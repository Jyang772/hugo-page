[{"content":"As software engineers, we absolutely love building and using abstractions.\nJust imagine how much stuff happens when you load a URL. You type something on a keyboard; key presses are somehow detected by the OS and get sent to the browser; browser parses the URL and asks the OS to make a network request; then comes DNS, routing, TCP, HTTP, and all the other OSI layers; browser parses HTML; JavaScript works its magic; some representation of a page gets sent over to GPU for rendering; image frames get sent to the monitor… and each of these steps probably involves doing dozens of more specific things in the process.\nAbstractions help us in reducing all this complexity down to a single interface that describes what a certain module can do without fixing a concrete implementation. This provides double benefits:\nEngineers working on higher-level modules only need to know the (much smaller) interface. Engineers working on the module itself get the freedom to optimize and refactor its implementation as long as it complies with its contracts. Hardware engineers love abstractions too. An abstraction of a CPU is called an instruction set architecture (ISA), and it defines how a computer should work from a programmer\u0026rsquo;s perspective. Similar to software interfaces, it gives computer engineers the ability to improve on existing CPU designs while also giving its users — us, programmers — the confidence that things that worked before won\u0026rsquo;t break on newer chips.\nAn ISA essentially defines how the hardware should interpret the machine language. Apart from instructions and their binary encodings, an ISA also defines the counts, sizes, and purposes of registers, the memory model, and the input/output model. Similar to software interfaces, ISAs can be extended too: in fact, they are often updated, mostly in a backward-compatible way, to add new and more specialized instructions that can improve performance.\n#RISC vs CISCHistorically, there have been many competing ISAs in use. But unlike character encodings and instant messaging protocols, developing and maintaining a completely separate ISA is costly, so mainstream CPU designs ended up converging to one of the two families:\nArm chips, which are used in almost all mobile devices, as well as other computer-like devices such as TVs, smart fridges, microwaves, car autopilots, and so on. They are designed by a British company of the same name, as well as a number of electronics manufacturers including Apple and Samsung. x861 chips, which are used in almost all servers and desktops, with a few notable exceptions such as Apple\u0026rsquo;s M1 MacBooks, AWS\u0026rsquo;s Graviton processors, and the current world\u0026rsquo;s fastest supercomputer, all of which use Arm-based CPUs. They are designed by a duopoly of Intel and AMD. The main difference between them is that of architectural complexity, which is more of a design philosophy rather than some strictly defined property:\nArm CPUs are reduced instruction set computers (RISC). They improve performance by keeping the instruction set small and highly optimized, although some less common operations have to be implemented with subroutines involving several instructions. x86 CPUs are complex instruction set computers (CISC). They improve performance by adding many specialized instructions, some of which may only be rarely used in practical programs. The main advantage of RISC designs is that they result in simpler and smaller chips, which projects to lower manufacturing costs and power usage. It\u0026rsquo;s not surprising that the market segmented itself with Arm dominating battery-powered, general-purpose devices, and leaving the complex neural network and Galois field calculations to server-grade, highly-specialized x86s.\nModern 64-bit versions of x86 are known as \u0026ldquo;AMD64,\u0026rdquo; \u0026ldquo;Intel 64,\u0026rdquo; or by the more vendor-neutral names of \u0026ldquo;x86-64\u0026rdquo; or just \u0026ldquo;x64.\u0026rdquo; A similar 64-bit extension of Arm is called \u0026ldquo;AArch64\u0026rdquo; or \u0026ldquo;ARM64.\u0026rdquo; In this book, we will just use plain \u0026ldquo;x86\u0026rdquo; and \u0026ldquo;Arm\u0026rdquo; implying the 64-bit versions.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":0,"path":"/hugo-page/hpc/architecture/isa/","title":"Instruction Set Architectures"},{"content":"CPUs are controlled with machine language, which is just a stream of binary-encoded instructions that specify\nthe instruction number (called opcode), what its operands are (if there are any), and where to store the result (if one is produced). A much more human-friendly rendition of machine language, called assembly language, uses mnemonic codes to refer to machine code instructions and symbolic names to refer to registers and other storage locations.\nJumping right into it, here is how you add two numbers (*c = *a + *b) in Arm assembly:\n; *a = x0, *b = x1, *c = x2 ldr w0, [x0] ; load 4 bytes from wherever x0 points into w0 ldr w1, [x1] ; load 4 bytes from wherever x1 points into w1 add w0, w0, w1 ; add w0 with w1 and save the result to w0 str w0, [x2] ; write contents of w0 to wherever x2 points Here is the same operation in x86 assembly:\n; *a = rsi, *b = rdi, *c = rdx mov eax, DWORD PTR [rsi] ; load 4 bytes from wherever rsi points into eax add eax, DWORD PTR [rdi] ; add whatever is stored at rdi to eax mov DWORD PTR [rdx], eax ; write contents of eax to wherever rdx points Assembly is very simple in the sense that it doesn\u0026rsquo;t have many syntactical constructions compared to high-level programming languages. From what you can observe from the examples above:\nA program is a sequence of instructions, each written as its name followed by a variable number of operands. The [reg] syntax is used for \u0026ldquo;dereferencing\u0026rdquo; a pointer stored in a register, and on x86 you need to prefix it with size information (DWORD here means 32 bit). The ; sign is used for line comments, similar to # and // in other languages. Assembly is a very minimal language because it needs to be. It reflects the machine language as closely as possible, up to the point where there is almost 1:1 correspondence between machine code and assembly. In fact, you can turn any compiled program back into its assembly form using a process called disassembly1 — although everything non-essential like comments will not be preserved.\nNote that the two snippets above are not just syntactically different. Both are optimized codes produced by a compiler, but the Arm version uses 4 instructions, while the x86 version uses 3. The add eax, [rdi] instruction is what\u0026rsquo;s called fused instruction that does a load and an add in one go — this is one of the perks that the CISC approach can provide.\nSince there are far more differences between the architectures than just this one, from here on and for the rest of the book we will only provide examples for x86, which is probably what most of our readers will optimize for, although many of the introduced concepts will be architecture-agnostic.\n#Instructions and RegistersFor historical reasons, instruction mnemonics in most assembly languages are very terse. Back when people used to write assembly by hand and repeatedly wrote the same set of common instructions, one less character to type was one step away from insanity.\nFor example, mov is for \u0026ldquo;store/load a word,\u0026rdquo; inc is for \u0026ldquo;increment by 1,\u0026rdquo; mul is for \u0026ldquo;multiply,\u0026rdquo; and idiv is for \u0026ldquo;integer division.\u0026rdquo; You can look up the description of an instruction by its name in one of x86 references, but most instructions do what you\u0026rsquo;d think they do.\nMost instructions write their result into the first operand, which can also be involved in the computation like in the add eax, [rdi] example we saw before. Operands can be either registers, constant values, or memory locations.\nRegisters are named rax, rbx, rcx, rdx, rdi, rsi, rbp, rsp, and r8-r15 for a total of 16 of them. The \u0026ldquo;letter\u0026rdquo; ones are named like that for historical reasons: rax is \u0026ldquo;accumulator,\u0026rdquo; rcx is \u0026ldquo;counter,\u0026rdquo; rdx is \u0026ldquo;data\u0026rdquo; and so on — but, of course, they don\u0026rsquo;t have to be used only for that.\nThere are also 32-, 16-bit and 8-bit registers that have similar names (rax → eax → ax → al). They are not fully separate but aliased: the lowest 32 bits of rax are eax, the lowest 16 bits of eax are ax, and so on. This is made to save die space while maintaining compatibility, and it is also the reason why basic type casts in compiled programming languages are usually free.\nThese are just the general-purpose registers that you can, with some exceptions, use however you like in most instructions. There is also a separate set of registers for floating-point arithmetic, a bunch of very wide registers used in vector extensions, and a few special ones that are needed for control flow, but we\u0026rsquo;ll get there in time.\nConstants are just integer or floating-point values: 42, 0x2a, 3.14, 6.02e23. They are more commonly called immediate values because they are embedded right into the machine code. Because it may considerably increase the complexity of the instruction encoding, some instructions don\u0026rsquo;t support immediate values or allow just a fixed subset of them. In some cases, you have to load a constant value into a register and then use it instead of an immediate value.\nApart from numeric values, there are also string constants such as hello or world\\n with their own little subset of operations, but that is a somewhat obscure corner of the assembly language that we are not going to explore here.\n#Moving DataSome instructions may have the same mnemonic, but have different operand types, in which case they are considered distinct instructions as they may perform slightly different operations and take different times to execute. The mov instruction is a vivid example of that, as it comes in around 20 different forms, all related to moving data: either between the memory and registers or just between two registers. Despite the name, it doesn\u0026rsquo;t move a value into a register, but copies it, preserving the original.\nWhen used to copy data between two registers, the mov instruction instead performs register renaming internally — informs the CPU that the value referred by register X is actually stored in register Y — without causing any additional delay except for maybe reading and decoding the instruction itself. For the same reason, the xchg instruction that swaps two registers also doesn\u0026rsquo;t cost anything.\nAs we\u0026rsquo;ve seen above with the fused add, you don\u0026rsquo;t have to use mov for every memory operation: some arithmetic instructions conveniently support memory locations as operands.\n#Addressing ModesMemory addressing is done with the [] operator, but it can do more than just reinterpret a value stored in a register as a memory location. The address operand takes up to 4 parameters presented in the syntax:\nSIZE PTR [base + index * scale + displacement] where displacement needs to be an integer constant and scale can be either 2, 4, or 8. What it does is calculate the pointer base + index * scale + displacement and dereferences it.\nUsing complex addressing is at most one cycle slower than dereferencing a pointer directly, and it can be useful when you have, for example, an array of structures and want to load a specific field of its $i$-th element.\nAddressing operator needs to be prefixed with a size specifier for how many bits of data are needed:\nBYTE for 8 bits WORD for 16 bits DWORD for 32 bits QWORD for 64 bits There is also a more rare TBYTE for 80 bits, and XMMWORD, YMMWORD, and ZMMWORD for 128, 256, and 512 bits respectively. All these types don\u0026rsquo;t have to be written in uppercase, but this is how most compilers emit them.\nThe address computation is often useful by itself: the lea (\u0026ldquo;load effective address\u0026rdquo;) instruction calculates the memory address of the operand and stores it in a register in one cycle, without doing any actual memory operations. While its intended use is for actually computing memory addresses, it is also often used as an arithmetic trick that would otherwise involve 1 multiplication and 2 additions — for example, you can multiply by 3, 5, and 9 with it.\nIt also frequently serves as a replacement for add because it doesn\u0026rsquo;t need a separate mov instruction if you need to move the result somewhere else: add only works in the two-register a += b mode, while lea lets you do a = b + c (or even a = b + c + d if one of them is a constant).\n#Alternative SyntaxThere are actually multiple assemblers (the programs that produce machine code from assembly) with different assembly languages, but only two x86 syntaxes are widely used now. They are commonly called after the two companies that used them and had a dominant influence on programming during that era:\nThe AT\u0026amp;T syntax, used by default by all Linux tools. The Intel syntax, used by default, well, by Intel. These syntaxes are also sometimes called GAS and NASM respectively, by the names of the two primary assemblers that use them (GNU Assembler and Netwide Assembler).\nWe used Intel syntax in this chapter and will continue to preferably use it for the rest of the book. For comparison, here is how the same *c = *a + *b example looks like in AT\u0026amp;T asm:\nmovl (%rsi), %eax addl (%rdi), %eax movl %eax, (%rdx) The key differences can be summarized as follows:\nThe last operand is used to specify the destination. Registers and constants need to be prefixed by % and $ respectively (e.g., addl $1, %rdx increments rdx). Memory addressing looks like this: displacement(%base, %index, scale). Both ; and # can be used for line comments, and also /* */ can be used for block comments. And, most importantly, in AT\u0026amp;T syntax, the instruction names need to be \u0026ldquo;suffixed\u0026rdquo; (addq, movl, cmpq, etc.) to specify what size operands are being manipulated:\nb = byte (8 bit) w = word (16 bit) l = long (32 bit integer or 64-bit floating-point) q = quad (64 bit) s = single (32-bit floating-point) t = ten bytes (80-bit floating-point) In Intel syntax, this information is inferred from operands (which is why you also need to specify sizes of pointers).\nMost tools that produce or consume x86 assembly can do so in both syntaxes, so you can just pick the one you like more and don\u0026rsquo;t worry.\nOn Linux, to disassemble a compiled program, you can call objdump -d {path-to-binary}.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1,"path":"/hugo-page/hpc/architecture/assembly/","title":"Assembly Language"},{"content":"In this section, we will derive a variant of gcd that is ~2x faster than the one in the C++ standard library.\n#Euclid\u0026rsquo;s AlgorithmEuclid\u0026rsquo;s algorithm solves the problem of finding the greatest common divisor (GCD) of two integer numbers $a$ and $b$, which is defined as the largest such number $g$ that divides both $a$ and $b$:\n$$ \\gcd(a, b) = \\max_{g: ; g|a , \\land , g | b} g $$\nYou probably already know this algorithm from a CS textbook, but I will summarize it here. It is based on the following formula, assuming that $a \u0026gt; b$:\n$$ \\gcd(a, b) = \\begin{cases} a, \u0026amp; b = 0 \\ \\gcd(b, a \\bmod b), \u0026amp; b \u0026gt; 0 \\end{cases} $$\nThis is true, because if $g = \\gcd(a, b)$ divides both $a$ and $b$, it should also divide $(a \\bmod b = a - k \\cdot b)$, but any larger divisor $d$ of $b$ will not: $d \u0026gt; g$ implies that $d$ couldn\u0026rsquo;t divide $a$ and thus won\u0026rsquo;t divide $(a - k \\cdot b)$.\nThe formula above is essentially the algorithm itself: you can simply apply it recursively, and since each time one of the arguments strictly decreases, it will eventually converge to the $b = 0$ case.\nThe textbook also probably mentioned that the worst possible input to Euclid\u0026rsquo;s algorithm — the one that maximizes the total number of steps — are consecutive Fibonacci numbers, and since they grow exponentially, the running time of the algorithm is logarithmic in the worst case. This is also true for its average running time if we define it as the expected number of steps for pairs of uniformly distributed integers. The Wikipedia article also has a cryptic derivation of a more precise $0.84 \\cdot \\ln n$ asymptotic estimate.\nYou can see bright blue lines at the proportions of the golden ratio There are many ways you can implement Euclid\u0026rsquo;s algorithm. The simplest would be just to convert the definition into code:\nint gcd(int a, int b) { if (b == 0) return a; else return gcd(b, a % b); } You can rewrite it more compactly like this:\nint gcd(int a, int b) { return (b ? gcd(b, a % b) : a); } You can rewrite it as a loop, which will be closer to how it is actually executed by the hardware. It won\u0026rsquo;t be faster though, because compilers can easily optimize tail recursion.\nint gcd(int a, int b) { while (b \u0026gt; 0) { a %= b; std::swap(a, b); } return a; } You can even write the body of the loop as this confusing one-liner — and it will even compile without causing undefined behavior warnings since C++17:\nint gcd(int a, int b) { while (b) b ^= a ^= b ^= a %= b; return a; } All of these, as well as std::gcd which was introduced in C++17, are almost equivalent and get compiled into functionally the following assembly loop:\n; a = eax, b = edx loop: ; modulo in assembly: mov r8d, edx cdq idiv r8d mov eax, r8d ; (a and b are already swapped now) ; continue until b is zero: test edx, edx jne loop If you run perf on it, you will see that it spends ~90% of the time on the idiv line. This isn\u0026rsquo;t surprising: general integer division works notoriously slow on all computers, including x86.\nBut there is one kind of division that works well in hardware: division by a power of 2.\n#Binary GCDThe binary GCD algorithm was discovered around the same time as Euclid\u0026rsquo;s, but on the other end of the civilized world, in ancient China. In 1967, it was rediscovered by Josef Stein for use in computers that either don\u0026rsquo;t have division instruction or have a very slow one — it wasn\u0026rsquo;t uncommon for CPUs of that era to use hundreds or thousands of cycles for rare or complex operations.\nAnalogous to the Euclidean algorithm, it is based on a few similar observations:\n$\\gcd(0, b) = b$ and symmetrically $\\gcd(a, 0) = a$; $\\gcd(2a, 2b) = 2 \\cdot \\gcd(a, b)$; $\\gcd(2a, b) = \\gcd(a, b)$ if $b$ is odd and symmetrically $\\gcd(a, b) = \\gcd(a, 2b)$ if $a$ is odd; $\\gcd(a, b) = \\gcd(|a − b|, \\min(a, b))$, if $a$ and $b$ are both odd. Likewise, the algorithm itself is just a repeated application of these identities.\nIts running time is still logarithmic, which is even easier to show because in each of these identities one of the arguments is divided by 2 — except for the last case, in which the new first argument, the absolute difference of two odd numbers, is guaranteed to be even and thus will be divided by 2 on the next iteration.\nWhat makes this algorithm especially interesting to us is that the only arithmetic operations it uses are binary shifts, comparisons, and subtractions, all of which typically take just one cycle.\n#ImplementationThe reason this algorithm is not in the textbooks is because it can\u0026rsquo;t be implemented as a simple one-liner anymore:\nint gcd(int a, int b) { // base cases (1) if (a == 0) return b; if (b == 0) return a; if (a == b) return a; if (a % 2 == 0) { if (b % 2 == 0) // a is even, b is even (2) return 2 * gcd(a / 2, b / 2); else // a is even, b is odd (3) return gcd(a / 2, b); } else { if (b % 2 == 0) // a is odd, b is even (3) return gcd(a, b / 2); else // a is odd, b is odd (4) return gcd(std::abs(a - b), std::min(a, b)); } } Let\u0026rsquo;s run it, and… it sucks. The difference in speed compared to std::gcd is indeed 2x, but on the other side of the equation. This is mainly because of all the branching needed to differentiate between the cases. Let\u0026rsquo;s start optimizing.\nFirst, let\u0026rsquo;s replace all divisions by 2 with divisions by whichever highest power of 2 we can. We can do it efficiently with __builtin_ctz, the \u0026ldquo;count trailing zeros\u0026rdquo; instruction available on modern CPUs. Whenever we are supposed to divide by 2 in the original algorithm, we will call this function instead, which will give us the exact number of bits to right-shift the number by. Assuming that the we are dealing with large random numbers, this is expected to decrease the number of iterations by almost a factor 2, because $1 + \\frac{1}{2} + \\frac{1}{4} + \\frac{1}{8} + \\ldots \\to 2$.\nSecond, we can notice that condition 2 can now only be true once — in the very beginning — because every other identity leaves at least one of the numbers odd. Therefore we can handle this case just once in the beginning and not consider it in the main loop.\nThird, we can notice that after we\u0026rsquo;ve entered condition 4 and applied its identity, $a$ will always be even and $b$ will always be odd, so we already know that on the next iteration we are going to be in condition 3. This means that we can actually \u0026ldquo;de-evenize\u0026rdquo; $a$ right away, and if we do so we will again hit condition 4 on the next iteration. This means that we can only ever be either in condition 4 or terminating by condition 1, which removes the need to branch.\nCombining these ideas, we get the following implementation:\nint gcd(int a, int b) { if (a == 0) return b; if (b == 0) return a; int az = __builtin_ctz(a); int bz = __builtin_ctz(b); int shift = std::min(az, bz); a \u0026gt;\u0026gt;= az, b \u0026gt;\u0026gt;= bz; while (a != 0) { int diff = a - b; b = std::min(a, b); a = std::abs(diff); a \u0026gt;\u0026gt;= __builtin_ctz(a); } return b \u0026lt;\u0026lt; shift; } It runs in 116ns, while std::gcd takes 198ns. Almost twice as fast — maybe we can even optimize it below 100ns?\nFor that we need to stare at its assembly again, in particular at this block:\n; a = edx, b = eax loop: mov ecx, edx sub ecx, eax ; diff = a - b cmp eax, edx cmovg eax, edx ; b = min(a, b) mov edx, ecx neg edx cmovs edx, ecx ; a = max(diff, -diff) = abs(diff) tzcnt ecx, edx ; az = __builtin_ctz(a) sarx edx, edx, ecx ; a \u0026gt;\u0026gt;= az test edx, edx ; a != 0? jne loop Let\u0026rsquo;s draw the dependency graph of this loop:\nModern processors can execute many instructions in parallel, essentially meaning that the true \u0026ldquo;cost\u0026rdquo; of this computation is roughly the sum of latencies on its critical path. In this case, it is the total latency of diff, abs, ctz, and shift.\nWe can decrease this latency using the fact that we can actually calculate ctz using just diff = a - b, because a negative number divisible by $2^k$ still has $k$ zeros at the end of its binary representation. This lets us not wait for max(diff, -diff) to be computed first, resulting in a shorter graph like this:\nHopefully you will be less confused when you think about how the final code will be executed:\nint gcd(int a, int b) { if (a == 0) return b; if (b == 0) return a; int az = __builtin_ctz(a); int bz = __builtin_ctz(b); int shift = std::min(az, bz); b \u0026gt;\u0026gt;= bz; while (a != 0) { a \u0026gt;\u0026gt;= az; int diff = b - a; az = __builtin_ctz(diff); b = std::min(a, b); a = std::abs(diff); } return b \u0026lt;\u0026lt; shift; } It runs in 91ns, which is good enough to leave it there.\nIf somebody wants to try to shave off a few more nanoseconds by rewriting the assembly by hand or trying a lookup table to save a few last iterations, please let me know.\n#AcknowledgementsThe main optimization ideas belong to Daniel Lemire and Ralph Corderoy, who had nothing better to do on the Christmas holidays of 2013.\n","id":2,"path":"/hugo-page/hpc/algorithms/gcd/","title":"Binary GCD"},{"content":" While improving the speed of user-facing applications is the end goal of performance engineering, people don\u0026rsquo;t really get excited over 5-10% improvements in some databases. Yes, this is what software engineers are paid for, but these types of optimizations tend to be too intricate and system-specific to be readily generalized to other software.\nInstead, the most fascinating showcases of performance engineering are multifold optimizations of textbook algorithms: the kinds that everybody knows and deemed so simple that it would never even occur to try to optimize them in the first place. These optimizations are simple and instructive and can very much be adopted elsewhere. And they are surprisingly not as rare as you\u0026rsquo;d think.\nIn this section, we focus on one such fundamental algorithm — binary search — and implement two of its variants that are, depending on the problem size, up to 4x faster than std::lower_bound, while being under just 15 lines of code.\nThe first algorithm achieves that by removing branches, and the second also optimizes the memory layout to achieve better cache system performance. This technically disqualifies it from being a drop-in replacement for std::lower_bound as it needs to permute the elements of the array before it can start answering queries — but I can\u0026rsquo;t recall a lot of scenarios where you obtain a sorted array but can\u0026rsquo;t afford to spend linear time on preprocessing.\nThe usual disclaimer: the CPU is a Zen 2, the RAM is a DDR4-2666, and the compiler we will be using by default is Clang 10. The performance on your machine may be different, so I highly encourage to go and test it for yourself.\n#Binary Search Here is the standard way of searching for the first element not less than x in a sorted array t of n integers that you can find in any introductory computer science textbook:\nint lower_bound(int x) { int l = 0, r = n - 1; while (l \u0026lt; r) { int m = (l + r) / 2; if (t[m] \u0026gt;= x) r = m; else l = m + 1; } return t[l]; } Find the middle element of the search range, compare it to x, shrink the range in half. Beautiful in its simplicity.\nA similar approach is employed by std::lower_bound, except that it needs to be more generic to support containers with non-random-access iterators and thus uses the first element and the size of the search interval instead of the two of its ends. To this end, implementations from both Clang and GCC use this metaprogramming monstrosity:\ntemplate \u0026lt;class _Compare, class _ForwardIterator, class _Tp\u0026gt; _LIBCPP_CONSTEXPR_AFTER_CXX17 _ForwardIterator __lower_bound(_ForwardIterator __first, _ForwardIterator __last, const _Tp\u0026amp; __value_, _Compare __comp) { typedef typename iterator_traits\u0026lt;_ForwardIterator\u0026gt;::difference_type difference_type; difference_type __len = _VSTD::distance(__first, __last); while (__len != 0) { difference_type __l2 = _VSTD::__half_positive(__len); _ForwardIterator __m = __first; _VSTD::advance(__m, __l2); if (__comp(*__m, __value_)) { __first = ++__m; __len -= __l2 + 1; } else __len = __l2; } return __first; } If the compiler is successful in removing the abstractions, it compiles to roughly the same machine code and yields roughly the same average latency, which expectedly grows with the array size:\nSince most people don\u0026rsquo;t implement binary search by hand, we will use std::lower_bound from Clang as the baseline.\n#The BottleneckBefore jumping to the optimized implementations, let\u0026rsquo;s briefly discuss why binary search is slow in the first place.\nIf you run std::lower_bound with perf, you\u0026rsquo;ll see that it spends most of its time on a conditional jump instruction:\n│35: mov %rax,%rdx 0.52 │ sar %rdx 0.33 │ lea (%rsi,%rdx,4),%rcx 4.30 │ cmp (%rcx),%edi 65.39 │ ↓ jle b0 0.07 │ sub %rdx,%rax 9.32 │ lea 0x4(%rcx),%rsi 0.06 │ dec %rax 1.37 │ test %rax,%rax 1.11 │ ↑ jg 35 This pipeline stall stops the search from progressing, and it is mainly caused by two factors:\nWe suffer a control hazard because we have a branch that is impossible to predict (queries and keys are drawn independently at random), and the processor has to halt for 10-15 cycles to flush the pipeline and fill it back on each branch mispredict. We suffer a data hazard because we have to wait for the preceding comparison to complete, which in turn waits for one of its operands to be fetched from the memory — and it may take anywhere between 0 and 300 cycles, depending on where it is located. Now, let\u0026rsquo;s try to get rid of these obstacles one by one.\n#Removing BranchesWe can replace branching with predication. To make the task easier, we can adopt the STL approach and rewrite the loop using the first element and the size of the search interval (instead of its first and last element):\nint lower_bound(int x) { int *base = t, len = n; while (len \u0026gt; 1) { int half = len / 2; if (base[half - 1] \u0026lt; x) { base += half; len = len - half; } else { len = half; } } return *base; } Note that, on each iteration, len is essentially just halved and then either floored or ceiled, depending on how the comparison went. This conditional update seems unnecessary; to avoid it, we can simply say that it\u0026rsquo;s always ceiled:\nint lower_bound(int x) { int *base = t, len = n; while (len \u0026gt; 1) { int half = len / 2; if (base[half - 1] \u0026lt; x) base += half; len -= half; // = ceil(len / 2) } return *base; } This way, we only need to update the first element of the search interval with a conditional move and halve its size on each iteration:\nint lower_bound(int x) { int *base = t, len = n; while (len \u0026gt; 1) { int half = len / 2; base += (base[half - 1] \u0026lt; x) * half; // will be replaced with a \u0026#34;cmov\u0026#34; len -= half; } return *base; } Note that this loop is not always equivalent to the standard binary search. Since it always rounds up the size of the search interval, it accesses slightly different elements and may perform one comparison more than needed. Apart from simplifying computations on each iteration, it also makes the number of iterations constant if the array size is constant, removing branch mispredictions completely.\nAs typical for predication, this trick is very fragile to compiler optimizations — depending on the compiler and how the function is invoked, it may still leave a branch or generate suboptimal code. It works fine on Clang 10, yielding a 2.5-3x improvement on small arrays:\nOne interesting detail is that it performs worse on large arrays. It seems weird: the total delay is dominated by the RAM latency, and since it does roughly the same memory accesses as the standard binary search, it should be roughly the same or even slightly better.\nThe real question you need to ask is not why the branchless implementation is worse but why the branchy version is better. It happens because when you have branching, the CPU can speculate on one of the branches and start fetching either the left or the right key before it can even confirm that it is the right one — which effectively acts as implicit prefetching.\nFor the branchless implementation, this doesn\u0026rsquo;t happen, as cmov is treated as every other instruction, and the branch predictor doesn\u0026rsquo;t try to peek into its operands to predict the future. To compensate for this, we can prefetch the data in software by explicitly requesting the left and right child key:\nint lower_bound(int x) { int *base = t, len = n; while (len \u0026gt; 1) { int half = len / 2; len -= half; __builtin_prefetch(\u0026amp;base[len / 2 - 1]); __builtin_prefetch(\u0026amp;base[half + len / 2 - 1]); base += (base[half - 1] \u0026lt; x) * half; } return *base; } With prefetching, the performance on large arrays becomes roughly the same:\nThe graph still grows faster as the branchy version also prefetches \u0026ldquo;grandchildren,\u0026rdquo; \u0026ldquo;great-grandchildren,\u0026rdquo; and so on — although the usefulness of each new speculative read diminishes exponentially as the prediction is less and less likely to be correct.\nIn the branchless version, we could also fetch ahead by more than one layer, but the number of fetches we\u0026rsquo;d need also grows exponentially. Instead, we will try a different approach to optimize memory operations.\n#Optimizing the LayoutThe memory requests we perform during binary search form a very specific access pattern:\nHow likely is it that the elements on each request are cached? How good is their data locality?\nSpatial locality seems to be okay for the last 3 to 4 requests that are likely to be on the same cache line — but all the previous requests require huge memory jumps. Temporal locality seems to be okay for the first dozen or so requests — there aren\u0026rsquo;t that many different comparison sequences of this length, so we will be comparing against the same middle elements over and over, which are likely to be cached. To illustrate how important the second type of cache sharing is, let\u0026rsquo;s try to pick the element we will compare to on each iteration randomly among the elements of the search interval, instead of the middle one:\nint lower_bound(int x) { int l = 0, r = n - 1; while (l \u0026lt; r) { int m = l + rand() % (r - l); if (t[m] \u0026gt;= x) r = m; else l = m + 1; } return t[l]; } Theoretically, this randomized binary search is expected to do 30-40% more comparisons than the normal one, but on a real computer, the running time goes ~6x on large arrays:\nThis isn\u0026rsquo;t just caused by the rand() call being slow. You can clearly see the point on the L2-L3 boundary where memory latency outweighs the random number generation and modulo. The performance degrades because all of the fetched elements are unlikely to be cached and not just some small suffix of them.\nAnother potential negative effect is that of cache associativity. If the array size is a multiple of a large power of two, then the indices of these \u0026ldquo;hot\u0026rdquo; elements will also be divisible by some large powers of two and map to the same cache line, kicking each other out. For example, binary searching over arrays of size $2^{20}$ takes about ~360ns per query while searching over arrays of size $(2^{20} + 123)$ takes ~300ns — a 20% difference. There are ways to fix this problem, but to not get distracted from more pressing matters, we are just going to ignore it: all array sizes we use are in the form of $\\lfloor 1.17^k \\rfloor$ for integer $k$ so that any cache side effects are unlikely.\nThe real problem with our memory layout is that it doesn\u0026rsquo;t make the most efficient use of temporal locality because it groups hot and cold elements together. For example, we likely store the element $\\lfloor n/2 \\rfloor$, which we request the first thing on each query, in the same cache line with $\\lfloor n/2 \\rfloor + 1$, which we almost never request.\nHere is the heatmap visualizing the expected frequency of comparisons for a 31-element array:\nSo, ideally, we\u0026rsquo;d want a memory layout where hot elements are grouped with hot elements, and cold elements are grouped with cold elements. And we can achieve this if we permute the array in a more cache-friendly way by renumbering them. The numeration we will use is actually half a millennium old, and chances are, you already know it.\n#Eytzinger LayoutMichaël Eytzinger is a 16th-century Austrian nobleman known for his work on genealogy, particularly for a system for numbering ancestors called ahnentafel (German for \u0026ldquo;ancestor table\u0026rdquo;).\nAncestry mattered a lot back then, but writing down that data was expensive. Ahnentafel allows displaying a person\u0026rsquo;s genealogy compactly, without wasting extra space by drawing diagrams.\nIt lists a person\u0026rsquo;s direct ancestors in a fixed sequence of ascent. First, the person themselves is listed as number 1, and then, recursively, for each person numbered $k$, their father is listed as $2k$ and their mother as $(2k+1)$.\nHere is the example for Paul I, the great-grandson of Peter the Great:\nPaul I Peter III (Paul\u0026rsquo;s father) Catherine II (Paul\u0026rsquo;s mother) Charles Frederick (Peter\u0026rsquo;s father, Paul\u0026rsquo;s paternal grandfather) Anna Petrovna (Peter\u0026rsquo;s mother, Paul\u0026rsquo;s paternal grandmother) Christian August (Catherine\u0026rsquo;s father, Paul\u0026rsquo;s maternal grandfather) Johanna Elisabeth (Catherine\u0026rsquo;s mother, Paul\u0026rsquo;s maternal grandmother) Apart from being compact, it has some nice properties, like that all even-numbered persons are male and all odd-numbered (possibly except for 1) are female. One can also find the number of a particular ancestor only knowing the genders of their descendants. For example, Peter the Great\u0026rsquo;s bloodline is Paul I → Peter III → Anna Petrovna → Peter the Great, so his number should be $((1 \\times 2) \\times 2 + 1) \\times 2 = 10$.\nIn computer science, this enumeration has been widely used for implicit (pointer-free) implementations of heaps, segment trees, and other binary tree structures — where instead of names, it stores underlying array items.\nHere is how this layout looks when applied to binary search:\nNote that the tree is slightly imbalanced (because of the last layer is continuous) When searching in this layout, we just need to start from the first element of the array, and then on each iteration jump to either $2 k$ or $(2k + 1)$, depending on how the comparison went:\nYou can immediately see how its temporal locality is better (and, in fact, theoretically optimal) as the elements closer to the root are closer to the beginning of the array and thus are more likely to be fetched from the cache.\nAnother way to look at it is that we write every even-indexed element to the end of the new array, then write every even-indexed element of the remaining ones right before them, and so on, until we place the root as the first element.\n#ConstructionTo construct the Eytzinger array, we could do this even-odd filtering $O(\\log n)$ times — and, perhaps, this is the fastest approach — but for brevity, we will instead build it by traversing the original search tree:\nint a[n], t[n + 1]; // the original sorted array and the eytzinger array we build // ^ we need one element more because of one-based indexing void eytzinger(int k = 1) { static int i = 0; // \u0026lt;- careful running it on multiple arrays if (k \u0026lt;= n) { eytzinger(2 * k); t[k] = a[i++]; eytzinger(2 * k + 1); } } This function takes the current node number k, recursively writes out all elements to the left of the middle of the search interval, writes out the current element we\u0026rsquo;d compare against, and then recursively writes out all the elements on the right. It seems a bit complicated, but to convince yourself that it works, you only need three observations:\nIt writes exactly n elements as we enter the body of if for each k from 1 to n just once. It writes out sequential elements from the original array as it increments the i pointer each time. By the time we write the element at node k, we will have already written all the elements to its left (exactly i). Despite being recursive, it is actually quite fast as all the memory reads are sequential, and the memory writes are only in $O(\\log n)$ different memory blocks at a time. Maintaining the permutation is both logically and computationally harder to maintain though: adding an element to a sorted array only requires shifting a suffix of its elements one position to the right, while Eytzinger array practically needs to be rebuilt from scratch.\nNote that this traversal and the resulting permutation are not exactly equivalent to the \u0026ldquo;tree\u0026rdquo; of vanilla binary search: for example, the left child subtree may be larger than the right child subtree — up to twice as large — but it doesn\u0026rsquo;t matter much since both approaches result in the same $\\lceil \\log_2 n \\rceil$ tree depth.\nAlso note that the Eytzinger array is one-indexed — this will be important for performance later. You can put in the zeroth element the value that you want to be returned in the case when the lower bound doesn\u0026rsquo;t exist (similar to a.end() for std::lower_bound).\n#Search ImplementationWe can now descend this array using only indices: we just start with $k=1$ and execute $k := 2k$ if we need to go left and $k := 2k + 1$ if we need to go right. We don\u0026rsquo;t even need to store and recalculate the search boundaries anymore. This simplicity also lets us avoid branching:\nint k = 1; while (k \u0026lt;= n) k = 2 * k + (t[k] \u0026lt; x); The only problem arises when we need to restore the index of the resulting element, as $k$ does not directly point to it. Consider this example (its corresponding tree is listed above):\narray: 0 1 2 3 4 5 6 7 8 9 eytzinger: 6 3 7 1 5 8 9 0 2 4 1st range: ------------?------ k := 2*k = 2 (6 ≥ 3) 2nd range: ------?------ k := 2*k = 4 (3 ≥ 3) 3rd range: --?---- k := 2*k + 1 = 9 (1 \u003c 3) 4th range: ?-- k := 2*k + 1 = 19 (2 \u003c 3) 5th range: ! Here we query the array of $[0, …, 9]$ for the lower bound of $x=3$. We compare it against $6$, $3$, $1$, and $2$, go left-left-right-right, and end up with $k = 19$, which isn\u0026rsquo;t even a valid array index.\nThe trick is to notice that, unless the answer is the last element of the array, we compare $x$ against it at some point, and after we\u0026rsquo;ve learned that it is not less than $x$, we go left exactly once and then keep going right until we reach a leaf (because we will only be comparing $x$ against lesser elements). Therefore, to restore the answer, we just need to \u0026ldquo;cancel\u0026rdquo; some number of right turns and then one more.\nThis can be done in an elegant way by observing that the right turns are recorded in the binary representation of $k$ as 1-bits, and so we just need to find the number of trailing 1s in the binary representation and right-shift $k$ by exactly that number of bits plus one. To do this, we can invert the number (~k) and call the \u0026ldquo;find first set\u0026rdquo; instruction:\nint lower_bound(int x) { int k = 1; while (k \u0026lt;= n) k = 2 * k + (t[k] \u0026lt; x); k \u0026gt;\u0026gt;= __builtin_ffs(~k); return t[k]; } We run it, and… well, it doesn\u0026rsquo;t look that good:\nThe latency on smaller arrays is on par with the branchless binary search implementation — which isn\u0026rsquo;t surprising as it is just two lines of code — but it starts taking off much sooner. The reason is that the Eytzinger binary search doesn\u0026rsquo;t get the advantage of spatial locality: the last 3-4 elements we compare against are not in the same cache line anymore, and we have to fetch them separately.\nIf you think about it deeper, you might object that the improved temporal locality should compensate for that. Before, we were using only about $\\frac{1}{16}$-th of the cache line to store one hot element, and now we are using all of it, so the effective cache size is larger by a factor of 16, which lets us cover $\\log_2 16 = 4$ more first requests.\nBut if you think about it more, you understand that this isn’t enough compensation. Caching the other 15 elements wasn’t completely useless, and also, the hardware prefetcher could fetch the neighboring cache lines of our requests. If this was one of our last requests, the rest of what we will be reading will probably be cached elements. So actually, the last 6-7 accesses are likely to be cached, not 3-4.\nIt seems like we did an overall stupid thing switching to this layout, but there is a way to make it worthwhile.\n#PrefetchingTo hide the memory latency, we can use software prefetching similar to how we did for branchless binary search. But instead of issuing two separate prefetch instructions for the left and right child nodes, we can notice that they are neighbors in the Eytzinger array: one has index $2 k$ and the other $(2k + 1)$, so they are likely in the same cache line, and we can use just one instruction.\nThis observation extends to the grand-children of node $k$ — they are also stored sequentially:\n2 * 2 * k = 4 * k 2 * 2 * k + 1 = 4 * k + 1 2 * (2 * k + 1) = 4 * k + 2 2 * (2 * k + 1) + 1 = 4 * k + 3 Their cache line can also be fetched with one instruction. Interesting… what if we continue this, and instead of fetching direct children, we fetch ahead as many descendants as we can cramp into one cache line? That would be $\\frac{64}{4} = 16$ elements, our great-great-grandchildren with indices from $16k$ to $(16k + 15)$.\nNow, if we prefetch just one of these 16 elements, we will probably only get some but not all of them, as they may cross a cache line boundary. We can prefetch the first and the last element, but to get away with just one memory request, we need to notice that the index of the first element, $16k$, is divisible by $16$, so its memory address will be the base address of the array plus something divisible by $16 \\cdot 4 = 64$, the cache line size. If the array were to begin on a cache line, then these $16$ great-great-grandchildren elements will be guaranteed to be on a single cache line, which is just what we needed.\nTherefore, we only need to align the array:\nt = (int*) std::aligned_alloc(64, 4 * (n + 1)); And then prefetch the element indexed $16 k$ on each iteration:\nint lower_bound(int x) { int k = 1; while (k \u0026lt;= n) { __builtin_prefetch(t + k * 16); k = 2 * k + (t[k] \u0026lt; x); } k \u0026gt;\u0026gt;= __builtin_ffs(~k); return t[k]; } The performance on large arrays improves 3-4x from the previous version and ~2x compared to std::lower_bound. Not bad for just two more lines of code:\nEssentially, what we do here is hide the latency by prefetching four steps ahead and overlapping memory requests. Theoretically, if the compute didn\u0026rsquo;t matter, we would expect a ~4x speedup, but in reality, we get a somewhat more moderate speedup.\nWe can also try to prefetch further than that four steps ahead, and we don\u0026rsquo;t even have to use more than one prefetch instruction for that: we can try to request only the first cache line and rely on the hardware to prefetch its neighbors. This trick may or may not improve actual performance — depends on the hardware:\n__builtin_prefetch(t + k * 32); Also, note that the last few prefetch requests are actually not needed, and in fact, they may even be outside the memory region allocated for the program. On most modern CPUs, invalid prefetch instructions get converted into no-ops, so it isn\u0026rsquo;t a problem, but on some platforms, this may cause a slowdown, so it may make sense, for example, to split off the last ~4 iterations from the loop to try to remove them.\nThis prefetching technique allows us to read up to four elements ahead, but it doesn\u0026rsquo;t really come for free — we are effectively trading off excess memory bandwidth for reduced latency. If you run more than one instance at a time on separate hardware threads or just any other memory-intensive computation in the background, it will significantly affect the benchmark performance.\nBut we can do better. Instead of fetching four cache lines at a time, we could fetch four times fewer cache lines. And in the next section, we will explore the approach.\n#Removing the Last BranchJust one finishing touch: did you notice the bumpiness of the Eytzinger search? This isn\u0026rsquo;t random noise — let\u0026rsquo;s zoom in:\nThe latency is ~10ns higher for the array sizes in the form of $1.5 \\cdot 2^k$. These are mispredicted branches from the loop itself — the last branch, to be exact. When the array size is far from a power of two, it is hard to predict whether the loop will make $\\lfloor \\log_2 n \\rfloor$ or $\\lfloor \\log_2 n \\rfloor + 1$ iterations, so we have a 50% chance to suffer exactly one branch mispredict.\nOne way to address it is to pad the array with infinities to the closest power of two, but this wastes memory. Instead, we get rid of that last branch by always executing a constant minimum number of iterations and then using predication to optionally make the last comparison against some dummy element — that is guaranteed to be less than $x$ so that its comparison will be canceled:\nt[0] = -1; // an element that is less than x iters = std::__lg(n + 1); int lower_bound(int x) { int k = 1; for (int i = 0; i \u0026lt; iters; i++) k = 2 * k + (t[k] \u0026lt; x); int *loc = (k \u0026lt;= n ? t + k : t); k = 2 * k + (*loc \u0026lt; x); k \u0026gt;\u0026gt;= __builtin_ffs(~k); return t[k]; } The graph is now smooth, and on small arrays, it is just a couple of cycles slower than the branchless binary search:\nInterestingly, now GCC fails to replace the branch with cmov, but Clang doesn\u0026rsquo;t. 1-1.\n#Appendix: Random Binary SearchBy the way, finding the exact expected number of comparisons for random binary search is quite an interesting math problem in and of itself. Try solving it yourself first!\nThe way to compute it algorithmically is through dynamic programming. If we denote $f_n$ as the expected number of comparisons to find a random lower bound on a search interval of size $n$, it can be calculated from the previous $f_n$ by considering all the $(n - 1)$ possible splits:\n$$ f_n = \\sum_{l = 1}^{n - 1} \\frac{1}{n-1} \\cdot \\left( f_l \\cdot \\frac{l}{n} + f_{n - l} \\cdot \\frac{n - l}{n} \\right) + 1 $$\nDirectly applying this formula gives us an $O(n^2)$ algorithm, but we can optimize it by rearranging the sum like this:\n$$ \\begin{aligned} f_n \u0026amp;= \\sum_{i = 1}^{n - 1} \\frac{ f_i \\cdot i + f_{n - i} \\cdot (n - i) }{ n \\cdot (n - 1) } + 1 \\ \u0026amp;= \\frac{2}{n \\cdot (n - 1)} \\cdot \\sum_{i = 1}^{n - 1} f_i \\cdot i + 1 \\end{aligned} $$\nTo update $f_n$, we only need to calculate the sum of $f_i \\cdot i$ for all $i \u0026lt; n$. To do that, let\u0026rsquo;s introduce two new variables:\n$$ g_n = f_n \\cdot n, ;; s_n = \\sum_{i=1}^{n} g_n $$\nNow they can be sequentially calculated as:\n$$ \\begin{aligned} g_n \u0026amp;= f_n \\cdot n = \\frac{2}{n-1} \\cdot \\sum_{i = 1}^{n - 1} g_i + n = \\frac{2}{n - 1} \\cdot s_{n - 1} + n \\ s_n \u0026amp;= s_{n - 1} + g_n \\end{aligned} $$\nThis way we get an $O(n)$ algorithm, but we can do even better. Let\u0026rsquo;s substitute $g_n$ in the update formula for $s_n$:\n$$ \\begin{aligned} s_n \u0026amp;= s_{n - 1} + \\frac{2}{n - 1} \\cdot s_{n - 1} + n \\ \u0026amp;= (1 + \\frac{2}{n - 1}) \\cdot s_{n - 1} + n \\ \u0026amp;= \\frac{n + 1}{n - 1} \\cdot s_{n - 1} + n \\end{aligned} $$\nThe next trick is more complicated. We define $r_n$ like this:\n$$ \\begin{aligned} r_n \u0026amp;= \\frac{s_n}{n} \\ \u0026amp;= \\frac{1}{n} \\cdot \\left(\\frac{n + 1}{n - 1} \\cdot s_{n - 1} + n\\right) \\ \u0026amp;= \\frac{n + 1}{n} \\cdot \\frac{s_{n - 1}}{n - 1} + 1 \\ \u0026amp;= \\left(1 + \\frac{1}{n}\\right) \\cdot r_{n - 1} + 1 \\end{aligned} $$\nWe can substitute it into the formula we got for $g_n$ before:\n$$ g_n = \\frac{2}{n - 1} \\cdot s_{n - 1} + n = 2 \\cdot r_{n - 1} + n $$\nRecalling that $g_n = f_n \\cdot n$, we can express $r_{n - 1}$ using $f_n$:\n$$ f_n \\cdot n = 2 \\cdot r_{n - 1} + n \\implies r_{n - 1} = \\frac{(f_n - 1) \\cdot n}{2} $$\nFinal step. We\u0026rsquo;ve just expressed $r_n$ through $r_{n - 1}$ and $r_{n - 1}$ through $f_n$. This lets us express $f_{n + 1}$ through $f_n$:\n$$ \\begin{aligned} \u0026amp;\u0026amp;\\quad r_n \u0026amp;= \\left(1 + \\frac{1}{n}\\right) \\cdot r_{n - 1} + 1 \\ \u0026amp;\\Rightarrow \u0026amp; \\frac{(f_{n + 1} - 1) \\cdot (n + 1)}{2} \u0026amp;= \\left(1 + \\frac{1}{n}\\right) \\cdot \\frac{(f_n - 1) \\cdot n}{2} + 1 \\ \u0026amp;\u0026amp;\u0026amp;= \\frac{n + 1}{2} \\cdot (f_n - 1) + 1 \\ \u0026amp;\\Rightarrow \u0026amp; (f_{n + 1} - 1) \u0026amp;= (f_{n} - 1) + \\frac{2}{n + 1} \\ \u0026amp;\\Rightarrow \u0026amp;f_{n + 1} \u0026amp;= f_{n} + \\frac{2}{n + 1} \\ \u0026amp;\\Rightarrow \u0026amp;f_{n} \u0026amp;= f_{n - 1} + \\frac{2}{n} \\ \u0026amp;\\Rightarrow \u0026amp;f_{n} \u0026amp;= \\sum_{k = 2}^{n} \\frac{2}{k} \\end{aligned} $$\nThe last expression is double the harmonic series, which is well known to approximate $\\ln n$ as $n \\to \\infty$. Therefore, the random binary search will perform $\\frac{2 \\ln n}{\\log_2 n} = 2 \\ln 2 \\approx 1.386$ more comparisons compared to the normal one.\n#AcknowledgementsThe article is loosely based on \u0026ldquo;Array Layouts for Comparison-Based Searching\u0026rdquo; by Paul-Virak Khuong and Pat Morin. It is 46 pages long and discusses these and many other (less successful) approaches in more detail. I highly recommend also checking it out — this is one of my favorite performance engineering papers.\nThanks to Marshall Lochbaum for providing the proof for the random binary search. No way I could do it myself.\nI also stole these lovely layout visualizations from some blog a long time ago, but I don\u0026rsquo;t remember the name of the blog and what license they had, and inverse image search doesn\u0026rsquo;t find them anymore. If you don\u0026rsquo;t sue me, thank you, whoever you are!\n","id":3,"path":"/hugo-page/hpc/data-structures/binary-search/","title":"Binary Search"},{"content":"If you ever opened a computer science textbook, it probably introduced computational complexity somewhere in the very beginning. Simply put, it is the total count of elementary operations (additions, multiplications, reads, writes…) that are executed during a computation, optionally weighted by their costs.\nComplexity is an old concept. It was systematically formulated in the early 1960s, and since then it has been universally used as the cost function for designing algorithms. The reason this model was so quickly adopted is that it was a good approximation of how computers worked at the time.\nClassical Complexity TheoryThe \u0026ldquo;elementary operations\u0026rdquo; of a CPU are called instructions, and their \u0026ldquo;costs\u0026rdquo; are called latencies. Instructions are stored in memory and executed one by one by the processor, which has some internal state stored in a number of registers. One of these registers is the instruction pointer, which indicates the address of the next instruction to read and execute. Each instruction changes the state of the processor in a certain way (including moving the instruction pointer), possibly modifies the main memory, and takes a different number of CPU cycles to complete before the next one can be started.\nTo estimate the real running time of a program, you need to sum all latencies for its executed instructions and divide it by the clock frequency, that is, the number of cycles a particular CPU does per second.\nThe clock frequency is a volatile and often unknown variable that depends on the CPU model, operating system settings, current microchip temperature, power usage of other components, and quite a few other things. In contrast, instruction latencies are static and even somewhat consistent across different CPUs when expressed in clock cycles, so counting them instead is much more useful for analytical purposes.\nFor example, the by-definition matrix multiplication algorithm requires the total of $n^2 \\cdot (n + n - 1)$ arithmetic operations: specifically, $n^3$ multiplications and $n^2 \\cdot (n - 1)$ additions. If we look up the latencies for these instructions (in special documents called instruction tables, like this one), we can find that, e.g., multiplication takes 3 cycles, while addition takes 1, so we need a total of $3 \\cdot n^3 + n^2 \\cdot (n - 1) = 4 \\cdot n^3 - n^2$ clock cycles for the entire computation (bluntly ignoring everything else that needs to be done to \u0026ldquo;feed\u0026rdquo; these instructions with the right data).\nSimilar to how the sum of instruction latencies can be used as a clock-independent proxy for total execution time, computational complexity can be used to quantify the intrinsic time requirements of an abstract algorithm, without relying on the choice of a specific computer.\nAsymptotic ComplexityThe idea to express execution time as a function of input size seems obvious now, but it wasn\u0026rsquo;t so in the 1960s. Back then, typical computers cost millions of dollars, were so large that they required a separate room, and had clock rates measured in kilohertz. They were used for practical tasks at hand, like predicting the weather, sending rockets into space, or figuring out how far a Soviet nuclear missile can fly from the coast of Cuba — all of which are finite-length problems. Engineers of that era were mainly concerned with how to multiply $3 \\times 3$ matrices rather than $n \\times n$ ones.\nWhat caused the shift was the acquired confidence among computer scientists that computers will continue to become faster — and indeed they have. Over time, people stopped counting execution time, then stopped counting cycles, and then even stopped counting operations exactly, replacing it with an estimate that, on sufficiently large inputs, is only off by no more than a constant factor. With asymptotic complexity, verbose \u0026ldquo;$4 \\cdot n^3 - n^2$ operations\u0026rdquo; turns into plain \u0026ldquo;$\\Theta(n^3)$,\u0026rdquo; hiding the initial costs of individual operations in the \u0026ldquo;Big O,\u0026rdquo; along with all the other intricacies of the hardware.\nThe reason we use asymptotic complexity is that it provides simplicity while still being just precise enough to yield useful results about relative algorithm performance on large datasets. Under the promise that computers will eventually become fast enough to handle any sufficiently large input in a reasonable amount of time, asymptotically faster algorithms will always be faster in real-time too, regardless of the hidden constant.\nBut this promise turned out to be not true — at least not in terms of clock speeds and instruction latencies — and in this chapter, we will try to explain why and how to deal with it.\n","id":4,"path":"/hugo-page/hpc/complexity/","title":"Complexity Models"},{"content":"The users of floating-point arithmetic deserve one of these IQ bell curve memes — because this is how the relationship between it and most people typically proceeds:\nBeginner programmers use it everywhere as if it was some magic unlimited-precision data type. Then they discover that 0.1 + 0.2 != 0.3 or some other quirk like that, freak out, start thinking that some random error term is added to every computation, and for many years avoid any real data types completely. Then they finally man up, read the specification of how IEEE-754 floats work and start using them appropriately. Unfortunately, too many people are still at stage 2, breeding various misconceptions about floating-point arithmetic — thinking that it is fundamentally imprecise and unstable, and slower than integer arithmetic.\nBut these are all just myths. Floating-point arithmetic is often faster than integer arithmetic because of specialized instructions, and real number representations are thoroughly standardized and follow simple and deterministic rules in terms of rounding, allowing you to manage computational errors reliably.\nIn fact, it is so reliable that some high-level programming languages, most notably JavaScript, don\u0026rsquo;t have integers at all. In JavaScript, there is only one number type, which is internally stored as a 64-bit double, and due to the way floating-point arithmetic works, all integer numbers between $-2^{53}$ and $2^{53}$ and results of computations involving them can be stored exactly, so from a programmer\u0026rsquo;s perspective, there is little practical need for a separate integer type.\nOne notable exception is when you need to perform bitwise operations with numbers, which floating-point units (the coprocessors responsible for operations on floating-point numbers) typically don\u0026rsquo;t support. In this case, they need to be converted to integers, which is so frequently used in JavaScript-enabled browsers that arm added a special \u0026ldquo;FJCVTZS\u0026rdquo; instruction that stands for \u0026ldquo;Floating-point Javascript Convert to Signed fixed-point, rounding toward Zero\u0026rdquo; and does what it says it does — converts real to integer the exact same way as JavaScript — which is an interesting example of the software-hardware feedback loop in action.\nBut unless you are a JavaScript developer who uses real types exclusively to emulate integer arithmetic, you probably need a more in-depth guide about floating-point arithmetic, so we are going to start with a broader subject.\n#Real Number RepresentationsIf you need to deal with real (non-integer) numbers, you have several options with varying applicability. Before jumping straight to floating-point numbers, which is what most of this article is about, we want to discuss the available alternatives and the motivation behind them — after all, people who avoid floating-point arithmetic do have a point.\n#Symbolic ExpressionsThe first and the most cumbersome approach is to store not the resulting values themselves but the algebraic expressions that produce them.\nHere is a simple example. In some applications, such as computational geometry, apart from adding, subtracting and multiplying numbers, you also need to divide without rounding, producing a rational number, which can be exactly represented with a ratio of two integers:\nstruct r { int x, y; }; r operator+(r a, r b) { return {a.x * b.y + a.y * b.x, a.y * b.y}; } r operator*(r a, r b) { return {a.x * b.x, a.y * b.y}; } r operator/(r a, r b) { return {a.x * b.x, a.y * b.y}; } bool operator\u0026lt;(r a, r b) { return a.x * b.y \u0026lt; b.x * a.y; } // ...and so on, you get the idea This ratio can be made irreducible, which would even make this representation unique:\nstruct r { int x, y; r(int x, int y) : x(x), y(y) { if (y \u0026lt; 0) x = -x, y = -y; int g = gcd(x, y); x /= g; y /= g; } }; This is how computer algebra systems such as WolframAlpha and SageMath work: they operate solely on symbolic expressions and avoid evaluating anything as real values.\nWith this method, you get absolute precision, and it works well when you have a limited scope such as only supporting rational numbers. But this comes at a large computational cost because in general, you would need to somehow store the whole history of operations that led to the result and take it into account each time you perform a new operation — which quickly becomes unfeasible as the history grows.\n#Fixed-Point NumbersAnother approach is to stick to integers, but treat them as if they were multiplied by a fixed constant. This is essentially the same as changing units of measurement for more up-to-scale ones.\nBecause some values can\u0026rsquo;t be represented exactly, this makes computations imprecise: you need to round the results to nearest representable value.\nThis approach is commonly used in financial software, where you really need a straightforward way to manage rounding errors so that the final numbers add up. For example, NASDAQ uses $\\frac{1}{10000}$-th of a dollar as its base unit in its stock listings, meaning that you get the precision of exactly 4 digits after comma in all transactions.\nstruct money { uint v; // 1/10000th of a dollar }; std::string to_string(money) { return std::format(\u0026#34;${0}.{1:04d}\u0026#34;, v / 10000, v % 10000); } money operator*(money x, money y) { return {x.v * y.v / 10000}; } Apart from introducing rounding errors, a bigger problem is that they become useless when the scaling constant is misplaced. If the numbers you are working with are too large, then the internal integer value will overflow, and if the numbers are too small, they will be just rounded down to zero. Interestingly, the former case once became an issue on NASDAQ when the Berkshire Hathaway stock price approached $\\frac{2^{32} - 1}{10000}$ = $429,496.7295 and could no longer fit in an unsigned 32-bit integer.\nThis problem makes fixed-point arithmetic fundamentally unsuitable for applications where you need to use both small and large numbers, for example, evaluating certain physics equations:\n$$ E = m c^2 $$\nIn this particular one, $m$ is typically of the same order of magnitude as the mass of a proton ($1.67 \\cdot 10^{-27}$ kg) and $c$ is the speed of light ($3 \\cdot 10^9$ m/s).\n#Floating-Point NumbersIn most numerical applications, we are mainly concerned with the relative error. We want the result of our computations to differ from the truth by no more than, say, $0.01\\%$, and we don\u0026rsquo;t really care what that $0.01\\%$ equates to in absolute units.\nFloating-point numbers solve this by storing a certain number of the most significant digits and the order of magnitude of the number. More precisely, they are represented with an integer (called significand or mantissa) and scaled using an exponent of some fixed base — most commonly, 2 or 10. For example:\n$$ 1.2345 = \\underbrace{12345}\\text{mantissa} \\times {\\underbrace{10}\\text{base}!!!!} ^{\\overbrace{-4}^\\text{exponent}} $$\nComputers operate on fixed-length binary words, so when designing a floating-point format for hardware, you\u0026rsquo;d want a fixed-length binary format where some bits are dedicated for the mantissa (for more precision) and some for the exponent (for more range).\nThis handmade float implementation hopefully conveys the idea:\n// DIY floating-point number struct fp { int m; // mantissa int e; // exponent }; This way we can represent numbers in the form $\\pm \\; m \\times 2^e$ where both $m$ and $e$ are bounded and possibly negative integers — which would correspond to negative or small numbers respectively. The distribution of these numbers is very much non-uniform: there are roughly as many numbers in the $[0, 1)$ range as in the $[1, +\\infty)$ range.\nNote that these representations are not unique for some numbers. For example, number $1$ can be represented as\n$$ 1 \\times 2^0 = 2 \\times 2^{-1} = 256 \\times 2^{-8} $$\nand in 28 other ways that don\u0026rsquo;t overflow the mantissa.\nThis can be problematic for some applications, such as comparisons or hashing. To fix this, we can normalize these representations using a certain convention. In decimal, the standard form is to always put the comma after the first digit (6.022e23), and for binary, we can do the same:\n$$ 42 = 10101_2 = 1.0101_2 \\times 2^5 $$\nNotice that, following this rule, the first bit is always 1. It is redundant to store it explicitly, so we will just pretend that it\u0026rsquo;s there and only store the other bits, which correspond to some rational number in the $[0, 1)$ range. The set of representable numbers is now roughly\n$$ { \\pm ; (1 + m) \\cdot 2^e ; | ; m = \\frac{x}{2^{32}}, ; x \\in [0, 2^{32}) } $$\nSince $m$ is now a nonnegative value, we will now make it unsigned integer, and instead add a separate Boolean field for the sign of the number:\nstruct fp { bool s; // sign: \u0026#34;0\u0026#34; for \u0026#34;+\u0026#34;, \u0026#34;1\u0026#34; for \u0026#34;-\u0026#34; unsigned m; // mantissa int e; // exponent }; Now, let\u0026rsquo;s try to implement some arithmetic operation — for example, multiplication — using our handmade floats. Using the new formula, the result should be:\n$$ \\begin{aligned} c \u0026amp;= a \\cdot b \\ \u0026amp;= (s_a \\cdot (1 + m_a) \\cdot 2^{e_a}) \\cdot (s_b \\cdot (1 + m_b) \\cdot 2^{e_b}) \\ \u0026amp;= s_a \\cdot s_b \\cdot (1 + m_a) \\cdot (1 + m_b) \\cdot 2^{e_a} \\cdot 2^{e_b} \\ \u0026amp;= \\underbrace{s_a \\cdot s_b}{s_c} \\cdot (1 + \\underbrace{m_a + m_b + m_a \\cdot m_b}{m_c}) \\cdot 2^{\\overbrace{e_a + e_b}^{e_c}} \\end{aligned} $$\nThe groupings now seem straightforward to calculate, but there are two nuances:\nThe new mantissa is now in the $[0, 3)$ range. We need to check if it is larger than $1$ and normalize the representation, applying the following formula: $1 + m = (1 + 1) + (m - 1) = (1 + \\frac{m - 1}{2}) \\cdot 2$. The resulting number can be (and very likely is) not representable exactly due to the lack of precision. We need twice as many bits to account for the $m_a \\cdot m_b$ term, and the best we can do here is to round it to the nearest representable number. Since we need some extra bits to properly handle the mantissa overflow issue, we will reserve one bit from $m$ thus limiting it to $[0,2^{31})$ range.\nfp operator*(fp a, fp b) { fp c; c.s = a.s ^ b.s; c.e = a.e + b.e; uint64_t x = a.m, y = b.m; // casting to wider types uint64_t m = (x \u0026lt;\u0026lt; 31) + (y \u0026lt;\u0026lt; 31) + x * y; // 62- or 63-bit intermediate result if (m \u0026amp; (1\u0026lt;\u0026lt;62)) { // checking for overflow m -= (1\u0026lt;\u0026lt;62); // m -= 1; m \u0026gt;\u0026gt;= 1; c.e++; } m += (1\u0026lt;\u0026lt;30); // \u0026#34;rounding\u0026#34; by adding 0.5 to a value that will be floored next c.m = m \u0026gt;\u0026gt; 31; return c; } Many applications that require higher levels of precision use software floating-point arithmetic in a similar fashion. But of course, you don\u0026rsquo;t want to execute a sequence of 10 or so instructions that this code compiles to each time you want to multiply two real numbers, so on modern CPUs, floating-point arithmetic is implemented in hardware — usually as separate coprocessors due to its complexity.\nThe floating-point unit of x86 (often referred to as x87) has separate registers and its own tiny instruction set that supports memory operations, basic arithmetic, trigonometry, and some common operations such as logarithm, exponent, and square root. To make these operations properly work together, some additional details of floating-point number representation need to be clarified — which we will do in the next section.\n","id":5,"path":"/hugo-page/hpc/arithmetic/float/","title":"Floating-Point Numbers"},{"content":" Instrumentation is an overcomplicated term that means inserting timers and other tracking code into programs. The simplest example is using the time utility in Unix-like systems to measure the duration of execution for the whole program.\nMore generally, we want to know which parts of the program need optimization. There are tools shipped with compilers and IDEs that can time designated functions automatically, but it is more robust to do it by hand using any methods of interacting with time that the language provides:\nclock_t start = clock(); do_something(); float seconds = float(clock() - start) / CLOCKS_PER_SEC; printf(\u0026#34;do_something() took %.4f\u0026#34;, seconds); One nuance here is that you can\u0026rsquo;t measure the execution time of particularly quick functions this way because the clock function returns the current timestamp in microseconds ($10^{-6}$) and also by itself takes up to a few hundred nanoseconds to complete. All other time-related utilities similarly have at least microsecond granularity, which is an eternity in the world of low-level optimization.\nTo achieve higher precision, you can invoke the function repeatedly in a loop, time the whole thing once, and then divide the total time by the number of iterations:\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;time.h\u0026gt; const int N = 1e6; int main() { clock_t start = clock(); for (int i = 0; i \u0026lt; N; i++) clock(); // benchmarking the clock function itself float duration = float(clock() - start) / CLOCKS_PER_SEC; printf(\u0026#34;%.2fns per iteration\\n\u0026#34;, 1e9 * duration / N); return 0; } You also need to ensure that nothing gets cached, optimized away by the compiler, or affected by similar side effects. This is a separate and highly complicated topic that we will discuss in more detail at the end of the chapter.\n#Event SamplingInstrumentation can also be used to collect other types of information that can give useful insights about the performance of a particular algorithm. For example:\nfor a hash function, we are interested in the average length of its input; for a binary tree, we care about its size and height; for a sorting algorithm, we we want to know how many comparisons it does. In a similar way, we can insert counters in the code that compute these algorithm-specific statistics.\nAdding counters has the disadvantage of introducing overhead, although you can mitigate it almost completely by only doing it randomly for a small fraction of invocations:\nvoid query() { if (rand() % 100 == 0) { // update statistics } // main logic } If the sample rate is small enough, the only remaining overhead per invocation will be random number generation and a condition check. Interestingly, we can optimize it a bit more with some stats magic.\nMathematically, what we are doing here is repeatedly sampling from Bernoulli distribution (with $p$ equal to sample rate) until we get a success. There is another distribution that tells us how many iterations of Bernoulli sampling we need until the first positive, called geometric distribution. What we can do is to sample from it instead and use that value as a decrementing counter:\nvoid query() { static next_sample = geometric_distribution(sample_rate); if (next_sample--) { next_sample = geometric_distribution(sample_rate); // ... } // ... } This way we can remove the need to sample a new random number on each invocation, only resetting the counter when we choose to calculate statistics.\nTechniques like that are frequently used by library algorithm developers inside large projects to collect profiling data without affecting the performance of the end program too much.\n","id":6,"path":"/hugo-page/hpc/profiling/instrumentation/","title":"Instrumentation"},{"content":"The most low-level way to use SIMD is to use the assembly vector instructions directly — they aren\u0026rsquo;t different from their scalar equivalents at all — but we are not going to do that. Instead, we will use intrinsic functions mapping to these instructions that are available in modern C/C++ compilers.\nIn this section, we will go through the basics of their syntax, and in the rest of this chapter, we will use them extensively to do things that are actually interesting.\n#SetupTo use x86 intrinsics, we need to do a little groundwork.\nFirst, we need to determine which extensions are supported by the hardware. On Linux, you can call cat /proc/cpuinfo, and on other platforms, you\u0026rsquo;d better go to WikiChip and look it up there using the name of the CPU. In either case, there should be a flags section that lists the codes of all supported vector extensions.\nThere is also a special CPUID assembly instruction that lets you query various information about the CPU, including the support of particular vector extensions. It is primarily used to get such information in runtime and avoid distributing a separate binary for each microarchitecture. Its output information is returned very densely in the form of feature masks, so compilers provide built-in methods to make sense of it. Here is an example:\n#include \u0026lt;iostream\u0026gt; using namespace std; int main() { cout \u0026lt;\u0026lt; __builtin_cpu_supports(\u0026#34;sse\u0026#34;) \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; __builtin_cpu_supports(\u0026#34;sse2\u0026#34;) \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; __builtin_cpu_supports(\u0026#34;avx\u0026#34;) \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; __builtin_cpu_supports(\u0026#34;avx2\u0026#34;) \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; __builtin_cpu_supports(\u0026#34;avx512f\u0026#34;) \u0026lt;\u0026lt; endl; return 0; } Second, we need to include a header file that contains the subset of intrinsics we need. Similar to \u0026lt;bits/stdc++.h\u0026gt; in GCC, there is the \u0026lt;x86intrin.h\u0026gt; header that contains all of them, so we will just use that.\nAnd last, we need to tell the compiler that the target CPU actually supports these extensions. This can be done either with #pragma GCC target(...) as we did before, or with the -march=... flag in the compiler options. If you are compiling and running the code on the same machine, you can set -march=native to auto-detect the microarchitecture.\nIn all further code examples, assume that they begin with these lines:\n#pragma GCC target(\u0026#34;avx2\u0026#34;) #pragma GCC optimize(\u0026#34;O3\u0026#34;) #include \u0026lt;x86intrin.h\u0026gt; #include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; We will focus on AVX2 and the previous SIMD extensions in this chapter, which should be available on 95% of all desktop and server computers, although the general principles transfer on AVX512, Arm Neon, and other SIMD architectures just as well.\n#SIMD RegistersThe most notable distinction between SIMD extensions is the support for wider registers:\nSSE (1999) added 16 128-bit registers called xmm0 through xmm15. AVX (2011) added 16 256-bit registers called ymm0 through ymm15. AVX512 (2017) added1 16 512-bit registers called zmm0 through zmm15. As you can guess from the naming, and also from the fact that 512 bits already occupy a full cache line, x86 designers are not planning to add wider registers anytime soon.\nC/C++ compilers implement special vector types that refer to the data stored in these registers:\n128-bit __m128, __m128d and __m128i types for single-precision floating-point, double-precision floating-point and various integer data respectively; 256-bit __m256, __m256d, __m256i; 512-bit __m512, __m512d, __m512i. Registers themselves can hold data of any kind: these types are only used for type checking. You can convert a vector variable to another vector type the same way you would normally convert any other type, and it won\u0026rsquo;t cost you anything.\n#SIMD IntrinsicsIntrinsics are just C-style functions that do something with these vector data types, usually by simply calling the associated assembly instruction.\nFor example, here is a cycle that adds together two arrays of 64-bit floating-point numbers using AVX intrinsics:\ndouble a[100], b[100], c[100]; // iterate in blocks of 4, // because that\u0026#39;s how many doubles can fit into a 256-bit register for (int i = 0; i \u0026lt; 100; i += 4) { // load two 256-bit segments into registers __m256d x = _mm256_loadu_pd(\u0026amp;a[i]); __m256d y = _mm256_loadu_pd(\u0026amp;b[i]); // add 4+4 64-bit numbers together __m256d z = _mm256_add_pd(x, y); // write the 256-bit result into memory, starting with c[i] _mm256_storeu_pd(\u0026amp;c[i], z); } The main challenge of using SIMD is getting the data into contiguous fixed-sized blocks suitable for loading into registers. In the code above, we may in general have a problem if the length of the array is not divisible by the block size. There are two common solutions to this:\nWe can \u0026ldquo;overshoot\u0026rdquo; by iterating over the last incomplete segment either way. To make sure we don\u0026rsquo;t segfault by trying to read from or write to a memory region we don\u0026rsquo;t own, we need to pad the arrays to the nearest block size (typically with some \u0026ldquo;neutral\u0026rdquo; element, e.g., zero). Make one iteration less and write a little loop in the end that calculates the remainder normally (with scalar operations). Humans prefer #1 because it is simpler and results in less code, and compilers prefer #2 because they don\u0026rsquo;t really have another legal option.\n#Instruction ReferencesMost SIMD intrinsics follow a naming convention similar to _mm\u0026lt;size\u0026gt;_\u0026lt;action\u0026gt;_\u0026lt;type\u0026gt; and correspond to a single analogously named assembly instruction. They become relatively self-explanatory once you get used to the assembly naming conventions, although sometimes it does seem like their names were generated by cats walking on keyboards (explain this: punpcklqdq).\nHere are a few more examples, just so that you get the gist of it:\n_mm_add_epi16: add two 128-bit vectors of 16-bit extended packed integers, or simply said, shorts. _mm256_acos_pd: calculate elementwise $\\arccos$ for 4 packed doubles. _mm256_broadcast_sd: broadcast (copy) a double from a memory location to all 4 elements of the result vector. _mm256_ceil_pd: round up each of 4 doubles to the nearest integer. _mm256_cmpeq_epi32: compare 8+8 packed ints and return a mask that contains ones for equal element pairs. _mm256_blendv_ps: pick elements from one of two vectors according to a mask. As you may have guessed, there is a combinatorially very large number of intrinsics, and in addition to that, some instructions also have immediate values — so their intrinsics require compile-time constant parameters: for example, the floating-point comparison instruction has 32 different modifiers.\nFor some reason, there are some operations that are agnostic to the type of data stored in registers, but only take a specific vector type (usually 32-bit float) — you just have to convert to and from it to use that intrinsic. To simplify the examples in this chapter, we will mostly work with 32-bit integers (epi32) in 256-bit AVX2 registers.\nA very helpful reference for x86 SIMD intrinsics is the Intel Intrinsics Guide, which has groupings by categories and extensions, descriptions, pseudocode, associated assembly instructions, and their latency and throughput on Intel microarchitectures. You may want to bookmark that page.\nThe Intel reference is useful when you know that a specific instruction exists and just want to look up its name or performance info. When you don\u0026rsquo;t know whether it exists, this cheat sheet may do a better job.\nInstruction selection. Note that compilers do not necessarily pick the exact instruction that you specify. Similar to the scalar c = a + b we discussed before, there is a fused vector addition instruction too, so instead of using 2+1+1=4 instructions per loop cycle, compiler rewrites the code above with blocks of 3 instructions like this:\nvmovapd ymm1, YMMWORD PTR a[rax] vaddpd ymm0, ymm1, YMMWORD PTR b[rax] vmovapd YMMWORD PTR c[rax], ymm0 Sometimes, although quite rarely, this compiler interference makes things worse, so it is always a good idea to check the assembly and take a closer look at the emitted vector instructions (they usually start with a \u0026ldquo;v\u0026rdquo;).\nAlso, some of the intrinsics don\u0026rsquo;t map to a single instruction but a short sequence of them, as a convenient shortcut: broadcasts and extracts are a notable example.\n#GCC Vector ExtensionsIf you feel like the design of C intrinsics is terrible, you are not alone. I\u0026rsquo;ve spent hundreds of hours writing SIMD code and reading the Intel Intrinsics Guide, and I still can\u0026rsquo;t remember whether I need to type _mm256 or __m256.\nIntrinsics are not only hard to use but also neither portable nor maintainable. In good software, you don\u0026rsquo;t want to maintain different procedures for each CPU: you want to implement it just once, in an architecture-agnostic way.\nOne day, compiler engineers from the GNU Project thought the same way and developed a way to define your own vector types that feel more like arrays with some operators overloaded to match the relevant instructions.\nIn GCC, here is how you can define a vector of 8 integers packed into a 256-bit (32-byte) register:\ntypedef int v8si __attribute__ (( vector_size(32) )); // type ^ ^ typename size in bytes ^ Unfortunately, this is not a part of the C or C++ standard, so different compilers use different syntax for that.\nThere is somewhat of a naming convention, which is to include size and type of elements into the name of the type: in the example above, we defined a \u0026ldquo;vector of 8 signed integers.\u0026rdquo; But you may choose any name you want, like vec, reg or whatever. The only thing you don\u0026rsquo;t want to do is to name it vector because of how much confusion there would be because of std::vector.\nThe main advantage of using these types is that for many operations you can use normal C++ operators instead of looking up the relevant intrinsic.\nv4si a = {1, 2, 3, 5}; v4si b = {8, 13, 21, 34}; v4si c = a + b; for (int i = 0; i \u0026lt; 4; i++) printf(\u0026#34;%d\\n\u0026#34;, c[i]); c *= 2; // multiply by scalar for (int i = 0; i \u0026lt; 4; i++) printf(\u0026#34;%d\\n\u0026#34;, c[i]); With vector types we can greatly simplify the \u0026ldquo;a + b\u0026rdquo; loop we implemented with intrinsics before:\ntypedef double v4d __attribute__ (( vector_size(32) )); v4d a[100/4], b[100/4], c[100/4]; for (int i = 0; i \u0026lt; 100/4; i++) c[i] = a[i] + b[i]; As you can see, vector extensions are much cleaner compared to the nightmare we have with intrinsic functions. Their downside is that there are some things that we may want to do are just not expressible with native C++ constructs, so we will still need intrinsics for them. Luckily, this is not an exclusive choice, because vector types support zero-cost conversion to the _mm types and back:\nv8f x; int mask = _mm256_movemask_ps((__m256) x) There are also many third-party libraries for different languages that provide a similar capability to write portable SIMD code and also implement some, and just in general are nicer to use than both intrinsics and built-in vector types. Notable examples for C++ are Highway, Expressive Vector Engine, Vector Class Library, and xsimd.\nUsing a well-established SIMD library is recommended as it greatly improves the developer experience. In this book, however, we will try to keep close to the hardware and mostly use intrinsics directly, occasionally switching to the vector extensions for simplicity when we can.\nAVX512 also added 8 so-called mask registers named k0 through k7, which are used for masking and blending data. We are not going to cover them and will mostly use AVX2 and previous standards.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":7,"path":"/hugo-page/hpc/simd/intrinsics/","title":"Intrinsics and Vector Types"},{"content":"On the data path between the CPU registers and the RAM, there is a hierarchy of caches that exist to speed up access to frequently used data: the layers closer to the processor are faster but also smaller in size. The word \u0026ldquo;faster\u0026rdquo; here applies to two closely related but separate timings:\nThe delay between the moment when a read or a write is initiated and when the data arrives (latency). The number of memory operations that can be processed per unit of time (bandwidth). For many algorithms, memory bandwidth is the most important characteristic of the cache system. And at the same time, it is also the easiest to measure, so we are going to start with it.\nFor our experiment, we create an array and iterate over it $K$ times, incrementing its values:\nint a[N]; for (int t = 0; t \u0026lt; K; t++) for (int i = 0; i \u0026lt; N; i++) a[i]++; Changing $N$ and adjusting $K$ so that the total number of array cells accessed remains roughly constant and expressing the total time in \u0026ldquo;operations per second,\u0026rdquo; we get a graph like this:\nDotted vertical lines are cache layer sizes You can clearly see the cache sizes on this graph:\nWhen the whole array fits into the lowest layer of cache, the program is bottlenecked by the CPU rather than the L1 cache bandwidth. As the array becomes larger, the overhead associated with the first iterations of the loop becomes smaller, and the performance gets closer to its theoretical maximum of 16 GFLOPS. But then the performance drops: first to 12-13 GFLOPS when it exceeds the L1 cache, and then gradually to about 2 GFLOPS when it can no longer fit in the L3 cache. This situation is typical for many lightweight loops.\n#Frequency ScalingAll CPU cache layers are placed on the same microchip as the processor, so the bandwidth, latency, and all its other characteristics scale with the clock frequency. The RAM, on the other side, lives on its own fixed clock, and its characteristics remain constant. We can observe this by re-running the same benchmarking with turbo boost on:\nThis detail comes into play when comparing algorithm implementations. When the working dataset fits in the cache, the relative performance of the two implementations may be different depending on the CPU clock rate because the RAM remains unaffected by it (while everything else does not).\nFor this reason, it is advised to keep the clock rate fixed, and as the turbo boost isn\u0026rsquo;t stable enough, we run most of the benchmarks in this book at plain 2GHz.\n#Directional AccessThis incrementing loop needs to perform both reads and writes during its execution: on each iteration, we fetch a value, increment it, and then write it back. In many applications, we only need to do one of them, so let’s try to measure unidirectional bandwidth.\nCalculating the sum of an array only requires memory reads:\nfor (int i = 0; i \u0026lt; N; i++) s += a[i]; And zeroing an array (or filling it with any other constant value) only requires memory writes:\nfor (int i = 0; i \u0026lt; N; i++) a[i] = 0; Both loops are trivially vectorized by the compiler, and the second one is actually replaced with a memset, so the CPU is also not the bottleneck here (except when the array fits into the L1 cache).\nThe reason why unidirectional and bidirectional memory accesses would perform differently is that they share the cache and memory buses and other CPU facilities. In the case of RAM, this causes a twofold difference in performance between the pure read and simultaneous read and write scenarios because the memory controller has to switch between the modes on the one-way memory bus, thus halving the bandwidth. The performance drop is less severe for the L2 cache: the bottleneck here is not the cache bus, so the incrementing loop loses by only ~15%.\nThere is one interesting anomaly on the graph, namely that the write-only loop performs the same as the read-and-write one when the array hits the L3 cache and the RAM. This is because the CPU moves the data to the highest level of cache on each access, whether it is a read or a write — which is typically a good optimization, as in many use cases we will be needing it soon. When reading data, this isn\u0026rsquo;t a problem, as the data travels through the cache hierarchy anyway, but when writing, this causes another implicit read to be dispatched right after a write — thus requiring twice the bus bandwidth.\n#Bypassing the CacheWe can prevent the CPU from prefetching the data that we just have written by using non-temporal memory accesses. To do this, we need to re-implement the zeroing loop more directly without relying on compiler vectorization.\nIgnoring a few special cases, what memset and auto-vectorized assignment loops do under the hood is they just move 32-byte blocks of data with SIMD instructions:\nconst __m256i zeros = _mm256_set1_epi32(0); for (int i = 0; i + 7 \u0026lt; N; i += 8) _mm256_store_si256((__m256i*) \u0026amp;a[i], zeros); We can replace the usual vector store intrinsic with a non-temporal one:\nconst __m256i zeros = _mm256_set1_epi32(0); for (int i = 0; i + 7 \u0026lt; N; i += 8) _mm256_stream_si256((__m256i*) \u0026amp;a[i], zeros); Non-temporal memory reads or writes are a way to tell the CPU that we won\u0026rsquo;t be needing the data that we have just accessed in the future, so there is no need to read the data back after a write.\nOn the one hand, if the array is small enough to fit into the cache, and we actually access it some short time after, this has a negative effect because we have to read entirely it from the RAM (or, in this case, we have to write it into the RAM instead of using a locally cached version). And on the other, this prevents read-backs and lets us use the memory bus more efficiently.\nIn fact, the performance increase in the case of the RAM is even more than 2x and faster than the read-only benchmark. This happens because:\nthe memory controller doesn\u0026rsquo;t have to switch the bus between read and write modes this way; the instruction sequence becomes simpler, allowing for more pending memory instructions; and, most importantly, the memory controller can simply \u0026ldquo;fire and forget\u0026rdquo; non-temporal write requests — while for reads, it needs to remember what to do with the data once it arrives (similar to connection handles in networking software). Theoretically, both requests should use the same bandwidth: a read request sends an address and gets data, and a non-temporal write request sends an address with data and gets nothing. Not accounting for the direction, we transmit the same data, but the read cycle will be longer because it needs to wait for the data to be fetched. Since there is a practical limit on how many concurrent requests the memory system can handle, this difference in read/write cycle latency also results in the difference in their bandwidth.\nAlso, for these reasons, a single CPU core usually can\u0026rsquo;t fully saturate the memory bandwidth.\nThe same technique generalizes to memcpy: it also just moves 32-byte blocks with SIMD load/store instructions, and it can be similarly made non-temporal, increasing the throughput twofold for large arrays. There is also a non-temporal load instruction (_mm256_stream_load_si256) for when you want to read without polluting cache (e.g., when you don\u0026rsquo;t need the original array after a memcpy, but will need some data that you had accessed before calling it).\n","id":8,"path":"/hugo-page/hpc/cpu-cache/bandwidth/","title":"Memory Bandwidth"},{"content":"Modern computer memory is highly hierarchical. It consists of multiple cache layers of varying speed and size, where higher levels typically store most frequently accessed data from lower levels to reduce latency: each next level is usually an order of magnitude faster, but also smaller and/or more expensive.\nAbstractly, various memory devices can be described as modules that have a certain storage capacity $M$ and can read or write data in blocks of $B$ (not individual bytes!), taking a fixed time to complete.\nFrom this perspective, each type of memory has a few important characteristics:\ntotal size $M$; block size $B$; latency, that is, how much time it takes to fetch one byte; bandwidth, which may be higher than just the block size times latency, meaning that I/O operations can \u0026ldquo;overlap\u0026rdquo;; cost in the amortized sense, including the price for the chip, its energy requirements, maintenance, and so on. Here is an approximate comparison table for commodity hardware in 2021:\nType $M$ $B$ Latency Bandwidth $/GB/mo1 L1 10K 64B 2ns 80G/s - L2 100K 64B 5ns 40G/s - L3 1M/core 64B 20ns 20G/s - RAM GBs 64B 100ns 10G/s 1.5 SSD TBs 4K 0.1ms 5G/s 0.17 HDD TBs - 10ms 1G/s 0.04 S3 $\\infty$ - 150ms $\\infty$ 0.022 In reality, there are many specifics about each type of memory, which we will now go through.\n#Volatile MemoryEverything up to the RAM level is called volatile memory because it does not persist data in case of a power shortage and other disasters. It is fast, which is why it is used to store temporary data while the computer is powered.\nFrom fastest to slowest:\nCPU registers, which are the zero-time access data cells CPU uses to store all its intermediate values, can also be thought of as a memory type. There is only a limited number of them (e.g., just 16 \u0026ldquo;general purpose\u0026rdquo; ones), and in some cases, you may want to use all of them for performance reasons. CPU caches. Modern CPUs have multiple layers of cache (L1, L2, often L3, and rarely even L4). The lowest layer is shared between cores and is usually scaled with their number (e.g., a 10-core CPU should have around 10M of L3 cache). Random access memory, which is the first scalable type of memory: nowadays you can rent machines with half a terabyte of RAM on the public clouds. This is the one where most of your working data is supposed to be stored. The CPU cache system has an important concept of a cache line, which is the basic unit of data transfer between the CPU and the RAM. The size of a cache line is 64 bytes on most architectures, meaning that all main memory is divided into blocks of 64 bytes, and whenever you request (read or write) a single byte, you are also fetching all its 63 cache line neighbors whether your want them or not.\nCaching on the CPU level happens automatically based on the last access times of cache lines. When accessed, the contents of a cache line are emplaced onto the lowest cache layer and then gradually evicted to higher levels unless accessed again in time. The programmer can\u0026rsquo;t control this process explicitly, but it is worthwhile to study how it works in detail, which we will do in the next chapter.\n#Non-Volatile MemoryWhile the data cells in CPU caches and the RAM only gently store just a few electrons (that periodically leak and need to be periodically refreshed), the data cells in non-volatile memory types store hundreds of them. This lets the data persist for prolonged periods of time without power but comes at the cost of performance and durability — because when you have more electrons, you also have more opportunities for them to collide with silicon atoms.\nThere are many ways to store data in a persistent way, but these are the main ones from a programmer\u0026rsquo;s perspective:\nSolid state drives. These have relatively low latency on the order of 0.1ms ($10^5$ ns), but they also have a high cost, amplified by the fact that they have limited lifespans as each cell can only be written to a limited number of times. This is what mobile devices and most laptops use because they are compact and have no moving parts. Hard disk drives are unusual because they are actually rotating physical disks with a read/write head attached to them. To read a memory location, you need to wait until the disk rotates to the right position and then very precisely move the head to it. This results in some very weird access patterns where reading one byte randomly may take the same time as reading the next 1MB of data — which is usually on the order of milliseconds. Since this is the only part of a computer, except for the cooling system, that has mechanically moving parts, hard disks break quite often (with the average lifespan of ~3 years for a data center HDD). Network-attached storage, which is the practice of using other networked devices to store data on them. There are two distinctive types. The first one is the Network File System (NFS), which is a protocol for mounting the file system of another computer over the network. The other is API-based distributed storage systems, most famously Amazon S3, that are backed by a fleet of storage-optimized machines of a public cloud, typically using cheap HDDs or some more exotic storage types internally. While NFS can sometimes work even faster than HDD if it is located in the same data center, object storage in the public cloud usually has latencies of 50-100ms. They are typically highly distributed and replicated for better availability. Since SDD/HDD are noticeably slower than RAM, everything on or below this level is usually called external memory.\nUnlike the CPU caches, external memory can be explicitly controlled. This is useful in many cases, but most programmers just want to abstract away from it and use it as an extension of the main memory, and operating systems have the capability to do so by the means of virtual memory.\nPricing information is taken from the Google Cloud Platform.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCloud storage typically has multiple tiers, becoming progressively cheaper if you access the data less frequently.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":9,"path":"/hugo-page/hpc/external-memory/hierarchy/","title":"Memory Hierarchy"},{"content":"The main disadvantage of the supercomputers of the 1960s wasn\u0026rsquo;t that they were slow — relatively speaking, they weren\u0026rsquo;t — but that they were giant, complex to use, and so expensive that only the governments of the world superpowers could afford them. Their size was the reason they were so expensive: they required a lot of custom components that had to be very carefully assembled in the macro-world, by people holding advanced degrees in electrical engineering, in a process that couldn\u0026rsquo;t be scaled up for mass production.\nThe turning point was the development of microchips — single, tiny, complete circuits — which revolutionized the industry and turned out to be probably the most important invention of the 20th century. What was a multimillion-dollar cupboard of computing machinery in 1965 could in 1975 fit on a 4mm × 4mm slice of silicon1 that you can buy for $25. This dramatic improvement in affordability started the home computer revolution during the following decade, with computers like Apple II, Atari 2600, Commodore 64, and IBM PC becoming available to the masses.\n#How Microchips are MadeMicrochips are \u0026ldquo;printed\u0026rdquo; on a slice of crystalline silicon using a process called photolithography, which involves\ngrowing and slicing a very pure silicon crystal, covering it with a layer of a substance that dissolves when photons hit it, hitting it with photons in a set pattern, chemically etching the now-exposed parts, removing the remaining photoresist, …and then performing another 40-50 steps over several months to complete the rest of the CPU.\nConsider now the \u0026ldquo;hit it with photons\u0026rdquo; part. For that, we can use a system of lenses that projects a pattern onto a much smaller area, effectively making a tiny circuit with all the desired properties. This way, the optics of the 1970s were able to fit a few thousand transistors on the size of a fingernail, which gives microchips several key advantages that macro-world computers didn\u0026rsquo;t have:\nhigher clock rates (that were previously limited by the speed of light); the ability to scale the production; much lower material and power usage, translating to much lower cost per unit. Apart from these immediate benefits, photolithography enabled a clear path to improve performance further: you can just make lenses stronger, which in turn would create smaller, but functionally identical devices with relatively little effort.\n#Dennard ScalingConsider what happens when we scale a microchip down. A smaller circuit requires proportionally fewer materials, and smaller transistors take less time to switch (along with all other physical processes in the chip), allowing reducing the voltage and increasing the clock rate.\nA more detailed observation, known as the Dennard scaling, states that reducing transistor dimensions by 30%\ndoubles the transistor density ($0.7^2 \\approx 0.5$), increases the clock speed by 40% ($\\frac{1}{0.7} \\approx 1.4$), and leaves the overall power density the same. Since the per-unit manufacturing cost is a function of area, and the exploitation cost is mostly the cost of power2, each new \u0026ldquo;generation\u0026rdquo; should have roughly the same total cost, but 40% higher clock and twice as many transistors, which can be promptly used, for example, to add new instructions or increase the word size — to keep up with the same miniaturization happening in memory microchips.\nDue to the trade-offs between energy and performance you can make during the design, the fidelity of the fabrication process itself, such as \u0026ldquo;180nm\u0026rdquo; or \u0026ldquo;65nm,\u0026rdquo; directly translating to the density of transistors, became the trademark for CPU efficiency3.\nThroughout most of the computing history, optical shrinking was the main driving force behind performance improvements. Gordon Moore, the former CEO of Intel, predicted in 1975 that the transistor count in microprocessors will double every two years. His prediction held to this day and became known as Moore\u0026rsquo;s law.\nBoth Dennard scaling and Moore\u0026rsquo;s law are not actual laws of physics, but just observations made by savvy engineers. They are both destined to stop at some point due to fundamental physical limitations, the ultimate one being the size of silicon atoms. In fact, Dennard scaling already did — due to power issues.\nThermodynamically, a computer is just a very efficient device for converting electrical power into heat. This heat eventually needs to be removed, and there are physical limits to how much power you can dissipate from a millimeter-scale crystal. Computer engineers, aiming to maximize performance, essentially just choose the maximum possible clock rate so that the overall power consumption stays the same. If transistors become smaller, they have less capacitance, meaning less required voltage to flip them, which in turn allows increasing the clock rate.\nAround 2005–2007, this strategy stopped working because of leakage effects: the circuit features became so small that their magnetic fields started to make the electrons in the neighboring circuitry move in directions they are not supposed to, causing unnecessary heating and occasional bit flipping.\nThe only way to mitigate this is to increase the voltage; and to balance off power consumption you need to reduce clock frequency, which in turn makes the whole process progressively less profitable as transistor density increases. At some point, clock rates could no longer be increased by scaling, and the miniaturization trend started to slow down.\n#Modern ComputingDennard scaling has ended, but Moore\u0026rsquo;s law is not dead yet.\nClock rates plateaued, but the transistor count is still increasing, allowing for the creation of new, parallel hardware. Instead of chasing faster cycles, CPU designs started to focus on getting more useful things done in a single cycle. Instead of getting smaller, transistors have been changing shape.\nThis resulted in increasingly complex architectures capable of doing dozens, hundreds, or even thousands of different things every cycle.\nDie shot of a Zen CPU core by AMD (~1,400,000,000 transistors) Here are some core approaches making use of more available transistors that are driving recent computer designs:\nOverlapping the execution of instructions so that different parts of the CPU are kept busy (pipelining); Executing operations without necessarily waiting for the previous ones to complete (speculative and out-of-order execution); Adding multiple execution units to process independent operations simultaneously (superscalar processors); Increasing the machine word size, to the point of adding instructions capable of executing the same operation on a block of 128, 256, or 512 bits of data split into groups (SIMD); Adding layers of cache on the chip to speed up RAM and external memory access time (memory doesn\u0026rsquo;t quite follow the laws of silicon scaling); Adding multiple identical cores on a chip (parallel computing, GPUs); Using multiple chips in a motherboard and multiple cheaper computers in a data center (distributed computing); Using custom hardware to solve a specific problem with better chip utilization (ASICs, FPGAs). For modern computers, the \u0026ldquo;let\u0026rsquo;s count all operations\u0026rdquo; approach for predicting algorithm performance isn\u0026rsquo;t just slightly wrong but is off by several orders of magnitude. This calls for new computation models and other ways of assessing algorithm performance.\nActual sizes of CPUs are about centimeter-scale because of power management, heat dissipation, and the need to plug it into the motherboard without excessive swearing.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe cost of electricity for running a busy server for 2-3 years roughly equals the cost of making the chip itself.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAt some point, when Moore\u0026rsquo;s law started to slow down, chip makers stopped delineating their chips by the size of their components — and it is now more like a marketing term. A special committee has a meeting every two years where they take the previous node name, divide it by the square root of two, round to the nearest integer, declare the result to be the new node name, and then drink lots of wine. The \u0026ldquo;nm\u0026rdquo; doesn\u0026rsquo;t mean nanometer anymore.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":10,"path":"/hugo-page/hpc/complexity/hardware/","title":"Modern Hardware"},{"content":" Computers usually store time as the number of seconds that have passed since the 1st of January, 1970 — the start of the \u0026ldquo;Unix era\u0026rdquo; — and use these timestamps in all computations that have to do with time.\nWe humans also keep track of time relative to some point in the past, which usually has a political or religious significance. For example, at the moment of writing, approximately 63882260594 seconds have passed since 1 AD — 6th century Eastern Roman monks\u0026rsquo; best estimate of the day Jesus Christ was born.\nBut unlike computers, we do not always need all that information. Depending on the task at hand, the relevant part may be that it\u0026rsquo;s 2 pm right now, and it\u0026rsquo;s time to go to dinner; or that it\u0026rsquo;s Thursday, and so Subway\u0026rsquo;s sub of the day is an Italian BMT. Instead of the whole timestamp, we use its remainder containing just the information we need: it is much easier to deal with 1- or 2-digit numbers than 11-digit ones.\nProblem. Today is Thursday. What day of the week will be exactly in a year?\nIf we enumerate each day of the week, starting with Monday, from $0$ to $6$ inclusive, Thursday gets number $3$. To find out what day it is going to be in a year from now, we need to add $365$ to it and then reduce modulo $7$. Conveniently, $365 \\bmod 7 = 1$, so we know that it will be Friday unless it is a leap year (in which case it will be Saturday).\n#ResiduesDefinition. Two integers $a$ and $b$ are said to be congruent modulo $m$ if $m$ divides their difference:\n$$ m \\mid (a - b) ; \\Longleftrightarrow ; a \\equiv b \\pmod m $$\nFor example, the 42nd day of the year is the same weekday as the 161st since $(161 - 42) = 119 = 17 \\times 7$.\nCongruence modulo $m$ is an equivalence relation that splits all integers into equivalence classes called residues. Each residue class modulo $m$ may be represented by any one of its members — although we commonly use the smallest nonnegative integer of that class (equal to the remainder $x \\bmod m$ for all nonnegative $x$).\nModular arithmetic studies these sets of residues, which are fundamental for number theory.\nProblem. Our \u0026ldquo;week\u0026rdquo; now consists of $m$ days, and our year consists of $a$ days (no leap years). How many distinct days of the week there will be among one, two, three and so on whole years from now?\nFor simplicity, assume that today is Monday, so that the initial day number $d_0$ is zero, and after each year, it changes to\n$$ d_{k + 1} = (d_k + a) \\bmod m $$\nAfter $k$ years, it will be\n$$ d_k = k \\cdot a \\bmod m $$\nSince there are only $m$ days in a week, at some point, it will be Monday again, and the sequence of day numbers is going to cycle. The number of distinct days is the length of this cycle, so we need to find the smallest $k$ such that\n$$ k \\cdot a \\equiv 0 \\pmod m $$\nFirst of all, if $a \\equiv 0$, it will be eternal Monday. Now, assuming the non-trivial case of $a \\not \\equiv 0$:\nFor a seven-day week, $m = 7$ is prime. There is no $k$ smaller than $m$ such that $k \\cdot a$ is divisible by $m$ because $m$ can not be decomposed in such a product by the definition of primality. So, if $m$ is prime, we will cycle through all of $m$ weekdays. If $m$ is not prime, but $a$ is coprime with it (that is, $a$ and $m$ do not have common divisors), then the answer is still $m$ for the same reason: the divisors of $a$ do not help in zeroing out the product any faster. If $a$ and $m$ share some divisors, then it is only possible to get residues that are also divisible by them. For example, if the week is $m = 10$ days long, and the year has $a = 42$ or any other even number of days, then we will cycle through all even day numbers, and if the number of days is a multiple of $5$, then we will only oscillate between $0$ and $5$. Otherwise, we will go through all the $10$ remainders. Therefore, in general, the answer is $\\frac{m}{\\gcd(a, m)}$, where $\\gcd(a, m)$ is the greatest common divisor of $a$ and $m$.\n#Fermat\u0026rsquo;s TheoremNow, consider what happens if, instead of adding a number $a$, we repeatedly multiply by it, writing out a sequence of\n$$ d_n = a^n \\bmod m $$\nAgain, since there is a finite number of residues, there is going to be a cycle. But what will its length be? Turns out, if $m$ is prime, it will span all $(m - 1)$ non-zero residues.\nTheorem. For any $a$ and a prime $p$:\n$$ a^p \\equiv a \\pmod p $$\nProof. Let $P(x_1, x_2, \\ldots, x_n) = \\frac{k}{\\prod (x_i!)}$ be the multinomial coefficient: the number of times the element $a_1^{x_1} a_2^{x_2} \\ldots a_n^{x_n}$ appears after the expansion of $(a_1 + a_2 + \\ldots + a_n)^k$. Then:\n$$ \\begin{aligned} a^p \u0026amp;= (\\underbrace{1+1+\\ldots+1+1}\\text{$a$ times})^p \u0026amp; \\\\ \u0026amp;= \\sum{x_1+x_2+\\ldots+x_a = p} P(x_1, x_2, \\ldots, x_a) \u0026amp; \\text{(by definition)} \\\\ \u0026amp;= \\sum_{x_1+x_2+\\ldots+x_a = p} \\frac{p!}{x_1! x_2! \\ldots x_a!} \u0026amp; \\text{(which terms will not be divisible by $p$?)} \\\\ \u0026amp;\\equiv P(p, 0, \\ldots, 0) + \\ldots + P(0, 0, \\ldots, p) \u0026amp; \\text{(everything else will be canceled)} \\\\ \u0026amp;= a \\end{aligned} $$\nNote that this is only true for prime $p$. We can use this fact to test whether a given number is prime faster than by factoring it: we can pick a number $a$ at random, calculate $a^{p} \\bmod p$, and check whether it is equal to $a$ or not.\nThis is called Fermat primality test, and it is probabilistic — only returning either \u0026ldquo;no\u0026rdquo; or \u0026ldquo;maybe\u0026rdquo; — since it may be that $a^p$ just happened to be equal to $a$ despite $p$ being composite, in which case you need to repeat the test with a different random $a$ until you are satisfied with the false positive probability.\nPrimality tests are commonly used to generate large primes (for cryptographic purposes). There are roughly $\\frac{n}{\\ln n}$ primes among the first $n$ numbers (a fact that we are not going to prove), and they are distributed more or less evenly. One can just pick a random number from the required range, perform a primality check, and repeat until a prime is found, performing $O(\\ln n)$ trials on average.\nAn extremely bad input to the Fermat test is the Carmichael numbers, which are composite numbers $n$ that satisfy $a^{n-1} \\equiv 1 \\pmod n$ for all relatively prime $a$. But these are rare, and the chance of randomly bumping into it is low.\n#Modular DivisionImplementing most \u0026ldquo;normal\u0026rdquo; arithmetic operations with residues is straightforward. You only need to take care of integer overflows and remember to take modulo:\nc = (a + b) % m; c = (a - b + m) % m; c = a * b % m; But there is an issue with division: we can\u0026rsquo;t just bluntly divide two residues. For example, $\\frac{8}{2} = 4$, but\n$$ \\frac{8 \\bmod 5}{2 \\bmod 5} = \\frac{3}{2} \\neq 4 $$\nTo perform modular division, we need to find an element that \u0026ldquo;acts\u0026rdquo; like the reciprocal $\\frac{1}{a} = a^{-1}$ and multiply by it. This element is called a modular multiplicative inverse, and Fermat\u0026rsquo;s theorem can help us find it when the modulo $p$ is a prime. When we divide its equivalence twice by $a$, we get:\n$$ a^p \\equiv a \\implies a^{p-1} \\equiv 1 \\implies a^{p-2} \\equiv a^{-1} $$\nTherefore, $a^{p-2}$ is like $a^{-1}$ for the purposes of multiplication, which is what we need from a modular inverse of $a$.\n","id":11,"path":"/hugo-page/hpc/number-theory/modular/","title":"Modular Arithmetic"},{"content":"Pipelining lets you hide the latencies of instructions by running them concurrently, but also creates some potential obstacles of its own — characteristically called pipeline hazards, that is, situations when the next instruction cannot execute on the following clock cycle.\nThere are multiple ways this may happen:\nA structural hazard happens when two or more instructions need the same part of CPU (e.g., an execution unit). A data hazard happens when you have to wait for an operand to be computed from some previous step. A control hazard happens when a CPU can\u0026rsquo;t tell which instructions it needs to execute next. The only way to resolve a hazard is to have a pipeline stall: stop the progress of all previous steps until the cause of congestion is gone. This creates bubbles in the pipeline — analogous with air bubbles in fluid pipes — a time-propagating condition when execution units are idling and no useful work is done.\nPipeline stall on the execution stage Different hazards have different penalties:\nIn structural hazards, you have to wait (usually one more cycle) until the execution unit is ready. They are fundamental bottlenecks on performance and can\u0026rsquo;t be avoided — you have to engineer around them. In data hazards, you have to wait for the required data to be computed (the latency of the critical path). Data hazards are solved by restructuring computations so that the critical path is shorter. In control hazards, you generally have to flush the entire pipeline and start over, wasting a whole 15-20 cycles. They are solved by either removing branches completely, or making them predictable so that the CPU can effectively speculate on what is going to be executed next. As they have very different impacts on performance, we are going to go in the reversed order and start with the more grave ones.\n","id":12,"path":"/hugo-page/hpc/pipelining/hazards/","title":"Pipeline Hazards"},{"content":"Before jumping straight to compiler optimizations, which is what most of this chapter is about, let\u0026rsquo;s briefly recap the \u0026ldquo;big picture\u0026rdquo; first. Skipping the boring parts, there are 4 stages of turning C programs into executables:\nPreprocessing expands macros, pulls included source from header files, and strips off comments from source code: gcc -E source.c (outputs preprocessed source to stdout) Compiling parses the source, checks for syntax errors, converts it into an intermediate representation, performs optimizations, and finally translates it into assembly language: gcc -S file.c (emits an .s file) Assembly turns assembly language into machine code, except that any external function calls like printf are substituted with placeholders: gcc -c file.c (emits an .o file, called object file) Linking finally resolves the function calls by plugging in their actual addresses, and produces an executable binary: gcc -o binary file.c There are possibilities to improve program performance in each of these stages.\n#Interprocedural OptimizationWe have the last stage, linking, because it is is both easier and faster to compile programs on a file-by-file basis and then link those files together — this way you can do this in parallel and also cache intermediate results.\nIt also gives the ability to distribute code as libraries, which can be either static or shared:\nStatic libraries are simply collections of precompiled object files that are merged with other sources by the compiler to produce a single executable, just as it normally would. Dynamic or shared libraries are precompiled executables that have additional meta-information about where their callables are, references to which are resolved during runtime. As the name suggests, this allows sharing the compiled binaries between multiple programs. The main advantage of using static libraries is that you can perform various interprocedural optimizations that require more context than just the signatures of library functions, such as function inlining or dead code elimination. To force the linker to look for and only accept static libraries, you can pass the -static option.\nThis process is called link-time optimization (LTO), and it is possible because modern compilers also store some form of intermediate representation in object files, which allows them to perform certain lightweight optimizations on the program as a whole. This also allows using different compiled languages in the same program, which can even be optimized across language barriers if their compilers use the same intermediate representation.\nLTO is a relatively recent feature (it appeared in GCC only around 2014), and it is still far from perfect. In C and C++, the way to make sure no performance is lost due to separate compilation is to create a header-only library. As the name suggests, they are just header files that contain full definitions of all functions, and so by simply including them, the compiler gets access to all optimizations possible. Although you do have to recompile the library code from scratch each time, this approach retains full control and makes sure that no performance is lost.\n#Inspecting the OutputExamining output from each of these stages can yield useful insights into what\u0026rsquo;s happening in your program.\nYou can get assembly from the source by passing the -S flag to the compiler, which will then generate a human-readable *.s file. If you pass -fverbose-asm, this file will also contain compiler comments about source code line numbers and some info about variables being used. If it is just a little snippet and you are feeling lazy, you can use Compiler Explorer, which is a very handy online tool that converts source code to assembly, highlights logical asm blocks by color, includes a small x86 instruction set reference, and also has a large selection of other compilers, targets, and languages.\nApart from the assembly, the other most helpful level of abstraction is the intermediate representation on which compilers perform optimizations. The IR defines the flow of computation itself and is much less dependent on architecture features like the number of registers or a particular instruction set. It is often useful to inspect these to get insight into how the compiler sees your program, but this is a bit out of the scope of this book.\nWe will mainly use GCC in this chapter, but also try to duplicate examples for Clang when necessary. The two compilers are largely compatible with each other, for the most part only differing in some optimization flags and minor syntax details.\n","id":13,"path":"/hugo-page/hpc/compilation/stages/","title":"Stages of Compilation"},{"content":"In modular arithmetic (and computational algebra in general), you often need to raise a number to the $n$-th power — to do modular division, perform primality tests, or compute some combinatorial values — ­and you usually want to spend fewer than $\\Theta(n)$ operations calculating it.\nBinary exponentiation, also known as exponentiation by squaring, is a method that allows for computation of the $n$-th power using $O(\\log n)$ multiplications, relying on the following observation:\n$$ \\begin{aligned} a^{2k} \u0026amp;= (a^k)^2 \\ a^{2k + 1} \u0026amp;= (a^k)^2 \\cdot a \\end{aligned} $$\nTo compute $a^n$, we can recursively compute $a^{\\lfloor n / 2 \\rfloor}$, square it, and then optionally multiply by $a$ if $n$ is odd, corresponding to the following recurrence:\n$$ a^n = f(a, n) = \\begin{cases} 1, \u0026amp;\u0026amp; n = 0 \\ f(a, \\frac{n}{2})^2, \u0026amp;\u0026amp; 2 \\mid n \\ f(a, n - 1) \\cdot a, \u0026amp;\u0026amp; 2 \\nmid n \\end{cases} $$\nSince $n$ is at least halved every two recursive transitions, the depth of this recurrence and the total number of multiplications will be at most $O(\\log n)$.\n#Recursive ImplementationAs we already have a recurrence, it is natural to implement the algorithm as a case matching recursive function:\nconst int M = 1e9 + 7; // modulo typedef unsigned long long u64; u64 binpow(u64 a, u64 n) { if (n == 0) return 1; if (n % 2 == 1) return binpow(a, n - 1) * a % M; else { u64 b = binpow(a, n / 2); return b * b % M; } } In our benchmark, we use $n = m - 2$ so that we compute the multiplicative inverse of $a$ modulo $m$:\nu64 inverse(u64 a) { return binpow(a, M - 2); } We use $m = 10^9+7$, which is a modulo value commonly used in competitive programming to calculate checksums in combinatorial problems — because it is prime (allowing inverse via binary exponentiation), sufficiently large, not overflowing int in addition, not overflowing long long in multiplication, and easy to type as 1e9 + 7.\nSince we use it as compile-time constant in the code, the compiler can optimize the modulo by replacing it with multiplication (even if it is not a compile-time constant, it is still cheaper to compute the magic constants by hand once and use them for fast reduction).\nThe execution path — and consequently the running time — depends on the value of $n$. For this particular $n$, the baseline implementation takes around 330ns per call. As recursion introduces some overhead, it makes sense to unroll the implementation into an iterative procedure.\n#Iterative ImplementationThe result of $a^n$ can be represented as the product of $a$ to some powers of two — those that correspond to 1s in the binary representation of $n$. For example, if $n = 42 = 32 + 8 + 2$, then\n$$ a^{42} = a^{32+8+2} = a^{32} \\cdot a^8 \\cdot a^2 $$\nTo calculate this product, we can iterate over the bits of $n$ maintaining two variables: the value of $a^{2^k}$ and the current product after considering $k$ lowest bits of $n$. On each step, we multiply the current product by $a^{2^k}$ if the $k$-th bit of $n$ is set, and, in either case, square $a^k$ to get $a^{2^k \\cdot 2} = a^{2^{k+1}}$ that will be used on the next iteration.\nu64 binpow(u64 a, u64 n) { u64 r = 1; while (n) { if (n \u0026amp; 1) r = res * a % M; a = a * a % M; n \u0026gt;\u0026gt;= 1; } return r; } The iterative implementation takes about 180ns per call. The heavy calculations are the same; the improvement mainly comes from the reduced dependency chain: a = a * a % M needs to finish before the loop can proceed, and it can now execute concurrently with r = res * a % M.\nThe performance also benefits from $n$ being a constant, making all branches predictable and letting the scheduler know what needs to be executed in advance. The compiler, however, does not take advantage of it and does not unroll the while(n) n \u0026gt;\u0026gt;= 1 loop. We can rewrite it as a for loop that performs constant 30 iterations:\nu64 inverse(u64 a) { u64 r = 1; #pragma GCC unroll(30) for (int l = 0; l \u0026lt; 30; l++) { if ( (M - 2) \u0026gt;\u0026gt; l \u0026amp; 1 ) r = r * a % M; a = a * a % M; } return r; } This forces the compiler to generate only the instructions we need, shaving off another 10ns and making the total running time ~170ns.\nNote that the performance depends not only on the binary length of $n$, but also on the number of binary 1s. If $n$ is $2^{30}$, it takes around 20ns less as we don\u0026rsquo;t have to to perform any off-path multiplications.\n","id":14,"path":"/hugo-page/hpc/number-theory/exponentiation/","title":"Binary Exponentiation"},{"content":"When I began learning how to optimize programs myself, one big mistake I made was to rely primarily on the empirical approach. Not understanding how computers really worked, I would semi-randomly swap nested loops, rearrange arithmetic, combine branch conditions, inline functions by hand, and follow all sorts of other performance tips I\u0026rsquo;ve heard from other people, blindly hoping for improvement.\nUnfortunately, this is how most programmers approach optimization. Most texts about performance do not teach you to reason about software performance qualitatively. Instead they give you general advice about certain implementation approaches — and general performance intuition is clearly not enough.\nIt would have probably saved me dozens, if not hundreds of hours if I learned computer architecture before doing algorithmic programming. So, even if most people aren\u0026rsquo;t excited about it, we are going to spend the first few chapters studying how CPUs work and start with learning assembly.\n","id":15,"path":"/hugo-page/hpc/architecture/","title":"Computer Architecture"},{"content":"The first step of getting high performance from the compiler is to ask for it, which is done with over a hundred different compiler options, attributes, and pragmas.\n#Optimization LevelsThere are 4 and a half main levels of optimization for speed in GCC:\n-O0 is the default one that does no optimizations (although, in a sense, it does optimize: for compilation time). -O1 (also aliased as -O) does a few \u0026ldquo;low-hanging fruit\u0026rdquo; optimizations, almost not affecting the compilation time. -O2 enables all optimizations that are known to have little to no negative side effects and take a reasonable time to complete (this is what most projects use for production builds). -O3 does very aggressive optimization, enabling almost all correct optimizations implemented in GCC. -Ofast does everything in -O3, plus a few more optimizations flags that may break strict standard compliance, but not in a way that would be critical for most applications (e.g., floating-point operations may be rearranged so that the result is off by a few bits in the mantissa). There are also many other optimization flags that are not included even in -Ofast, because they are very situational, and enabling them by default is more likely to hurt performance rather than improve it — we will talk about some of them in the next section.\n#Specifying TargetsThe next thing you may want to do is to tell the compiler more about the computer(s) this code is supposed to be run on: the smaller the set of platforms is, the better. By default, it will generate binaries that can run on any relatively new (\u0026gt;2000) x86 CPU. The simplest way to narrow it down is to pass -march flag to specify the exact microarchitecture: -march=haswell. If you are compiling on the same computer that will run the binary, you can use -march=native for auto-detection.\nThe instruction sets are generally backward-compatible, so it is often enough to just use the name of the oldest microarchitecture you need to support. A more robust approach is to list specific features that the CPU is guaranteed to have: -mavx2, -mpopcnt. When you just want to tune the program for a particular machine without using any instructions that may crash it on incompatible CPUs, you can use the -mtune flag (by default -march=x also implies -mtune=x).\nThese options can also be specified for a compilation unit with pragmas instead of compilation flags:\n#pragma GCC optimize(\u0026#34;O3\u0026#34;) #pragma GCC target(\u0026#34;avx2\u0026#34;) This is useful when you need to optimize a single high-performance procedure without increasing the build time for the entire project.\n#Multiversioned FunctionsSometimes you may also want to provide several architecture-specific implementations in a single library. You can use attribute-based syntax to select between multiversioned functions automatically during compile time:\n__attribute__(( target(\u0026#34;default\u0026#34;) )) // fallback implementation int popcnt(int x) { int s = 0; for (int i = 0; i \u0026lt; 32; i++) s += (x\u0026gt;\u0026gt;i\u0026amp;1); return s; } __attribute__(( target(\u0026#34;popcnt\u0026#34;) )) // used if popcnt flag is enabled int popcnt(int x) { return __builtin_popcount(x); } In Clang, you can\u0026rsquo;t use pragmas to set target and optimization flags from the source code, but you can use attributes the same way as in GCC.\n","id":16,"path":"/hugo-page/hpc/compilation/flags/","title":"Flags and Targets"},{"content":"When we designed our DIY floating-point type, we omitted quite a lot of important little details:\nHow many bits do we dedicate for the mantissa and the exponent? Does a 0 sign bit mean +, or is it the other way around? How are these bits stored in memory? How do we represent 0? How exactly does rounding happen? What happens if we divide by zero? What happens if we take the square root of a negative number? What happens if we increment the largest representable number? Can we somehow detect if one of the above three happened? Most of the early computers didn\u0026rsquo;t support floating-point arithmetic, and when vendors started adding floating-point coprocessors, they had slightly different visions for what the answers to these questions should be. Diverse implementations made it difficult to use floating-point arithmetic reliably and portably — especially for the people who develop compilers.\nIn 1985, the Institute of Electrical and Electronics Engineers published a standard (called IEEE 754) that provided a formal specification of how floating-point numbers should work, which was quickly adopted by the vendors and is now used in virtually all general-purpose computers.\n#Float FormatsSimilar to our handmade float implementation, hardware floats use one bit for sign and a variable number of bits for the exponent and the mantissa parts. For example, the standard 32-bit float encoding uses the first (highest) bit for sign, the next 8 bits for the exponent, and the 23 remaining bits for the mantissa.\nOne of the reasons why they are stored in this exact order is that it is easier to compare and sort them: you can use mostly the same comparator circuit as for unsigned integers, except for maybe flipping some bits in case one of the numbers is negative.\nFor the same reason, the exponent is biased: the actual value is 127 less than the stored unsigned integer, which lets us also cover the values less than one (with negative exponents). In the example above:\n$$ (-1)^0 \\times 2^{01111100_2 - 127} \\times (1 + 2^{-2}) = 2^{124 - 127} \\times 1.25 = \\frac{1.25}{8} = 0.15625 $$\nIEEE 754 and a few consequent standards define not one but several representations that differ in sizes, most notably:\nType Sign Exponent Mantissa Total bits Approx. decimal digits single 1 8 23 32 ~7.2 double 1 11 52 64 ~15.9 half 1 5 10 16 ~3.3 extended 1 15 64 80 ~19.2 quadruple 1 15 112 128 ~34.0 bfloat16 1 8 7 16 ~2.3 Their availability ranges from chip to chip:\nMost CPUs support single- and double-precision — which is what float and double types refer to in C. Extended formats are exclusive to x86, and are available in C as the long double type, which falls back to double precision on Arm CPUs. The choice of 64 bits for mantissa is so that every long long integer can be represented exactly. There is also a 40-bit format that similarly allocates 32 mantissa bits. Quadruple as well as the 256-bit \u0026ldquo;octuple\u0026rdquo; formats are only used for specific scientific computations and are not supported by general-purpose hardware. Half-precision arithmetic only supports a small subset of operations and is generally used for applications such as machine learning, especially neural networks, because they tend to perform large amounts of calculations but don\u0026rsquo;t require high levels of precision. Half-precision is being gradually replaced by bfloat, which trades off 3 mantissa bits to have the same range as single-precision, enabling interoperability with it. It is mostly being adopted by specialized hardware: TPUs, FGPAs, and GPUs. The name stands for \u0026ldquo;Brain float.\u0026rdquo; Lower-precision types need less memory bandwidth to move them around and usually take fewer cycles to operate on (e.g., the division instruction may take $x$, $y$, or $z$ cycles depending on the type), which is why they are preferred when error tolerance allows it.\nDeep learning, emerging as a very popular and computationally-intensive field, created a huge demand for low-precision matrix multiplication, which led to manufacturers developing separate hardware or at least adding specialized instructions that support these types of computations — most notably, Google developing a custom chip called TPU (tensor processing unit) that specializes on multiplying 128-by-128 bfloat matrices, and NVIDIA adding \u0026ldquo;tensor cores,\u0026rdquo; capable of performing 4-by-4 matrix multiplication in one go, to all their newer GPUs.\nApart from their sizes, most of the behavior is the same between all floating-point types, which we will now clarify.\n#Handling Corner CasesThe default way integer arithmetic deals with corner cases such as division by zero is to crash.\nSometimes a software crash, in turn, causes a real, physical one. In 1996, the maiden flight of the Ariane 5 (the space launch vehicle that ESA uses to lift stuff into low Earth orbit) ended in a catastrophic explosion due to the policy of aborting computation on arithmetic error, which in this case was a floating-point to integer conversion overflow, that led to the navigation system thinking that it was off course and making a large correction, eventually causing the disintegration of a $200M rocket.\nThere is a way to gracefully handle corner cases like these: hardware interrupts. When an exception occurs, the CPU\ninterrupts the execution of a program; packs all relevant information into a data structure called \u0026ldquo;interrupt vector\u0026rdquo;; passes it to the operating system, which in turn either calls the handling code if it exists (the \u0026ldquo;try-except\u0026rdquo; block) or terminates the program otherwise. This is a complex mechanism that deserves an article of its own, but since this is a book about performance, the only thing you need to know is that they are quite slow and not desirable in real-time systems such as navigating rockets.\n#NaNs, Zeros and InfinitiesFloating-point arithmetic often deals with noisy, real-world data. Exceptions there are much more common than in the integer case, and for this reason, the default behavior when handling them is different. Instead of crashing, the result is substituted with a special value without interrupting the program execution (unless the programmer explicitly wants it to).\nThe first type of such value is the two infinities: a positive and a negative one. They are generated if the result of an operation can\u0026rsquo;t fit within the representable range, and they are treated as such in arithmetic.\n$$ \\begin{aligned} -∞ \u0026lt; x \u0026amp;\u0026lt; ∞ \\ ∞ + x \u0026amp;= ∞ \\ x ÷ ∞ \u0026amp;= 0 \\end{aligned} $$\nWhat happens if we, say, divide a value by zero? Should it be a negative or a positive infinity? This case is actually unambiguous because, somewhat less intuitively, there are also two zeros: a positive and a negative one.\n$$ \\frac{1}{+0} = +∞ ;;;; \\frac{1}{-0} = -∞ $$\nFun fact: x + 0.0 can\u0026rsquo;t be folded to x, but x + (-0.0) can, so the negative zero is a better initializer value than the positive zero as it is more likely to be optimized away by the compiler. The reason why +0.0 doesn\u0026rsquo;t work is that IEEE says that +0.0 + -0.0 == +0.0, so it will give a wrong answer for x = -0.0. The presence of two zeros frequently causes headaches like this — good news that you can pass -fno-signed-zeros to the compiler if you want to disable this behavior.\nZeros are encoded by setting all bits to zero, except for the sign bit in the negative case. Infinities are encoded by setting all their exponent bits to one and all mantissa bits to zero, with the sign bit distinguishing between positive and negative infinity.\nThe other type is the \u0026ldquo;not-a-number” (NaN), which is generated as the result of mathematically incorrect operations:\n$$ \\log(-1),; \\arccos(1.01),; ∞ − ∞,; −∞ + ∞,; 0 × ∞,; 0 ÷ 0,; ∞ ÷ ∞ $$\nThere are two types of NaNs: a signaling NaN and a quiet NaN. A signaling NaN raises an exception flag, which may or may not cause immediate hardware interrupt depending on the FPU configuration, while a quiet NaN just propagates through almost every arithmetic operation, resulting in more NaNs.\nIn binary, both NaNs have their exponent bits all set and the mantissa part being anything other than all zeros (to distinguish them from infinities). Note that there are very many valid encodings for a NaN.\n#Further ReadingIf you are so inclined, you can read the classic \u0026ldquo;What Every Computer Scientist Should Know About Floating-Point Arithmetic\u0026rdquo; (1991) and the paper introducing Grisu3, the current state-of-the-art for printing floating-point numbers.\n","id":17,"path":"/hugo-page/hpc/arithmetic/ieee-754/","title":"IEEE 754 Floats"},{"content":"Let\u0026rsquo;s consider a slightly more complex example:\nloop: add edx, DWORD PTR [rax] add rax, 4 cmp rax, rcx jne loop It calculates the sum of a 32-bit integer array, just as a simple for loop would.\nThe \u0026ldquo;body\u0026rdquo; of the loop is add edx, DWORD PTR [rax]: this instruction loads data from the iterator rax and adds it to the accumulator edx. Next, we move the iterator 4 bytes forward with add rax, 4. Then, a slightly more complicated thing happens.\n#JumpsAssembly doesn\u0026rsquo;t have if-s, for-s, functions, or other control flow structures that high-level languages have. What it does have is goto, or \u0026ldquo;jump,\u0026rdquo; how it is known in the world of low-level programming.\nJump moves the instruction pointer to a location specified by its operand. This location may be either an absolute address in memory, relative to the current address or even computed during runtime. To avoid the headache of managing these addresses directly, you can mark any instruction with a string followed by :, and then use this string as a label which gets replaced by the relative address of this instruction when converted to machine code.\nLabels can be any string, but compilers don\u0026rsquo;t get creative and typically just use the line numbers in the source code and function names with their signatures when picking names for labels.\nUnconditional jump jmp can only be used to implement while (true) kind of loops or stitch parts of a program together. A family of conditional jumps is used to implement actual control flow.\nIt is reasonable to think that these conditions are computed as bool-s somewhere and passed to conditional jumps as operands: after all, this is how it works in programming languages. But that is not how it is implemented in hardware. Conditional operations use a special FLAGS register, which first needs to be populated by executing instructions that perform some kind of check.\nIn our example, cmp rax, rcx compares the iterator rax with the end-of-array pointer rcx. This updates the FLAGS register, and now it can be used by jne loop, which looks up a certain bit there that tells whether the two values are equal or not, and then either jumps back to the beginning or continues to the next instruction, thus breaking the loop.\n#Loop UnrollingOne thing you might have noticed about the loop above is that there is a lot of overhead to process a single element. During each cycle, there is only one useful instruction executed, and the other 3 are incrementing the iterator and trying to find out if we are done yet.\nWhat we can do is to unroll the loop by grouping iterations together — equivalent to writing something like this in C:\nfor (int i = 0; i \u0026lt; n; i += 4) { s += a[i]; s += a[i + 1]; s += a[i + 2]; s += a[i + 3]; } In assembly, it would look something like this:\nloop: add edx, [rax] add edx, [rax+4] add edx, [rax+8] add edx, [rax+12] add rax, 16 cmp rax, rsi jne loop Now we only need 3 loop control instructions for 4 useful ones (an improvement from $\\frac{1}{4}$ to $\\frac{4}{7}$ in terms of efficiency), and this can be continued to reduce the overhead almost to zero.\nIn practice, unrolling loops isn\u0026rsquo;t always necessary for performance because modern processors don\u0026rsquo;t actually execute instructions one-by-one, but maintain a queue of pending instructions so that two independent operations can be executed concurrently without waiting for each other to finish.\nThis is our case too: the real speedup from unrolling won\u0026rsquo;t be fourfold, because the operations of incrementing the counter and checking if we are done are independent from the loop body, and can be scheduled to run concurrently with it. But may still be beneficial to ask the compiler to unroll it to some extent.\n#An Alternative ApproachYou don\u0026rsquo;t have to explicitly use cmp or a similar instruction to make a conditional jump. Many other instructions either read or modify the FLAGS register, sometimes as a by-product enabling optional exception checks.\nFor example, add always sets a number of flags, denoting whether the result is zero, is negative, whether an overflow or an underflow occurred, and so on. Taking advantage of this mechanism, compilers often produce loops like this:\nmov rax, -100 ; replace 100 with the array size loop: add edx, DWORD PTR [rax + 100 + rcx] add rax, 4 jnz loop ; checks if the result is zero This code is a bit harder to read for a human, but it is one instruction shorter in the repeated part, which may meaningfully affect performance.\n","id":18,"path":"/hugo-page/hpc/architecture/loops/","title":"Loops and Conditionals"},{"content":"Despite that bandwidth is a more complicated concept, it is much easier to observe and measure than latency: you can simply execute a long series of independent read or write queries, and the scheduler, having access to them in advance, reorders and overlaps them, hiding their latency and maximizing the total throughput.\nTo measure latency, we need to design an experiment where the CPU can\u0026rsquo;t cheat by knowing the memory locations we will request in advance. One way to ensure this is to generate a random permutation of size $N$ that corresponds to a cycle and then repeatedly follow the permutation:\nint p[N], q[N]; // generating a random permutation iota(p, p + N, 0); random_shuffle(p, p + N); // this permutation may contain multiple cycles, // so instead we use it to construct another permutation with a single cycle int k = p[N - 1]; for (int i = 0; i \u0026lt; N; i++) k = q[k] = p[i]; for (int t = 0; t \u0026lt; K; t++) for (int i = 0; i \u0026lt; N; i++) k = q[k]; Compared to linear iteration, it is much slower — by multiple orders of magnitude — to visit all elements of an array this way. Not only does it make SIMD impossible, but it also stalls the pipeline, creating a large traffic jam of instructions, all waiting for a single piece of data to be fetched from the memory.\nThis performance anti-pattern is known as pointer chasing, and it is very frequent in data structures, especially those written high-level languages that use lots of heap-allocated objects and pointers to them necessary for dynamic typing.\nWhen talking about latency, it makes more sense to use cycles or nanoseconds rather than throughput units, so we replace this graph with its reciprocal:\nNote that the cliffs on both graphs aren\u0026rsquo;t as distinctive as they were for the bandwidth. This is because we still have some chance of hitting the previous layer of cache even if the array can\u0026rsquo;t fit into it entirely.\n#Theoretical LatencyMore formally, if there are $k$ levels in the cache hierarchy with sizes $s_i$ and latencies $l_i$, then, instead of being equal to the slowest access, their expected latency will be:\n$$ E[L] = \\frac{ s_1 \\cdot l_1 + (s_2 - s_1) \\cdot l_2 % + (s_3 - s_2) \\cdot l_3 + \\ldots + (N - s_k) \\cdot l_{RAM} }{N} $$\nIf we abstract away from all that happens before the slowest cache layer, we can reduce the formula to just this:\n$$ E[L] = \\frac{N \\cdot l_{last} - C}{N} = l_{last} - \\frac{C}{N} $$\nAs $N$ increases, the expected latency slowly approaches $l_{last}$, and if you squint hard enough, the graph of the throughput (reciprocal latency) should roughly look like if it is composed of a few transposed and scaled hyperbolas:\n$$ \\begin{aligned} E[L]^{-1} \u0026amp;= \\frac{1}{l_{last} - \\frac{C}{N}} \\ \u0026amp;= \\frac{N}{N \\cdot l_{last} - C} \\ \u0026amp;= \\frac{1}{l_{last}} \\cdot \\frac{N + \\frac{C}{l_{last}} - \\frac{C}{l_{last}}}{N - \\frac{C}{l_{last}}} \\ \u0026amp;= \\frac{1}{l_{last}} \\cdot \\left(\\frac{1}{N \\cdot \\frac{l_{last}}{C} - 1} + 1\\right) \\ \u0026amp;= \\frac{1}{k \\cdot (x - x_0)} + y_0 \\end{aligned} $$\nTo get the actual latency numbers, we can iteratively apply the first formula to deduce $l_1$, then $l_2$, and so on. Or just look at the values right before the cliff — they should be within 10-15% of the true latency.\nThere are more direct ways to measure latency, including the use of non-temporal reads, but this benchmark is more representable of practical access patterns.\n#Frequency ScalingSimilar to bandwidth, the latency of all CPU caches proportionally scales with its clock frequency, while the RAM does not. We can also observe this difference if we change the frequency by turning turbo boost on.\nThe graph starts making more sense if we plot it as a relative speedup.\nYou would expect 2x rates for array sizes that fit into CPU cache entirely, but then roughly equal for arrays stored in RAM. But this is not quite what is happening: there is a small, fixed-latency delay on lower clocked run even for RAM accesses. This happens because the CPU first has to check its cache before dispatching a read query to the main memory — to save RAM bandwidth for other processes that potentially need it.\nMemory latency is also slightly affected by some details of the virtual memory implementation and RAM-specific timings, which we will discuss later.\n","id":19,"path":"/hugo-page/hpc/cpu-cache/latency/","title":"Memory Latency"},{"content":"If you took some time to study the reference, you may have noticed that there are essentially two major groups of vector operations:\nInstructions that perform some elementwise operation (+, *, \u0026lt;, acos, etc.). Instructions that load, store, mask, shuffle, and generally move data around. While using the elementwise instructions is easy, the largest challenge with SIMD is getting the data in vector registers in the first place, with low enough overhead so that the whole endeavor is worthwhile.\n#Aligned Loads and StoresOperations of reading and writing the contents of a SIMD register into memory have two versions each: load / loadu and store / storeu. The letter \u0026ldquo;u\u0026rdquo; here stands for \u0026ldquo;unaligned.\u0026rdquo; The difference is that the former ones only work correctly when the read / written block fits inside a single cache line (and crash otherwise), while the latter work either way, but with a slight performance penalty if the block crosses a cache line.\nSometimes, especially when the \u0026ldquo;inner\u0026rdquo; operation is very lightweight, the performance difference becomes significant (at least because you need to fetch two cache lines instead of one). As an extreme example, this way of adding two arrays together:\nfor (int i = 3; i + 7 \u0026lt; n; i += 8) { __m256i x = _mm256_loadu_si256((__m256i*) \u0026amp;a[i]); __m256i y = _mm256_loadu_si256((__m256i*) \u0026amp;b[i]); __m256i z = _mm256_add_epi32(x, y); _mm256_storeu_si256((__m256i*) \u0026amp;c[i], z); } …is ~30% slower than its aligned version:\nfor (int i = 0; i \u0026lt; n; i += 8) { __m256i x = _mm256_load_si256((__m256i*) \u0026amp;a[i]); __m256i y = _mm256_load_si256((__m256i*) \u0026amp;b[i]); __m256i z = _mm256_add_epi32(x, y); _mm256_store_si256((__m256i*) \u0026amp;c[i], z); } In the first version, assuming that arrays a, b and c are all 64-byte aligned (the addresses of their first elements are divisible by 64, and so they start at the beginning of a cache line), roughly half of reads and writes will be \u0026ldquo;bad\u0026rdquo; because they cross a cache line boundary.\nNote that the performance difference is caused by the cache system and not by the instructions themselves. On most modern architectures, the loadu / storeu intrinsics should be equally as fast as load / store given that in both cases the blocks only span one cache line. The advantage of the latter is that they can act as free run time assertions that all reads and writes are aligned.\nThis makes it important to properly align arrays and other data on allocation, and it is also one of the reasons why compilers can\u0026rsquo;t always auto-vectorize efficiently. For most purposes, we only need to guarantee that any 32-byte SIMD block will not cross a cache line boundary, and we can specify this alignment with the alignas specifier:\nalignas(32) float a[n]; for (int i = 0; i \u0026lt; n; i += 8) { __m256 x = _mm256_load_ps(\u0026amp;a[i]); // ... } The built-in vector types already have corresponding alignment requirements and assume aligned memory reads and writes — so you are always safe when allocating an array of v8si, but when converting it from int* you have to make sure it is aligned.\nSimilar to the scalar case, many arithmetic instructions take memory addresses as operands — vector addition is an example — although you can\u0026rsquo;t explicitly use it as an intrinsic and have to rely on the compiler. There are also a few other instructions for reading a SIMD block from memory, notably the non-temporal load and store operations that don\u0026rsquo;t lift accessed data in the cache hierarchy.\n#Register AliasingThe first SIMD extension, MMX, started quite small. It only used 64-bit vectors, which were conveniently aliased to the mantissa part of a 80-bit float so that there is no need to introduce a separate set of registers. As the vector size grew with later extensions, the same register aliasing mechanism used in general-purpose registers was adopted for the vector registers to maintain backward compatibility: xmm0 is the first half (128 bits) of ymm0, xmm1 is the first half of ymm1, and so on.\nThis feature, combined with the fact that the vector registers are located in the FPU, makes moving data between them and the general-purpose registers slightly complicated.\n#Extract and InsertTo extract a specific value from a vector, you can use _mm256_extract_epi32 and similar intrinsics. It takes the index of the integer to be extracted as the second parameter and generates different instruction sequences depending on its value.\nIf you need to extract the first element, it generates the vmovd instruction (for xmm0, the first half of the vector):\nvmovd eax, xmm0 For other elements of an SSE vector, it generates possibly slightly slower vpextrd:\nvpextrd eax, xmm0, 1 To extract anything from the second half of an AVX vector, it first has to extract that second half, and then the scalar itself. For example, here is how it extracts the last (eighth) element,\nvextracti128 xmm0, ymm0, 0x1 vpextrd eax, xmm0, 3 There is a similar _mm256_insert_epi32 intrinsic for overwriting specific elements:\nmov eax, 42 ; v = _mm256_insert_epi32(v, 42, 0); vpinsrd xmm2, xmm0, eax, 0 vinserti128 ymm0, ymm0, xmm2, 0x0 ; v = _mm256_insert_epi32(v, 42, 7); vextracti128 xmm1, ymm0, 0x1 vpinsrd xmm2, xmm1, eax, 3 vinserti128 ymm0, ymm0, xmm2, 0x1 Takeaway: moving scalar data to and from vector registers is slow, especially when this isn\u0026rsquo;t the first element.\n#Making ConstantsIf you need to populate not just one element but the entire vector, you can use the _mm256_setr_epi32 intrinsic:\n__m256 iota = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7); The \u0026ldquo;r\u0026rdquo; here stands for \u0026ldquo;reversed\u0026rdquo; — from the CPU point of view, not for humans. There is also the _mm256_set_epi32 (without \u0026ldquo;r\u0026rdquo;) that fills the values from the opposite direction. Both are mostly used to create compile-time constants that are then fetched into the register with a block load. If your use case is filling a vector with zeros, use the _mm256_setzero_si256 instead: it xor-s the register with itself.\nIn built-in vector types, you can just use normal braced initialization:\nvec zero = {}; vec iota = {0, 1, 2, 3, 4, 5, 6, 7}; #BroadcastInstead of modifying just one element, you can also broadcast a single value into all its positions:\n; __m256i v = _mm256_set1_epi32(42); mov eax, 42 vmovd xmm0, eax vpbroadcastd ymm0, xmm0 This is a frequently used operation, so you can also use a memory location:\n; __m256 v = _mm256_broadcast_ss(\u0026amp;a[i]); vbroadcastss ymm0, DWORD PTR [rdi] When using built-in vector types, you can create a zero vector and add a scalar to it:\nvec v = 42 + vec{}; #Mapping to ArraysIf you want to avoid all this complexity, you can just dump the vector in memory and read its values back as scalars:\nvoid print(__m256i v) { auto t = (unsigned*) \u0026amp;v; for (int i = 0; i \u0026lt; 8; i++) std::cout \u0026lt;\u0026lt; std::bitset\u0026lt;32\u0026gt;(t[i]) \u0026lt;\u0026lt; \u0026#34; \u0026#34;; std::cout \u0026lt;\u0026lt; std::endl; } This may not be fast or technically legal (the C++ standard doesn\u0026rsquo;t specify what happens when you cast data like this), but it is simple, and I frequently use this code to print out the contents of a vector during debugging.\n#Non-Contiguous LoadLater SIMD extensions added special \u0026ldquo;gather\u0026rdquo; and \u0026ldquo;scatter instructions that read/write data non-sequentially using arbitrary array indices. These don\u0026rsquo;t work 8 times faster though and are usually limited by the memory rather than the CPU, but they are still helpful for certain applications such as sparse linear algebra.\nGather is available since AVX2, and various scatter instructions are available since AVX512.\nLet\u0026rsquo;s see if they work faster than scalar reads. First, we create an array of size $N$ and $Q$ random read queries:\nint a[N], q[Q]; for (int i = 0; i \u0026lt; N; i++) a[i] = rand(); for (int i = 0; i \u0026lt; Q; i++) q[i] = rand() % N; In the scalar code, we add the elements specified by the queries to a checksum one by one:\nint s = 0; for (int i = 0; i \u0026lt; Q; i++) s += a[q[i]]; And in the SIMD code, we use the gather instruction to do that for 8 different indexes in parallel:\nreg s = _mm256_setzero_si256(); for (int i = 0; i \u0026lt; Q; i += 8) { reg idx = _mm256_load_si256( (reg*) \u0026amp;q[i] ); reg x = _mm256_i32gather_epi32(a, idx, 4); s = _mm256_add_epi32(s, x); } They perform roughly the same, except when the array fits into the L1 cache:\nThe purpose of gather and scatter is not to perform memory operations faster, but to get the data into registers to perform heavy computations on them. For anything costlier than just one addition, they are hugely favorable.\nThe lack of (fast) gather and scatter instructions makes SIMD programming on CPUs very different from proper parallel computing environments that support independent memory access. You have to always engineer around it and employ various ways of organizing your data sequentially so that it be loaded into registers.\n","id":20,"path":"/hugo-page/hpc/simd/moving/","title":"Moving Data"},{"content":"If you are reading this book, then somewhere on your computer science journey you had a moment when you first started to care about the efficiency of your code.\nMine was in high school, when I realized that making websites and doing useful programming won\u0026rsquo;t get you into a university, and entered the exciting world of algorithmic programming olympiads. I was an okay programmer, especially for a highschooler, but I had never really wondered how much time it took for my code to execute before. But suddenly it started to matter: each problem now has a strict time limit. I started counting my operations. How many can you do in one second?\nI didn\u0026rsquo;t know much about computer architecture to answer this question. But I also didn\u0026rsquo;t need the right answer — I needed a rule of thumb. My thought process was: \u0026ldquo;2-3GHz means 2 to 3 billion instructions executed every second, and in a simple loop that does something with array elements, I also need to increment loop counter, check end-of-loop condition, do array indexing and stuff like that, so let\u0026rsquo;s add room for 3-5 more instructions for every useful one\u0026rdquo; and ended up with using $5 \\cdot 10^8$ as an estimate. None of these statements are true, but counting how many operations my algorithm needed and dividing it by this number was a good rule of thumb for my use case.\nThe real answer, of course, is much more complicated and highly dependent on what kind of \u0026ldquo;operation\u0026rdquo; you have in mind. It can be as low as $10^7$ for things like pointer chasing and as high as $10^{11}$ for SIMD-accelerated linear algebra. To demonstrate these striking differences, we will use the case study of matrix multiplication implemented in different languages — and dig deeper into how computers execute them.\n#Types of Languages On the lowest level, computers execute machine code consisting of binary-encoded instructions which are used to control the CPU. They are specific, quirky, and require a great deal of intellectual effort to work with, so one of the first things people did after creating computers was create programming languages, which abstract away some details of how computers operate to simplify the process of programming.\nA programming language is fundamentally just an interface. Any program written in it is just a nicer higher-level representation which still at some point needs to be transformed into the machine code to be executed on the CPU — and there are several different means of doing that:\nFrom a programmer\u0026rsquo;s perspective, there are two types of languages: compiled, which pre-process before executing, and interpreted, which are executed during runtime using a separate program called an interpreter. From a computer\u0026rsquo;s perspective, there are also two types of languages: native, which directly execute machine code, and managed, which rely on some sort of runtime to do it. Since running machine code in an interpreter doesn\u0026rsquo;t make sense, this makes a total of three types of languages:\nInterpreted languages, such as Python, JavaScript, or Ruby. Compiled languages with a runtime, such as Java, C#, or Erlang (and languages that work on their VMs, such as Scala, F#, or Elixir). Compiled native languages, such as C, Go, or Rust. There is no \u0026ldquo;right\u0026rdquo; way of executing computer programs: each approach has its own gains and drawbacks. Interpreters and virtual machines provide flexibility and enable some nice high-level programming features such as dynamic typing, run time code alteration, and automatic memory management, but these come with some unavoidable performance trade-offs, which we will now talk about.\n#Interpreted languagesHere is an example of a by-definition $1024 \\times 1024$ matrix multiplication in pure Python:\nimport time import random n = 1024 a = [[random.random() for row in range(n)] for col in range(n)] b = [[random.random() for row in range(n)] for col in range(n)] c = [[0 for row in range(n)] for col in range(n)] start = time.time() for i in range(n): for j in range(n): for k in range(n): c[i][j] += a[i][k] * b[k][j] duration = time.time() - start print(duration) This code runs in 630 seconds. That\u0026rsquo;s more than 10 minutes!\nLet\u0026rsquo;s try to put this number in perspective. The CPU that ran it has a clock frequency of 1.4GHz, meaning that it does $1.4 \\cdot 10^9$ cycles per second, totaling to almost $10^{15}$ for the entire computation, and about 880 cycles per multiplication in the innermost loop.\nThis is not surprising if you consider the things that Python needs to do to figure out what the programmer meant:\nit parses the expression c[i][j] += a[i][k] * b[k][j]; tries to figure out what a, b, and c are and looks up their names in a special hash table with type information; understands that a is a list, fetches its [] operator, retrieves the pointer for a[i], figures out it\u0026rsquo;s also a list, fetches its [] operator again, gets the pointer for a[i][k], and then the element itself; looks up its type, figures out that it\u0026rsquo;s a float, and fetches the method implementing * operator; does the same things for b and c and finally add-assigns the result to c[i][j]. Granted, the interpreters of widely used languages such as Python are well-optimized, and they can skip through some of these steps on repeated execution of the same code. But still, some quite significant overhead is unavoidable due to the language design. If we get rid of all this type checking and pointer chasing, perhaps we can get cycles per multiplication ratio closer to 1, or whatever the \u0026ldquo;cost\u0026rdquo; of native multiplication is?\n#Managed LanguagesThe same matrix multiplication procedure, but implemented in Java:\nimport java.util.Random; public class Matmul { static int n = 1024; static double[][] a = new double[n][n]; static double[][] b = new double[n][n]; static double[][] c = new double[n][n]; public static void main(String[] args) { Random rand = new Random(); for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { a[i][j] = rand.nextDouble(); b[i][j] = rand.nextDouble(); c[i][j] = 0; } } long start = System.nanoTime(); for (int i = 0; i \u0026lt; n; i++) for (int j = 0; j \u0026lt; n; j++) for (int k = 0; k \u0026lt; n; k++) c[i][j] += a[i][k] * b[k][j]; double diff = (System.nanoTime() - start) * 1e-9; System.out.println(diff); } } It now runs in 10 seconds, which amounts to roughly 13 CPU cycles per multiplication — 63 times faster than Python. Considering that we need to read elements of b non-sequentially from the memory, the running time is roughly what it is supposed to be.\nJava is a compiled, but not native language. The program first compiles to bytecode, which is then interpreted by a virtual machine (JVM). To achieve higher performance, frequently executed parts of the code, such as the innermost for loop, are compiled into the machine code during runtime and then executed with almost no overhead. This technique is called just-in-time compilation.\nJIT compilation is not a feature of the language itself, but of its implementation. There is also a JIT-compiled version of Python called PyPy, which needs about 12 seconds to execute the code above without any changes to it.\n#Compiled LanguagesNow it\u0026rsquo;s turn for C:\n#include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;time.h\u0026gt; #define n 1024 double a[n][n], b[n][n], c[n][n]; int main() { for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { a[i][j] = (double) rand() / RAND_MAX; b[i][j] = (double) rand() / RAND_MAX; } } clock_t start = clock(); for (int i = 0; i \u0026lt; n; i++) for (int j = 0; j \u0026lt; n; j++) for (int k = 0; k \u0026lt; n; k++) c[i][j] += a[i][k] * b[k][j]; float seconds = (float) (clock() - start) / CLOCKS_PER_SEC; printf(\u0026#34;%.4f\\n\u0026#34;, seconds); return 0; } It takes 9 seconds when you compile it with gcc -O3.\nIt doesn\u0026rsquo;t seem like a huge improvement — the 1-3 second advantage over Java and PyPy can be attributed to the additional time of JIT-compilation — but we haven\u0026rsquo;t yet taken advantage of a far better C compiler ecosystem. If we add -march=native and -ffast-math flags, time suddenly goes down to 0.6 seconds!\nWhat happened here is we communicated to the compiler the exact model of the CPU we are running (-march=native) and gave it the freedom to rearrange floating-point computations (-ffast-math), and so it took advantage of it and used vectorization to achieve this speedup.\nIt\u0026rsquo;s not like it is impossible to tune the JIT-compilers of PyPy and Java to achieve the same performance without significant changes to the source code, but it is certainly easier for languages that compile directly to native code.\n#BLASFinally, let\u0026rsquo;s take a look at what an expert-optimized implementation is capable of. We will test a widely-used optimized linear algebra library called OpenBLAS. The easiest way to use it is to go back to Python and just call it from numpy:\nimport time import numpy as np n = 1024 a = np.random.rand(n, n) b = np.random.rand(n, n) start = time.time() c = np.dot(a, b) duration = time.time() - start print(duration) Now it takes ~0.12 seconds: a ~5x speedup over the auto-vectorized C version and ~5250x speedup over our initial Python implementation!\nYou don\u0026rsquo;t typically see such dramatic improvements. For now, we are not ready to tell you exactly how this is achieved. Implementations of dense matrix multiplication in OpenBLAS are typically 5000 lines of handwritten assembly tailored separately for each architecture. In later chapters, we will explain all the relevant techniques one by one, and then return to this example and develop our own BLAS-level implementation using just under 40 lines of C.\n#TakeawayThe key lesson here is that using a native, low-level language doesn\u0026rsquo;t necessarily give you performance; but it does give you control over performance.\nComplementary to the \u0026ldquo;N operations per second\u0026rdquo; simplification, many programmers also have a misconception that using different programming languages has some sort of multiplier on that number. Thinking this way and comparing languages in terms of performance doesn\u0026rsquo;t make much sense: programming languages are fundamentally just tools that take away some control over performance in exchange for convenient abstractions. Regardless of the execution environment, it is still largely a programmer\u0026rsquo;s job to use the opportunities that the hardware provides.\n","id":21,"path":"/hugo-page/hpc/complexity/languages/","title":"Programming Languages"},{"content":"The way rounding works in hardware floats is remarkably simple: it occurs if and only if the result of the operation is not representable exactly, and by default gets rounded to the nearest representable number (in case of a tie preferring the number that ends with a zero).\nConsider the following code snippet:\nfloat x = 0; for (int i = 0; i \u0026lt; (1 \u0026lt;\u0026lt; 25); i++) x++; printf(\u0026#34;%f\\n\u0026#34;, x); Instead of printing $2^{25} = 33554432$ (what the result mathematically should be), it outputs $16777216 = 2^{24}$. Why?\nWhen we repeatedly increment a floating-point number $x$, we eventually hit a point where it becomes so big that $(x + 1)$ gets rounded back to $x$. The first such number is $2^{24}$ (the number of mantissa bits plus one) because\n$$2^{24} + 1 = 2^{24} \\cdot 1.\\underbrace{0\\ldots0}_{\\times 23} 1$$\nhas the exact same distance from $2^{24}$ and $(2^{24} + 1)$ but gets rounded down to $2^{24}$ by the above-stated tie-breaker rule. At the same time, the increment of everything lower than that can be represented exactly, so no rounding happens in the first place.\n#Rounding Errors and Operation OrderThe result of a floating-point computation may depend on the order of operations despite being algebraically correct.\nFor example, while the operations of addition and multiplication are commutative and associative in the pure mathematical sense, their rounding errors are not: when we have three floating-point variables $x$, $y$, and $z$, the result of $(x+y+z)$ depends on the order of summation. The same non-commutativity principle applies to most if not all other floating-point operations.\nCompilers are not allowed to produce non-spec-compliant results, so this annoying nuance disables some potential optimizations that involve rearranging operands in arithmetic. You can disable this strict compliance with the -ffast-math flag in GCC and Clang. If we add it and re-compile the code snippet above, it runs considerably faster and also happens to output the correct result, 33554432 (although you need to be aware that the compiler also could have chosen a less precise computation path).\n#Rounding ModesApart from the default mode (also known as Banker\u0026rsquo;s rounding), you can set other rounding logic with 4 more modes:\nround to nearest, with perfect ties always rounding \u0026ldquo;away\u0026rdquo; from zero; round up (toward $+∞$; negative results thus round toward zero); round down (toward $-∞$; negative results thus round away from zero); round toward zero (a truncation of the binary result). For example, if you call fesetround(FE_UPWARD) before running the loop above, it outputs not $2^{24}$, and not even $2^{25}$, but $67108864 = 2^{26}$. This happens because when we get to $2^{24}$, $(x + 1)$ starts rounding to the next nearest representable number $(x + 2)$, and we reach $2^{25}$ in half the time, and after that, $(x + 1)$ rounds up to $(x+4)$, and we start going four times as fast.\nOne of the uses for the alternative rounding modes is for diagnosing numerical instability. If the results of an algorithm substantially vary when switching between rounding to the positive and negative infinities, it indicates susceptibility to round-off errors.\nThis test is often better than switching all computations to lower precision and checking whether the result changed by too much because the default round-to-nearest policy converges to the correct “expected” value given enough averaging: half of the time the errors are rounding up, and the other they are rounding down — so, statistically, they cancel each other.\n#Measuring ErrorsIt seems surprising to expect this guarantee from hardware that performs complex calculations such as natural logarithms and square roots, but this is it: you are guaranteed to get the highest precision possible from all operations. This makes it remarkably easy to analyze round-off errors, as we will see in a bit.\nThere are two natural ways to measure computational errors:\nThe engineers who create hardware or spec-compliant exact software are concerned with units in the last place (ulps), which is the distance between two numbers in terms of how many representable numbers can fit between the precise real value and the actual result of the computation. People that are working on numerical algorithms care about relative precision, which is the absolute value of the approximation error divided by the real answer: $|\\frac{v-v\u0026rsquo;}{v}|$. In either case, the usual tactic to analyze errors is to assume the worst case and simply bound them.\nIf you perform a single basic arithmetic operation, then the worst thing that can happen is the result rounding to the nearest representable number, meaning that the error does not exceed 0.5 ulps. To reason about relative errors the same way, we can define a number $\\epsilon$ called machine epsilon, equal to the difference between $1$ and the next representable value (which should be equal to 2 to the negative power of however many bits are dedicated to mantissa).\nThis means that if after a single arithmetic operation you get result $x$, then the real value is somewhere in the range\n$$ [x \\cdot (1-\\epsilon),; x \\cdot (1 + \\epsilon)] $$\nThe omnipresence of errors is especially important to remember when making discrete \u0026ldquo;yes or no\u0026rdquo; decisions based on the results of floating-point calculations. For example, here is how you should check for equality:\nconst float eps = std::numeric_limits\u0026lt;float\u0026gt;::epsilon; // ~2^(-23) bool eq(float a, float b) { return abs(a - b) \u0026lt;= eps; } The value of eps should depend on the application: the one above — the machine epsilon for float — is only good for no more than one floating-point operation.\n#Interval ArithmeticAn algorithm is called numerically stable if its error, whatever its cause, does not grow much larger during the calculation. This can only happen if the problem itself is well-conditioned, meaning that the solution changes only by a small amount if the input data are changed by a small amount.\nWhen analyzing numerical algorithms, it is often useful to adopt the same method that is used in experimental physics: instead of working with unknown real values, we will work with the intervals where they may be in.\nFor example, consider a chain of operations where we consecutively multiply a variable by arbitrary real numbers:\nfloat x = 1; for (int i = 0; i \u0026lt; n; i++) x *= a[i]; After the first multiplication, the value of $x$ relative to the value of the real product is bounded by $(1 + \\epsilon)$, and after each additional multiplication, this upper bound is multiplied by another $(1 + \\epsilon)$. By induction, after $n$ multiplications, the computed value is bound by $(1 + \\epsilon)^n = 1 + n \\epsilon + O(\\epsilon^2)$ and a similar lower bound.\nThis implies that the relative error is $O(n \\epsilon)$, which is sort of okay, because usually $n \\ll \\frac{1}{\\epsilon}$.\nFor example of a numerically unstable computation, consider the function\n$$ f(x, y) = x^2 - y^2 $$\nAssuming $x \u0026gt; y$, the maximum value this function can return is roughly\n$$ x^2 \\cdot (1 + \\epsilon) - y^2 \\cdot (1 - \\epsilon) $$\ncorresponding to the absolute error of\n$$ x^2 \\cdot (1 + \\epsilon) - y^2 \\cdot (1 - \\epsilon) - (x^2 - y^2) = (x^2 + y^2) \\cdot \\epsilon $$\nand hence the relative error of\n$$ \\frac{x^2 + y^2}{x^2 - y^2} \\cdot \\epsilon $$\nIf $x$ and $y$ are close in magnitude, the error will be $O(\\epsilon \\cdot |x|)$.\nUnder direct computation, the subtraction \u0026ldquo;magnifies\u0026rdquo; the errors of squaring. But this can be fixed by instead using the following formula:\n$$ f(x, y) = x^2 - y^2 = (x + y) \\cdot (x - y) $$\nIn this one, it is easy to show that the error is bound by $\\epsilon \\cdot |x - y|$. It is also faster because it needs 2 additions and 1 multiplication: one fast addition more and one slow multiplication less compared to the original.\n#Kahan SummationFrom the previous example, we can see that long chains of operations are not a problem, but adding and subtracting numbers of different magnitude is. The general approach to dealing with such problems is to try to keep big numbers with big numbers and small numbers with small numbers.\nConsider the standard summation algorithm:\nfloat s = 0; for (int i = 0; i \u0026lt; n; i++) s += a[i]; Since we are performing summations and not multiplications, its relative error is no longer just bounded by $O(\\epsilon \\cdot n)$, but heavily depends on the input.\nIn the most ridiculous case, if the first value is $2^{24}$ and the other values are equal to $1$, the sum is going to be $2^{24}$ regardless of $n$, which can be verified by executing the following code and observing that it simply prints $16777216 = 2^{24}$ twice:\nconst int n = (1\u0026lt;\u0026lt;24); printf(\u0026#34;%d\\n\u0026#34;, n); float s = n; for (int i = 0; i \u0026lt; n; i++) s += 1.0; printf(\u0026#34;%f\\n\u0026#34;, s); This happens because float has only 23 mantissa bits, and so $2^{24} + 1$ is the first integer number that can\u0026rsquo;t be represented exactly and has to be rounded down, which happens every time we try to add $1$ to $s = 2^{24}$. The error is indeed $O(n \\cdot \\epsilon)$ but in terms of the absolute error, not the relative one: in the example above, it is $2$, and it would go up to infinity if the last number happened to be $-2^{24}$.\nThe obvious solution is to switch to a larger type such as double, but this isn\u0026rsquo;t really a scalable method. An elegant solution is to store the parts that weren\u0026rsquo;t added in a separate variable, which is then added to the next variable:\nfloat s = 0, c = 0; for (int i = 0; i \u0026lt; n; i++) { float y = a[i] - c; // c is zero on the first iteration float t = s + y; // s may be big and y may be small, losing low-order bits of y c = (t - s) - y; // (t - s) cancels high-order part of y s = t; } This trick is known as Kahan summation. Its relative error is bounded by $2 \\epsilon + O(n \\epsilon^2)$: the first term comes from the very last summation, and the second term is due to the fact that we work with less-than-epsilon errors on each step.\nOf course, a more general approach that works not just for array summation would be to switch to a more precise data type, like double, also effectively squaring the machine epsilon. Furthermore, it can (sort of) be scaled by bundling two double variables together: one for storing the value and another for its non-representable errors so that they represent the value $a+b$. This approach is known as double-double arithmetic, and it can be similarly generalized to define quad-double and higher precision arithmetic.\n","id":22,"path":"/hugo-page/hpc/arithmetic/errors/","title":"Rounding Errors"},{"content":"This section is a follow-up to the previous one, where we optimized binary search by the means of removing branching and improving the memory layout. Here, we will also be searching in sorted arrays, but this time we are not limited to fetching and comparing only one element at a time.\nIn this section, we generalize the techniques we developed for binary search to static B-trees and accelerate them further using SIMD instructions. In particular, we develop two new implicit data structures:\nThe first is based on the memory layout of a B-tree, and, depending on the array size, it is up to 8x faster than std::lower_bound while using the same space as the array and only requiring a permutation of its elements. The second is based on the memory layout of a B+ tree, and it is up to 15x faster than std::lower_bound while using just 6-7% more memory — or 6-7% of the memory if we can keep the original sorted array. To distinguish them from B-trees — the structures with pointers, hundreds to thousands of keys per node, and empty spaces in them — we will use the names S-tree and S+ tree respectively to refer to these particular memory layouts1.\nTo the best of my knowledge, this is a significant improvement over the existing approaches. As before, we are using Clang 10 targeting a Zen 2 CPU, but the performance improvements should approximately transfer to most other platforms, including Arm-based chips. Use this single-source benchmark of the final implementation if you want to test it on your machine.\nThis is a long article, and since it also serves as a textbook case study, we will improve the algorithm incrementally for pedagogical goals. If you are already an expert and feel comfortable reading intrinsic-heavy code with little to no context, you can jump straight to the final implementation.\n#B-Tree LayoutB-trees generalize the concept of binary search trees by allowing nodes to have more than two children. Instead of a single key, a node of a B-tree of order $k$ can contain up to $B = (k - 1)$ keys stored in sorted order and up to $k$ pointers to child nodes. Each child $i$ satisfies the property that all keys in its subtree are between keys $(i - 1)$ and $i$ of the parent node (if they exist).\nA B-tree of order 4 The main advantage of this approach is that it reduces the tree height by $\\frac{\\log_2 n}{\\log_k n} = \\frac{\\log k}{\\log 2} = \\log_2 k$ times, while fetching each node still takes roughly the same time — as long it fits into a single memory block.\nB-trees were primarily developed for the purpose of managing on-disk databases, where the latency of randomly fetching a single byte is comparable with the time it takes to read the next 1MB of data sequentially. For our use case, we will be using the block size of $B = 16$ elements — or $64$ bytes, the size of the cache line — which makes the tree height and the total number of cache line fetches per query $\\log_2 17 \\approx 4$ times smaller compared to the binary search.\n#Implicit B-TreeStoring and fetching pointers in a B-tree node wastes precious cache space and decreases performance, but they are essential for changing the tree structure on inserts and deletions. But when there are no updates and the structure of a tree is static, we can get rid of the pointers, which makes the structure implicit.\nOne of the ways to achieve this is by generalizing the Eytzinger numeration to $(B + 1)$-ary trees:\nThe root node is numbered $0$. Node $k$ has $(B + 1)$ child nodes numbered $\\{k \\cdot (B + 1) + i + 1\\}$ for $i \\in [0, B]$. This way, we can only use $O(1)$ additional memory by allocating one large two-dimensional array of keys and relying on index arithmetic to locate children nodes in the tree:\nconst int B = 16; int nblocks = (n + B - 1) / B; int btree[nblocks][B]; int go(int k, int i) { return k * (B + 1) + i + 1; } This numeration automatically makes the B-tree complete or almost complete with the height of $\\Theta(\\log_{B + 1} n)$. If the length of the initial array is not a multiple of $B$, the last block is padded with the largest value of its data type.\n#ConstructionWe can construct the B-tree similar to how we constructed the Eytzinger array — by traversing the search tree:\nvoid build(int k = 0) { static int t = 0; if (k \u0026lt; nblocks) { for (int i = 0; i \u0026lt; B; i++) { build(go(k, i)); btree[k][i] = (t \u0026lt; n ? a[t++] : INT_MAX); } build(go(k, B)); } } It is correct because each value of the initial array will be copied to a unique position in the resulting array, and the tree height is $\\Theta(\\log_{B+1} n)$ because $k$ is multiplied by $(B + 1)$ each time we descend into a child node.\nNote that this numeration causes a slight imbalance: left-er children may have larger subtrees, although this is only true for $O(\\log_{B+1} n)$ parent nodes.\n#SearchesTo find the lower bound, we need to fetch the $B$ keys in a node, find the first key $a_i$ not less than $x$, descend to the $i$-th child — and continue until we reach a leaf node. There is some variability in how to find that first key. For example, we could do a tiny internal binary search that makes $O(\\log B)$ iterations, or maybe just compare each key sequentially in $O(B)$ time until we find the local lower bound, hopefully exiting from the loop a bit early.\nBut we are not going to do that — because we can use SIMD. It doesn\u0026rsquo;t work well with branching, so essentially what we want to do is to compare against all $B$ elements regardless, compute a bitmask out of these comparisons, and then use the ffs instruction to find the bit corresponding to the first non-lesser element:\nint mask = (1 \u0026lt;\u0026lt; B); for (int i = 0; i \u0026lt; B; i++) mask |= (btree[k][i] \u0026gt;= x) \u0026lt;\u0026lt; i; int i = __builtin_ffs(mask) - 1; // now i is the number of the correct child node Unfortunately, the compilers are not smart enough to auto-vectorize this code yet, so we have to optimize it manually. In AVX2, we can load 8 elements, compare them against the search key, producing a vector mask, and then extract the scalar mask from it with movemask. Here is a minimized illustrated example of what we want to do:\ny = 4 17 65 103 x = 42 42 42 42 y ≥ x = 00000000 00000000 11111111 11111111 ├┬┬┬─────┴────────┴────────┘ movemask = 0011 ┌─┘ ffs = 3 Since we are limited to processing 8 elements at a time (half our block / cache line size), we have to split the elements into two groups and then combine the two 8-bit masks. To do this, it will be slightly easier to swap the condition for x \u0026gt; y and compute the inverted mask instead:\ntypedef __m256i reg; int cmp(reg x_vec, int* y_ptr) { reg y_vec = _mm256_load_si256((reg*) y_ptr); // load 8 sorted elements reg mask = _mm256_cmpgt_epi32(x_vec, y_vec); // compare against the key return _mm256_movemask_ps((__m256) mask); // extract the 8-bit mask } Now, to process the entire block, we need to call it twice and combine the masks:\nint mask = ~( cmp(x, \u0026amp;btree[k][0]) + (cmp(x, \u0026amp;btree[k][8]) \u0026lt;\u0026lt; 8) ); To descend down the tree, we use ffs on that mask to get the correct child number and just call the go function we defined earlier:\nint i = __builtin_ffs(mask) - 1; k = go(k, i); To actually return the result in the end, we\u0026rsquo;d want to just fetch btree[k][i] in the last node we visited, but the problem is that sometimes the local lower bound doesn\u0026rsquo;t exist ($i \\ge B$) because $x$ happens to be greater than all the keys in the node. We could, in theory, do the same thing we did for the Eytzinger binary search and restore the correct element after we calculate the last index, but we don\u0026rsquo;t have a nice bit trick this time and have to do a lot of divisions by 17 to compute it, which will be slow and almost certainly not worth it.\nInstead, we can just remember and return the last local lower bound we encountered when we descended the tree:\nint lower_bound(int _x) { int k = 0, res = INT_MAX; reg x = _mm256_set1_epi32(_x); while (k \u0026lt; nblocks) { int mask = ~( cmp(x, \u0026amp;btree[k][0]) + (cmp(x, \u0026amp;btree[k][8]) \u0026lt;\u0026lt; 8) ); int i = __builtin_ffs(mask) - 1; if (i \u0026lt; B) res = btree[k][i]; k = go(k, i); } return res; } This implementation outperforms all previous binary search implementations, and by a huge margin:\nThis is very good — but we can optimize it even further.\n#OptimizationBefore everything else, let\u0026rsquo;s allocate the memory for the array on a hugepage:\nconst int P = 1 \u0026lt;\u0026lt; 21; // page size in bytes (2MB) const int T = (64 * nblocks + P - 1) / P * P; // can only allocate whole number of pages btree = (int(*)[16]) std::aligned_alloc(P, T); madvise(btree, T, MADV_HUGEPAGE); This slightly improves the performance on larger array sizes:\nIdeally, we\u0026rsquo;d also need to enable hugepages for all previous implementations to make the comparison fair, but it doesn\u0026rsquo;t matter that much because they all have some form of prefetching that alleviates this problem.\nWith that settled, let\u0026rsquo;s begin real optimization. First of all, we\u0026rsquo;d want to use compile-time constants instead of variables as much as possible because it lets the compiler embed them in the machine code, unroll loops, optimize arithmetic, and do all sorts of other nice stuff for us for free. Specifically, we want to know the tree height in advance:\nconstexpr int height(int n) { // grow the tree until its size exceeds n elements int s = 0, // total size so far l = B, // size of the next layer h = 0; // height so far while (s + l - B \u0026lt; n) { s += l; l *= (B + 1); h++; } return h; } const int H = height(N); Next, we can find the local lower bound in nodes faster. Instead of calculating it separately for two 8-element blocks and merging two 8-bit masks, we combine the vector masks using the packs instruction and readily extract it using movemask just once:\nunsigned rank(reg x, int* y) { reg a = _mm256_load_si256((reg*) y); reg b = _mm256_load_si256((reg*) (y + 8)); reg ca = _mm256_cmpgt_epi32(a, x); reg cb = _mm256_cmpgt_epi32(b, x); reg c = _mm256_packs_epi32(ca, cb); int mask = _mm256_movemask_epi8(c); // we need to divide the result by two because we call movemask_epi8 on 16-bit masks: return __tzcnt_u32(mask) \u0026gt;\u0026gt; 1; } This instruction converts 32-bit integers stored in two registers to 16-bit integers stored in one register — in our case, effectively joining the vector masks into one. Note that we\u0026rsquo;ve swapped the order of comparison — this lets us not invert the mask in the end, but we have to subtract2 one from the search key once in the beginning to make it correct (otherwise, it works as upper_bound).\nThe problem is, it does this weird interleaving where the result is written in the a1 b1 a2 b2 order instead of a1 a2 b1 b2 that we want — many AVX2 instructions tend to do that. To correct this, we need to permute the resulting vector, but instead of doing it during the query time, we can just permute every node during preprocessing:\nvoid permute(int *node) { const reg perm = _mm256_setr_epi32(4, 5, 6, 7, 0, 1, 2, 3); reg* middle = (reg*) (node + 4); reg x = _mm256_loadu_si256(middle); x = _mm256_permutevar8x32_epi32(x, perm); _mm256_storeu_si256(middle, x); } Now we just call permute(\u0026amp;btree[k]) right after we are done building the node. There are probably faster ways to swap the middle elements, but we will leave it here as the preprocessing time is not that important for now.\nThis new SIMD routine is significantly faster because the extra movemask is slow, and also blending the two masks takes quite a few instructions. Unfortunately, we now can\u0026rsquo;t just do the res = btree[k][i] update anymore because the elements are permuted. We can solve this problem with some bit-level trickery in terms of i, but indexing a small lookup table turns out to be faster and also doesn\u0026rsquo;t require a new branch:\nconst int translate[17] = { 0, 1, 2, 3, 8, 9, 10, 11, 4, 5, 6, 7, 12, 13, 14, 15, 0 }; void update(int \u0026amp;res, int* node, unsigned i) { int val = node[translate[i]]; res = (i \u0026lt; B ? val : res); } This update procedure takes some time, but it\u0026rsquo;s not on the critical path between the iterations, so it doesn\u0026rsquo;t affect the actual performance that much.\nStitching it all together (and leaving out some other minor optimizations):\nint lower_bound(int _x) { int k = 0, res = INT_MAX; reg x = _mm256_set1_epi32(_x - 1); for (int h = 0; h \u0026lt; H - 1; h++) { unsigned i = rank(x, \u0026amp;btree[k]); update(res, \u0026amp;btree[k], i); k = go(k, i); } // the last branch: if (k \u0026lt; nblocks) { unsigned i = rank(x, btree[k]); update(res, \u0026amp;btree[k], i); } return res; } All this work saved us 15-20% or so:\nIt doesn\u0026rsquo;t feel very satisfying so far, but we will reuse these optimization ideas later.\nThere are two main problems with the current implementation:\nThe update procedure is quite costly, especially considering that it is very likely going to be useless: 16 out of 17 times, we can just fetch the result from the last block. We do a non-constant number of iterations, causing branch prediction problems similar to how it did for the Eytzinger binary search; you can also see it on the graph this time, but the latency bumps have a period of $2^4$. To address these problems, we need to change the layout a little bit.\n#B+ Tree LayoutMost of the time, when people talk about B-trees, they really mean B+ trees, which is a modification that distinguishes between the two types of nodes:\nInternal nodes store up to $B$ keys and $(B + 1)$ pointers to child nodes. The key number $i$ is always equal to the smallest key in the subtree of the $(i + 1)$-th child node. Data nodes or leaves store up to $B$ keys, the pointer to the next leaf node, and, optionally, an associated value for each key — if the structure is used as a key-value map. The advantages of this approach include faster search time (as the internal nodes only store keys) and the ability to quickly iterate over a range of entries (by following next leaf node pointers), but this comes at the cost of some memory overhead: we have to store copies of keys in the internal nodes.\nA B\u0026#43; tree of order 4 Back to our use case, this layout can help us solve our two problems:\nEither the last node we descend into has the local lower bound, or it is the first key of the next leaf node, so we don\u0026rsquo;t need to call update on each iteration. The depth of all leaves is constant because B+ trees grow at the root and not at the leaves, which removes the need for branching. The disadvantage is that this layout is not succinct: we need some additional memory to store the internal nodes — about $\\frac{1}{16}$-th of the original array size, to be exact — but the performance improvement will be more than worth it.\n#Implicit B+ TreeTo be more explicit with pointer arithmetic, we will store the entire tree in a single one-dimensional array. To minimize index computations during run time, we will store each layer sequentially in this array and use compile time computed offsets to address them: the keys of the node number k on layer h start with btree[offset(h) + k * B], and its i-th child will at btree[offset(h - 1) + (k * (B + 1) + i) * B].\nTo implement all that, we need slightly more constexpr functions:\n// number of B-element blocks in a layer with n keys constexpr int blocks(int n) { return (n + B - 1) / B; } // number of keys on the layer previous to one with n keys constexpr int prev_keys(int n) { return (blocks(n) + B) / (B + 1) * B; } // height of a balanced n-key B+ tree constexpr int height(int n) { return (n \u0026lt;= B ? 1 : height(prev_keys(n)) + 1); } // where the layer h starts (layer 0 is the largest) constexpr int offset(int h) { int k = 0, n = N; while (h--) { k += blocks(n) * B; n = prev_keys(n); } return k; } const int H = height(N); const int S = offset(H); // the tree size is the offset of the (non-existent) layer H int *btree; // the tree itself is stored in a single hugepage-aligned array of size S Note that we store the layers in reverse order, but the nodes within a layer and data in them are still left-to-right, and also the layers are numbered bottom-up: the leaves form the zeroth layer, and the root is the layer H - 1. These are just arbitrary decisions — it is just slightly easier to implement in code.\n#ConstructionTo construct the tree from a sorted array a, we first need to copy it into the zeroth layer and pad it with infinities:\nmemcpy(btree, a, 4 * N); for (int i = N; i \u0026lt; S; i++) btree[i] = INT_MAX; Now we build the internal nodes, layer by layer. For each key, we need to descend to the right of it in, always go left until we reach a leaf node, and then take its first key — it will be the smallest on the subtree:\nfor (int h = 1; h \u0026lt; H; h++) { for (int i = 0; i \u0026lt; offset(h + 1) - offset(h); i++) { // i = k * B + j int k = i / B, j = i - k * B; k = k * (B + 1) + j + 1; // compare to the right of the key // and then always to the left for (int l = 0; l \u0026lt; h - 1; l++) k *= (B + 1); // pad the rest with infinities if the key doesn\u0026#39;t exist btree[offset(h) + i] = (k * B \u0026lt; N ? btree[k * B] : INT_MAX); } } And just the finishing touch — we need to permute keys in internal nodes to search them faster:\nfor (int i = offset(1); i \u0026lt; S; i += B) permute(btree + i); We start from offset(1), and we specifically don\u0026rsquo;t permute leaf nodes and leave the array in the original sorted order. The motivation is that we\u0026rsquo;d need to do this complex index translation we do in update if the keys were permuted, and it is on the critical path when this is the last operation. So, just for this layer, we switch to the original mask-blending local lower bound procedure.\n#SearchingThe search procedure becomes simpler than for the B-tree layout: we don\u0026rsquo;t need to do update and only execute a fixed number of iterations — although the last one with some special treatment:\nint lower_bound(int _x) { unsigned k = 0; // we assume k already multiplied by B to optimize pointer arithmetic reg x = _mm256_set1_epi32(_x - 1); for (int h = H - 1; h \u0026gt; 0; h--) { unsigned i = permuted_rank(x, btree + offset(h) + k); k = k * (B + 1) + i * B; } unsigned i = direct_rank(x, btree + k); return btree[k + i]; } Switching to the B+ layout more than paid off: the S+ tree is 1.5-3x faster compared to the optimized S-tree:\nThe spikes at the high end of the graph are caused by the L1 TLB not being large enough: it has 64 entries, so it can handle at most 64 × 2 = 128MB of data, which is exactly what is required for storing 2^25 integers. The S+ tree hits this limit slightly sooner because of the ~7% memory overhead.\n#Comparison with std::lower_boundWe\u0026rsquo;ve come a long way from binary search:\nOn these scales, it makes more sense to look at the relative speedup:\nThe cliffs at the beginning of the graph are because the running time of std::lower_bound grows smoothly with the array size, while for an S+ tree, it is locally flat and increases in discrete steps when a new layer needs to be added.\nOne important asterisk we haven\u0026rsquo;t discussed is that what we are measuring is not real latency, but the reciprocal throughput — the total time it takes to execute a lot of queries divided by the number of queries:\nclock_t start = clock(); for (int i = 0; i \u0026lt; m; i++) checksum ^= lower_bound(q[i]); float seconds = float(clock() - start) / CLOCKS_PER_SEC; printf(\u0026#34;%.2f ns per query\\n\u0026#34;, 1e9 * seconds / m); To measure actual latency, we need to introduce a dependency between the loop iterations so that the next query can\u0026rsquo;t start before the previous one finishes:\nint last = 0; for (int i = 0; i \u0026lt; m; i++) { last = lower_bound(q[i] ^ last); checksum ^= last; } In terms of real latency, the speedup is not that impressive:\nA lot of the performance boost of the S+ tree comes from removing branching and minimizing memory requests, which allows overlapping the execution of more adjacent queries — apparently, around three on average.\nAlthough nobody except maybe the HFT people cares about real latency, and everybody actually measures throughput even when using the word \u0026ldquo;latency,\u0026rdquo; this nuance is still something to take into account when predicting the possible speedup in user applications.\n#Modifications and Further Optimizations To minimize the number of memory accesses during a query, we can increase the block size. To find the local lower bound in a 32-element node (spanning two cache lines and four AVX2 registers), we can use a similar trick that uses two packs_epi32 and one packs_epi16 to combine masks.\nWe can also try to use the cache more efficiently by controlling where each tree layer is stored in the cache hierarchy. We can do that by prefetching nodes to a specific level and using non-temporal reads during queries.\nI implemented two versions of these optimizations: the one with a block size of 32 and the one where the last read is non-temporal. They don\u0026rsquo;t improve the throughput:\n…but they do make the latency lower:\nIdeas that I have not yet managed to implement but consider highly perspective are:\nMake the block size non-uniform. The motivation is that the slowdown from having one 32-element layer is less than from having two separate layers. Also, the root is often not full, so perhaps sometimes it should have only 8 keys or even just one key. Picking the optimal layer configuration for a given array size should remove the spikes from the relative speedup graph and make it look more like its upper envelope.\nI know how to do it with code generation, but I went for a generic solution and tried to implement it with the facilities of modern C++, but the compiler can\u0026rsquo;t produce optimal code this way.\nGroup nodes with one or two generations of its descendants (~300 nodes / ~5k keys) so that they are close in memory — in the spirit of what FAST calls hierarchical blocking. This reduces the severity of TLB misses and also may improve the latency as the memory controller may choose to keep the RAM row buffer open, anticipating local reads.\nOptionally use prefetching on some specific layers. Aside from to the $\\frac{1}{17}$-th chance of it fetching the node we need, the hardware prefetcher may also get some of its neighbors for us if the data bus is not busy. It also has the same TLB and row buffer effects as with blocking.\nOther possible minor optimizations include:\nPermuting the nodes of the last layer as well — if we only need the index and not the value. Reversing the order in which the layers are stored to left-to-right so that the first few layers are on the same page. Rewriting the whole thing in assembly, as the compiler seems to struggle with pointer arithmetic. Using blending instead of packs: you can odd-even shuffle node keys ([1 3 5 7] [2 4 6 8]), compare against the search key, and then blend the low 16 bits of the first register mask with the high 16 bits of the second. Blending is slightly faster on many architectures, and it may also help to alternate between packing and blending as they use different subsets of ports. (Thanks to Const-me from HackerNews for suggesting it.) Using popcount instead of tzcnt: the index i is equal to the number of keys less than x, so we can compare x against all keys, combine the vector mask any way we want, call maskmov, and then calculate the number of set bits with popcnt. This removes the need to store the keys in any particular order, which lets us skip the permutation step and also use this procedure on the last layer as well. Defining the key $i$ as the maximum key in the subtree of child $i$ instead of the minimum key in the subtree of child $(i + 1)$. The correctness doesn\u0026rsquo;t change, but this guarantees that the result will be stored in the last node we access (and not in the first element of the next neighbor node), which lets us fetch slightly fewer cache lines. Note that the current implementation is specific to AVX2 and may require some non-trivial changes to adapt to other platforms. It would be interesting to port it for Intel CPUs with AVX-512 and Arm CPUs with 128-bit NEON, which may require some trickery to work.\nWith these optimizations implemented, I wouldn\u0026rsquo;t be surprised to see another 10-30% improvement and over 10x speedup over std::lower_bound on large arrays for some platforms.\n#As a Dynamic TreeThe comparison is even more favorable against std::set and other pointer-based trees. In our benchmark, we add the same elements (without measuring the time it takes to add them) and use the same lower bound queries, and the S+ tree is up to 30x faster:\nThis suggests that we can probably use this approach to also improve on dynamic search trees by a large margin.\nTo validate this hypothesis, I added an array of 17 indices for each node that point to where their children should be and used this array to descend the tree instead of the usual implicit numbering. This array is separate from the tree, not aligned, and isn\u0026rsquo;t even on a hugepage — the only optimization we do is prefetch the first and the last pointer of a node.\nI also added B-tree from Abseil to the comparison, which is the only widely-used B-tree implementation I know of. It performs just slightly better than std::lower_bound, while the S+ tree with pointers is ~15x faster for large arrays:\nOf course, this comparison is not fair, as implementing a dynamic search tree is a more high-dimensional problem.\nWe\u0026rsquo;d also need to implement the update operation, which will not be that efficient, and for which we\u0026rsquo;d need to sacrifice the fanout factor. But it still seems possible to implement a 10-20x faster std::set and a 3-5x faster absl::btree_set, depending on how you define \u0026ldquo;faster\u0026rdquo; — and this is one of the things we\u0026rsquo;ll attempt to do next.\n#AcknowledgementsThis StackOverflow answer by Cory Nelson is where I took the permuted 16-element search trick from.\nSimilar to B-trees, \u0026ldquo;the more you think about what the S in S-trees means, the better you understand S-trees.\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIf you need to work with floating-point keys, consider whether upper_bound will suffice — because if you need lower_bound specifically, then subtracting one or the machine epsilon from the search key doesn\u0026rsquo;t work: you need to get the previous representable number instead. Aside from some corner cases, this essentially means reinterpreting its bits as an integer, subtracting one, and reinterpreting it back as a float (which magically works because of how IEEE-754 floating-point numbers are stored in memory).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":23,"path":"/hugo-page/hpc/data-structures/s-tree/","title":"Static B-Trees"},{"content":"Instrumentation is a rather tedious way of doing profiling, especially if you are interested in multiple small sections of the program. And even if it can be partially automated by the tooling, it still won\u0026rsquo;t help you gather some fine-grained statistics because of its inherent overhead.\nAnother, less invasive approach to profiling is to interrupt the execution of a program at random intervals and look where the instruction pointer is. The number of times the pointer stopped in each function\u0026rsquo;s block would be roughly proportional to the total time spent executing these functions. You can also get some other useful information this way, like finding out which functions are called by which functions by inspecting the call stack.\nThis could, in principle, be done by just running a program with gdb and ctrl+c\u0026lsquo;ing it at random intervals but modern CPUs and operating systems provide special utilities for this type of profiling.\n#Hardware EventsHardware performance counters are special registers built into microprocessors that can store the counts of certain hardware-related activities. They are cheap to add on a microchip, as they are basically just binary counters with an activation wire connected to them.\nEach performance counter is connected to a large subset of circuitry and can be configured to be incremented on a particular hardware event, such as a branch mispredict or a cache miss. You can reset a counter at the start of a program, run it, and output its stored value at the end, and it will be equal to the exact number of times a certain event has been triggered throughout the execution.\nYou can also keep track of multiple events by multiplexing between them, that is, stopping the program in even intervals and reconfiguring the counters. The result in this case will not be exact, but a statistical approximation. One nuance here is that its accuracy can’t be improved by simply increasing the sampling frequency because it would affect the performance too much and thus skew the distribution, so to collect multiple statistics, you would need to run the program for longer periods of time.\nOverall, event-driven statistical profiling is usually the most effective and easy way to diagnose performance issues.\n#Profiling with perfPerformance analysis tools that rely on the event sampling techniques described above are called statistical profilers. There are many of them, but the one we will mainly use in this book is perf, which is a statistical profiler shipped with the Linux kernel. On non-Linux systems, you can use VTune from Intel, which provides roughly the same functionality for our purposes. It is available for free, although it is proprietary, and you need to refresh your community license every 90 days, while perf is free as in freedom.\nPerf is a command-line application that generates reports based on the live execution of programs. It does not need the source and can profile a very wide range of applications, even those that involve multiple processes and interaction with the operating system.\nFor explanation purposes, I have written a small program that creates an array of a million random integers, sorts it, and then does a million binary searches on it:\nvoid setup() { for (int i = 0; i \u0026lt; n; i++) a[i] = rand(); std::sort(a, a + n); } int query() { int checksum = 0; for (int i = 0; i \u0026lt; n; i++) { int idx = std::lower_bound(a, a + n, rand()) - a; checksum += idx; } return checksum; } After compiling it (g++ -O3 -march=native example.cc -o run), we can run it with perf stat ./run, which outputs the counts of basic performance events during its execution:\nPerformance counter stats for \u0026#39;./run\u0026#39;: 646.07 msec task-clock:u # 0.997 CPUs utilized 0 context-switches:u # 0.000 K/sec 0 cpu-migrations:u # 0.000 K/sec 1,096 page-faults:u # 0.002 M/sec 852,125,255 cycles:u # 1.319 GHz (83.35%) 28,475,954 stalled-cycles-frontend:u # 3.34% frontend cycles idle (83.30%) 10,460,937 stalled-cycles-backend:u # 1.23% backend cycles idle (83.28%) 479,175,388 instructions:u # 0.56 insn per cycle # 0.06 stalled cycles per insn (83.28%) 122,705,572 branches:u # 189.925 M/sec (83.32%) 19,229,451 branch-misses:u # 15.67% of all branches (83.47%) 0.647801770 seconds time elapsed 0.647278000 seconds user 0.000000000 seconds sys You can see that the execution took 0.53 seconds or 852M cycles at an effective 1.32 GHz clock rate, over which 479M instructions were executed. There were also 122.7M branches, and 15.7% of them were mispredicted.\nYou can get a list of all supported events with perf list, and then specify a list of specific events you want with the -e option. For example, for diagnosing binary search, we mostly care about cache misses:\n\u0026gt; perf stat -e cache-references,cache-misses ./run 91,002,054 cache-references:u 44,991,746 cache-misses:u # 49.440 % of all cache refs By itself, perf stat simply sets up performance counters for the whole program. It can tell you the total number of branch mispredictions, but it won\u0026rsquo;t tell you where they are happening, let alone why they are happening.\nTo try the stop-the-world approach we discussed previously, we need to use perf record \u0026lt;cmd\u0026gt;, which records profiling data and dumps it as a perf.data file, and then call perf report to inspect it. I highly advise you to go and try it yourselves because the last command is interactive and colorful, but for those that can\u0026rsquo;t do it right now, I\u0026rsquo;ll try to describe it the best I can.\nWhen you call perf report, it first displays a top-like interactive report that tells you which functions are taking how much time:\nOverhead Command Shared Object Symbol 63.08% run run [.] query 24.98% run run [.] std::__introsort_loop\u0026lt;...\u0026gt; 5.02% run libc-2.33.so [.] __random 3.43% run run [.] setup 1.95% run libc-2.33.so [.] __random_r 0.80% run libc-2.33.so [.] rand Note that, for each function, just its overhead is listed and not the total running time (e.g., setup includes std::__introsort_loop but only its own overhead is accounted as 3.43%). There are tools for constructing flame graphs out of perf reports to make them more clear. You also need to account for possible inlining, which is apparently what happened with std::lower_bound here. Perf also tracks shared libraries (like libc) and, in general, any other spawned processes: if you want, you can launch a web browser with perf and see what\u0026rsquo;s happening inside.\nNext, you can \u0026ldquo;zoom in\u0026rdquo; on any of these functions, and, among others things, it will offer to show you its disassembly with an associated heatmap. For example, here is the assembly for query:\n│20: → call rand@plt │ mov %r12,%rsi │ mov %eax,%edi │ mov $0xf4240,%eax │ nop │30: test %rax,%rax 4.57 │ ↓ jle 52 │35: mov %rax,%rdx 0.52 │ sar %rdx 0.33 │ lea (%rsi,%rdx,4),%rcx 4.30 │ cmp (%rcx),%edi 65.39 │ ↓ jle b0 0.07 │ sub %rdx,%rax 9.32 │ lea 0x4(%rcx),%rsi 0.06 │ dec %rax 1.37 │ test %rax,%rax 1.11 │ ↑ jg 35 │52: sub %r12,%rsi 2.22 │ sar $0x2,%rsi 0.33 │ add %esi,%ebp 0.20 │ dec %ebx │ ↑ jne 20 On the left column is the fraction of times that the instruction pointer stopped on a specific line. You can see that we spend ~65% of the time on the jump instruction because it has a comparison operator before it, indicating that the control flow waits there for this comparison to be decided.\nBecause of intricacies such as pipelining and out-of-order execution, \u0026ldquo;now\u0026rdquo; is not a well-defined concept in modern CPUs, so the data is slightly inaccurate as the instruction pointer drifts a little bit forward. The instruction-level data is still useful, but at the individual cycle level, we need to switch to something more precise.\n","id":24,"path":"/hugo-page/hpc/profiling/events/","title":"Statistical Profiling"},{"content":"When a CPU encounters a conditional jump or any other type of branching, it doesn\u0026rsquo;t just sit idle until its condition is computed — instead, it starts speculatively executing the branch that seems more likely to be taken immediately. During execution, the CPU computes statistics about branches taken on each instruction, and after some time, they start to predict them by recognizing common patterns.\nFor this reason, the true \u0026ldquo;cost\u0026rdquo; of a branch largely depends on how well it can be predicted by the CPU. If it is a pure 50/50 coin toss, you have to suffer a control hazard and discard the entire pipeline, taking another 15-20 cycles to build up again. And if the branch is always or never taken, you pay almost nothing except checking the condition.\n#An ExperimentAs a case study, we are going to create an array of random integers between 0 and 99 inclusive:\nfor (int i = 0; i \u0026lt; N; i++) a[i] = rand() % 100; Then we create a loop where we sum up all its elements under 50:\nvolatile int s; for (int i = 0; i \u0026lt; N; i++) if (a[i] \u0026lt; 50) s += a[i]; We set $N = 10^6$ and run this loop many times over so that the cold cache effect doesn\u0026rsquo;t mess up our results. We mark our accumulator variable as volatile so that the compiler doesn\u0026rsquo;t vectorize the loop, interleave its iterations, or \u0026ldquo;cheat\u0026rdquo; in any other way.\nOn Clang, this produces assembly that looks like this:\nmov rcx, -4000000 jmp body counter: add rcx, 4 jz finished ; \u0026#34;jump if rcx became zero\u0026#34; body: mov edx, dword ptr [rcx + a + 4000000] cmp edx, 49 jg counter add dword ptr [rsp + 12], edx jmp counter Our goal is to simulate a completely unpredictable branch, and we successfully achieve it: the code takes ~14 CPU cycles per element. For a very rough estimate of what it is supposed to be, we can assume that the branches alternate between \u0026lt; and \u0026gt;=, and the pipeline is mispredicted every other iteration. Then, every two iterations:\nWe discard the pipeline, which is 19 cycles deep on Zen 2 (i.e., it has 19 stages, each taking one cycle). We need a memory fetch and a comparison, which costs ~5 cycles. We can check the conditions of even and odd iterations concurrently, so let\u0026rsquo;s assume we only pay it once per 2 iterations. In the case of the \u0026lt; branch, we need another ~4 cycles to add a[i] to a volatile (memory-stored) variable s. Therefore, on average, we need to spend $(4 + 5 + 19) / 2 = 14$ cycles per element, matching what we measured.\n#Branch PredictionWe can replace the hardcoded 50 with a tweakable parameter P that effectively sets the probability of the \u0026lt; branch:\nfor (int i = 0; i \u0026lt; N; i++) if (a[i] \u0026lt; P) s += a[i]; Now, if we benchmark it for different values of P, we get an interesting-looking graph:\nIts peak is at 50-55%, as expected: branch misprediction is the most expensive thing here. This graph is asymmetrical: it takes just ~1 cycle to only check conditions that are never satisfied (P = 0), and ~7 cycles for the sum if the branch is always taken (P = 100).\nThis graph is not unimodal: there is another local minimum at around 85-90%. We spend ~6.15 cycles per element there or about 10-15% faster than when we always take the branch, accounting for the fact that we need to perform fewer additions. Branch misprediction stops affecting the performance at this point because when it happens, not the whole instruction buffer is discarded, but only the operations that were speculatively scheduled. Essentially, that 10-15% mispredict rate is the equilibrium point where we can see far enough in the pipeline not to stall but still save 10-15% on taking the cheaper \u0026gt;= branch.\nNote that it costs almost nothing to check for a condition that never or almost never occurs. This is why programmers use runtime exceptions and base case checks so profusely: if they are indeed rare, they don\u0026rsquo;t really cost anything.\n#Pattern DetectionIn our example, everything that was needed for efficient branch prediction is a hardware statistics counter. If we historically took branch A more often than branch B, then it makes sense to speculatively execute branch A. But branch predictors on modern CPUs are considerably more advanced than that and can detect much more complicated patterns.\nLet\u0026rsquo;s fix P back at 50, and then sort the array first before the main summation loop:\nfor (int i = 0; i \u0026lt; N; i++) a[i] = rand() % 100; std::sort(a, a + n); We are still processing the same elements, but in a different order, and instead of 14 cycles, it now runs in a little bit more than 4, which is exactly the average of the cost of the pure \u0026lt; and \u0026gt;= branches.\nThe branch predictor can pick up on much more complicated patterns than just \u0026ldquo;always left, then always right\u0026rdquo; or \u0026ldquo;left-right-left-right.\u0026rdquo; If we just decrease the size of the array $N$ to 1000 (without sorting it), then the branch predictor memorizes the entire sequence of comparisons, and the benchmark again measures at around 4 cycles — in fact, even slightly fewer than in the sorted array case, because in the former case branch predictor needs to spend some time flicking between the \u0026ldquo;always yes\u0026rdquo; and \u0026ldquo;always no\u0026rdquo; states.\n#Hinting Likeliness of BranchesIf you know beforehand which branch is more likely, it may be beneficial to pass that information to the compiler:\nfor (int i = 0; i \u0026lt; N; i++) if (a[i] \u0026lt; P) [[likely]] s += a[i]; When P = 75, it measures around ~7.3 cycles per element, while the original version without the hint needs ~8.3.\nThis hint does not eliminate the branch or communicate anything to the branch predictor, but it changes the machine code layout in a way that lets the CPU front-end process the more likely branch slightly faster (although usually by no more than one cycle).\nThis optimization is only beneficial when you know which branch is more likely to be taken before the compilation stage. When the branch is fundamentally unpredictable, we can try to remove it completely using predication — a profoundly important technique that we are going to explore in the next section.\n#AcknowledgementsThis case study is inspired by the most upvoted Stack Overflow question ever.\n","id":25,"path":"/hugo-page/hpc/pipelining/branching/","title":"The Cost of Branching"},{"content":"Early operating systems gave every process the freedom of reading and modifying any memory region they want, including those allocated for other processes. While this keeps things simple, it also poses some problems:\nWhat if one of the processes is buggy or outright malicious? How do we prevent it from modifying the memory allocated for other processes while still keeping inter-process communication through memory possible? How do we deal with memory fragmentation? Say, we have 4MB of memory, process A allocates the first 1MB for itself, then process B claims the next 2MB, then A terminates and releases its memory, and then process C comes and asks for a contiguous 2MB region — and can\u0026rsquo;t get it because we only have two separate 1MB slices. Restarting process B or somehow stopping it and shifting all its data and pointers by one megabyte doesn\u0026rsquo;t seem like a good solution. How do we access non-RAM memory types? How do we plug a flash drive and read a specific file from it? These problems are not that critical for some specialized computer systems such as GPUs, where you typically solve just one task at a time and have full control over the computation, but they are absolutely essential for modern multitasking operating systems — and they solve all these problems with a technique called virtual memory.\n#Memory PagingVirtual memory gives each process the impression that it fully controls a contiguous region of memory, which in reality may be mapped to multiple smaller blocks of the physical memory — which includes both the main memory (RAM) and external memory (HDD, SSD).\nTo achieve this, the memory address space is divided into pages (typically 4KB in size), which are the base units of memory that the programs can request from the operating system. The memory system maintains a special hardware data structure called the page table, which contains the mappings of virtual page addresses to the physical ones. When a process accesses data using its virtual memory address, the memory system calculates its page number (by right-shifting it by $12$ if $4096=2^{12}$ is the page size), looks up in the page table that its physical address is, and forwards the read or write request to where that data is actually stored.\nSince the address translation needs to be done for each memory request, and the number of memory pages itself may be large (e.g., 16G RAM / 4K page size = 4M pages), address translation poses a difficult problem in itself. One way to speed it up is to use a special cache for the page table itself called translation lookaside buffer (TLB), and the other is to increase the page size so that the total number of memory pages is made smaller at the cost of reduced granularity.\n#Mapping External MemoryThe mechanism of virtual memory also allows using external memory types quite transparently. Modern operating systems support memory mapping, which lets you open a file and use its contents as if they were in the main memory:\n// open a file containing 1024 random integers for reading and writing int fd = open(\u0026#34;input.bin\u0026#34;, O_RDWR); // map it into memory size allow reads and writes write changes back to the file int* data = (int*) mmap(0, 4096, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0); // sort it like if it was a normal integer array std::sort(data, data + 1024); // changes are eventually propagated to the file Here we map a 4K file, which can fit entirely on just a single memory page, but when we open larger files, its reads will be done lazily when we request a certain page, and its writes will be buffered and committed to the file system when the operating decides to (usually on the program termination or when the system runs out of RAM).\nA technique that has the same operating principle, but the reverse intention is the swap file, which lets the operating system automatically use parts of an SSD or an HDD as an extension of the main memory when there is not enough real RAM. This lets the systems that run out of memory just experience a terrible slowdown instead of crashing.\nThis seamless integration of the main and external memory essentially turns RAM into an \u0026ldquo;L4 cache\u0026rdquo; for the external memory, which is a convenient way to think about it from the algorithm design perspective.\n","id":26,"path":"/hugo-page/hpc/external-memory/virtual/","title":"Virtual Memory"},{"content":"As we established in the previous section, branches that can\u0026rsquo;t be effectively predicted by the CPU are expensive as they may cause a long pipeline stall to fetch new instructions after a branch mispredict. In this section, we discuss the means of removing branches in the first place.\n#PredicationWe are going to continue the same case study we\u0026rsquo;ve started before — we create an array of random numbers and sum up all its elements below 50:\nfor (int i = 0; i \u0026lt; N; i++) a[i] = rand() % 100; volatile int s; for (int i = 0; i \u0026lt; N; i++) if (a[i] \u0026lt; 50) s += a[i]; Our goal is to eliminate the branch caused by the if statement. We can try to get rid of it like this:\nfor (int i = 0; i \u0026lt; N; i++) s += (a[i] \u0026lt; 50) * a[i]; The loop now takes ~7 cycles per element instead of the original ~14. Also, the performance remains constant if we change 50 to some other threshold, so it doesn\u0026rsquo;t depend on the branch probability.\nBut wait… shouldn\u0026rsquo;t there still be a branch? How does (a[i] \u0026lt; 50) map to assembly?\nThere are no Boolean types in assembly, nor any instructions that yield either one or zero based on the result of the comparison, but we can compute it indirectly like this: (a[i] - 50) \u0026gt;\u0026gt; 31. This trick relies on the binary representation of integers, specifically on the fact that if the expression a[i] - 50 is negative (implying a[i] \u0026lt; 50), then the highest bit of the result will be set to one, which we can then extract using a right-shift.\nmov ebx, eax ; t = x sub ebx, 50 ; t -= 50 sar ebx, 31 ; t \u0026gt;\u0026gt;= 31 imul eax, ebx ; x *= t Another, more complicated way to implement this whole sequence is to convert this sign bit into a mask and then use bitwise and instead of multiplication: ((a[i] - 50) \u0026gt;\u0026gt; 31 - 1) \u0026amp; a[i]. This makes the whole sequence one cycle faster, considering that, unlike other instructions, imul takes 3 cycles:\nmov ebx, eax ; t = x sub ebx, 50 ; t -= 50 sar ebx, 31 ; t \u0026gt;\u0026gt;= 31 ; imul eax, ebx ; x *= t sub ebx, 1 ; t -= 1 (causing underflow if t = 0) and eax, ebx ; x \u0026amp;= t Note that this optimization is not technically correct from the compiler\u0026rsquo;s perspective: for the 50 lowest representable integers — those in the $[-2^{31}, - 2^{31} + 49]$ range — the result will be wrong due to underflow. We know that all numbers are all between 0 and 100, and this won\u0026rsquo;t happen, but the compiler doesn\u0026rsquo;t.\nBut the compiler actually elects to do something different. Instead of going with this arithmetic trick, it used a special cmov (\u0026ldquo;conditional move\u0026rdquo;) instruction that assigns a value based on a condition (which is computed and checked using the flags register, the same way as for jumps):\nmov ebx, 0 ; cmov doesn\u0026#39;t support immediate values, so we need a zero register cmp eax, 50 cmovge eax, ebx ; eax = (eax \u0026gt;= 50 ? eax : ebx=0) So the code above is actually closer to using a ternary operator like this:\nfor (int i = 0; i \u0026lt; N; i++) s += (a[i] \u0026lt; 50 ? a[i] : 0); Both variants are optimized by the compiler and produce the following assembly:\nmov eax, 0 mov ecx, -4000000 loop: mov esi, dword ptr [rdx + a + 4000000] ; load a[i] cmp esi, 50 cmovge esi, eax ; esi = (esi \u0026gt;= 50 ? esi : eax=0) add dword ptr [rsp + 12], esi ; s += esi add rdx, 4 jnz loop ; \u0026#34;iterate while rdx is not zero\u0026#34; This general technique is called predication, and it is roughly equivalent to this algebraic trick:\n$$ x = c \\cdot a + (1 - c) \\cdot b $$\nThis way you can eliminate branching, but this comes at the cost of evaluating both branches and the cmov itself. Because evaluating the \u0026ldquo;\u0026gt;=\u0026rdquo; branch costs nothing, the performance is exactly equal to the \u0026ldquo;always yes\u0026rdquo; case in the branchy version.\n#When Predication Is BeneficialUsing predication eliminates a control hazard but introduces a data hazard. There is still a pipeline stall, but it is a cheaper one: you only need to wait for cmov to be resolved and not flush the entire pipeline in case of a mispredict.\nHowever, there are many situations when it is more efficient to leave branchy code as it is. This is the case when the cost of computing both branches instead of just one outweighs the penalty for the potential branch mispredictions.\nIn our example, the branchy code wins when the branch can be predicted with a probability of more than ~75%.\nThis 75% threshold is commonly used by the compilers as a heuristic for determining whether to use the cmov or not. Unfortunately, this probability is usually unknown at the compile time, so it needs to be provided in one of several ways:\nWe can use profile-guided optimization which will decide for itself whether to use predication or not. We can use likeliness attributes and compiler-specific intrinsics to hint at the likeliness of branches: __builtin_expect_with_probability in GCC and __builtin_unpredictable in Clang. We can rewrite branchy code using the ternary operator or various arithmetic tricks, which acts as sort of an implicit contract between programmers and compilers: if the programmer wrote the code this way, then it was probably meant to be branchless. The \u0026ldquo;right way\u0026rdquo; is to use branching hints, but unfortunately, the support for them is lacking. Right now these hints seem to be lost by the time the compiler back-end decides whether a cmov is more beneficial. There is some progress towards making it possible, but currently, there is no good way of forcing the compiler to generate branch-free code, so sometimes the best hope is to just write a small snippet in assembly.\n#Larger ExamplesStrings. Oversimplifying things, an std::string is comprised of a pointer to a null-terminated char array (also known as a \u0026ldquo;C-string\u0026rdquo;) allocated somewhere on the heap and one integer containing the string size.\nA common value for a string is the empty string — which is also its default value. You also need to handle them somehow, and the idiomatic approach is to assign nullptr as the pointer and 0 as the string size, and then check if the pointer is null or if the size is zero at the beginning of every procedure involving strings.\nHowever, this requires a separate branch, which is costly (unless the majority of strings are either empty or non-empty). To remove the check and thus also the branch, we can allocate a \u0026ldquo;zero C-string,\u0026rdquo; which is just a zero byte allocated somewhere, and then simply point all empty strings there. Now all string operations with empty strings have to read this useless zero byte, but this is still much cheaper than a branch misprediction.\nBinary search. The standard binary search can be implemented without branches, and on small arrays (that fit into cache) it works ~4x faster than the branchy std::lower_bound:\nint lower_bound(int x) { int *base = t, len = n; while (len \u0026gt; 1) { int half = len / 2; base += (base[half - 1] \u0026lt; x) * half; // will be replaced with a \u0026#34;cmov\u0026#34; len -= half; } return *base; } Other than being more complex, it has another slight drawback in that it potentially does more comparisons (constant $\\lceil \\log_2 n \\rceil$ instead of either $\\lfloor \\log_2 n \\rfloor$ or $\\lceil \\log_2 n \\rceil$) and can\u0026rsquo;t speculate on future memory reads (which acts as prefetching, so it loses on very large arrays).\nIn general, data structures are made branchless by implicitly or explicitly padding them so that their operations take a constant number of iterations. Refer to the article for more complex examples.\nData-parallel programming. Branchless programming is very important for SIMD applications because they don\u0026rsquo;t have branching in the first place.\nIn our array sum example, removing the volatile type qualifier from the accumulator allows the compiler to vectorize the loop:\n/* volatile */ int s = 0; for (int i = 0; i \u0026lt; N; i++) if (a[i] \u0026lt; 50) s += a[i]; It now works in ~0.3 per element, which is mainly bottlenecked by the memory.\nThe compiler is usually able to vectorize any loop that doesn\u0026rsquo;t have branches or dependencies between the iterations — and some specific small deviations from that, such as reductions or simple loops that contain just one if-without-else. Vectorization of anything more complex is a very nontrivial problem, which may involve various techniques such as masking and in-register permutations.\n","id":27,"path":"/hugo-page/hpc/pipelining/branchless/","title":"Branchless Programming"},{"content":"The basic units of data transfer in the CPU cache system are not individual bits and bytes, but cache lines. On most architectures, the size of a cache line is 64 bytes, meaning that all memory is divided in blocks of 64 bytes, and whenever you request (read or write) a single byte, you are also fetching all its 63 cache line neighbors whether your want them or not.\nTo demonstrate this, we add a \u0026ldquo;step\u0026rdquo; parameter to our incrementing loop. Now we only touch every $D$-th element:\nfor (int i = 0; i \u0026lt; N; i += D) a[i]++; If we run it with $D=1$ and $D=16$, we can observe something interesting:\nPerformance is normalized by the total time to run benchmark, not the total number of elements incremented As the problem size grows, the graphs of the two loops meet, despite one doing 16 times less work than the other. This is because, in terms of cache lines, we are fetching the exact same memory in both loops, and the fact that the strided loop only needs one-sixteenth of it is irrelevant.\nWhen the array fits into the L1 cache, the strided version completes faster — although not 16 but just two times as fast. This is because it only needs to do half the work: it only executes a single inc DWORD PTR [rdx] instruction for every 16 elements, while the original loop needed two 8-element vector instructions to process the same 16 elements. Both computations are bottlenecked by writing the result back: Zen 2 can only write one word per cycle — regardless of whether it is composed of one integer or eight.\nWhen we change the step parameter to 8, the graphs equalize, as we now also need two increments and two write-backs per every 16 elements:\nWe can use this effect to minimize cache sharing in our latency benchmark to measure it more precisely. We need to pad the indices of a permutation so that each of them lies in its own cache line:\nstruct padded_int { int val; int padding[15]; }; padded_int q[N / 16]; // constructing a cycle from a random permutation // ... for (int i = 0; i \u0026lt; N / 16; i++) k = q[k].val; Now, each index is much more likely to be kicked out of the cache by the time we loop around and request it again:\nThe important practical lesson when designing and analyzing memory-bound algorithms is to count the number of cache lines accessed and not just the total count of memory reads and writes.\n","id":28,"path":"/hugo-page/hpc/cpu-cache/cache-lines/","title":"Cache Lines"},{"content":"Fermat’s theorem allows us to calculate modular multiplicative inverses through binary exponentiation in $O(\\log n)$ operations, but it only works with prime modula. There is a generalization of it, Euler\u0026rsquo;s theorem, stating that if $m$ and $a$ are coprime, then\n$$ a^{\\phi(m)} \\equiv 1 \\pmod m $$\nwhere $\\phi(m)$ is Euler\u0026rsquo;s totient function defined as the number of positive integers $x \u0026lt; m$ that are coprime with $m$. In the special case when $m$ is a prime, then all the $m - 1$ residues are coprime and $\\phi(m) = m - 1$, yielding the Fermat\u0026rsquo;s theorem.\nThis lets us calculate the inverse of $a$ as $a^{\\phi(m) - 1}$ if we know $\\phi(m)$, but in turn, calculating it is not so fast: you usually need to obtain the factorization of $m$ to do it. There is a more general method that works by modifying the the Euclidean algorthm.\n#AlgorithmExtended Euclidean algorithm, apart from finding $g = \\gcd(a, b)$, also finds integers $x$ and $y$ such that\n$$ a \\cdot x + b \\cdot y = g $$\nwhich solves the problem of finding modular inverse if we substitute $b$ with $m$ and $g$ with $1$:\n$$ a^{-1} \\cdot a + k \\cdot m = 1 $$\nNote that, if $a$ is not coprime with $m$, there is no solution since no integer combination of $a$ and $m$ can yield anything that is not a multiple of their greatest common divisor.\nThe algorithm is also recursive: it calculates the coefficients $x\u0026rsquo;$ and $y\u0026rsquo;$ for $\\gcd(b, a \\bmod b)$ and restores the solution for the original number pair. If we have a solution $(x\u0026rsquo;, y\u0026rsquo;)$ for the pair $(b, a \\bmod b)$\n$$ b \\cdot x\u0026rsquo; + (a \\bmod b) \\cdot y\u0026rsquo; = g $$\nthen, to get the solution for the initial input, we can rewrite the expression $(a \\bmod b)$ as $(a - \\lfloor \\frac{a}{b} \\rfloor \\cdot b)$ and subsitute it into the aforementioned equation:\n$$ b \\cdot x\u0026rsquo; + (a - \\Big \\lfloor \\frac{a}{b} \\Big \\rfloor \\cdot b) \\cdot y\u0026rsquo; = g $$\nNow we rearrange the terms grouping by $a$ and $b$ to get\n$$ a \\cdot \\underbrace{y\u0026rsquo;}_x + b \\cdot \\underbrace{(x\u0026rsquo; - \\Big \\lfloor \\frac{a}{b} \\Big \\rfloor \\cdot y\u0026rsquo;)}_y = g $$\nComparing it with the initial expression, we infer that we can just use coefficients of $a$ and $b$ for the initial $x$ and $y$.\n#ImplementationWe implement the algorithm as a recursive function. Since its output is not one but three integers, we pass the coefficients to it by reference:\nint gcd(int a, int b, int \u0026amp;x, int \u0026amp;y) { if (a == 0) { x = 0; y = 1; return b; } int x1, y1; int d = gcd(b % a, a, x1, y1); x = y1 - (b / a) * x1; y = x1; return d; } To calculate the inverse, we simply pass $a$ and $m$ and return the $x$ coefficient the algorithm finds. Since we pass two positive numbers, one of the coefficient will be positive and the other one is negative (which one depends on whether the number of iterations is odd or even), so we need to optionally check if $x$ is negative and add $m$ to get a correct residue:\nint inverse(int a) { int x, y; gcd(a, M, x, y); if (x \u0026lt; 0) x += M; return x; } It works in ~160ns — 10ns faster than inverting numbers with binary exponentiation. To optimize it further, we can similarly turn it iterative ­— which takes 135ns:\nint inverse(int a) { int b = M, x = 1, y = 0; while (a != 1) { y -= b / a * x; b %= a; swap(a, b); swap(x, y); } return x \u0026lt; 0 ? x + M : x; } Note that, unlike binary exponentiation, the running time depends on the value of $a$. For example, for this particular value of $m$ ($10^9 + 7$), the worst input happens to be 564400443, for which the algorithm performs 37 iterations and takes 250ns.\nExercise. Try to adapt the same technique for the binary GCD (it won\u0026rsquo;t give performance speedup though unless you are better than me at optimization).\n","id":29,"path":"/hugo-page/hpc/number-theory/euclid-extended/","title":"Extended Euclidean Algorithm"},{"content":"To reason about the performance of memory-bound algorithms, we need to develop a cost model that is more sensitive to expensive block I/O operations but is not too rigorous to still be useful.\n#Cache-Aware ModelIn the standard RAM model, we ignore the fact that primitive operations take unequal time to complete. Most importantly, it does not differentiate between operations on different types of memory, equating a read from RAM taking ~50ns in real-time with a read from HDD taking ~5ms, or about a $10^5$ times as much.\nSimilar in spirit, in the external memory model, we simply ignore every operation that is not an I/O operation. More specifically, we consider one level of cache hierarchy and assume the following about the hardware and the problem:\nThe size of the dataset is $N$, and it is all stored in external memory, which we can read and write in blocks of $B$ elements in a unit time (reading a whole block and just one element takes the same time). We can store $M$ elements in internal memory, meaning that we can store up to $\\left \\lfloor \\frac{M}{B} \\right \\rfloor$ blocks. We only care about I/O operations: any computations done in-between the reads and the writes are free. We additionally assume $N \\gg M \\gg B$. In this model, we measure the performance of an algorithm in terms of its high-level I/O operations, or IOPS — that is, the total number of blocks read or written to external memory during execution.\nWe will mostly focus on the case where the internal memory is RAM and the external memory is SSD or HDD, although the underlying analysis techniques that we will develop are applicable to any layer in the cache hierarchy. Under these settings, reasonable block size $B$ is about 1MB, internal memory size $M$ is usually a few gigabytes, and $N$ is up to a few terabytes.\n#Array Scan As a simple example, when we calculate the sum of an array by iterating through it one element at a time, we implicitly load it by chunks of $O(B)$ elements and, in terms of the external memory model, process these chunks one by one:\n$$ \\underbrace{a_1, a_2, a_3,} _ {B_1} \\underbrace{a_4, a_5, a_6,} _ {B_2} \\ldots \\underbrace{a_{n-3}, a_{n-2}, a_{n-1}} _ {B_{m-1}} $$\nThus, in the external memory model, the complexity of summation and other linear array scans is\n$$ SCAN(N) \\stackrel{\\text{def}}{=} O\\left(\\left \\lceil \\frac{N}{B} \\right \\rceil \\right) ; \\text{IOPS} $$\nYou can implement external array scan explicitly like this:\nFILE *input = fopen(\u0026#34;input.bin\u0026#34;, \u0026#34;rb\u0026#34;); const int M = 1024; int buffer[M], sum = 0; // while the file is not fully processed while (true) { // read up to M of 4-byte elements from the input stream int n = fread(buffer, 4, M, input); // ^ the number of elements that were actually read // if we can\u0026#39;t read any more elements, finish if (n == 0) break; // sum elements in-memory for (int i = 0; i \u0026lt; n; i++) sum += buffer[i]; } fclose(input); printf(\u0026#34;%d\\n\u0026#34;, sum); Note that, in most cases, operating systems do this buffering automatically. Even when the data is just redirected to the standard input from a normal file, the operating system buffers its stream and reads it in blocks of ~4KB (by default).\n","id":30,"path":"/hugo-page/hpc/external-memory/model/","title":"External Memory Model"},{"content":"To \u0026ldquo;call a function\u0026rdquo; in assembly, you need to jump to its beginning and then jump back. But then two important problems arise:\nWhat if the caller stores data in the same registers as the callee? Where is \u0026ldquo;back\u0026rdquo;? Both of these concerns can be solved by having a dedicated location in memory where we can write all the information we need to return from the function before calling it. This location is called the stack.\n#The StackThe hardware stack works the same way software stacks do and is similarly implemented as just two pointers:\nThe base pointer marks the start of the stack and is conventionally stored in rbp. The stack pointer marks the last element of the stack and is conventionally stored in rsp. When you need to call a function, you push all your local variables onto the stack (which you can also do in other circumstances; e.g., when you run out of registers), push the current instruction pointer, and then jump to the beginning of the function. When exiting from a function, you look at the pointer stored on top of the stack, jump there, and then carefully read all the variables stored on the stack back into their registers.\nYou can implement all that with the usual memory operations and jumps, but because of how frequently it is used, there are 4 special instructions for doing this:\npush writes data at the stack pointer and decrements it. pop reads data from the stack pointer and increments it. call puts the address of the following instruction on top of the stack and jumps to a label. ret reads the return address from the top of the stack and jumps to it. You would call them \u0026ldquo;syntactic sugar\u0026rdquo; if they weren\u0026rsquo;t actual hardware instructions — they are just fused equivalents of these two-instruction snippets:\n; \u0026#34;push rax\u0026#34; sub rsp, 8 mov QWORD PTR[rsp], rax ; \u0026#34;pop rax\u0026#34; mov rax, QWORD PTR[rsp] add rsp, 8 ; \u0026#34;call func\u0026#34; push rip ; \u0026lt;- instruction pointer (although accessing it like that is probably illegal) jmp func ; \u0026#34;ret\u0026#34; pop rcx ; \u0026lt;- choose any unused register jmp rcx The memory region between rbp and rsp is called a stack frame, and this is where local variables of functions are typically stored. It is pre-allocated at the start of the program, and if you push more data on the stack than its capacity (8MB by default on Linux), you encounter a stack overflow error. Because modern operating systems don\u0026rsquo;t actually give you memory pages until you read or write to their address space, you can freely specify a very large stack size, which acts more like a limit on how much stack memory can be used, and not a fixed amount every program has to use.\n#Calling ConventionsThe people who develop compilers and operating systems eventually came up with conventions on how to write and call functions. These conventions enable some important software engineering marvels such as splitting compilation into separate units, reusing already-compiled libraries, and even writing them in different programming languages.\nConsider the following example in C:\nint square(int x) { return x * x; } int distance(int x, int y) { return square(x) + square(y); } By convention, a function should take its arguments in rdi, rsi, rdx, rcx, r8, r9 (and the rest in the stack if those weren\u0026rsquo;t enough), put the return value into rax, and then return. Thus, square, being a simple one-argument function, can be implemented like this:\nsquare: ; x = edi, ret = eax imul edi, edi mov eax, edi ret Each time we call it from distance, we just need to go through some trouble preserving its local variables:\ndistance: ; x = rdi/edi, y = rsi/esi, ret = rax/eax push rdi push rsi call square ; eax = square(x) pop rsi pop rdi mov ebx, eax ; save x^2 mov rdi, rsi ; move new x=y push rdi push rsi call square ; eax = square(x=y) pop rsi pop rdi add eax, ebx ; x^2 + y^2 ret There are a lot more nuances, but we won\u0026rsquo;t go into detail here because this book is about performance, and the best way to deal with functions calls is actually to avoid making them in the first place.\n#InliningMoving data to and from the stack creates noticeable overhead for small functions like these. The reason you have to do this is that, in general, you don\u0026rsquo;t know whether the callee is modifying the registers where you store your local variables. But when you have access to the code of square, you can solve this problem by stashing the data in registers that you know won\u0026rsquo;t be modified.\ndistance: call square mov ebx, eax mov edi, esi call square add eax, ebx ret This is better, but we are still implicitly accessing stack memory: you need to push and pop the instruction pointer on each function call. In simple cases like this, we can inline function calls by stitching the callee\u0026rsquo;s code into the caller and resolving conflicts over registers. In our example:\ndistance: imul edi, edi ; edi = x^2 imul esi, esi ; esi = y^2 add edi, esi mov eax, edi ; there is no \u0026#34;add eax, edi, esi\u0026#34;, so we need a separate mov ret This is fairly close to what optimizing compilers produce out of this snippet — only they use the lea trick to make the resulting machine code sequence a few bytes smaller:\ndistance: imul edi, edi ; edi = x^2 imul esi, esi ; esi = y^2 lea eax, [rdi+rsi] ; eax = x^2 + y^2 ret In situations like these, function inlining is clearly beneficial, and compilers mostly do it automatically, but there are cases when it\u0026rsquo;s not — and we will talk about them in a bit.\n#Tail Call EliminationInlining is straightforward to do when the callee doesn\u0026rsquo;t make any other function calls, or at least if these calls are not recursive. Let\u0026rsquo;s move on to a more complex example. Consider this recursive computation of a factorial:\nint factorial(int n) { if (n == 0) return 1; return factorial(n - 1) * n; } Equivalent assembly:\n; n = edi, ret = eax factorial: test edi, edi ; test if a value is zero jne nonzero ; (the machine code of \u0026#34;cmp rax, 0\u0026#34; would be one byte longer) mov eax, 1 ; return 1 ret nonzero: push edi ; save n to use later in multiplication sub edi, 1 call factorial ; call f(n - 1) pop edi imul eax, edi ret If the function is recursive, it is still often possible to make it \u0026ldquo;call-less\u0026rdquo; by restructuring it. This is the case when the function is tail recursive, that is, it returns right after making a recursive call. Since no actions are required after the call, there is also no need for storing anything on the stack, and a recursive call can be safely replaced with a jump to the beginning — effectively turning the function into a loop.\nTo make our factorial function tail-recursive, we can pass a \u0026ldquo;current product\u0026rdquo; argument to it:\nint factorial(int n, int p = 1) { if (n == 0) return p; return factorial(n - 1, p * n); } Then this function can be easily folded into a loop:\n; assuming n \u0026gt; 0 factorial: mov eax, 1 loop: imul eax, edi sub edi, 1 jne loop ret The primary reason why recursion can be slow is that it needs to read and write data to the stack, while iterative and tail-recursive algorithms do not. This concept is very important in functional programming, where there are no loops and all you can use are functions. Without tail call elimination, functional programs would require way more time and memory to execute.\n","id":31,"path":"/hugo-page/hpc/architecture/functions/","title":"Functions and Recursion"},{"content":" Interleaving the stages of execution is a general idea in digital electronics, and it is applied not only in the main CPU pipeline, but also on the level of separate instructions and memory. Most execution units have their own little pipelines and can take another instruction just one or two cycles after the previous one.\nIn this context, it makes sense to use two different \u0026ldquo;costs\u0026rdquo; for instructions:\nLatency: how many cycles are needed to receive the results of an instruction. Throughput: how many instructions can be, on average, executed per cycle. You can get latency and throughput numbers for a specific architecture from special documents called instruction tables. Here are some sample values for my Zen 2 (all specified for 32-bit operands, if there is any difference):\nInstruction Latency RThroughput jmp - 2 mov r, r - 1/4 mov r, m 4 1/2 mov m, r 3 1 add 1 1/3 cmp 1 1/4 popcnt 1 1/4 mul 3 1 div 13-28 13-28 Some comments:\nBecause our minds are so used to the cost model where \u0026ldquo;more\u0026rdquo; means \u0026ldquo;worse,\u0026rdquo; people mostly use reciprocals of throughput instead of throughput. If a certain instruction is especially frequent, its execution unit could be duplicated to increase its throughput — possibly to even more than one, but not higher than the decode width. Some instructions have a latency of 0. This means that these instruction are used to control the scheduler and don\u0026rsquo;t reach the execution stage. They still have non-zero reciprocal throughput because the CPU front-end still needs to process them. Most instructions are pipelined, and if they have the reciprocal throughput of $n$, this usually means that their execution unit can take another instruction after $n$ cycles (and if it is below 1, this means that there are multiple execution units, all capable of taking another instruction on the next cycle). One notable exception is integer division: it is either very poorly pipelined or not pipelined at all. Some instructions have variable latency, depending on not only the size, but also the values of the operands. For memory operations (including fused ones like add), the latency is usually specified for the best case (an L1 cache hit). There are many more important little details, but this mental model will suffice for now.\n","id":32,"path":"/hugo-page/hpc/pipelining/tables/","title":"Instruction Tables"},{"content":"When programmers hear the word parallelism, they mostly think about multi-core parallelism, the practice of explicitly splitting a computation into semi-independent threads that work together to solve a common problem.\nThis type of parallelism is mainly about reducing latency and achieving scalability, but not about improving efficiency. You can solve a problem ten times as big with a parallel algorithm, but it would take at least ten times as many computational resources. Although parallel hardware is becoming ever more abundant and parallel algorithm design is becoming an increasingly important area, for now, we will limit ourselves to considering only a single CPU core.\nBut there are other types of parallelism, already existing inside a CPU core, that you can use for free.\nInstruction PipeliningTo execute any instruction, processors need to do a lot of preparatory work first, which includes:\nfetching a chunk of machine code from memory, decoding it and splitting into instructions, executing these instructions, which may involve doing some memory operations, and writing the results back into registers. This whole sequence of operations is long. It takes up to 15-20 CPU cycles even for something simple like add-ing two register-stored values together. To hide this latency, modern CPUs use pipelining: after an instruction passes through the first stage, they start processing the next one right away, without waiting for the previous one to fully complete.\nPipelining does not reduce actual latency but functionally makes it seem like if it was composed of only the execution and memory stage. You still need to pay these 15-20 cycles, but you only need to do it once after you\u0026rsquo;ve found the sequence of instructions you are going to execute.\nHaving this in mind, hardware manufacturers prefer to use cycles per instruction (CPI) instead of something like \u0026ldquo;average instruction latency\u0026rdquo; as the main performance indicator for CPU designs. It is a pretty good metric for algorithm designs too, if we only consider useful instructions.\nThe CPI of a perfectly pipelined processor should tend to one, but it can actually be even lower if we make each stage of the pipeline \u0026ldquo;wider\u0026rdquo; by duplicating it, so that more than one instruction can be processed at a time. Because the cache and most of the ALU can be shared, this ends up being cheaper than adding a fully separate core. Such architectures, capable of executing more than one instruction per cycle, are called superscalar, and most modern CPUs are.\nYou can only take advantage of superscalar processing if the stream of instructions contains groups of logically independent operations that can be processed separately. The instructions don\u0026rsquo;t always arrive in the most convenient order, so, when possible, modern CPUs can execute them out of order to improve overall utilization and minimize pipeline stalls. How this magic works is a topic for a more advanced discussion, but for now, you can assume that the CPU maintains a buffer of pending instructions up to some distance in the future, and executes them as soon as the values of its operands are computed and there is an execution unit available.\nAn Education AnalogyConsider how our education system works:\nTopics are taught to groups of students instead of individuals as broadcasting the same things to everyone at once is more efficient. An intake of students is split into groups led by different teachers; assignments and other course materials are shared between groups. Each year the same course is taught to a new intake so that the teachers are kept busy. These innovations greatly increase the throughput of the whole system, although the latency (time to graduation for a particular student) remains unchanged (and maybe increases a little bit because personalized tutoring is more effective).\nYou can find many analogies with modern CPUs:\nCPUs use SIMD parallelism to execute the same operation on a block of different data points (comprised of 16, 32, or 64 bytes). There are multiple execution units that can process these instructions simultaneously while sharing other CPU facilities (usually 2-4 execution units). Instructions are processed in pipelined fashion (saving roughly the same number of cycles as the number of years between kindergarten and PhD). In addition to that, several other aspects also match:\nExecution paths become more divergent with time and need different execution units. Some instructions may be stalled for various reasons. Some instructions are even speculated (executed ahead of time), but then discarded. Some instructions may be split in several distinct micro-operations that can proceed on their own. Programming pipelined and superscalar processors presents its own challenges, which we are going to address in this chapter.\n","id":33,"path":"/hugo-page/hpc/pipelining/","title":"Instruction-Level Parallelism"},{"content":"The problem of factoring integers into primes is central to computational number theory. It has been studied since at least the 3rd century BC, and many methods have been developed that are efficient for different inputs.\nIn this case study, we specifically consider the factorization of word-sized integers: those on the order of $10^9$ and $10^{18}$. Untypical for this book, in this one, you may actually learn an asymptotically better algorithm: we start with a few basic approaches and gradually build up to the $O(\\sqrt[4]{n})$-time Pollard\u0026rsquo;s rho algorithm and optimize it to the point where it can factorize 60-bit semiprimes in 0.3-0.4ms and ~3 times faster than the previous state-of-the-art.\n#BenchmarkFor all methods, we will implement find_factor function that takes a positive integer $n$ and returns any of its non-trivial divisors (or 1 if the number is prime):\n// I don\u0026#39;t feel like typing \u0026#34;unsigned long long\u0026#34; each time typedef __uint16_t u16; typedef __uint32_t u32; typedef __uint64_t u64; typedef __uint128_t u128; u64 find_factor(u64 n); To find the full factorization, you can apply it to $n$, reduce it, and continue until a new factor can no longer be found:\nvector\u0026lt;u64\u0026gt; factorize(u64 n) { vector\u0026lt;u64\u0026gt; factorization; do { u64 d = find_factor(n); factorization.push_back(d); n /= d; } while (d != 1); return factorization; } After each removed factor, the problem becomes considerably smaller, so the worst-case running time of full factorization is equal to the worst-case running time of a find_factor call.\nFor many factorization algorithms, including those presented in this section, the running time scales with the smaller prime factor. Therefore, to provide worst-case input, we use semiprimes: products of two prime numbers $p \\le q$ that are on the same order of magnitude. We generate a $k$-bit semiprime as the product of two random $\\lfloor k / 2 \\rfloor$-bit primes.\nSince some of the algorithms are inherently randomized, we also tolerate a small (\u0026lt;1%) percentage of false-negative errors (when find_factor returns 1 despite number $n$ being composite), although this rate can be reduced to almost zero without significant performance penalties.\n#Trial division The most basic approach is to try every integer smaller than $n$ as a divisor:\nu64 find_factor(u64 n) { for (u64 d = 2; d \u0026lt; n; d++) if (n % d == 0) return d; return 1; } We can notice that if $n$ is divided by $d \u0026lt; \\sqrt n$, then it is also divided by $\\frac{n}{d} \u0026gt; \\sqrt n$, and there is no need to check for it separately. This lets us stop trial division early and only check for potential divisors that do not exceed $\\sqrt n$:\nu64 find_factor(u64 n) { for (u64 d = 2; d * d \u0026lt;= n; d++) if (n % d == 0) return d; return 1; } In our benchmark, $n$ is a semiprime, and we always find the lesser divisor, so both $O(n)$ and $O(\\sqrt n)$ implementations perform the same and are able to factorize ~2k 30-bit numbers per second — while taking whole 20 seconds to factorize a single 60-bit number.\n#Lookup TableNowadays, you can type factor 57 in your Linux terminal or Google search bar to get the factorization of any number. But before computers were invented, it was more practical to use factorization tables: special books containing factorizations of the first $N$ numbers.\nWe can also use this approach to compute these lookup tables during compile time. To save space, we can store only the smallest divisor of a number. Since the smallest divisor does not exceed the $\\sqrt n$, we need just one byte per a 16-bit integer:\ntemplate \u0026lt;int N = (1\u0026lt;\u0026lt;16)\u0026gt; struct Precalc { unsigned char divisor[N]; constexpr Precalc() : divisor{} { for (int i = 0; i \u0026lt; N; i++) divisor[i] = 1; for (int i = 2; i * i \u0026lt; N; i++) if (divisor[i] == 1) for (int k = i * i; k \u0026lt; N; k += i) divisor[k] = i; } }; constexpr Precalc P{}; u64 find_factor(u64 n) { return P.divisor[n]; } With this approach, we can process 3M 16-bit integers per second, although it would probably get slower for larger numbers. While it requires just a few milliseconds and 64KB of memory to calculate and store the divisors of the first $2^{16}$ numbers, it does not scale well for larger inputs.\n#Wheel factorizationTo save paper space, pre-computer era factorization tables typically excluded numbers divisible by $2$ and $5$, making the factorization table ½ × ⅘ = 0.4 of its original size. In the decimal numeral system, you can quickly determine whether a number is divisible by $2$ or $5$ (by looking at its last digit) and keep dividing the number $n$ by $2$ or $5$ while it is possible, eventually arriving at some entry in the factorization table.\nWe can apply a similar trick to trial division by first checking if the number is divisible by $2$ and then only considering odd divisors:\nu64 find_factor(u64 n) { if (n % 2 == 0) return 2; for (u64 d = 3; d * d \u0026lt;= n; d += 2) if (n % d == 0) return d; return 1; } With 50% fewer divisions to perform, this algorithm works twice as fast.\nThis method can be extended: if the number is not divisible by $3$, we can also ignore all multiples of $3$, and the same goes for all other divisors. The problem is, as we increase the number of primes to exclude, it becomes less straightforward to iterate only over the numbers not divisible by them as they follow an irregular pattern — unless the number of primes is small.\nFor example, if we consider $2$, $3$, and $5$, then, among the first $90$ numbers, we only need to check:\n(1,) 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 49, 53, 59, 61, 67, 71, 73, 77, 79, 83, 89… You can notice a pattern: the sequence repeats itself every $30$ numbers. This is not surprising since the remainder modulo $2 \\times 3 \\times 5 = 30$ is all we need to determine whether a number is divisible by $2$, $3$, or $5$. This means that we only need to check $8$ numbers with specific remainders out of every $30$, proportionally improving the performance:\nu64 find_factor(u64 n) { for (u64 d : {2, 3, 5}) if (n % d == 0) return d; u64 offsets[] = {0, 4, 6, 10, 12, 16, 22, 24}; for (u64 d = 7; d * d \u0026lt;= n; d += 30) { for (u64 offset : offsets) { u64 x = d + offset; if (n % x == 0) return x; } } return 1; } As expected, it works $\\frac{30}{8} = 3.75$ times faster than the naive trial division, processing about 7.6k 30-bit numbers per second. The performance can be improved further by considering more primes, but the returns are diminishing: adding a new prime $p$ reduces the number of iterations by $\\frac{1}{p}$ but increases the size of the skip-list by a factor of $p$, requiring proportionally more memory.\n#Precomputed PrimesIf we keep increasing the number of primes in wheel factorization, we eventually exclude all composite numbers and only check for prime factors. In this case, we don\u0026rsquo;t need this array of offsets but just the array of primes:\nconst int N = (1 \u0026lt;\u0026lt; 16); struct Precalc { u16 primes[6542]; // # of primes under N=2^16 constexpr Precalc() : primes{} { bool marked[N] = {}; int n_primes = 0; for (int i = 2; i \u0026lt; N; i++) { if (!marked[i]) { primes[n_primes++] = i; for (int j = 2 * i; j \u0026lt; N; j += i) marked[j] = true; } } } }; constexpr Precalc P{}; u64 find_factor(u64 n) { for (u16 p : P.primes) if (n % p == 0) return p; return 1; } This approach lets us process almost 20k 30-bit integers per second, but it does not work for larger (64-bit) numbers unless they have small ($\u0026lt; 2^{16}$) factors.\nNote that this is actually an asymptotic optimization: there are $O(\\frac{n}{\\ln n})$ primes among the first $n$ numbers, so this algorithm performs $O(\\frac{\\sqrt n}{\\ln \\sqrt n})$ operations, while wheel factorization only eliminates a large but constant fraction of divisors. If we extend it to 64-bit numbers and precompute every prime under $2^{32}$ (storing which would require several hundred megabytes of memory), the relative speedup would grow by a factor of $\\frac{\\ln \\sqrt{n^2}}{\\ln \\sqrt n} = 2 \\cdot \\frac{1/2}{1/2} \\cdot \\frac{\\ln n}{\\ln n} = 2$.\nAll variants of trial division, including this one, are bottlenecked by the speed of integer division, which can be optimized if we know the divisors in advance and allow for some additional precomputation. In our case, it is suitable to use the Lemire division check:\n// ...precomputation is the same as before, // but we store the reciprocal instead of the prime number itself u64 magic[6542]; // for each prime i: magic[n_primes++] = u64(-1) / i + 1; u64 find_factor(u64 n) { for (u64 m : P.magic) if (m * n \u0026lt; m) return u64(-1) / m + 1; return 1; } This makes the algorithm ~18x faster: we can now factorize ~350k 30-bit numbers per second, which is actually the most efficient algorithm we have for this number range. While it can probably be optimized even further by performing these checks in parallel with SIMD, we will stop there and try a different, asymptotically better approach.\n#Pollard\u0026rsquo;s Rho Algorithm Pollard\u0026rsquo;s rho is a randomized $O(\\sqrt[4]{n})$ integer factorization algorithm that makes use of the birthday paradox:\nOne only needs to draw $d = \\Theta(\\sqrt{n})$ random numbers between $1$ and $n$ to get a collision with high probability.\nThe reasoning behind it is that each of the $d$ added element has a $\\frac{d}{n}$ chance of colliding with some other element, implying that the expected number of collisions is $\\frac{d^2}{n}$. If $d$ is asymptotically smaller than $\\sqrt n$, then this ratio grows to zero as $n \\to \\infty$, and to infinity otherwise.\nConsider some function $f(x)$ that takes a remainder $x \\in [0, n)$ and maps it to some other remainder of $n$ in a way that seems random from the number theory point of view. Specifically, we will use $f(x) = x^2 + 1 \\bmod n$, which is random enough for our purposes.\nNow, consider a graph where each number-vertex $x$ has an edge pointing to $f(x)$. Such graphs are called functional. In functional graphs, the \u0026ldquo;trajectory\u0026rdquo; of any element — the path we walk if we start from that element and keep following the edges — is a path that eventually loops around (because the set of vertices is limited, and at some point, we have to go to a vertex we have already visited).\nThe trajectory of an element resembles the greek letter ρ (rho), which is what the algorithm is named after Consider a trajectory of some particular element $x_0$:\n$$ x_0, ; f(x_0), ; f(f(x_0)), ; \\ldots $$\nLet\u0026rsquo;s make another sequence out of this one by reducing each element modulo $p$, the smallest prime divisor of $n$.\nLemma. The expected length of the reduced sequence before it turns into a cycle is $O(\\sqrt[4]{n})$.\nProof: Since $p$ is the smallest divisor, $p \\leq \\sqrt n$. Each time we follow a new edge, we essentially generate a random number between $0$ and $p$ (we treat $f$ as a \u0026ldquo;deterministically-random\u0026rdquo; function). The birthday paradox states that we only need to generate $O(\\sqrt p) = O(\\sqrt[4]{n})$ numbers until we get a collision and thus enter a loop.\nSince we don\u0026rsquo;t know $p$, this mod-$p$ sequence is only imaginary, but if find a cycle in it — that is, $i$ and $j$ such that\n$$ f^i(x_0) \\equiv f^j(x_0) \\pmod p $$\nthen we can also find $p$ itself as\n$$ p = \\gcd(|f^i(x_0) - f^j(x_0)|, n) $$\nThe algorithm itself just finds this cycle and $p$ using this GCD trick and Floyd\u0026rsquo;s \u0026ldquo;tortoise and hare\u0026rdquo; algorithm: we maintain two pointers $i$ and $j = 2i$ and check that\n$$ \\gcd(|f^i(x_0) - f^j(x_0)|, n) \\neq 1 $$\nwhich is equivalent to comparing $f^i(x_0)$ and $f^j(x_0)$ modulo $p$. Since $j$ (hare) is increasing at twice the rate of $i$ (tortoise), their difference is increasing by $1$ each iteration and eventually will become equal to (or a multiple of) the cycle length, with $i$ and $j$ pointing to the same elements. And as we proved half a page ago, reaching a cycle would only require $O(\\sqrt[4]{n})$ iterations:\nu64 f(u64 x, u64 mod) { return ((u128) x * x + 1) % mod; } u64 diff(u64 a, u64 b) { // a and b are unsigned and so is their difference, so we can\u0026#39;t just call abs(a - b) return a \u0026gt; b ? a - b : b - a; } const u64 SEED = 42; u64 find_factor(u64 n) { u64 x = SEED, y = SEED, g = 1; while (g == 1) { x = f(f(x, n), n); // advance x twice y = f(y, n); // advance y once g = gcd(diff(x, y)); } return g; } While it processes only ~25k 30-bit integers — which is almost 15 times slower than by checking each prime using a fast division trick — it dramatically outperforms every $\\tilde{O}(\\sqrt n)$ algorithm for 60-bit numbers, factorizing around 90 of them per second.\n#Pollard-Brent AlgorithmFloyd\u0026rsquo;s cycle-finding algorithm has a problem in that it moves iterators more than necessary: at least half of the vertices are visited one additional time by the slower iterator.\nOne way to solve it is to memorize the values $x_i$ that the faster iterator visits and, every two iterations, compute the GCD using the difference of $x_i$ and $x_{\\lfloor i / 2 \\rfloor}$. But it can also be done without extra memory using a different principle: the tortoise doesn\u0026rsquo;t move on every iteration, but it gets reset to the value of the faster iterator when the iteration number becomes a power of two. This lets us save additional iterations while still using the same GCD trick to compare $x_i$ and $x_{2^{\\lfloor \\log_2 i \\rfloor}}$ on each iteration:\nu64 find_factor(u64 n) { u64 x = SEED; for (int l = 256; l \u0026lt; (1 \u0026lt;\u0026lt; 20); l *= 2) { u64 y = x; for (int i = 0; i \u0026lt; l; i++) { x = f(x, n); if (u64 g = gcd(diff(x, y), n); g != 1) return g; } } return 1; } Note that we also set an upper limit on the number of iterations so that the algorithm finishes in a reasonable amount of time and returns 1 if $n$ turns out to be a prime.\nIt actually does not improve performance and even makes the algorithm ~1.5x slower, which probably has something to do with the fact that $x$ is stale. It spends most of the time computing the GCD and not advancing the iterator — in fact, the time requirement of this algorithm is currently $O(\\sqrt[4]{n} \\log n)$ because of it.\nInstead of optimizing the GCD itself, we will optimize the number of its invocations. We can use the fact that if one of $a$ and $b$ contains factor $p$, then $a \\cdot b \\bmod n$ will also contain it, so instead of computing $\\gcd(a, n)$ and $\\gcd(b, n)$, we can compute $\\gcd(a \\cdot b \\bmod n, n)$. This way, we can group the calculations of GCP in groups of $M = O(\\log n)$ we remove $\\log n$ out of the asymptotic:\nconst int M = 1024; u64 find_factor(u64 n) { u64 x = SEED; for (int l = M; l \u0026lt; (1 \u0026lt;\u0026lt; 20); l *= 2) { u64 y = x, p = 1; for (int i = 0; i \u0026lt; l; i += M) { for (int j = 0; j \u0026lt; M; j++) { y = f(y, n); p = (u128) p * diff(x, y) % n; } if (u64 g = gcd(p, n); g != 1) return g; } } return 1; } Now it performs 425 factorizations per second, bottlenecked by the speed of modulo.\n#Optimizing the ModuloThe final step is to apply Montgomery multiplication. Since the modulo is constant, we can perform all computations — advancing the iterator, multiplication, and even computing the GCD — in the Montgomery space where reduction is cheap:\nstruct Montgomery { u64 n, nr; Montgomery(u64 n) : n(n) { nr = 1; for (int i = 0; i \u0026lt; 6; i++) nr *= 2 - n * nr; } u64 reduce(u128 x) const { u64 q = u64(x) * nr; u64 m = ((u128) q * n) \u0026gt;\u0026gt; 64; return (x \u0026gt;\u0026gt; 64) + n - m; } u64 multiply(u64 x, u64 y) { return reduce((u128) x * y); } }; u64 f(u64 x, u64 a, Montgomery m) { return m.multiply(x, x) + a; } const int M = 1024; u64 find_factor(u64 n, u64 x0 = 2, u64 a = 1) { Montgomery m(n); u64 x = SEED; for (int l = M; l \u0026lt; (1 \u0026lt;\u0026lt; 20); l *= 2) { u64 y = x, p = 1; for (int i = 0; i \u0026lt; l; i += M) { for (int j = 0; j \u0026lt; M; j++) { x = f(x, m); p = m.multiply(p, diff(x, y)); } if (u64 g = gcd(p, n); g != 1) return g; } } return 1; } This implementation can processes around 3k 60-bit integers per second, which is ~3x faster than what PARI / SageMath\u0026rsquo;s factor / cat semiprimes.txt | time factor measures.\n#Further ImprovementsOptimizations. There is still a lot of potential for optimization in our implementation of the Pollard\u0026rsquo;s algorithm:\nWe could probably use a better cycle-finding algorithm, exploiting the fact that the graph is random. For example, there is little chance that we enter the loop in within the first few iterations (the length of the cycle and the path we walk before entering it should be equal in expectation since before we loop around, we choose the vertex of the path we\u0026rsquo;ve walked independently), so we may just advance the iterator for some time before starting the trials with the GCD trick. Our current approach is bottlenecked by advancing the iterator (the latency of Montgomery multiplication is much higher than its reciprocal throughput), and while we are waiting for it to complete, we could perform more than just one trial using the previous values. If we run $p$ independent instances of the algorithm with different seeds in parallel and stop when one of them finds the answer, it would finish $\\sqrt p$ times faster (the reasoning is similar to the Birthday paradox; try to prove it yourself). We don\u0026rsquo;t have to use multiple cores for that: there is a lot of untapped instruction-level parallelism, so we could concurrently run two or three of the same operations on the same thread, or use SIMD instructions to perform 4 or 8 multiplications in parallel. I would not be surprised to see another 3x improvement and throughput of ~10k/sec. If you implement some of these ideas, please let me know.\nErrors. Another aspect that we need to handle in a practical implementation is possible errors. Our current implementation has a 0.7% error rate for 60-bit integers, and it grows higher if the numbers are lower. These errors come from three main sources:\nA cycle simply not being found (the algorithm is inherently random, and there is no guarantee that it will be found). In this case, we need to perform a primality test and optionally start again. The p variable becoming zero (because both $p$ and $q$ can get into the product). It becomes increasingly more likely as we decrease size of the inputs or increase the constant M. In this case, we need to either restart the process or (better) roll back the last $M$ iterations and perform the trials one by one. Overflows in the Montgomery multiplication. Our current implementation is pretty loose with them, and if $n$ is large, we need to add more x \u0026gt; mod ? x - mod : x kind of statements to deal with overflows. Larger numbers. These issues become less important if we exclude small numbers and numbers with small prime factors using the algorithms we\u0026rsquo;ve implemented before. In general, the optimal approach should depend on the size of the numbers:\nSmaller than $2^{16}$: use a lookup table; Smaller than $2^{32}$: use a list of precomputed primes with a fast divisibility check; Smaller than $2^{64}$ or so: use Pollard\u0026rsquo;s rho algorithm with Montgomery multiplication; Smaller than $10^{50}$: switch to Lenstra elliptic curve factorization; Smaller than $10^{100}$: switch to Quadratic Sieve; Larger than $10^{100}$: switch to General Number Field Sieve. The last three approaches are very different from what we\u0026rsquo;ve been doing and require much more advanced number theory, and they deserve an article (or a full-length university course) of their own.\n","id":34,"path":"/hugo-page/hpc/algorithms/factorization/","title":"Integer Factorization"},{"content":"Starting at some level of the hierarchy, the cache becomes shared between different cores. This reduces the total die area and lets you add more cores on a single chip but also poses some \u0026ldquo;noisy neighbor\u0026rdquo; problems as it limits the effective cache size and bandwidth available to a single execution thread.\nOn most CPUs, only the last layer of cache is shared, and not always in a uniform manner. On my machine, there are 8 physical cores, and the size of the L3 cache is 8M, but it is split into two halves: two groups of 4 cores have access to their own 4M region of the L3 cache, and not all of it.\nThere are even more complex topologies, where accessing certain regions of memory takes non-constant time, different for each core (which is sometimes unintended). Such architectural feature is called non-uniform memory access (NUMA), and it is the case for multi-socket systems that have several separate CPU chips installed.\nOn Linux, the topology of the memory system can be retrieved with lstopo:\nCache hierarchy of my Ryzen 7 4700U generated by lstopo This has some important implications for parallel algorithms: the performance of multi-threaded memory accesses depends on which cores are running which execution threads. To demonstrate this, we will run the bandwidth benchmarks in parallel.\n#CPU AffinityInstead of modifying the source code to run on multiple threads, we can simply run multiple identical processes with GNU parallel. To control which cores are executing them, we set their processor affinity with taskset. This combined command runs 4 processes that can run on the first 4 cores of the CPU:\nparallel taskset -c 0,1,2,3 ./run ::: {0..3} Here is what we get when we change the number of processes running simultaneously:\nYou can now see that the performance decreases with more processes when the array exceeds the L2 cache (which is private to each core), as the cores start competing for the shared L3 cached and the RAM.\nWe specifically set all processes to run on the first 4 cores because they have a unified L3 cache. If some of the processes were to be scheduled on the other half of the cores, there would be less contention for the L3 cache. The operating system doesn\u0026rsquo;t monitor such activities — what a process does is its own private business — so by default, it assigns threads to cores arbitrarily during execution, without caring about cache affinity and only taking into account the core load.\nLet\u0026rsquo;s run another benchmark, but now with pinning the processes to different 4-core groups that don\u0026rsquo;t share L3 cache:\nparallel taskset -c 0,1 ./run ::: {0..1} # L3 cache sharing parallel taskset -c 0,4 ./run ::: {0..1} # no L3 cache sharing It performs better — as if there were twice as much L3 cache and RAM bandwidth available:\nThese issues are especially tricky when benchmarking and are a huge source of noise when timing parallel applications.\n#Saturating BandwidthWhen looking at the RAM section of the first graph, it may seem that with more cores, the per-process throughput goes ½, ⅓, ¼, and so on, and the total bandwidth remains constant. But this isn\u0026rsquo;t quite true: the contention hurts, but a single CPU core usually can\u0026rsquo;t saturate all of the RAM bandwidth.\nIf we plot it more carefully, we see that the total bandwidth actually increases with the number of cores — although not proportionally, and eventually approaches its theoretical maximum of ~42.4 GB/s:\nNote that we still specify processor affinity: the $k$-threaded run uses the first $k$ cores. This is why we have such a huge performance increase when switching from 4 cores to 5: you can have more RAM bandwidth if the requests go through separate L3 caches.\nIn general, to achieve maximum bandwidth, you should always split the threads of an application symmetrically.\n","id":35,"path":"/hugo-page/hpc/cpu-cache/sharing/","title":"Memory Sharing"},{"content":"Reaching the maximum possible precision is rarely required from a practical algorithm. In real-world data, modeling and measurement errors are usually several orders of magnitude larger than the errors that come from rounding floating-point numbers and such, and we are often perfectly happy with picking an approximate method that trades off precision for speed.\nIn this section, we introduce one of the most important building blocks in such approximate, numerical algorithms: Newton\u0026rsquo;s method.\n#Newton\u0026rsquo;s MethodNewton\u0026rsquo;s method is a simple yet very powerful algorithm for finding approximate roots of real-valued functions, that is, the solutions to the following generic equation:\n$$ f(x) = 0 $$\nThe only thing assumed about the function $f$ is that at least one root exists and that $f(x)$ is continuous and differentiable on the search interval. There are also some boring corner cases, but they almost never occur in practice, so we will just informally say that the function is \u0026ldquo;good.\u0026rdquo;\nThe main idea of the algorithm is to start with some initial approximation $x_0$ and then iteratively improve it by drawing the tangent to the graph of the function at $x = x_i$ and setting the next approximation $x_{i+1}$ equal to the $x$-coordinate of its intersection with the $x$-axis. The intuition is that if the function $f$ is \u0026ldquo;good\u0026rdquo; and $x_i$ is already close enough to the root, then $x_{i+1}$ will be even closer.\nTo obtain the point of intersection for $x_n$, we need to equal its tangent line function to zero:\n$$ 0 = f(x_i) + (x_{i+1} - x_i) f\u0026rsquo;(x_i) $$\nfrom which we derive\n$$ x_{i+1} = x_i - \\frac{f(x_i)}{f\u0026rsquo;(x_i)} $$\nNewton\u0026rsquo;s method is very important: it is the basis of a wide range of optimization solvers in science and engineering.\n#Square RootAs a simple example, let\u0026rsquo;s derive the algorithm for the problem of finding square roots:\n$$ x = \\sqrt n \\iff x^2 = n \\iff f(x) = x^2 - n = 0 $$\nIf we substitute $f(x) = x^2 - n$ into the generic formula above, we can obtain the following update rule:\n$$ x_{i+1} = x_i - \\frac{x_i^2 - n}{2 x_i} = \\frac{x_i + n / x_i}{2} $$\nIn practice we also want to stop it as soon as it is close enough to the right answer, which we can simply check after each iteration:\nconst double EPS = 1e-9; double sqrt(double n) { double x = 1; while (abs(x * x - n) \u0026gt; eps) x = (x + n / x) / 2; return x; } The algorithm converges for many functions, although it does so reliably and provably only for a certain subset of them (e.g., convex functions). Another question is how fast the convergence is, if it occurs.\n#Rate of ConvergenceLet\u0026rsquo;s run a few iterations of Newton\u0026rsquo;s method to find the square root of $2$, starting with $x_0 = 1$, and check how many digits it got correct after each iteration:\n1.0000000000000000000000000000000000000000000000000000000000000 1.5000000000000000000000000000000000000000000000000000000000000 1.4166666666666666666666666666666666666666666666666666666666675 1.4142156862745098039215686274509803921568627450980392156862745 1.4142135623746899106262955788901349101165596221157440445849057 1.4142135623730950488016896235025302436149819257761974284982890 1.4142135623730950488016887242096980785696718753772340015610125 1.4142135623730950488016887242096980785696718753769480731766796 Looking carefully, we can see that the number of accurate digits approximately doubles on each iteration. This fantastic convergence rate is not a coincidence.\nTo analyze convergence rate quantitatively, we need to consider a small relative error $\\delta_i$ on the $i$-th iteration and determine how much smaller the error $\\delta_{i+1}$ is on the next iteration:\n$$ |\\delta_i| = \\frac{|x_n - x|}{x} $$\nWe can express $x_i$ as $x \\cdot (1 + \\delta_i)$. Plugging it into the Newton iteration formula and dividing both sides by $x$ we get\n$$ 1 + \\delta_{i+1} = \\frac{1}{2} (1 + \\delta_i + \\frac{1}{1 + \\delta_i}) = \\frac{1}{2} (1 + \\delta_i + 1 - \\delta_i + \\delta_i^2 + o(\\delta_i^2)) = 1 + \\frac{\\delta_i^2}{2} + o(\\delta_i^2) $$\nHere we have Taylor-expanded $(1 + \\delta_i)^{-1}$ at $0$, using the assumption that the error $d_i$ is small (since the sequence converges, $d_i \\ll 1$ for sufficiently large $n$).\nRearranging for $\\delta_{i+1}$, we obtain\n$$ \\delta_{i+1} = \\frac{\\delta_i^2}{2} + o(\\delta_i^2) $$\nwhich means that the error roughly squares (and halves) on each iteration once we are close to the solution. Since the logarithm $(- \\log_{10} \\delta_i)$ is roughly the number of accurate significant digits in the answer $x_i$, squaring the relative error corresponds precisely to doubling the number of significant digits that we had observed.\nThis is known as quadratic convergence, and in fact, this is not limited to finding square roots. With detailed proof being left as an exercise to the reader, it can be shown that, in general\n$$ |\\delta_{i+1}| = \\frac{|f\u0026rsquo;\u0026rsquo;(x_i)|}{2 \\cdot |f\u0026rsquo;(x_n)|} \\cdot \\delta_i^2 $$\nwhich results in at least quadratic convergence under a few additional assumptions, namely $f\u0026rsquo;(x)$ not being equal to $0$ and $f\u0026rsquo;\u0026rsquo;(x)$ being continuous.\n#Further ReadingIntroduction to numerical methods at MIT.\n","id":36,"path":"/hugo-page/hpc/arithmetic/newton/","title":"Newton's Method"},{"content":"The last approach to profiling (or rather a group of them) is not to gather the data by actually running the program but to analyze what should happen by simulating it with specialized tools.\nThere are many subcategories of such profilers, differing in which aspect of computation is simulated. In this article, we are going to focus on caching and branch prediction, and use Cachegrind for that, which is a profiling-oriented part of Valgrind, a well-established tool for memory leak detection and memory debugging in general.\n#Profiling with CachegrindCachegrind essentially inspects the binary for \u0026ldquo;interesting\u0026rdquo; instructions — that perform memory reads / writes and conditional / indirect jumps — and replaces them with code that simulates corresponding hardware operations using software data structures. It therefore doesn\u0026rsquo;t need access to the source code and can work with already compiled programs, and can be run on any program like this:\nvalgrind --tool=cachegrind --branch-sim=yes ./run # also simulate branch prediction ^ ^ any command, not necessarily one process It instruments all involved binaries, runs them, and outputs a summary similar to perf stat:\nI refs: 483,664,426 I1 misses: 1,858 LLi misses: 1,788 I1 miss rate: 0.00% LLi miss rate: 0.00% D refs: 115,204,359 (88,016,970 rd + 27,187,389 wr) D1 misses: 9,722,664 ( 9,656,463 rd + 66,201 wr) LLd misses: 72,587 ( 8,496 rd + 64,091 wr) D1 miss rate: 8.4% ( 11.0% + 0.2% ) LLd miss rate: 0.1% ( 0.0% + 0.2% ) LL refs: 9,724,522 ( 9,658,321 rd + 66,201 wr) LL misses: 74,375 ( 10,284 rd + 64,091 wr) LL miss rate: 0.0% ( 0.0% + 0.2% ) Branches: 90,575,071 (88,569,738 cond + 2,005,333 ind) Mispredicts: 19,922,564 (19,921,919 cond + 645 ind) Mispred rate: 22.0% ( 22.5% + 0.0% ) We\u0026rsquo;ve fed Cachegrind exactly the same example code as in the previous section: we create an array of a million random integers, sort it, and then perform a million binary searches on it. Cachegrind shows roughly the same numbers as perf does, except that that perf\u0026rsquo;s measured numbers of memory reads and branches are slightly inflated due to speculative execution: they really happen in hardware and thus increment hardware counters, but are discarded and don\u0026rsquo;t affect actual performance, and thus ignored in the simulation.\nCachegrind only models the first (D1 for data, I1 for instructions) and the last (LL, unified) levels of cache, the characteristics of which are inferred from the system. It doesn\u0026rsquo;t limit you in any way as you can also set them from the command line, e g., to model the L2 cache: --LL=\u0026lt;size\u0026gt;,\u0026lt;associativity\u0026gt;,\u0026lt;line size\u0026gt;.\nIt seems like it only slowed down our program so far and hasn\u0026rsquo;t provided us any information that perf stat couldn\u0026rsquo;t. To get more out of it than just the summary info, we can inspect a special file with profiling info, which it dumps by default in the same directory named as cachegrind.out.\u0026lt;pid\u0026gt;. It is human-readable, but is expected to be read via the cg_annotate command:\ncg_annotate cachegrind.out.4159404 --show=Dr,D1mr,DLmr,Bc,Bcm # ^ we are only interested in data reads and branches First it shows the parameters that were used during the run, including the characteristics of the cache system:\nI1 cache: 32768 B, 64 B, 8-way associative D1 cache: 32768 B, 64 B, 8-way associative LL cache: 8388608 B, 64 B, direct-mapped It didn\u0026rsquo;t get the L3 cache quite right: it is not unified (8M in total, but a single core only sees 4M) and also 16-way associative, but we will ignore that for now.\nNext, it outputs a per-function summary similar to perf report:\nDr D1mr DLmr Bc Bcm file:function -------------------------------------------------------------------------------- 19,951,476 8,985,458 3 41,902,938 11,005,530 ???:query() 24,832,125 585,982 65 24,712,356 7,689,480 ???:void std::__introsort_loop\u0026lt;...\u0026gt; 16,000,000 60 3 9,935,484 129,044 ???:random_r 18,000,000 2 1 6,000,000 1 ???:random 4,690,248 61,999 17 5,690,241 1,081,230 ???:setup() 2,000,000 0 0 0 0 ???:rand You can see there are a lot of branch mispredicts in the sorting stage, and also a lot of both L1 cache misses and branch mispredicts during binary searching. We couldn\u0026rsquo;t get this information with perf — it would only tell use these counts for the whole program.\nAnother great feature that Cachegrind has is the line-by-line annotation of source code. For that, you need to compile the program with debug information (-g) and either explicitly tell cg_annotate which source files to annotate or just pass the --auto=yes option so that it annotates everything it can reach (including the standard library source code).\nThe whole source-to-analysis process would therefore go like this:\ng++ -O3 -g sort-and-search.cc -o run valgrind --tool=cachegrind --branch-sim=yes --cachegrind-out-file=cachegrind.out ./run cg_annotate cachegrind.out --auto=yes --show=Dr,D1mr,DLmr,Bc,Bcm Since the glibc implementations are not the most readable, for exposition purposes, we replace lower_bound with our own binary search, which will be annotated like this:\nDr D1mr DLmr Bc Bcm . . . . . int binary_search(int x) { 0 0 0 0 0 int l = 0, r = n - 1; 0 0 0 20,951,468 1,031,609 while (l \u0026lt; r) { 0 0 0 0 0 int m = (l + r) / 2; 19,951,468 8,991,917 63 19,951,468 9,973,904 if (a[m] \u0026gt;= x) . . . . . r = m; . . . . . else 0 0 0 0 0 l = m + 1; . . . . . } . . . . . return l; . . . . . } Unfortunately, Cachegrind only tracks memory accesses and branches. When the bottleneck is caused by something else, we need other simulation tools.\n","id":37,"path":"/hugo-page/hpc/profiling/simulation/","title":"Program Simulation"},{"content":"Reduction (also known as folding in functional programming) is the action of computing the value of some associative and commutative operation (i.e., $(a \\circ b) \\circ c = a \\circ (b \\circ c)$ and $a \\circ b = b \\circ a$) over a range of arbitrary elements.\nThe simplest example of reduction is calculating the sum an array:\nint sum(int *a, int n) { int s = 0; for (int i = 0; i \u0026lt; n; i++) s += a[i]; return s; } The naive approach is not so straightforward to vectorize, because the state of the loop (sum $s$ on the current prefix) depends on the previous iteration. The way to overcome this is to split a single scalar accumulator $s$ into 8 separate ones, so that $s_i$ would contain the sum of every 8th element of the original array, shifted by $i$:\n$$ s_i = \\sum_{j=0}^{n / 8} a_{8 \\cdot j + i } $$\nIf we store these 8 accumulators in a single 256-bit vector, we can update them all at once by adding consecutive 8-element segments of the array. With vector extensions, this is straightforward:\nint sum_simd(v8si *a, int n) { // ^ you can just cast a pointer normally, like with any other pointer type v8si s = {0}; for (int i = 0; i \u0026lt; n / 8; i++) s += a[i]; int res = 0; // sum 8 accumulators into one for (int i = 0; i \u0026lt; 8; i++) res += s[i]; // add the remainder of a for (int i = n / 8 * 8; i \u0026lt; n; i++) res += a[i]; return res; } You can use this approach for other reductions, such as for finding the minimum or the xor-sum of an array.\n#Instruction-Level ParallelismOur implementation matches what the compiler produces automatically, but it is actually suboptimal: when we use just one accumulator, we have to wait one cycle between the loop iterations for a vector addition to complete, while the throughput of corresponding instruction is 2 on this microarchitecture.\nIf we again divide the array in $B \\geq 2$ parts and use a separate accumulator for each, we can saturate the throughput of vector addition and increase the performance twofold:\nconst int B = 2; // how many vector accumulators to use int sum_simd(v8si *a, int n) { v8si b[B] = {0}; for (int i = 0; i + (B - 1) \u0026lt; n / 8; i += B) for (int j = 0; j \u0026lt; B; j++) b[j] += a[i + j]; // sum all vector accumulators into one for (int i = 1; i \u0026lt; B; i++) b[0] += b[i]; int s = 0; // sum 8 scalar accumulators into one for (int i = 0; i \u0026lt; 8; i++) s += b[0][i]; // add the remainder of a for (int i = n / (8 * B) * (8 * B); i \u0026lt; n; i++) s += a[i]; return s; } If you have more than 2 relevant execution ports, you can increase the B constant accordingly, but the $n$-fold performance increase will only apply to arrays that fit into L1 cache — memory bandwidth will be the bottleneck for anything larger.\n#Horizontal SummationThe part where we sum up the 8 accumulators stored in a vector register into a single scalar to get the total sum is called \u0026ldquo;horizontal summation.\u0026rdquo;\nAlthough extracting and adding every scalar one by one only takes a constant number of cycles, it can be computed slightly faster using a special instruction that adds together pairs of adjacent elements in a register.\nHorizontal summation in SSE/AVX. Note how the output is stored: the (a b a b) interleaving is common for reducing operations Since it is a very specific operation, it can only be done with SIMD intrinsics — although the compiler probably emits roughly the same procedure for the scalar code anyway:\nint hsum(__m256i x) { __m128i l = _mm256_extracti128_si256(x, 0); __m128i h = _mm256_extracti128_si256(x, 1); l = _mm_add_epi32(l, h); l = _mm_hadd_epi32(l, l); return _mm_extract_epi32(l, 0) + _mm_extract_epi32(l, 1); } There are other similar instructions, e.g., for integer multiplication or calculating absolute differences between adjacent elements (used in image processing).\nThere is also one specific instruction, _mm_minpos_epu16, that calculates the horizontal minimum and its index among eight 16-bit integers. This is the only horizontal reduction that works in one go: all others are computed in multiple steps.\n","id":38,"path":"/hugo-page/hpc/simd/reduction/","title":"Reductions"},{"content":"In the previous article, we designed and implemented static B-trees to speed up binary searching in sorted arrays. In its last section, we briefly discussed how to make them dynamic back while retaining the performance gains from SIMD and validated our predictions by adding and following explicit pointers in the internal nodes of the S+ tree.\nIn this article, we follow up on that proposition and design a minimally functional search tree for integer keys, achieving up to 18x/8x speedup over std::set and up to 7x/2x speedup over absl::btree for lower_bound and insert queries, respectively — with yet ample room for improvement.\nThe memory overhead of the structure is around 30% for 32-bit integers, and the final implementation is under 150 lines of C++. It can be easily generalized to other arithmetic types and small/fixed-length strings such as hashes, country codes, and stock symbols.\n#B− TreeInstead of making small incremental improvements like we usually do in other case studies, in this article, we will implement just one data structure that we name B− tree, which is based on the B+ tree, with a few minor differences:\nNodes in the B− tree do not store pointers or any metadata except for the pointers to internal node children (while the B+ tree leaf nodes store a pointer to the next leaf node). This lets us perfectly place the keys in the leaf nodes on cache lines. We define key $i$ to be the maximum key in the subtree of the child $i$ instead of the minimum key in the subtree of the child $(i + 1)$. This lets us not fetch any other nodes after we reach a leaf (in the B+ tree, all keys in the leaf node may be less than the search key, so we need to go to the next leaf node to fetch its first element). We also use a node size of $B=32$, which is smaller than typical. The reason why it is not $16$, which was optimal for the S+ tree, is because we have the additional overhead associated with fetching the pointer, and the benefit of reducing the tree height by ~20% outweighs the cost of processing twice the elements per node, and also because it improves the running time of the insert query that needs to perform a costly node split every $\\frac{B}{2}$ insertions on average.\n#Memory LayoutAlthough this is probably not the best approach in terms of software engineering, we will simply store the entire tree in a large pre-allocated array, without discriminating between leaves and internal nodes:\nconst int R = 1e8; alignas(64) int tree[R]; We also pre-fill this array with infinities to simplify the implementation:\nfor (int i = 0; i \u0026lt; R; i++) tree[i] = INT_MAX; (In general, it is technically cheating to compare against std::set or other structures that use new under the hood, but memory allocation and initialization are not the bottlenecks here, so this does not significantly affect the evaluation.)\nBoth nodes types store their keys sequentially in sorted order and are identified by the index of its first key in the array:\nA leaf node has up to $(B - 1)$ keys but is padded to $B$ elements with infinities. An internal node has up to $(B - 2)$ keys padded to $B$ elements and up to $(B - 1)$ indices of its child nodes, also padded to $B$ elements. These design decisions are not arbitrary:\nThe padding ensures that leaf nodes occupy exactly 2 cache lines and internal nodes occupy exactly 4 cache lines. We specifically use indices instead of pointers to save cache space and make moving them with SIMD faster.\n(We will use \u0026ldquo;pointer\u0026rdquo; and \u0026ldquo;index\u0026rdquo; interchangeably from now on.) We store indices right after the keys even though they are stored in separate cache lines because we have reasons. We intentionally \u0026ldquo;waste\u0026rdquo; one array cell in leaf nodes and $2+1=3$ cells in internal nodes because we need it to store temporary results during a node split. Initially, we only have one empty leaf node as the root:\nconst int B = 32; int root = 0; // where the keys of the root start int n_tree = B; // number of allocated array cells int H = 1; // current tree height To \u0026ldquo;allocate\u0026rdquo; a new node, we simply increase n_tree by $B$ if it is a leaf node or by $2 B$ if it is an internal node.\nSince new nodes can only be created by splitting a full node, each node except for the root will be at least half full. This implies that we need between 4 and 8 bytes per integer element (the internal nodes will contribute $\\frac{1}{16}$-th or so to that number), the former being the case when the inserts are sequential, and the latter being the case when the input is adversarial. When the queries are uniformly distributed, the nodes are ~75% full on average, projecting to ~5.2 bytes per element.\nB-trees are very memory-efficient compared to the pointer-based binary trees. For example, std::set needs at least three pointers (the left child, the right child, and the parent), alone costing $3 \\times 8 = 24$ bytes, plus at least another $8$ bytes to store the key and the meta-information due to structure padding.\n#SearchingIt is a very common scenario when \u0026gt;90% of operations are lookups, and even if this is not the case, every other tree operation typically begins with locating a key anyway, so we will start with implementing and optimizing the searches.\nWhen we implemented S-trees, we ended up storing the keys in permuted order due to the intricacies of how the blending/packs instructions work. For the dynamic tree problem, storing the keys in permuted order would make inserts much harder to implement, so we will change the approach instead.\nAn alternative way to think about finding the would-be position of the element x in a sorted array is not \u0026ldquo;the index of the first element that is not less than x\u0026rdquo; but \u0026ldquo;the number of elements that are less than x.\u0026rdquo; This observation generates the following idea: compare the keys against x, aggregate the vector masks into a 32-bit mask (where each bit can correspond to any element as long as the mapping is bijective), and then call popcnt on it, returning the number of elements less than x.\nThis trick lets us perform the local search efficiently and without requiring any shuffling:\ntypedef __m256i reg; reg cmp(reg x, int *node) { reg y = _mm256_load_si256((reg*) node); return _mm256_cmpgt_epi32(x, y); } // returns how many keys are less than x unsigned rank32(reg x, int *node) { reg m1 = cmp(x, node); reg m2 = cmp(x, node + 8); reg m3 = cmp(x, node + 16); reg m4 = cmp(x, node + 24); // take lower 16 bits from m1/m3 and higher 16 bits from m2/m4 m1 = _mm256_blend_epi16(m1, m2, 0b01010101); m3 = _mm256_blend_epi16(m3, m4, 0b01010101); m1 = _mm256_packs_epi16(m1, m3); // can also use blendv here, but packs is simpler unsigned mask = _mm256_movemask_epi8(m1); return __builtin_popcount(mask); } Note that, because of this procedure, we have to pad the \u0026ldquo;key area\u0026rdquo; with infinities, which prevents us from storing metadata in the vacated cells (unless we are also willing to spend a few cycles to mask it out when loading a SIMD lane).\nNow, to implement lower_bound, we can descend the tree just like we did in the S+ tree, but fetching the pointer after we compute the child number:\nint lower_bound(int _x) { unsigned k = root; reg x = _mm256_set1_epi32(_x); for (int h = 0; h \u0026lt; H - 1; h++) { unsigned i = rank32(x, \u0026amp;tree[k]); k = tree[k + B + i]; } unsigned i = rank32(x, \u0026amp;tree[k]); return tree[k + i]; } Implementing search is easy, and it doesn\u0026rsquo;t introduce much overhead. The hard part is implementing insertion.\n#InsertionOn the one side, correctly implementing insertion takes a lot of code, but on the other, most of that code is executed very infrequently, so we don\u0026rsquo;t have to care about its performance that much. Most often, all we need to do is to reach the leaf node (which we\u0026rsquo;ve already figured out how to do) and then insert a new key into it, moving some suffix of the keys one position to the right. Occasionally, we also need to split the node and/or update some ancestors, but this is relatively rare, so let\u0026rsquo;s focus on the most common execution path first.\nTo insert a key into an array of $(B - 1)$ sorted elements, we can load them in vector registers and then mask-store them one position to the right using a precomputed mask that tells which elements need to be written for a given i:\nstruct Precalc { alignas(64) int mask[B][B]; constexpr Precalc() : mask{} { for (int i = 0; i \u0026lt; B; i++) for (int j = i; j \u0026lt; B - 1; j++) // everything from i to B - 2 inclusive needs to be moved mask[i][j] = -1; } }; constexpr Precalc P; void insert(int *node, int i, int x) { // need to iterate right-to-left to not overwrite the first element of the next lane for (int j = B - 8; j \u0026gt;= 0; j -= 8) { // load the keys reg t = _mm256_load_si256((reg*) \u0026amp;node[j]); // load the corresponding mask reg mask = _mm256_load_si256((reg*) \u0026amp;P.mask[i][j]); // mask-write them one position to the right _mm256_maskstore_epi32(\u0026amp;node[j + 1], mask, t); } node[i] = x; // finally, write the element itself } This constexpr magic is the only C++ feature we use.\nThere are other ways to do it, some possibly more efficient, but we are going to stop there for now.\nWhen we split a node, we need to move half of the keys to another node, so let\u0026rsquo;s write another primitive that does it:\n// move the second half of a node and fill it with infinities void move(int *from, int *to) { const reg infs = _mm256_set1_epi32(INT_MAX); for (int i = 0; i \u0026lt; B / 2; i += 8) { reg t = _mm256_load_si256((reg*) \u0026amp;from[B / 2 + i]); _mm256_store_si256((reg*) \u0026amp;to[i], t); _mm256_store_si256((reg*) \u0026amp;from[B / 2 + i], infs); } } With these two vector functions implemented, we can now very carefully implement insertion:\nvoid insert(int _x) { // the beginning of the procedure is the same as in lower_bound, // except that we save the path in case we need to update some of our ancestors unsigned sk[10], si[10]; // k and i on each iteration // ^------^ We assume that the tree height does not exceed 10 // (which would require at least 16^10 elements) unsigned k = root; reg x = _mm256_set1_epi32(_x); for (int h = 0; h \u0026lt; H - 1; h++) { unsigned i = rank32(x, \u0026amp;tree[k]); // optionally update the key i right away tree[k + i] = (_x \u0026gt; tree[k + i] ? _x : tree[k + i]); sk[h] = k, si[h] = i; // and save the path k = tree[k + B + i]; } unsigned i = rank32(x, \u0026amp;tree[k]); // we can start computing the is-full check before insertion completes bool filled = (tree[k + B - 2] != INT_MAX); insert(tree + k, i, _x); if (filled) { // the node needs to be split, so we create a new leaf node move(tree + k, tree + n_tree); int v = tree[k + B / 2 - 1]; // new key to be inserted int p = n_tree; // pointer to the newly created node n_tree += B; for (int h = H - 2; h \u0026gt;= 0; h--) { // ascend and repeat until we reach the root or find a the node is not split k = sk[h], i = si[h]; filled = (tree[k + B - 3] != INT_MAX); // the node already has a correct key (the right one) // and a correct pointer (the left one) insert(tree + k, i, v); insert(tree + k + B, i + 1, p); if (!filled) return; // we\u0026#39;re done // create a new internal node move(tree + k, tree + n_tree); // move keys move(tree + k + B, tree + n_tree + B); // move pointers v = tree[k + B / 2 - 1]; tree[k + B / 2 - 1] = INT_MAX; p = n_tree; n_tree += 2 * B; } // if reach here, this means we\u0026#39;ve reached the root, // and it was split into two, so we need a new root tree[n_tree] = v; tree[n_tree + B] = root; tree[n_tree + B + 1] = p; root = n_tree; n_tree += 2 * B; H++; } } There are many inefficiencies, but, luckily, the body of if (filled) is executed very infrequently — approximately every $\\frac{B}{2}$ insertions — and the insertion performance is not really our top priority, so we will just leave it there.\n#EvaluationWe have only implemented insert and lower_bound, so this is what we will measure.\nWe want the evaluation to take a reasonable time, so our benchmark is a loop that alternates between two steps:\nIncrease the structure size from $1.17^k$ to $1.17^{k+1}$ using individual inserts and measure the time it took. Perform $10^6$ random lower_bound queries and measure the time it took. We start at the size $10^4$ and end at $10^7$, for around $50$ data points in total. We generate the data for both query types uniformly in the $[0, 2^{30})$ range and independently between the stages. Since the data generation process allows for repeated keys, we compared against std::multiset and absl::btree_multiset1, although we still refer to them as std::set and absl::btree for brevity. We also enable hugepages on the system level for all three runs.\nThe performance of the B− tree matches what we originally predicted — at least for the lookups:\nThe relative speedup varies with the structure size — 7-18x/3-8x over STL and 3-7x/1.5-2x over Abseil:\nInsertions are only 1.5-2 faster than for absl::btree, which uses scalar code to do everything. My best guess why insertions are that slow is due to data dependency: since the tree nodes may change, the CPU can\u0026rsquo;t start processing the next query before the previous one finishes (the true latency of both queries is roughly equal and ~3x of the reciprocal throughput of lower_bound).\nWhen the structure size is small, the reciprocal throughput of lower_bound increases in discrete steps: it starts with 3.5ns when there is only the root to visit, then grows to 6.5ns (two nodes), and then to 12ns (three nodes), and then hits the L2 cache (not shown on the graphs) and starts increasing more smoothly, but still with noticeable spikes when the tree height increases.\nInterestingly, B− tree outperforms absl::btree even when it only stores a single key: it takes around 5ns stalling on branch misprediction, while (the search in) the B− tree is entirely branchless.\n#Possible OptimizationsIn our previous endeavors in data structure optimization, it helped a lot to make as many variables as possible compile-time constants: the compiler can hardcode these constants into the machine code, simplify the arithmetic, unroll all the loops, and do many other nice things for us.\nThis would not be a problem at all if our tree were of constant height, but it is not. It is largely constant, though: the height rarely changes, and in fact, under the constraints of the benchmark, the maximum height was only 6.\nWhat we can do is pre-compile the insert and lower_bound functions for several different compile-time constant heights and switch between them as the tree grows. The idiomatic C++ way is to use virtual functions, but I prefer to be explicit and use raw function pointers like this:\nvoid (*insert_ptr)(int); int (*lower_bound_ptr)(int); void insert(int x) { insert_ptr(x); } int lower_bound(int x) { return lower_bound_ptr(x); } We now define template functions that have the tree height as a parameter, and in the grow-tree block inside the insert function, we change the pointers as the tree grows:\ntemplate \u0026lt;int H\u0026gt; void insert_impl(int _x) { // ... } template \u0026lt;int H\u0026gt; void insert_impl(int _x) { // ... if (/* tree grows */) { // ... insert_ptr = \u0026amp;insert_impl\u0026lt;H + 1\u0026gt;; lower_bound_ptr = \u0026amp;lower_bound_impl\u0026lt;H + 1\u0026gt;; } } template \u0026lt;\u0026gt; void insert_impl\u0026lt;10\u0026gt;(int x) { std::cerr \u0026lt;\u0026lt; \u0026#34;This depth was not supposed to be reached\u0026#34; \u0026lt;\u0026lt; std::endl; exit(1); } I tried but could not get any performance improvement with this, but I still have high hope for this approach because the compiler can (theoretically) remove sk and si, completely removing any temporary storage and only reading and computing everything once, greatly optimizing the insert procedure.\nInsertion can also probably be optimized by using a larger block size as node splits would become rare, but this comes at the cost of slower lookups. We could also try different node sizes for different layers: leaves should probably be larger than the internal nodes.\nAnother idea is to move extra keys on insert to a sibling node, delaying the node split as long as possible.\nOne such particular modification is known as the B* tree. It moves the last key to the next node if the current one is full, and when both nodes become full, it jointly splits both of them, producing three nodes that are ⅔ full. This reduces the memory overhead (the nodes will be ⅚ full on average) and increases the fanout factor, reducing the height, which helps all operations.\nThis technique can even be extended to, say, three-to-four splits, although further generalization would come at the cost of a slower insert.\nAnd yet another idea is to get rid of (some) pointers. For example, for large trees, we can probably afford a small S+ tree for $16 \\cdot 17$ or so elements as the root, which we rebuild from scratch on each infrequent occasion when it changes. You can\u0026rsquo;t extend it to the whole tree, unfortunately: I believe there is a paper somewhere saying that we can\u0026rsquo;t turn a dynamic structure fully implicit without also having to do $\\Omega(\\sqrt n)$ operations per query.\nWe could also try some non-tree data structures, such as the skip list. There has even been a successful attempt to vectorize it — although the speedup was not that impressive. I have low hope that skip-list, in particular, can be improved, although it may achieve a higher total throughput in the concurrent setting.\n#Other OperationsTo delete a key, we can similarly locate and remove it from a node with the same mask-store trick. After that, if the node is at least half-full, we\u0026rsquo;re done. Otherwise, we try to borrow a key from the next sibling. If the sibling has more than $\\frac{B}{2}$ keys, we append its first key and shift its keys one to the left. Otherwise, both the current node and the next node have less than $\\frac{B}{2}$ keys, so we can merge them, after which we go to the parent and iteratively delete a key there.\nAnother thing we may want to implement is iteration. Bulk-loading each key from l to r is a very common pattern — for example, in SELECT abc ORDER BY xyz type of queries in databases — and B+ trees usually store pointers to the next node in the data layer to allow for this type of rapid iteration. In B− trees, as we\u0026rsquo;re using a much smaller node size, we can experience pointer chasing problems if we do this. Going to the parent and reading all its $B$ pointers is probably faster as it negates this problem. Therefore, a stack of ancestors (the sk and si arrays we used in insert) can serve as an iterator and may even be better than separately storing pointers in nodes.\nWe can easily implement almost everything that std::set does, but the B− tree, like any other B-tree, is very unlikely to become a drop-in replacement to std::set due to the requirement of pointer stability: a pointer to an element should remain valid unless the element is deleted, which is hard to achieve when we split and merge nodes all the time. This is a major problem not only for search trees but most data structures in general: having both pointer stability and high performance at the same time is next to impossible.\n#AcknowledgementsThanks to Danila Kutenin from Google for meaningful discussions of applicability and the usage of B-trees in Abseil.\nIf you also think that only comparing with Abseil\u0026rsquo;s B-tree is not convincing enough, feel free to add your favorite search tree to the benchmark.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":39,"path":"/hugo-page/hpc/data-structures/b-tree/","title":"Search Trees"},{"content":" Most compiler optimizations enabled by -O2 and -O3 are guaranteed to either improve or at least not seriously hurt performance. Those that aren\u0026rsquo;t included in -O3 are either not strictly standard-compliant, or highly circumstantial and require some additional input from the programmer to help decide whether using them is beneficial.\nLet\u0026rsquo;s discuss the most frequently used ones that we\u0026rsquo;ve also previously covered in this book.\n#Loop UnrollingLoop unrolling is disabled by default, unless the loop takes a small constant number of iterations known at compile time — in which case it will be replaced with a completely jump-free, repeated sequence of instructions. It can be enabled globally with the -funroll-loops flag, which will unroll all loops whose number of iterations can be determined at compile time or upon entry to the loop.\nYou can also use a pragma to target a specific loop:\n#pragma GCC unroll 4 for (int i = 0; i \u0026lt; n; i++) { // ... } Loop unrolling makes binary larger, and may or may not make it run faster. Don\u0026rsquo;t use it fanatically.\n#Function InliningInlining is best left for the compiler to decide, but you can influence it with inline keyword:\ninline int square(int x) { return x * x; } The hint may be ignored though if the compiler thinks that the potential performance gains are not worth it. You can force inlining by adding the always_inline attribute:\n#define FORCE_INLINE inline __attribute__((always_inline)) There is also the -finline-limit=n option which lets you set a specific threshold on the size of inlined functions (in terms of the number of instructions). Its Clang equivalent is -inline-threshold.\n#Likeliness of BranchesLikeliness of branches can be hinted by [[likely]] and [[unlikely]] attributes in if-s and switch-es:\nint factorial(int n) { if (n \u0026gt; 1) [[likely]] return n * factorial(n - 1); else [[unlikely]] return 1; } This is a new feature that only appeared in C++20. Before that, there were compiler-specific intrinsics similarly used to wrap condition expressions. The same example in older GCC:\nint factorial(int n) { if (__builtin_expect(n \u0026gt; 1, 1)) return n * factorial(n - 1); else return 1; } There are many other cases like this when you need to point the compiler in the right direction, but we will get to them later when they become more relevant.\n#Profile-Guided OptimizationAdding all this metadata to the source code is tedious. People already hate writing C++ even without having to do it.\nIt is also not always obvious whether certain optimizations are beneficial or not. To make a decision about branch reordering, function inlining, or loop unrolling, we need answers to questions like these:\nHow often is this branch taken? How often is this function called? What is the average number of iterations in this loop? Luckily for us, there is a way to provide this real-world information automatically.\nProfile-guided optimization (PGO, also called \u0026ldquo;pogo\u0026rdquo; because it\u0026rsquo;s easier and more fun to pronounce) is a technique that uses profiling data to improve performance beyond what can be achieved with just static analysis. In a nutshell, it involves adding timers and counters to the points of interest in the program, compiling and running it on real data, and then compiling it again, but this time supplying additional information from the test run.\nThe whole process is automated by modern compilers. For example, the -fprofile-generate flag will let GCC instrument the program with profiling code:\ng++ -fprofile-generate [other flags] source.cc -o binary After we run the program — preferably on input that is as representative of the real use case as possible — it will create a bunch of *.gcda files that contain log data for the test run, after which we can rebuild the program, but now adding the -fprofile-use flag:\ng++ -fprofile-use [other flags] source.cc -o binary It usually improves performance by 10-20% for large codebases, and for this reason it is commonly included in the build process of performance-critical projects. This is more reason to invest in solid benchmarking code.\n","id":40,"path":"/hugo-page/hpc/compilation/situational/","title":"Situational Optimizations"},{"content":"The main benefit of learning assembly language is not the ability to write programs in it, but the understanding of what is happening during the execution of compiled code and its performance implications.\nThere are rare cases where we really need to switch to handwritten assembly for maximal performance, but most of the time compilers are capable of producing near-optimal code all by themselves. When they do not, it is usually because the programmer knows more about the problem than what can be inferred from the source code but failed to communicate this extra information to the compiler.\nIn this chapter, we will discuss the intricacies of getting the compiler to do exactly what we want and gathering useful information that can guide further optimizations.\n","id":41,"path":"/hugo-page/hpc/compilation/","title":"Compilation"},{"content":"Now, let\u0026rsquo;s try to design some actually useful algorithms for the new external memory model. Our goal in this section is to slowly build up more complex things and eventually get to external sorting and its interesting applications.\nThe algorithm will be based on the standard merge sorting algorithm, so we need to derive its main primitive first.\n#MergeProblem. Given two sorted arrays $a$ and $b$ of lengths $N$ and $M$, produce a single sorted array $c$ of length $N + M$ containing all of their elements.\nThe standard two-pointer technique for merging sorted arrays looks like this:\nvoid merge(int *a, int *b, int *c, int n, int m) { int i = 0, j = 0; for (int k = 0; k \u0026lt; n + m; k++) { if (i \u0026lt; n \u0026amp;\u0026amp; (j == m || a[i] \u0026lt; b[j])) c[k] = a[i++]; else c[k] = b[j++]; } } In terms of memory operations, we just linearly read all elements of $a$ and $b$ and linearly write all elements of $c$. Since these reads and writes can be buffered, it works in $SCAN(N+M)$ I/O operations.\nSo far the examples have been simple, and their analysis doesn\u0026rsquo;t differ too much from the RAM model, except that we divide the final answer by the block size $B$. But here is a case where this is not so.\n$k$-way merging. Consider the modification of this algorithm where we need to merge not just two arrays, but $k$ arrays of total size $N$ — by likewise looking at $k$ values, choosing the minimum between them, writing it into $c$, and incrementing one of the iterators.\nIn the standard RAM model, the asymptotic complexity would be multiplied $k$, since we would need to perform $O(k)$ comparisons to fill each next element. But in the external memory model, since everything we do in-memory doesn\u0026rsquo;t cost us anything, its asymptotic complexity would not change as long as we can fit $(k+1)$ full blocks in memory, that is, if $k = O(\\frac{M}{B})$.\nRemember the $M \\gg B$ assumption when we introduced the computational model? If we have $M \\geq B^{1+ε}$ for $\\epsilon \u0026gt; 0$, then we can fit any sub-polynomial number of blocks in memory, certainly including $O(\\frac{M}{B})$. This condition is called tall cache assumption, and it is usually required in many other external memory algorithms.\n#Merge SortingThe \u0026ldquo;normal\u0026rdquo; complexity of the standard mergesort algorithm is $O(N \\log_2 N)$: on each of its $O(\\log_2 N)$ \u0026ldquo;layers,\u0026rdquo; the algorithms need to go through all $N$ elements in total and merge them in linear time.\nIn the external memory model, when we read a block of size $M$, we can sort its elements \u0026ldquo;for free,\u0026rdquo; since they are already in memory. This way we can split the arrays into $O(\\frac{N}{M})$ blocks of consecutive elements and sort them separately as the base step, and only then merge them.\nThis effectively means that, in terms of I/O operations, the first $O(\\log M)$ layers of mergesort are free, and there are only $O(\\log_2 \\frac{N}{M})$ non-zero-cost layers, each mergeable in $O(\\frac{N}{B})$ IOPS in total. This brings total I/O complexity to\n$$ O\\left(\\frac{N}{B} \\log_2 \\frac{N}{M}\\right) $$\nThis is quite fast. If we have 1GB of memory and 10GB of data, this essentially means that we need a little bit more than 3 times the effort than just reading the data to sort it. Interestingly enough, we can do better.\n#$k$-way MergesortHalf of a page ago we have learned that in the external memory model, we can merge $k$ arrays just as easily as two arrays — at the cost of reading them. Why don\u0026rsquo;t we apply this fact here?\nLet\u0026rsquo;s sort each block of size $M$ in-memory just as we did before, but during each merge stage, we will split sorted blocks not just in pairs to be merged, but take as many blocks we can fit into our memory during a $k$-way merge. This way the height of the merge tree would be greatly reduced, while each layer would still be done in $O(\\frac{N}{B})$ IOPS.\nHow many sorted arrays can we merge at once? Exactly $k = \\frac{M}{B}$, since we need memory for one block for each array. Since the total number of layers will be reduced to $\\log_{\\frac{M}{B}} \\frac{N}{M}$, the total complexity will be reduced to\n$$ SORT(N) \\stackrel{\\text{def}}{=} O\\left(\\frac{N}{B} \\log_{\\frac{M}{B}} \\frac{N}{M} \\right) $$\nNote that, in our example, we have 10GB of data, 1GB of memory, and the block size is around 1MB for HDD. This makes $\\frac{M}{B} = 1000$ and $\\frac{N}{M} = 10$, and so the logarithm is less than one (namely, $\\log_{1000} 10 = \\frac{1}{3}$). Of course, we can\u0026rsquo;t sort an array faster than reading it, so this analysis applies to the cases when we have a very large dataset, small memory, and/or large block sizes, which rarely happens in real life these days.\n#Practical ImplementationUnder more realistic constraints, instead of using $\\log_{\\frac{M}{B}} \\frac{N}{M}$ layers, we can use just two: one for sorting data in blocks of $M$ elements, and another one for merging all of them at once. This way, from the I/O operations perspective, we just loop around our dataset twice. And with a gigabyte of RAM and a block size of 1MB, this way can sort arrays up to a terabyte in size.\nHere is how the first phase looks in C++. This program opens a multi-gigabyte binary file with unsorted integers, reads it in blocks of 256MB, sorts them in memory, and then writes them back in files named part-000.bin, part-001.bin, part-002.bin, and so on:\nconst int B = (1\u0026lt;\u0026lt;20) / 4; // 1 MB blocks of integers const int M = (1\u0026lt;\u0026lt;28) / 4; // available memory FILE *input = fopen(\u0026#34;input.bin\u0026#34;, \u0026#34;rb\u0026#34;); std::vector\u0026lt;FILE*\u0026gt; parts; while (true) { static int part[M]; // better delete it right after int n = fread(part, 4, M, input); if (n == 0) break; // sort a block in-memory std::sort(part, part + n); char fpart[sizeof \u0026#34;part-999.bin\u0026#34;]; sprintf(fpart, \u0026#34;part-%03d.bin\u0026#34;, parts.size()); printf(\u0026#34;Writing %d elements into %s...\\n\u0026#34;, n, fpart); FILE *file = fopen(fpart, \u0026#34;wb\u0026#34;); fwrite(part, 4, n, file); fclose(file); file = fopen(fpart, \u0026#34;rb\u0026#34;); parts.push_back(file); } fclose(input); What is left now is to merge them together. The bandwidth of modern HDDs can be quite high, and there may be a lot of parts to merge, so the I/O efficiency of this stage is not our only concern: we also need a faster way to merge $k$ arrays than by finding minima with $O(k)$ comparisons. We can do that in $O(\\log k)$ time per element if we maintain a min-heap for these $k$ elements, in a manner almost identical to heapsort.\nHere is how to implement it. First, we are going to need a heap (priority_queue in C++):\nstruct Pointer { int key, part; // the element itself and the number of its part bool operator\u0026lt;(const Pointer\u0026amp; other) const { return key \u0026gt; other.key; // std::priority_queue is a max-heap by default } }; std::priority_queue\u0026lt;Pointer\u0026gt; q; Then, we need to allocate and fill the buffers:\nconst int nparts = parts.size(); auto buffers = new int[nparts][B]; // buffers for each part int *l = new int[nparts], // # of already processed buffer elements *r = new int[nparts]; // buffer size (in case it isn\u0026#39;t full) // now we add fill the buffer for each part and add their elements to the heap for (int part = 0; part \u0026lt; nparts; part++) { l[part] = 1; // if the element is in the heap, we also consider it \u0026#34;processed\u0026#34; r[part] = fread(buffers[part], 4, B, parts[part]); q.push({buffers[part][0], part}); } Now we just need to pop elements from the heap into the result file until it is empty, carefully writing and reading elements in batches:\nFILE *output = fopen(\u0026#34;output.bin\u0026#34;, \u0026#34;w\u0026#34;); int outbuffer[B]; // the output buffer int buffered = 0; // number of elements in it while (!q.empty()) { auto [key, part] = q.top(); q.pop(); // write the minimum to the output buffer outbuffer[buffered++] = key; // check if it needs to be committed to the file if (buffered == B) { fwrite(outbuffer, 4, B, output); buffered = 0; } // fetch a new block of that part if needed if (l[part] == r[part]) { r[part] = fread(buffers[part], 4, B, parts[part]); l[part] = 0; } // read a new element from that part unless we\u0026#39;ve already processed all of it if (l[part] \u0026lt; r[part]) { q.push({buffers[part][l[part]], part}); l[part]++; } } // write what\u0026#39;s left of the output buffer fwrite(outbuffer, 4, buffered, output); //clean up delete[] buffers; for (FILE *file : parts) fclose(file); fclose(output); This implementation is not particularly effective or safe-looking (well, this is basically plain C), but is a good educational example of how to work with low-level memory APIs.\n#JoiningSorting is mainly used not by itself, but as an intermediate step for other operations. One important real-world use case of external sorting is joining (as in \u0026ldquo;SQL join\u0026rdquo;), used in databases and other data processing applications.\nProblem. Given two lists of tuples $(x_i, a_{x_i})$ and $(y_i, b_{y_i})$, output a list $(k, a_{x_k}, b_{y_k})$ such that $x_k = y_k$\nThe optimal solution would be to sort the two lists and then use the standard two-pointer technique to merge them. The I/O complexity here would be the same as sorting, and just $O(\\frac{N}{B})$ if the arrays are already sorted. This is why most data processing applications (databases, MapReduce systems) like to keep their tables at least partially sorted.\nOther approaches. Note that this analysis is only applicable in the external memory setting — that is, if you don\u0026rsquo;t have the memory to read the entire dataset. In the real world, alternative methods may be faster.\nThe simplest of them is probably hash join, which goes something like this:\ndef join(a, b): d = dict(a) for x, y in b: if x in d: yield d[x] In external memory, joining two lists with a hash table would be unfeasible, as it would involve doing $O(M)$ block reads, even though only one element is used in each of them.\nAnother method is to use alternative sorting algorithms such as radix sort. In particular, radix sort would work in $O(\\frac{N}{B} \\cdot w)$ block reads if enough memory is available to maintain buffers for all possible keys, and it could be faster in the case of small keys and large datasets.\n","id":42,"path":"/hugo-page/hpc/external-memory/sorting/","title":"External Sorting"},{"content":"The inverse square root of a floating-point number $\\frac{1}{\\sqrt x}$ is used in calculating normalized vectors, which are in turn extensively used in various simulation scenarios such as computer graphics (e.g., to determine angles of incidence and reflection to simulate lighting).\n$$ \\hat{v} = \\frac{\\vec v}{\\sqrt {v_x^2 + v_y^2 + v_z^2}} $$\nCalculating an inverse square root directly — by first calculating a square root and then dividing $1$ by it — is extremely slow because both of these operations are slow even though they are implemented in hardware.\nBut there is a surprisingly good approximation algorithm that takes advantage of the way floating-point numbers are stored in memory. In fact, it is so good that it has been implemented in hardware, so the algorithm is no longer relevant by itself for software engineers, but we are nonetheless going to walk through it for its intrinsic beauty and great educational value.\nApart from the method itself, quite interesting is the history of its creation. It is attributed to a game studio id Software that used it in their iconic 1999 game Quake III Arena, although apparently, it got there by a chain of \u0026ldquo;I learned it from a guy who learned it from a guy\u0026rdquo; that seems to end on William Kahan (the same one that is responsible for IEEE 754 and Kahan summation algorithm).\nIt became popular in game developing community around 2005 when they released the source code of the game. Here is the relevant excerpt from it, including the comments:\nfloat Q_rsqrt(float number) { long i; float x2, y; const float threehalfs = 1.5F; x2 = number * 0.5F; y = number; i = * ( long * ) \u0026amp;y; // evil floating point bit level hacking i = 0x5f3759df - ( i \u0026gt;\u0026gt; 1 ); // what the fuck? y = * ( float * ) \u0026amp;i; y = y * ( threehalfs - ( x2 * y * y ) ); // 1st iteration // y = y * ( threehalfs - ( x2 * y * y ) ); // 2nd iteration, this can be removed return y; } We will go through what it does step by step, but first, we need to take a small detour.\n#Approximate LogarithmBefore computers (or at least affordable calculators) became an everyday thing, people computed multiplication and related operations using logarithm tables — by looking up the logarithms of $a$ and $b$, adding them, and then finding the inverse logarithm of the result.\n$$ a \\times b = 10^{\\log a + \\log b} = \\log^{-1}(\\log a + \\log b) $$\nYou can do the same trick when computing $\\frac{1}{\\sqrt x}$ using the identity:\n$$ \\log \\frac{1}{\\sqrt x} = - \\frac{1}{2} \\log x $$\nThe fast inverse square root is based on this identity, and so it needs to calculate the logarithm of $x$ very quickly. Turns out, it can be approximated by just reinterpreting a 32-bit float as an integer.\nRecall that floating-point numbers sequentially store the sign bit (equal to zero for positive values, which is our case), exponent $e_x$ and mantissa $m_x$, which corresponds to\n$$ x = 2^{e_x} \\cdot (1 + m_x) $$\nIts logarithm is therefore\n$$ \\log_2 x = e_x + \\log_2 (1 + m_x) $$\nSince $m_x \\in [0, 1)$, the logarithm on the right-hand side can be approximated by\n$$ \\log_2 (1 + m_x) \\approx m_x $$\nThe approximation is exact at both ends of the intervals, but to account for the average case we need to shift it by a small constant $\\sigma$, therefore\n$$ \\log_2 x = e_x + \\log_2 (1 + m_x) \\approx e_x + m_x + \\sigma $$\nNow, having this approximation in mind and defining $L=2^{23}$ (the number of mantissa bits in a float) and $B=127$ (the exponent bias), when we reinterpret the bit-pattern of $x$ as an integer $I_x$, we essentially get\n$$ \\begin{aligned} I_x \u0026amp;= L \\cdot (e_x + B + m_x) \\ \u0026amp;= L \\cdot (e_x + m_x + \\sigma +B-\\sigma ) \\ \u0026amp;\\approx L \\cdot \\log_2 (x) + L \\cdot (B-\\sigma ) \\end{aligned} $$\n(Multiplying an integer by $L=2^{23}$ is equivalent to left-shifting it by 23.)\nWhen you tune $\\sigma$ to minimize the mean square error, this results in a surprisingly accurate approximation.\nReinterpreting a floating-point number $x$ as an integer (blue) compared to its scaled and shifted logarithm (gray) Now, expressing the logarithm from the approximation, we get\n$$ \\log_2 x \\approx \\frac{I_x}{L} - (B - \\sigma) $$\nCool. Now, where were we? Oh, yes, we wanted to calculate the inverse square root.\n#Approximating the ResultTo calculate $y = \\frac{1}{\\sqrt x}$ using the identity $\\log_2 y = - \\frac{1}{2} \\log_2 x$, we can plug it into our approximation formula and get\n$$ \\frac{I_y}{L} - (B - \\sigma) \\approx\n\\frac{1}{2} ( \\frac{I_x}{L} - (B - \\sigma) ) $$ Solving for $I_y$:\n$$ I_y \\approx \\frac{3}{2} L (B - \\sigma) - \\frac{1}{2} I_x $$\nIt turns out, we don\u0026rsquo;t even need to calculate the logarithm in the first place: the formula above is just a constant minus half the integer reinterpretation of $x$. It is written in the code as:\ni = * ( long * ) \u0026amp;y; i = 0x5f3759df - ( i \u0026gt;\u0026gt; 1 ); We reinterpret y as an integer in the first line, and then it plug into the formula on the second, the first term of which is the magic number $\\frac{3}{2} L (B - \\sigma) = \\mathtt{0x5F3759DF}$, while the second is calculated with a binary shift instead of division.\n#Iterating with Newton\u0026rsquo;s MethodWhat we have next is a couple hand-coded iterations of Newton\u0026rsquo;s method with $f(y) = \\frac{1}{y^2} - x$ and a very good initial value. Its update rule is\n$$ f\u0026rsquo;(y) = - \\frac{2}{y^3} \\implies y_{i+1} = y_{i} (\\frac{3}{2} - \\frac{x}{2} y_i^2) = \\frac{y_i (3 - x y_i^2)}{2} $$\nwhich is written in the code as\nx2 = number * 0.5F; y = y * ( threehalfs - ( x2 * y * y ) ); The initial approximation is so good that just one iteration was enough for game development purposes. It falls within 99.8% of the correct answer after just the first iteration and can be reiterated further to improve accuracy — which is what is done in the hardware: the x86 instruction does a few of them and guarantees a relative error of no more than $1.5 \\times 2^{-12}$.\n#Further ReadingWikipedia article on fast inverse square root.\n","id":43,"path":"/hugo-page/hpc/arithmetic/rsqrt/","title":"Fast Inverse Square Root"},{"content":"During assembly, all labels are converted to addresses (absolute or relative) and then encoded into jump instructions.\nYou can also jump by a non-constant value stored inside a register, which is called a computed jump:\njmp rax This has a few interesting applications related to dynamic languages and implementing more complex control flow.\n#Multiway BranchIf you have already forgotten what a switch statement does, here is a little subroutine for calculating GPA in the American grading system:\nswitch (grade) { case \u0026#39;A\u0026#39;: return 4.0; break; case \u0026#39;B\u0026#39;: return 3.0; break; case \u0026#39;C\u0026#39;: return 2.0; break; case \u0026#39;D\u0026#39;: return 1.0; break; case \u0026#39;E\u0026#39;: case \u0026#39;F\u0026#39;: return 0.0; break; default: return NAN; } I personally don\u0026rsquo;t remember the last time I used a switch in a non-educational context. In general, switch statements are equivalent to a sequence of \u0026ldquo;if, else if, else if, else if…\u0026rdquo; and so on, and for this reason many languages don\u0026rsquo;t even have them. Nonetheless, such control flow structures are important for implementing parsers, interpreters, and other state machines, which are often comprised of a single while (true) loop and a switch (state) statement inside.\nWhen we have control over the range of values that the variable can take, we can use the following trick utilizing computed jumps. Instead of making $n$ conditional branches, we can create a branch table that contains pointers/offsets to possible jump locations, and then just index it with the state variable taking values in the $[0, n)$ range.\nCompilers use this technique when the values are densely packed together (not necessarily strictly sequentially, but it has to be worth having blank fields in the table). It can also be implemented explicitly with a computed goto:\nvoid weather_in_russia(int season) { static const void* table[] = {\u0026amp;\u0026amp;winter, \u0026amp;\u0026amp;spring, \u0026amp;\u0026amp;summer, \u0026amp;\u0026amp;fall}; goto *table[season]; winter: printf(\u0026#34;Freezing\\n\u0026#34;); return; spring: printf(\u0026#34;Dirty\\n\u0026#34;); return; summer: printf(\u0026#34;Dry\\n\u0026#34;); return; fall: printf(\u0026#34;Windy\\n\u0026#34;); return; } Switch-based code is not always straightforward for compilers to optimize, so in the context of state machines, goto statements are often used directly. The I/O-related part of glibc is full of examples.\n#Dynamic DispatchIndirect branching is also instrumental in implementing runtime polymorphism.\nConsider the cliché example when we have an abstract class of Animal with a virtual .speak() method, and two concrete implementations: a Dog that barks and a Cat that meows:\nstruct Animal { virtual void speak() { printf(\u0026#34;\u0026lt;abstract animal sound\u0026gt;\\n\u0026#34;);} }; struct Dog { void speak() override { printf(\u0026#34;Bark\\n\u0026#34;); } }; struct Cat { void speak() override { printf(\u0026#34;Meow\\n\u0026#34;); } }; We want to create an animal and, without knowing its type in advance, call its .speak() method, which should somehow invoke the right implementation:\nDog sparkles; Cat mittens; Animal *catdog = (rand() \u0026amp; 1) ? \u0026amp;sparkles : \u0026amp;mittens; catdog-\u0026gt;speak(); There are many ways to implement this behavior, but C++ does it using a virtual method table.\nFor all concrete implementations of Animal, compiler pads all their methods (that is, their instruction sequences) so that they have the exact same length for all classes (by inserting some filler instructions after ret) and then just writes them sequentially somewhere in the instruction memory. Then it adds a run-time type information field to the structure (that is, to all its instances), which is essentially just the offset in the memory region that points to the right implementation of the virtual methods of the class.\nWith a virtual method call, that offset field is fetched from the instance of a structure and a normal function call is made with it, using the fact that all methods and other fields of every derived class have exactly the same offsets.\nOf course, this adds some overhead:\nYou may need to spend another 15 cycles or so for the same pipeline flushing reasons as for branch misprediction. The compiler most likely won\u0026rsquo;t be able to inline the function call itself. Class size increases by a couple of bytes or so (this is implementation-specific). The binary size itself increases a little bit. For these reasons, runtime polymorphism is usually avoided in performance-critical applications.\n","id":44,"path":"/hugo-page/hpc/architecture/indirect/","title":"Indirect Branching"},{"content":"A machine code analyzer is a program that takes a small snippet of assembly code and simulates its execution on a particular microarchitecture using information available to compilers, and outputs the latency and throughput of the whole block, as well as cycle-perfect utilization of various resources within the CPU.\n#Using llvm-mcaThere are many different machine code analyzers, but I personally prefer llvm-mca, which you can probably install via a package manager together with clang. You can also access it through a web-based tool called UICA or in the Compiler Explorer by selecting \u0026ldquo;Analysis\u0026rdquo; as the language.\nWhat llvm-mca does is it runs a set number of iterations of a given assembly snippet and computes statistics about the resource usage of each instruction, which is useful for finding out where the bottleneck is.\nWe will consider the array sum as our simple example:\nloop: addl (%rax), %edx addq $4, %rax cmpq %rcx, %rax jne\tloop Here is its analysis with llvm-mca for the Skylake microarchitecture:\nIterations: 100 Instructions: 400 Total Cycles: 108 Total uOps: 500 Dispatch Width: 6 uOps Per Cycle: 4.63 IPC: 3.70 Block RThroughput: 0.8 First, it outputs general information about the loop and the hardware:\nIt \u0026ldquo;ran\u0026rdquo; the loop 100 times, executing 400 instructions in total in 108 cycles, which is the same as executing $\\frac{400}{108} \\approx 3.7$ instructions per cycle on average (IPC). The CPU is theoretically capable of executing up to 6 instructions per cycle (dispatch width). Each cycle in theory can be executed in 0.8 cycles on average (block reciprocal throughput). The \u0026ldquo;uOps\u0026rdquo; here are the micro-operations that the CPU splits each instruction into (e.g., fused load-add is composed of two uOps). Then it proceeds to give information about each individual instruction:\nInstruction Info: [1]: uOps [2]: Latency [3]: RThroughput [4]: MayLoad [5]: MayStore [6]: HasSideEffects (U) [1] [2] [3] [4] [5] [6] Instructions: 2 6 0.50 * addl\t(%rax), %edx 1 1 0.25 addq\t$4, %rax 1 1 0.25 cmpq\t%rcx, %rax 1 1 0.50 jne\t-11 There is nothing there that there isn\u0026rsquo;t in the instruction tables:\nhow many uOps each instruction is split into; how many cycles each instruction takes to complete (latency); how many cycles each instruction takes to complete in the amortized sense (reciprocal throughput), considering that several copies of it can be executed simultaneously. Then it outputs probably the most important part — which instructions are executing when and where:\nResource pressure by instruction: [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] Instructions: - - 0.01 0.98 0.50 0.50 - - 0.01 - addl (%rax), %edx - - - - - - - 0.01 0.99 - addq $4, %rax - - - 0.01 - - - 0.99 - - cmpq %rcx, %rax - - 0.99 - - - - - 0.01 - jne -11 As the contention for execution ports causes structural hazards, ports often become the bottleneck for throughput-oriented loops, and this chart helps diagnose why. It does not give you a cycle-perfect Gantt chart of something like that, but it gives you the aggregate statistics of the execution ports used for each instruction, which lets you find which one is overloaded.\n","id":45,"path":"/hugo-page/hpc/profiling/mca/","title":"Machine Code Analyzers"},{"content":"One of the bigger challenges of SIMD programming is that its options for control flow are very limited — because the operations you apply to a vector are the same for all its elements.\nThis makes the problems that are usually trivially resolved with an if or any other type of branching much harder. With SIMD, they have to be dealt with by the means of various branchless programming techniques, which aren\u0026rsquo;t always that straightforward to apply.\n#MaskingThe main way to make a computation branchless is through predication — computing the results of both branches and then using either some arithmetic trick or a special \u0026ldquo;conditional move\u0026rdquo; instruction:\nfor (int i = 0; i \u0026lt; N; i++) a[i] = rand() % 100; int s = 0; // branch: for (int i = 0; i \u0026lt; N; i++) if (a[i] \u0026lt; 50) s += a[i]; // no branch: for (int i = 0; i \u0026lt; N; i++) s += (a[i] \u0026lt; 50) * a[i]; // also no branch: for (int i = 0; i \u0026lt; N; i++) s += (a[i] \u0026lt; 50 ? a[i] : 0); To vectorize this loop, we are going to need two new instructions:\n_mm256_cmpgt_epi32, which compares the integers in two vectors and produces a mask of all ones if the first element is more than the second and a mask of full zeros otherwise. _mm256_blendv_epi8, which blends (combines) the values of two vectors based on the provided mask. By masking and blending the elements of a vector so that only the selected subset of them is affected by computation, we can perform predication in a manner similar to the conditional move:\nconst reg c = _mm256_set1_epi32(49); const reg z = _mm256_setzero_si256(); reg s = _mm256_setzero_si256(); for (int i = 0; i \u0026lt; N; i += 8) { reg x = _mm256_load_si256( (reg*) \u0026amp;a[i] ); reg mask = _mm256_cmpgt_epi32(x, c); x = _mm256_blendv_epi8(x, z, mask); s = _mm256_add_epi32(s, x); } (Minor details such as horizontal summation and accounting for the remainder of the array are omitted for brevity.)\nThis is how predication is usually done in SIMD, but it isn\u0026rsquo;t always the most optimal approach. We can use the fact that one of the blended values is zero, and use bitwise and with the mask instead of blending:\nconst reg c = _mm256_set1_epi32(50); reg s = _mm256_setzero_si256(); for (int i = 0; i \u0026lt; N; i += 8) { reg x = _mm256_load_si256( (reg*) \u0026amp;a[i] ); reg mask = _mm256_cmpgt_epi32(c, x); x = _mm256_and_si256(x, mask); s = _mm256_add_epi32(s, x); } This loop performs slightly faster because on this particular CPU, the vector and takes one cycle less than blend.\nSeveral other instructions support masks as inputs, most notably:\nThe _mm256_blend_epi32 intrinsic is a blend that takes an 8-bit integer mask instead of a vector (which is why it doesn\u0026rsquo;t have v at the end). The _mm256_maskload_epi32 and _mm256_maskstore_epi32 intrinsics that load/store a SIMD block from memory and and it with a mask in one go. We can also use predication with built-in vector types:\nvec *v = (vec*) a; vec s = {}; for (int i = 0; i \u0026lt; N / 8; i++) s += (v[i] \u0026lt; 50 ? v[i] : 0); All these versions work at around 13 GFLOPS as this example is so simple that the compiler can vectorize the loop all by itself. Let\u0026rsquo;s move on to more complex examples that can\u0026rsquo;t be auto-vectorized.\n#SearchingIn the next example, we need to find a specific value in an array and return its position (aka std::find):\nconst int N = (1\u0026lt;\u0026lt;12); int a[N]; int find(int x) { for (int i = 0; i \u0026lt; N; i++) if (a[i] == x) return i; return -1; } To benchmark the find function, we fill the array with numbers from $0$ to $(N - 1)$ and then repeatedly search for a random element:\nfor (int i = 0; i \u0026lt; N; i++) a[i] = i; for (int t = 0; t \u0026lt; K; t++) checksum ^= find(rand() % N); The scalar version gives ~4 GFLOPS of performance. This number includes the elements we haven\u0026rsquo;t had to process, so divide this number by two in your head (the expected fraction of the elements we have to check).\nTo vectorize it, we need to compare a vector of its elements with the searched value for equality, producing a mask, and then somehow check if this mask is zero. If it isn\u0026rsquo;t, the needed element is somewhere within this block of 8.\nTo check if the mask is zero, we can use the _mm256_movemask_ps intrinsic, which takes the first bit of each 32-bit element in a vector and produces an 8-bit integer mask out of them. We can then check if this mask is non-zero — and if it is, also immediately get the index with the ctz instruction:\nint find(int needle) { reg x = _mm256_set1_epi32(needle); for (int i = 0; i \u0026lt; N; i += 8) { reg y = _mm256_load_si256( (reg*) \u0026amp;a[i] ); reg m = _mm256_cmpeq_epi32(x, y); int mask = _mm256_movemask_ps((__m256) m); if (mask != 0) return i + __builtin_ctz(mask); } return -1; } This version gives ~20 GFLOPS or about 5 times faster than the scalar one. It only uses 3 instructions in the hot loop:\nvpcmpeqd ymm0, ymm1, YMMWORD PTR a[0+rdx*4] vmovmskps eax, ymm0 test eax, eax je loop Checking if a vector is zero is a common operation, and there is an operation similar to test in SIMD that we can use:\nint find(int needle) { reg x = _mm256_set1_epi32(needle); for (int i = 0; i \u0026lt; N; i += 8) { reg y = _mm256_load_si256( (reg*) \u0026amp;a[i] ); reg m = _mm256_cmpeq_epi32(x, y); if (!_mm256_testz_si256(m, m)) { int mask = _mm256_movemask_ps((__m256) m); return i + __builtin_ctz(mask); } } return -1; } We are still using movemask to do ctz later, but the hot loop is now one instruction shorter:\nvpcmpeqd ymm0, ymm1, YMMWORD PTR a[0+rdx*4] vptest ymm0, ymm0 je loop This doesn\u0026rsquo;t improve performance much because both both vptest and vmovmskps have a throughput of one and will bottleneck the computation regardless of anything else we do in the loop.\nTo work around this limitation, we can iterate in blocks of 16 elements and combine the results of independent comparisons of two 256-bit AVX2 registers using a bitwise or:\nint find(int needle) { reg x = _mm256_set1_epi32(needle); for (int i = 0; i \u0026lt; N; i += 16) { reg y1 = _mm256_load_si256( (reg*) \u0026amp;a[i] ); reg y2 = _mm256_load_si256( (reg*) \u0026amp;a[i + 8] ); reg m1 = _mm256_cmpeq_epi32(x, y1); reg m2 = _mm256_cmpeq_epi32(x, y2); reg m = _mm256_or_si256(m1, m2); if (!_mm256_testz_si256(m, m)) { int mask = (_mm256_movemask_ps((__m256) m2) \u0026lt;\u0026lt; 8) + _mm256_movemask_ps((__m256) m1); return i + __builtin_ctz(mask); } } return -1; } With this obstacle removed, the performance now peaks at ~34 GFLOPS. But why not 40? Shouldn\u0026rsquo;t it be twice as fast?\nHere is how one iteration of the loop looks in assembly:\nvpcmpeqd ymm2, ymm1, YMMWORD PTR a[0+rdx*4] vpcmpeqd ymm3, ymm1, YMMWORD PTR a[32+rdx*4] vpor ymm0, ymm3, ymm2 vptest ymm0, ymm0 je loop Every iteration, we need to execute 5 instructions. While the throughputs of all relevant execution ports allow to do that in one cycle on average, we can\u0026rsquo;t do that because the decode width of this particular CPU (Zen 2) is 4. Therefore, the performance is limited by ⅘ of what it could have been.\nTo mitigate this, we can once again double the number of SIMD blocks we process on each iteration:\nunsigned get_mask(reg m) { return _mm256_movemask_ps((__m256) m); } reg cmp(reg x, int *p) { reg y = _mm256_load_si256( (reg*) p ); return _mm256_cmpeq_epi32(x, y); } int find(int needle) { reg x = _mm256_set1_epi32(needle); for (int i = 0; i \u0026lt; N; i += 32) { reg m1 = cmp(x, \u0026amp;a[i]); reg m2 = cmp(x, \u0026amp;a[i + 8]); reg m3 = cmp(x, \u0026amp;a[i + 16]); reg m4 = cmp(x, \u0026amp;a[i + 24]); reg m12 = _mm256_or_si256(m1, m2); reg m34 = _mm256_or_si256(m3, m4); reg m = _mm256_or_si256(m12, m34); if (!_mm256_testz_si256(m, m)) { unsigned mask = (get_mask(m4) \u0026lt;\u0026lt; 24) + (get_mask(m3) \u0026lt;\u0026lt; 16) + (get_mask(m2) \u0026lt;\u0026lt; 8) + get_mask(m1); return i + __builtin_ctz(mask); } } return -1; } It now shows the throughput of 43 GFLOPS — or about 10x faster than the original scalar implementation.\nExtending it to 64 values per cycle doesn\u0026rsquo;t help: small arrays suffer from the overhead of all these additional movemask-s when we hit the condition, and larger arrays are bottlenecked by memory bandwidth anyway.\n#Counting ValuesAs the final exercise, let\u0026rsquo;s find the count of a value in an array instead of just its first occurrence:\nint count(int x) { int cnt = 0; for (int i = 0; i \u0026lt; N; i++) cnt += (a[i] == x); return cnt; } To vectorize it, we just need to convert the comparison mask to either one or zero per element and calculate the sum:\nconst reg ones = _mm256_set1_epi32(1); int count(int needle) { reg x = _mm256_set1_epi32(needle); reg s = _mm256_setzero_si256(); for (int i = 0; i \u0026lt; N; i += 8) { reg y = _mm256_load_si256( (reg*) \u0026amp;a[i] ); reg m = _mm256_cmpeq_epi32(x, y); m = _mm256_and_si256(m, ones); s = _mm256_add_epi32(s, m); } return hsum(s); } Both implementations yield ~15 GFLOPS: the compiler can vectorize the first one all by itself.\nBut a trick that the compiler can\u0026rsquo;t find is to notice that the mask of all ones is minus one when reinterpreted as an integer. So we can skip the and-the-lowest-bit part and use the mask itself, and then just negate the final result:\nint count(int needle) { reg x = _mm256_set1_epi32(needle); reg s = _mm256_setzero_si256(); for (int i = 0; i \u0026lt; N; i += 8) { reg y = _mm256_load_si256( (reg*) \u0026amp;a[i] ); reg m = _mm256_cmpeq_epi32(x, y); s = _mm256_add_epi32(s, m); } return -hsum(s); } This doesn\u0026rsquo;t improve the performance in this particular architecture because the throughput is actually bottlenecked by updating s: there is a dependency on the previous iteration, so the loop can\u0026rsquo;t proceed faster than one iteration per CPU cycle. We can make use of instruction-level parallelism if we split the accumulator in two:\nint count(int needle) { reg x = _mm256_set1_epi32(needle); reg s1 = _mm256_setzero_si256(); reg s2 = _mm256_setzero_si256(); for (int i = 0; i \u0026lt; N; i += 16) { reg y1 = _mm256_load_si256( (reg*) \u0026amp;a[i] ); reg y2 = _mm256_load_si256( (reg*) \u0026amp;a[i + 8] ); reg m1 = _mm256_cmpeq_epi32(x, y1); reg m2 = _mm256_cmpeq_epi32(x, y2); s1 = _mm256_add_epi32(s1, m1); s2 = _mm256_add_epi32(s2, m2); } s1 = _mm256_add_epi32(s1, s2); return -hsum(s1); } It now gives ~22 GFLOPS of performance, which is as high as it can get.\nWhen adapting this code for shorter data types, keep in mind that the accumulator may overflow. To work around this, add another accumulator of larger size and regularly stop the loop to add the values in the local accumulator to it and then reset the local accumulator. For example, for 8-bit integers, this means creating another inner loop that does $\\lfloor \\frac{256-1}{8} \\rfloor = 15$ iterations.\n","id":46,"path":"/hugo-page/hpc/simd/masking/","title":"Masking and Blending"},{"content":"Unsurprisingly, a large fraction of computation in modular arithmetic is often spent on calculating the modulo operation, which is as slow as general integer division and typically takes 15-20 cycles, depending on the operand size.\nThe best way to deal this nuisance is to avoid modulo operation altogether, delaying or replacing it with predication, which can be done, for example, when calculating modular sums:\nconst int M = 1e9 + 7; // input: array of n integers in the [0, M) range // output: sum modulo M int slow_sum(int *a, int n) { int s = 0; for (int i = 0; i \u0026lt; n; i++) s = (s + a[i]) % M; return s; } int fast_sum(int *a, int n) { int s = 0; for (int i = 0; i \u0026lt; n; i++) { s += a[i]; // s \u0026lt; 2 * M s = (s \u0026gt;= M ? s - M : s); // will be replaced with cmov } return s; } int faster_sum(int *a, int n) { long long s = 0; // 64-bit integer to handle overflow for (int i = 0; i \u0026lt; n; i++) s += a[i]; // will be vectorized return s % M; } However, sometimes you only have a chain of modular multiplications, and there is no good way to eel out of computing the remainder of the division — other than with the integer division tricks requiring a constant modulo and some precomputation.\nBut there is another technique designed specifically for modular arithmetic, called Montgomery multiplication.\n#Montgomery SpaceMontgomery multiplication works by first transforming the multipliers into Montgomery space, where modular multiplication can be performed cheaply, and then transforming them back when their actual values are needed. Unlike general integer division methods, Montgomery multiplication is not efficient for performing just one modular reduction and only becomes worthwhile when there is a chain of modular operations.\nThe space is defined by the modulo $n$ and a positive integer $r \\ge n$ coprime to $n$. The algorithm involves modulo and division by $r$, so in practice, $r$ is chosen to be $2^{32}$ or $2^{64}$, so that these operations can be done with a right-shift and a bitwise AND respectively.\nDefinition. The representative $\\bar x$ of a number $x$ in the Montgomery space is defined as\n$$ \\bar{x} = x \\cdot r \\bmod n $$\nComputing this transformation involves a multiplication and a modulo — an expensive operation that we wanted to optimize away in the first place — which is why we only use this method when the overhead of transforming numbers to and from the Montgomery space is worth it and not for general modular multiplication.\nInside the Montgomery space, addition, substraction, and checking for equality is performed as usual:\n$$ x \\cdot r + y \\cdot r \\equiv (x + y) \\cdot r \\bmod n $$\nHowever, this is not the case for multiplication. Denoting multiplication in the Montgomery space as $*$ and the \u0026ldquo;normal\u0026rdquo; multiplication as $\\cdot$, we expect the result to be:\n$$ \\bar{x} * \\bar{y} = \\overline{x \\cdot y} = (x \\cdot y) \\cdot r \\bmod n $$\nBut the normal multiplication in the Montgomery space yields:\n$$ \\bar{x} \\cdot \\bar{y} = (x \\cdot y) \\cdot r \\cdot r \\bmod n $$\nTherefore, the multiplication in the Montgomery space is defined as\n$$ \\bar{x} * \\bar{y} = \\bar{x} \\cdot \\bar{y} \\cdot r^{-1} \\bmod n $$\nThis means that, after we normally multiply two numbers in the Montgomery space, we need to reduce the result by multiplying it by $r^{-1}$ and taking the modulo — and there is an efficent way to do this particular operation.\n#Montgomery reductionAssume that $r=2^{32}$, the modulo $n$ is 32-bit, and the number $x$ we need to reduce is 64-bit (the product of two 32-bit numbers). Our goal is to calculate $y = x \\cdot r^{-1} \\bmod n$.\nSince $r$ is coprime with $n$, we know that there are two numbers $r^{-1}$ and $n^\\prime$ in the $[0, n)$ range such that\n$$ r \\cdot r^{-1} + n \\cdot n^\\prime = 1 $$\nand both $r^{-1}$ and $n^\\prime$ can be computed, e.g., using the extended Euclidean algorithm.\nUsing this identity, we can express $r \\cdot r^{-1}$ as $(1 - n \\cdot n^\\prime)$ and write $x \\cdot r^{-1}$ as\n$$ \\begin{aligned} x \\cdot r^{-1} \u0026amp;= x \\cdot r \\cdot r^{-1} / r \\ \u0026amp;= x \\cdot (1 - n \\cdot n^{\\prime}) / r \\ \u0026amp;= (x - x \\cdot n \\cdot n^{\\prime} ) / r \\ \u0026amp;\\equiv (x - x \\cdot n \\cdot n^{\\prime} + k \\cdot r \\cdot n) / r \u0026amp;\\pmod n \u0026amp;;;\\text{(for any integer $k$)} \\ \u0026amp;\\equiv (x - (x \\cdot n^{\\prime} - k \\cdot r) \\cdot n) / r \u0026amp;\\pmod n \\end{aligned} $$\nNow, if we choose $k$ to be $\\lfloor x \\cdot n^\\prime / r \\rfloor$ (the upper 64 bits of the $x \\cdot n^\\prime$ product), it will cancel out, and $(k \\cdot r - x \\cdot n^{\\prime})$ will simply be equal to $x \\cdot n^{\\prime} \\bmod r$ (the lower 32 bits of $x \\cdot n^\\prime$), implying:\n$$ x \\cdot r^{-1} \\equiv (x - x \\cdot n^{\\prime} \\bmod r \\cdot n) / r $$\nThe algorithm itself just evaluates this formula, performing two multiplications to calculate $q = x \\cdot n^{\\prime} \\bmod r$ and $m = q \\cdot n$, and then subtracts it from $x$ and right-shifts the result to divide it by $r$.\nThe only remaining thing to handle is that the result may not be in the $[0, n)$ range; but since\n$$ x \u0026lt; n \\cdot n \u0026lt; r \\cdot n \\implies x / r \u0026lt; n $$\nand\n$$ m = q \\cdot n \u0026lt; r \\cdot n \\implies m / r \u0026lt; n $$\nit is guaranteed that\n$$ -n \u0026lt; (x - m) / r \u0026lt; n $$\nTherefore, we can simply check if the result is negative and in that case, add $n$ to it, giving the following algorithm:\ntypedef __uint32_t u32; typedef __uint64_t u64; const u32 n = 1e9 + 7, nr = inverse(n, 1ull \u0026lt;\u0026lt; 32); u32 reduce(u64 x) { u32 q = u32(x) * nr; // q = x * n\u0026#39; mod r u64 m = (u64) q * n; // m = q * n u32 y = (x - m) \u0026gt;\u0026gt; 32; // y = (x - m) / r return x \u0026lt; m ? y + n : y; // if y \u0026lt; 0, add n to make it be in the [0, n) range } This last check is relatively cheap, but it is still on the critical path. If we are fine with the result being in the $[0, 2 \\cdot n - 2]$ range instead of $[0, n)$, we can remove it and add $n$ to the result unconditionally:\nu32 reduce(u64 x) { u32 q = u32(x) * nr; u64 m = (u64) q * n; u32 y = (x - m) \u0026gt;\u0026gt; 32; return y + n } We can also move the \u0026gt;\u0026gt; 32 operation one step earlier in the computation graph and compute $\\lfloor x / r \\rfloor - \\lfloor m / r \\rfloor$ instead of $(x - m) / r$. This is correct because the lower 32 bits of $x$ and $m$ are equal anyway since\n$$ m = x \\cdot n^\\prime \\cdot n \\equiv x \\pmod r $$\nBut why would we voluntarily choose to perfom two right-shifts instead of just one? This is beneficial because for ((u64) q * n) \u0026gt;\u0026gt; 32 we need to do a 32-by-32 multiplication and take the upper 32 bits of the result (which the x86 mul instruction already writes in a separate register, so it doesn\u0026rsquo;t cost anything), and the other right-shift x \u0026gt;\u0026gt; 32 is not on the critical path.\nu32 reduce(u64 x) { u32 q = u32(x) * nr; u32 m = ((u64) q * n) \u0026gt;\u0026gt; 32; return (x \u0026gt;\u0026gt; 32) + n - m; } One of the main advantages of Montgomery multiplication over other modular reduction methods is that it doesn\u0026rsquo;t require very large data types: it only needs a $r \\times r$ multiplication that extracts the lower and higher $r$ bits of the result, which has special support on most hardware also makes it easily generalizable to SIMD and larger data types:\ntypedef __uint128_t u128; u64 reduce(u128 x) const { u64 q = u64(x) * nr; u64 m = ((u128) q * n) \u0026gt;\u0026gt; 64; return (x \u0026gt;\u0026gt; 64) + n - m; } Note that a 128-by-64 modulo is not possible with general integer division tricks: the compiler falls back to calling a slow long arithmetic library function to support it.\n#Faster Inverse and TransformMontgomery multiplication itself is fast, but it requires some precomputation:\ninverting $n$ modulo $r$ to compute $n^\\prime$, transforming a number to the Montgomery space, transforming a number from the Montgomery space. The last operation is already efficiently performed with the reduce procedure we just implemented, but the first two can be slightly optimized.\nComputing the inverse $n^\\prime = n^{-1} \\bmod r$ can be done faster than with the extended Euclidean algorithm by taking advantage of the fact that $r$ is a power of two and using the following identity:\n$$ a \\cdot x \\equiv 1 \\bmod 2^k \\implies a \\cdot x \\cdot (2 - a \\cdot x) \\equiv 1 \\bmod 2^{2k} $$\nProof:\n$$ \\begin{aligned} a \\cdot x \\cdot (2 - a \\cdot x) \u0026amp;= 2 \\cdot a \\cdot x - (a \\cdot x)^2 \\ \u0026amp;= 2 \\cdot (1 + m \\cdot 2^k) - (1 + m \\cdot 2^k)^2 \\ \u0026amp;= 2 + 2 \\cdot m \\cdot 2^k - 1 - 2 \\cdot m \\cdot 2^k - m^2 \\cdot 2^{2k} \\ \u0026amp;= 1 - m^2 \\cdot 2^{2k} \\ \u0026amp;\\equiv 1 \\bmod 2^{2k}. \\end{aligned} $$\nWe can start with $x = 1$ as the inverse of $a$ modulo $2^1$ and apply this identity exactly $\\log_2 r$ times, each time doubling the number of bits in the inverse — somewhat reminiscent of the Newton\u0026rsquo;s method.\nTransforming a number into the Montgomery space can be done by multiplying it by $r$ and computing modulo the usual way, but we can also take advantage of this relation:\n$$ \\bar{x} = x \\cdot r \\bmod n = x * r^2 $$\nTransforming a number into the space is just a multiplication by $r^2$. Therefore, we can precompute $r^2 \\bmod n$ and perform a multiplication and reduction instead — which may or may not be actually faster because multiplying a number by $r=2^{k}$ can be implemented with a left-shift, while multiplication by $r^2 \\bmod n$ can not.\n#Complete ImplementationIt is convenient to wrap everything into a single constexpr structure:\nstruct Montgomery { u32 n, nr; constexpr Montgomery(u32 n) : n(n), nr(1) { // log(2^32) = 5 for (int i = 0; i \u0026lt; 5; i++) nr *= 2 - n * nr; } u32 reduce(u64 x) const { u32 q = u32(x) * nr; u32 m = ((u64) q * n) \u0026gt;\u0026gt; 32; return (x \u0026gt;\u0026gt; 32) + n - m; // returns a number in the [0, 2 * n - 2] range // (add a \u0026#34;x \u0026lt; n ? x : x - n\u0026#34; type of check if you need a proper modulo) } u32 multiply(u32 x, u32 y) const { return reduce((u64) x * y); } u32 transform(u32 x) const { return (u64(x) \u0026lt;\u0026lt; 32) % n; // can also be implemented as multiply(x, r^2 mod n) } }; To test its performance, we can plug Montgomery multiplication into the binary exponentiation:\nconstexpr Montgomery space(M); int inverse(int _a) { u64 a = space.transform(_a); u64 r = space.transform(1); #pragma GCC unroll(30) for (int l = 0; l \u0026lt; 30; l++) { if ( (M - 2) \u0026gt;\u0026gt; l \u0026amp; 1 ) r = space.multiply(r, a); a = space.multiply(a, a); } return space.reduce(r); } While vanilla binary exponentiation with a compiler-generated fast modulo trick requires ~170ns per inverse call, this implementation takes ~166ns, going down to ~158ns we omit transform and reduce (a reasonable use case is for inverse to be used as a subprocedure in a bigger modular computation). This is a small improvement, but Montgomery multiplication becomes much more advantageous for SIMD applications and larger data types.\nExercise. Implement efficient modular matix multiplication.\n","id":47,"path":"/hugo-page/hpc/number-theory/montgomery/","title":"Montgomery Multiplication"},{"content":"The lessons learned from optimizing binary search can be applied to a broad range of data structures.\nIn this article, instead of trying to optimize something from the STL again, we focus on segment trees, the structures that may be unfamiliar to most normal programmers and perhaps even most computer science researchers1, but that are used very extensively in programming competitions for their speed and simplicity of implementation.\n(If you already know the context, jump straight to the last section for the novelty: the wide segment tree that works 4 to 12 times faster than the Fenwick tree.)\n#Dynamic Prefix Sum Segment trees are cool and can do lots of different things, but in this article, we will focus on their simplest non-trivial application — the dynamic prefix sum problem:\nvoid add(int k, int x); // react to a[k] += x (zero-based indexing) int sum(int k); // return the sum of the first k elements (from 0 to k - 1) As we have to support two types of queries, our optimization problem becomes multi-dimensional, and the optimal solution depends on the distribution of queries. For example, if one type of the queries were extremely rare, we would only optimize for the other, which is relatively easy to do:\nIf we only cared about the cost of updating the array, we would store it as it is and calculate the sum directly on each sum query. If we only cared about the cost of prefix sum queries, we would keep it ready and re-calculate them entirely from scratch on each update. Both of these options perform $O(1)$ work on one query type but $O(n)$ work on the other. When the query frequencies are relatively close, we can trade off some performance on one type of query for increased performance on the other. Segment trees let you do exactly that, achieving the equilibrium of $O(\\log n)$ work for both queries.\n#Segment Tree StructureThe main idea behind segment trees is this:\ncalculate the sum of the entire array and write it down somewhere; split the array into two halves, calculate the sum on both halves, and also write them down somewhere; split these halves into halves, calculate the total of four sums on them, and also write them down; …and so on, until we recursively reach segments of length one. These computed subsegment sums can be logically represented as a binary tree — which is what we call a segment tree:\nA segment tree with with the nodes relevant for the sum(11) and add(10) queries highlighted Segment trees have some nice properties:\nIf the underlying array has $n$ elements, the segment tree has exactly $(2n - 1)$ nodes — $n$ leaves and $(n - 1)$ internal nodes — because each internal node splits a segment in two, and you only need $(n - 1)$ of them to completely split the original $[0, n-1]$ range. The height of the tree is $\\Theta(\\log n)$: on each next level starting from the root, the number of nodes roughly doubles and the size of their segments roughly halves. Each segment can be split into $O(\\log n)$ non-intersecting segments that correspond to the nodes of the segment tree: you need at most two from each layer. When $n$ is not a perfect power of two, not all levels are filled entirely — the last layer may be incomplete — but the truthfulness of these properties remains unaffected. The first property allows us to use only $O(n)$ memory to store the tree, and the last two let us solve the problem in $O(\\log n)$ time:\nThe add(k, x) query can be handled by adding the value x to all nodes whose segments contain the element k, and we\u0026rsquo;ve already established that there are only $O(\\log n)$ of them. The sum(k) query can be answered by finding all nodes that collectively compose the [0, k) prefix and summing the values stored in them — and we\u0026rsquo;ve also established that there would be at most $O(\\log n)$ of them. But this is still theory. As we\u0026rsquo;ll see later, there are remarkably many ways one can implement this data structure.\n#Pointer-Based ImplementationThe most straightforward way to implement a segment tree is to store everything we need in a node explicitly: including the array segment boundaries, the sum, and the pointers to its children.\nIf we were at the \u0026ldquo;Introduction to OOP\u0026rdquo; class, we would implement a segment tree recursively like this:\nstruct segtree { int lb, rb; // the range this node is responsible for int s = 0; // the sum of elements [lb, rb) segtree *l = nullptr, *r = nullptr; // pointers to its children segtree(int lb, int rb) : lb(lb), rb(rb) { if (lb + 1 \u0026lt; rb) { // if the node is not a leaf, create children int m = (lb + rb) / 2; l = new segtree(lb, m); r = new segtree(m, rb); } } void add(int k, int x) { /* react to a[k] += x */ } int sum(int k) { /* compute the sum of the first k elements */ } }; If we needed to build it over an existing array, we would rewrite the body of the constructor like this:\nif (lb + 1 == rb) { s = a[lb]; // the node is a leaf -- its sum is just the element a[lb] } else { int t = (lb + rb) / 2; l = new segtree(lb, t); r = new segtree(t, rb); s = l-\u0026gt;s + r-\u0026gt;s; // we can use the sums of children that we\u0026#39;ve just calculated } The construction time is of no significant interest to us, so to reduce the mental burden, we will just assume that the array is zero-initialized in all future implementations.\nNow, to implement add, we need to descend down the tree until we reach a leaf node, adding the delta to the s fields:\nvoid add(int k, int x) { s += x; if (l != nullptr) { // check whether it is a leaf node if (k \u0026lt; l-\u0026gt;rb) l-\u0026gt;add(k, x); else r-\u0026gt;add(k, x); } } To calculate the sum on a segment, we can check if the query covers the current segment fully or doesn\u0026rsquo;t intersect with it at all — and return the result for this node right away. If neither is the case, we recursively pass the query to the children so that they figure it out themselves:\nint sum(int lq, int rq) { if (rb \u0026lt;= lq \u0026amp;\u0026amp; rb \u0026lt;= rq) // if we\u0026#39;re fully inside the query, return the sum return s; if (rq \u0026lt;= lb || lq \u0026gt;= rb) // if we don\u0026#39;t intersect with the query, return zero return 0; return l-\u0026gt;sum(lq, rq) + r-\u0026gt;sum(lq, rq); } This function visits a total of $O(\\log n)$ nodes because it only spawns children when a segment only partially intersects with the query, and there are at most $O(\\log n)$ of such segments.\nFor prefix sums, these checks can be simplified as the left border of the query is always zero:\nint sum(int k) { if (rb \u0026lt;= k) return s; if (lb \u0026gt;= k) return 0; return l-\u0026gt;sum(k) + r-\u0026gt;sum(k); } Since we have two types of queries, we also got two graphs to look at:\nWhile this object-oriented implementation is quite good in terms of software engineering practices, there are several aspects that make it terrible in terms of performance:\nBoth query implementations use recursion — although the add query can be tail-call optimized. Both query implementations use unpredictable branching, which stalls the CPU pipeline. The nodes store extra metadata. The structure takes $4+4+4+8+8=28$ bytes and gets padded to 32 bytes for memory alignment reasons, while only 4 bytes are really necessary to hold the integer sum. Most importantly, we are doing a lot of pointer chasing: we have to fetch the pointers to the children to descend into them, even though we can infer, ahead of time, which segments we\u0026rsquo;ll need just from the query. Pointer chasing outweighs all other issues by orders of magnitude — and to negate it, we need to get rid of pointers, making the structure implicit.\n#Implicit Segment TreesAs a segment tree is a type of binary tree, we can use the Eytzinger layout to store its nodes in one large array and use index arithmetic instead of explicit pointers to navigate it.\nMore formally, we define node $1$ to be the root, holding the sum of the entire array $[0, n)$. Then, for every node $v$ corresponding to the range $[l, r]$, we define:\nthe node $2v$ to be its left child corresponding to the range $[l, \\lfloor \\frac{l+r}{2} \\rfloor)$; the node $(2v+1)$ to be its right child corresponding to the range $[\\lfloor \\frac{l+r}{2} \\rfloor, r)$. When $n$ is a perfect power of two, this layout packs the entire tree very nicely:\nThe memory layout of the implicit segment tree with the same query path highlighted However, when $n$ is not a power of two, the layout stops being compact: although we still have exactly $(2n - 1)$ nodes regardless of how we split segments, they are no longer mapped perfectly to the $[1, 2n)$ range.\nFor example, consider what happens when we descend to the rightmost leaf in a segment tree of size $17 = 2^4 + 1$:\nwe start with the root numbered $1$ representing the range $[0, 16]$, we go to node $3 = 2 \\times 1 + 1$ representing the range $[8, 16]$, we go to node $7 = 2 \\times 2 + 1$ representing the range $[12, 16]$, we go to node $15 = 2 \\times 7 + 1$ representing the range $[14, 16]$, we go to node $31 = 2 \\times 15 + 1$ representing the range $[15, 16]$, and we finally reach node $63 = 2 \\times 31 + 1$ representing the range $[16, 16]$. So, as $63 \u0026gt; 2 \\times 17 - 1 = 33$, there are some empty spaces in the layout, but the structure of the tree is still the same, and its height is still $O(\\log n)$. For now, we can ignore this problem and just allocate a larger array for storing the nodes — it can be shown that the index of the rightmost leaf never exceeds $4n$, so allocating that many cells will always suffice:\nint t[4 * N]; // contains the node sums Now, to implement add, we create a similar recursive function but using index arithmetic instead of pointers. Since we\u0026rsquo;ve also stopped storing the borders of the segment in the nodes, we need to re-calculate them and pass them as parameters for each recursive call:\nvoid add(int k, int x, int v = 1, int l = 0, int r = N) { t[v] += x; if (l + 1 \u0026lt; r) { int m = (l + r) / 2; if (k \u0026lt; m) add(k, x, 2 * v, l, m); else add(k, x, 2 * v + 1, m, r); } } The implementation of the prefix sum query is largely the same:\nint sum(int k, int v = 1, int l = 0, int r = N) { if (l \u0026gt;= k) return 0; if (r \u0026lt;= k) return t[v]; int m = (l + r) / 2; return sum(k, 2 * v, l, m) + sum(k, 2 * v + 1, m, r); } Passing around five variables in a recursive function seems clumsy, but the performance gains are clearly worth it:\nApart from requiring much less memory, which is good for fitting into the CPU caches, the main advantage of this implementation is that we can now make use of the memory parallelism and fetch the nodes we need in parallel, considerably improving the running time for both queries.\nTo improve the performance further, we can:\nmanually optimize the index arithmetic (e.g., noticing that we need to multiply v by 2 either way), replace division by two with an explicit binary shift (because compilers aren\u0026rsquo;t always able to do it themselves), and, most importantly, get rid of recursion and make the implementation fully iterative. As add is tail-recursive and has no return value, it is easy turn it into a single while loop:\nvoid add(int k, int x) { int v = 1, l = 0, r = N; while (l + 1 \u0026lt; r) { t[v] += x; v \u0026lt;\u0026lt;= 1; int m = (l + r) \u0026gt;\u0026gt; 1; if (k \u0026lt; m) r = m; else l = m, v++; } t[v] += x; } Doing the same for the sum query is slightly harder as it has two recursive calls. The key trick is to notice that when we make these calls, one of them is guaranteed to terminate immediately as k can only be in one of the halves, so we can simply check this condition before descending the tree:\nint sum(int k) { int v = 1, l = 0, r = N, s = 0; while (true) { int m = (l + r) \u0026gt;\u0026gt; 1; v \u0026lt;\u0026lt;= 1; if (k \u0026gt;= m) { s += t[v++]; if (k == m) break; l = m; } else { r = m; } } return s; } This doesn\u0026rsquo;t improve the performance for the update query by a lot (because it was tail-recursive, and the compiler already performed a similar optimization), but the running time on the prefix sum query has roughly halved for all problem sizes:\nThis implementation still has some problems: we are using up to twice as much memory as necessary, we have costly branching, and we have to maintain and re-compute array bounds on each iteration. To get rid of these problems, we need to change our approach a little bit.\n#Bottom-Up ImplementationLet\u0026rsquo;s change the definition of the implicit segment tree layout. Instead of relying on the parent-to-child relationship, we first forcefully assign all the leaf nodes numbers in the $[n, 2n)$ range, and then recursively define the parent of node $k$ to be equal to node $\\lfloor \\frac{k}{2} \\rfloor$.\nThis structure is largely the same as before: you can still reach the root (node $1$) by dividing any node number by two, and each node still has at most two children: $2k$ and $(2k + 1)$, as anything else yields a different parent number when floor-divided by two. The advantage we get is that we\u0026rsquo;ve forced the last layer to be contiguous and start from $n$, so we can use the array of half the size:\nint t[2 * N]; When $n$ is a power of two, the structure of the tree is exactly the same as before and when implementing the queries, we can take advantage of this bottom-up approach and start from the $k$-th leaf node (simply indexed $N + k$) and ascend the tree until we reach the root:\nvoid add(int k, int x) { k += N; while (k != 0) { t[k] += x; k \u0026gt;\u0026gt;= 1; } } To calculate the sum on the $[l, r)$ subsegment, we can maintain pointers to the first and the last element that needs to be added, increase/decrease them respectively when we add a node and stop after they converge to the same node (which would be their least common ancestor):\nint sum(int l, int r) { l += N; r += N - 1; int s = 0; while (l \u0026lt;= r) { if ( l \u0026amp; 1) s += t[l++]; // l is a right child: add it and move to a cousin if (~r \u0026amp; 1) s += t[r--]; // r is a left child: add it and move to a cousin l \u0026gt;\u0026gt;= 1, r \u0026gt;\u0026gt;= 1; } return s; } Surprisingly, both queries work correctly even when $n$ is not a power of two. To understand why, consider a 13-element segment tree:\nThe first index of the last layer is always a power of two, but when the array size is not a perfect power of two, some prefix of the leaf elements gets wrapped around to the right side of the tree. Magically, this fact does not pose a problem for our implementation:\nThe add query still updates its parent nodes, even though some of them correspond to some prefix and some suffix of the array instead of a contiguous subsegment. The sum query still computes the sum on the correct subsegment, even when l is on that wrapped prefix and logically \u0026ldquo;to the right\u0026rdquo; of r because eventually l becomes the last node on a layer and gets incremented, suddenly jumping to the first element of the next layer and proceeding normally after adding just the right nodes on the wrapped-around part of the tree (look at the dimmed nodes in the illustration). Compared to the top-down approach, we use half the memory and don\u0026rsquo;t have to maintain query ranges, which results in simpler and consequently faster code:\nWhen running the benchmarks, we use the sum(l, r) procedure for computing a general subsegment sum and just fix l equal to 0. To achieve higher performance on the prefix sum query, we want to avoid maintaining l and only move the right border like this:\nint sum(int k) { int s = 0; k += N - 1; while (k != 0) { if (~k \u0026amp; 1) // if k is a right child s += t[k--]; k = k \u0026gt;\u0026gt; 1; } return s; } In contrast, this prefix sum implementation doesn\u0026rsquo;t work unless $n$ is not a power of two — because k could be on that wrapped-around part, and we\u0026rsquo;d sum almost the entire array instead of a small prefix.\nTo make it work for arbitrary array sizes, we can permute the leaves so that they are in the left-to-right logical order in the last two layers of the tree. In the example above, this would mean adding $3$ to all leaf indexes and then moving the last three leaves one level higher by subtracting $13$.\nIn the general case, this can be done using predication in a few cycles like this:\nconst int last_layer = 1 \u0026lt;\u0026lt; __lg(2 * N - 1); // calculate the index of the leaf k int leaf(int k) { k += last_layer; k -= (k \u0026gt;= 2 * N) * N; return k; } When implementing the queries, all we need to do is to call the leaf function to get the correct leaf index:\nvoid add(int k, int x) { k = leaf(k); while (k != 0) { t[k] += x; k \u0026gt;\u0026gt;= 1; } } int sum(int k) { k = leaf(k - 1); int s = 0; while (k != 0) { if (~k \u0026amp; 1) s += t[k--]; k \u0026gt;\u0026gt;= 1; } return s; } The last touch: by replacing the s += t[k--] line with predication, we can make the implementation branchless (except for the last branch — we still need to check the loop condition):\nint sum(int k) { k = leaf(k - 1); int s = 0; while (k != 0) { s += (~k \u0026amp; 1) ? t[k] : 0; // will be replaced with a cmov k = (k - 1) \u0026gt;\u0026gt; 1; } return s; } When combined, these optimizations make the prefix sum queries run much faster:\nNotice that the bump in the latency for the prefix sum query starts at $2^{19}$ and not at $2^{20}$, the L3 cache boundary. This is because we are still storing $2n$ integers and also fetching the t[k] element regardless of whether we will add it to s or not. We can actually solve both of these problems.\n#Fenwick treesImplicit structures are great: they avoid pointer chasing, allow visiting all the relevant nodes in parallel, and take less space as they don\u0026rsquo;t store metadata in nodes. Even better than implicit structures are succinct structures: they only require the information-theoretical minimum space to store the structure, using only $O(1)$ additional memory.\nTo make a segment tree succinct, we need to look at the values stored in the nodes and search for redundancies — the values that can be inferred from others — and remove them. One way to do this is to notice that in every implementation of prefix sum, we\u0026rsquo;ve never used the sums stored in right children — therefore, for computing prefix sums, such nodes are redundant:\nThe Fenwick tree (also called binary indexed tree — soon you\u0026rsquo;ll understand why) is a type of segment tree that uses this consideration and gets rid of all right children, essentially removing every second node in each layer and making the total node count the same as the underlying array.\nint t[N + 1]; // +1 because we use use one-based indexing To store these segment sums compactly, the Fenwick tree ditches the Eytzinger layout: instead, in place of every element $k$ that would be a leaf in the last layer of a segment tree, it stores the sum of its first non-removed ancestor. For example:\nthe element $7$ would hold the sum on the $[0, 7]$ range ($282$), the element $9$ would hold the sum on the $[8, 9]$ range ($-86$), the element $10$ would hold the sum on the $[10, 10]$ range ($-52$, the element itself). How to compute this range for a given element $k$ (the left boundary, to be more specific: the right boundary is always the element $k$ itself) quicker than simulating the descend down the tree? Turns out, there is a smart bit trick that works when the tree size is a power of two and we use one-based indexing — just remove the least significant bit of the index:\nthe left bound for element $7 + 1 = 8 = 1000_2$ is $0000_2 = 0$, the left bound for element $9 + 1 = 10 = 1010_2$ is $1000_2 = 8$, the left bound for element $10 + 1 = 11 = 1011_2$ is $1010_2 = 10$. And to get the last set bit of an integer, we can use this procedure:\nint lowbit(int x) { return x \u0026amp; -x; } This trick works by the virtue of how signed numbers are stored in binary using two\u0026rsquo;s complement. When we compute -x, we implicitly subtract it from a large power of two: some prefix of the number flips, some suffix of zeros at the end remains, and the only one-bit that stays unchanged is the last set bit — which will be the only one surviving x \u0026amp; -x. For example:\n+90 = 64 + 16 + 8 + 2 = (0)10110 -90 = 00000 - 10110 = (1)01010 → (+90) \u0026amp; (-90) = (0)00010 We\u0026rsquo;ve established what a Fenwick tree is just an array of size n where each element k is defined to be the sum of elements from k - lowbit(k) + 1 and k inclusive in the original array, and now it\u0026rsquo;s time to implement some queries.\nImplementing the prefix sum query is easy. The t[k] holds the sum we need except for the first k - lowbit(k) elements, so we can just add it to the result and then jump to k - lowbit(k) and continue doing this until we reach the beginning of the array:\nint sum(int k) { int s = 0; for (; k != 0; k -= lowbit(k)) s += t[k]; return s; } Since we are repeatedly removing the lowest set bit from k, and also since this procedure is equivalent to visiting the same left-child nodes in a segment tree, each sum query can touch at most $O(\\log n)$ nodes:\nA path for a prefix sum query in a Fenwick tree To slightly improve the performance of the sum query, we use k \u0026amp;= k - 1 to remove the lowest bit in one go, which is one instruction faster than k -= k \u0026amp; -k:\nint sum(int k) { int s = 0; for (; k != 0; k \u0026amp;= k - 1) s += t[k]; return s; } Unlike all previous segment tree implementations, a Fenwick tree is a structure where it is easier and more efficient to calculate the sum on a subsegment as the difference of two prefix sums:\n// [l, r) int sum (int l, int r) { return sum(r) - sum(l); } The update query is easier to code but less intuitive. We need to add a value x to all nodes that are left-child ancestors of leaf k. Such nodes have indices m larger than k but m - lowbit(m) \u0026lt; k so that k is included in their ranges.\nAll such indices need to have a common prefix with k, then a 1 where it was 0 in k, and then a suffix of zeros so that that 1 canceled and the result of m - lowbit(m) is less than k. All such indices can be generated iteratively like this:\nvoid add(int k, int x) { for (k += 1; k \u0026lt;= N; k += k \u0026amp; -k) t[k] += x; } Repeatedly adding the lowest set bit to k makes it \u0026ldquo;more even\u0026rdquo; and lifts it to its next left-child segment tree ancestor:\nA path for an update query in a Fenwick tree Now, if we leave all the code as it is, it works correctly even when $n$ is not a power of two. In this case, the Fenwick tree is not equivalent to a segment tree of size $n$ but to a forest of up to $O(\\log n)$ segment trees of power-of-two sizes — or to a single segment tree padded with zeros to a large power of two, if you like to think this way. In either case, all procedures still work correctly as they never touch anything outside the $[1, n]$ range.\nThe performance of the Fenwick tree is similar to the optimized bottom-up segment tree for the update queries and slightly faster for the prefix sum queries:\nThere is one weird thing on the graph. After we cross the L3 cache boundary, the performance takes off very rapidly. This is a cache associativity effect: the most frequently used cells all have their indices divisible by large powers of two, so they get aliased to the same cache set, kicking each other out and effectively reducing the cache size.\nOne way to negate this effect is to insert \u0026ldquo;holes\u0026rdquo; in the layout like this:\ninline constexpr int hole(int k) { return k + (k \u0026gt;\u0026gt; 10); } int t[hole(N) + 1]; void add(int k, int x) { for (k += 1; k \u0026lt;= N; k += k \u0026amp; -k) t[hole(k)] += x; } int sum(int k) { int res = 0; for (; k != 0; k \u0026amp;= k - 1) res += t[hole(k)]; return res; } Computing the hole function is not on the critical path between iterations, so it does not introduce any significant overhead but completely removes the cache associativity problem and shrinks the latency by up to 3x on large arrays:\nFenwick trees are fast, but there are still other minor issues with them. Similar to binary search, the temporal locality of their memory accesses is not the greatest, as rarely accessed elements are grouped with the most frequently accessed ones. Fenwick trees also execute a non-constant number of iterations and have to perform end-of-loop checks, very likely causing a branch misprediction — although just a single one.\nThere are probably still some things to optimize, but we are going to leave it there and focus on an entirely different approach, and if you know S-trees, you probably already know where this is headed.\n#Wide Segment TreesHere is the main idea: if the memory system is fetching a full cache line for us anyway, let\u0026rsquo;s fill it to the maximum with information that lets us process the query quicker. For segment trees, this means storing more than one data point in a node. This lets us reduce the tree height and perform fewer iterations when descending or ascending it:\nWe will use the term wide (B-ary) segment tree to refer to this modification.\nTo implement this layout, we can use a similar constexpr-based approach we used in S+ trees:\nconst int b = 4, B = (1 \u0026lt;\u0026lt; b); // cache line size (in integers, not bytes) // the height of the tree over an n-element array constexpr int height(int n) { return (n \u0026lt;= B ? 1 : height(n / B) + 1); } // where the h-th layer starts constexpr int offset(int h) { int s = 0, n = N; while (h--) { n = (n + B - 1) / B; s += n * B; } return s; } constexpr int H = height(N); alignas(64) int t[offset(H)]; // an array for storing nodes This way, we effectively reduce the height of the tree by approximately $\\frac{\\log_B n}{\\log_2 n} = \\log_2 B$ times ($\\sim4$ times if $B = 16$), but it becomes non-trivial to implement in-node operations efficiently. For our problem, we have two main options:\nWe could store $B$ sums in each node (for each of its $B$ children). We could store $B$ prefix sums in each node (the $i$-th being the sum of the first $(i + 1)$ children). If we go with the first option, the add query would be largely the same as in the bottom-up segment tree, but the sum query would need to add up to $B$ scalars in each node it visits. And if we go with the second option, the sum query would be trivial, but the add query would need to add x to some suffix on each node it visits.\nIn either case, one operation would perform $O(\\log_B n)$ operations, touching just one scalar in each node, while the other would perform $O(B \\cdot \\log_B n)$ operations, touching up to $B$ scalars in each node. We can, however, use SIMD to accelerate the slower operation, and since there are no fast horizontal reductions in SIMD instruction sets, but it is easy to add a vector to a vector, we will choose the second approach and store prefix sums in each node.\nThis makes the sum query extremely fast and easy to implement:\nint sum(int k) { int s = 0; for (int h = 0; h \u0026lt; H; h++) s += t[offset(h) + (k \u0026gt;\u0026gt; (h * b))]; return s; } The add query is more complicated and slower. We need to add a number only to a suffix of a node, and we can do this by masking out the positions that should not be modified.\nWe can pre-calculate a $B \\times B$ array corresponding to $B$ such masks that tell, for each of $B$ positions within a node, whether a certain prefix sum value needs to be updated or not:\nstruct Precalc { alignas(64) int mask[B][B]; constexpr Precalc() : mask{} { for (int k = 0; k \u0026lt; B; k++) for (int i = 0; i \u0026lt; B; i++) mask[k][i] = (i \u0026gt; k ? -1 : 0); } }; constexpr Precalc T; Apart from this masking trick, the rest of the computation is simple enough to be handled with GCC vector types only. When processing the add query, we just use these masks to bitwise-and them with the broadcasted x value to mask it and then add it to the values stored in the node:\ntypedef int vec __attribute__ (( vector_size(32) )); constexpr int round(int k) { return k \u0026amp; ~(B - 1); // = k / B * B } void add(int k, int x) { vec v = x + vec{}; for (int h = 0; h \u0026lt; H; h++) { auto a = (vec*) \u0026amp;t[offset(h) + round(k)]; auto m = (vec*) T.mask[k % B]; for (int i = 0; i \u0026lt; B / 8; i++) a[i] += v \u0026amp; m[i]; k \u0026gt;\u0026gt;= b; } } This speeds up the sum query by more than 10x and the add query by up to 4x compared to the Fenwick tree:\nUnlike S-trees, the block size can be easily changed in this implementation (by literally changing one character). Expectedly, when we increase it, the update time also increases as we need to fetch more cache lines and process them, but the sum query time decreases as the height of the tree becomes smaller:\nSimilar to the S+ trees, the optimal memory layout probably has non-uniform block sizes, depending on the problem size and the distribution of queries, but we are not going to explore this idea and just leave the optimization here.\n#ComparisonsWide segment trees are significantly faster compared to other popular segment tree implementations:\nThe relative speedup is in the orders of magnitude:\nCompared to the original pointer-based implementation, the wide segment tree is up to 200 and 40 times faster for the prefix sum and update queries, respectively — although, for sufficiently large arrays, both implementations become purely memory-bound, and this speedup goes down to around 60 and 15 respectively.\n#ModificationsWe have only focused on the prefix sum problem for 32-bit integers — to make this already long article slightly less long and also to make the comparison with the Fenwick tree fair — but wide segment trees can be used for other common range operations, although implementing them efficiently with SIMD requires some creativity.\nDisclaimer: I haven\u0026rsquo;t implemented any of these ideas, so some of them may be fatally flawed.\nOther data types can be trivially supported by changing the vector type and, if they differ in size, the node size $B$ — which also changes the tree height and hence the total number of iterations for both queries.\nIt may also be that the queries have different limits on the updates and the prefix sum queries. For example, it is not uncommon to have only \u0026ldquo;$\\pm 1$\u0026rdquo; update queries with a guarantee that the result of the prefix sum query always fits into a 32-bit integer. If the result could fit into 8 bits, we\u0026rsquo;d simply use a 8-bit char with block size of $B=64$ bytes, making the total tree height $\\frac{\\log_{16} n}{\\log_{64} n} = \\log_{16} 64 = 1.5$ times smaller and both queries proportionally faster.\nUnfortunately, that doesn\u0026rsquo;t work in the general case, but we still have a way to speed up queries when the update deltas are small: we can buffer the updates queries. Using the same \u0026ldquo;$\\pm 1$\u0026rdquo; example, we can make the branching factor $B=64$ as we wanted, and in each node, we store $B$ 32-bit integers, $B$ 8-bit signed chars, and a single 8-bit counter variable that starts at $127$ and decrements each time we update a node. Then, when we process the queries in nodes:\nFor the update query, we add a vector of masked 8-bit plus-or-minus ones to the char array, decrement the counter, and, if it is zero, convert the values in the char array to 32-bit integers, add them to the integer array, set the char array to zero, and reset the counter back to 127. For the prefix sum query, we visit the same nodes but add both int and char values to the result. This update accumulation trick lets us increase the performance by up to 1.5x at the cost of using ~25% more memory.\nHaving a conditional branch in the add query and adding the char array to the int array is rather slow, but since we only have to do it every 127 iterations, it doesn\u0026rsquo;t cost us anything in the amortized sense. The processing time for the sum query increases, but not significantly — because it mostly depends on the slowest read rather than the number of iterations.\nGeneral range queries can be supported the same way as in the Fenwick tree: just decompose the range $[l, r)$ as the difference of two prefix sums $[0, r)$ and $[0, l)$.\nThis also works for some operations other than addition (multiplication modulo prime, xor, etc.), although they have to be reversible: there should be a way to quickly \u0026ldquo;cancel\u0026rdquo; the operation on the left prefix from the final result.\nNon-reversible operations can also be supported, although they should still satisfy some other properties:\nThey must be associative: $(a \\circ b) \\circ c = a \\circ (b \\circ c)$. They must have an identity element: $a \\circ e = e \\circ a = a$. (Such algebraic structures are called monoids if you\u0026rsquo;re a snob.)\nUnfortunately, the prefix sum trick doesn\u0026rsquo;t work when the operation is not reversible, so we have to switch to option one and store the results of these operations separately for each segment. This requires some significant changes to the queries:\nThe update query should replace one scalar at the leaf, perform a horizontal reduction at the leaf node, and then continue upwards, replacing one scalar of its parent and so on. The range reduction query should, separately for left and right borders, calculate a vector with vertically reduced values on their paths, combine these two vectors into one, and then reduce it horizontally to return the final answer. Note that we still need to use masking to replace values outside of query with neutral elements, and this time, it probably requires some conditional moves/blending and either $B \\times B$ precomputed masks or using two masks to account for both left and right borders of the query. This makes both queries much slower — especially the reduction — but this should still be faster compared to the bottom-up segment tree.\nMinimum is a nice exception where the update query can be made slightly faster if the new value of the element is less than the current one: we can skip the horizontal reduction part and just update $\\log_B n$ nodes using a scalar procedure.\nThis works very fast when we mostly have such updates, which is the case, e.g., for the sparse-graph Dijkstra algorithm when we have more edges than vertices. For this problem, the wide segment tree can serve as an efficient fixed-universe min-heap.\nLazy propagation can be done by storing a separate array for the delayed operations in a node. To propagate the updates, we need to go top to bottom (which can be done by simply reversing the direction of the for loop and using k \u0026gt;\u0026gt; (h * b) to calculate the h-th ancestor), broadcast and reset the delayed operation value stored in the parent of the current node, and apply it to all values stored in the current node with SIMD.\nOne minor problem is that for some operations, we need to know the lengths of the segments: for example, when we need to support a sum and a mass assignment. It can be solved by either padding the elements so that each segment on a layer is uniform in size, pre-calculating the segment lengths and storing them in the node, or using predication to check for the problematic nodes (there will be at most one on each layer).\n#AcknowledgementsMany thanks to Giulio Ermanno Pibiri for collaborating on this case study, which is largely based on his 2020 paper \u0026ldquo;Practical Trade-Offs for the Prefix-Sum Problem\u0026rdquo; co-authored with Rossano Venturini. I highly recommend reading the original article if you are interested in the details we\u0026rsquo;ve skipped through here for brevity.\nThe code and some ideas regarding bottom-up segment trees were adapted from a 2015 blog post \u0026ldquo;Efficient and easy segment trees\u0026rdquo; by Oleksandr Bacherikov.\nSegment trees are rarely mentioned in the theoretical computer science literature because they are relatively novel (invented ~2000), mostly don\u0026rsquo;t do anything that any other binary tree can\u0026rsquo;t do, and asymptotically aren\u0026rsquo;t faster — although, in practice, they often win by a lot in terms of speed.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":48,"path":"/hugo-page/hpc/data-structures/segment-trees/","title":"Segment Trees"},{"content":"Optimizing for latency is usually quite different from optimizing for throughput:\nWhen optimizing data structure queries or small one-time or branchy algorithms, you need to look up the latencies of its instructions, mentally construct the execution graph of the computation, and then try to reorganize it so that the critical path is shorter. When optimizing hot loops and large-dataset algorithms, you need to look up the throughputs of their instructions, count how many times each one is used per iteration, determine which of them is the bottleneck, and then try to restructure the loop so that it is used less often. The last advice only works for data-parallel loops, where each iteration is fully independent of the previous one. When there is some interdependency between consecutive iterations, there may potentially be a pipeline stall caused by a data hazard as the next iteration is waiting for the previous one to complete.\n#ExampleAs a simple example, consider how the sum of an array is computed:\nint s = 0; for (int i = 0; i \u0026lt; n; i++) s += a[i]; Let\u0026rsquo;s assume for a moment that the compiler doesn\u0026rsquo;t vectorize this loop, the memory bandwidth isn\u0026rsquo;t a concern, and that the loop is unrolled so that we don\u0026rsquo;t pay any additional cost associated with maintaining the loop variables. In this case, the computation becomes very simple:\nint s = 0; s += a[0]; s += a[1]; s += a[2]; s += a[3]; // ... How fast can we compute this? At exactly one cycle per element — because we need one cycle each iteration to add another value to s. The latency of the memory read doesn\u0026rsquo;t matter because the CPU can start it ahead of time.\nBut we can go higher than that. The throughput of add1 is 2 on my CPU (Zen 2), meaning we could theoretically execute two of them every cycle. But right now this isn\u0026rsquo;t possible: while s is being used to accumulate $i$-th element, it can\u0026rsquo;t be used for $(i+1)$-th for at least one cycle.\nThe solution is to use two accumulators and just sum up odd and and even elements separately:\nint s0 = 0, s1 = 0; s0 += a[0]; s1 += a[1]; s0 += a[2]; s1 += a[3]; // ... int s = s0 + s1; Now our superscalar CPU can execute these two \u0026ldquo;threads\u0026rdquo; simultaneously, and our computation no longer has any critical paths that limit the throughput.\n#The General CaseIf an instruction has a latency of $x$ and a throughput of $y$, then you would need to use $x \\cdot y$ accumulators to saturate it. This also implies that you need $x \\cdot y$ logical registers to hold their values, which is an important consideration for CPU designs, limiting the maximum number of usable execution units for high-latency instructions.\nThis technique is mostly used with SIMD and not in scalar code. You can generalize the code above and compute sums and other reductions faster than the compiler.\nIn general, when optimizing loops, you usually have just one or a few execution ports that you want to utilize to their fullest, and you engineer the rest of the loop around them. As different instructions may use different sets of ports, it is not always clear which one is going to be overused. In situations like this, machine code analyzers can be very helpful for finding the bottlenecks of small assembly loops.\nThe throughput of register-register add is 4, but since we are reading its second operand from memory, it is bottlenecked by the throughput of memory mov, which is 2 on Zen 2.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":49,"path":"/hugo-page/hpc/pipelining/throughput/","title":"Throughput Computing"},{"content":"This is an upcoming high performance computing book titled \u0026ldquo;Algorithms for Modern Hardware\u0026rdquo; by Sergey Slotin.\nIts intended audience is everyone from performance engineers and practical algorithm researchers to undergraduate computer science students who have just finished an advanced algorithms course and want to learn more practical ways to speed up a program than by going from $O(n \\log n)$ to $O(n \\log \\log n)$.\nAll book materials are hosted on GitHub, with code in a separate repository. This isn\u0026rsquo;t a collaborative project, but any contributions and feedback are very much welcome.\nFAQBug/typo fixes. If you spot an error on any page, please do one of these — in the order of preference:\nfix it right away by either clicking on the pencil icon on the top right on any page (opens the Prose editor) or, more traditionally, by modifying the page directly on GitHub (the link to the source is also on the top right); create an issue on GitHub; tell me about it directly; or leave a comment on some other website where it is being discussed — I read most of HackerNews, CodeForces, and Twitter threads where I\u0026rsquo;m tagged.\nRelease date. The book is split into several parts that I plan to finish sequentially with long breaks in-between. Part I, Performance Engineering, is ~75% complete as of March 2022 and will hopefully be \u0026gt;95% complete by this summer.\nA \u0026ldquo;release\u0026rdquo; for an open-source book like this essentially means:\nfinishing all essential sections and filling all the TODOs, mostly freezing the table of contents (except for the case studies), doing one final round of heavy copyediting (hopefully, with the help of a professional editor — I still haven’t figured out how commas work in English), drawing illustrations (I stole a lot of those that are currently displayed), making a print-optimized PDF and figuring out the best way to distribute it. After that, I will mostly be fixing errors and only doing some minor edits reflecting the changes in technology or new algorithm advancements. The e-book/printed editions will most likely be sold on a \u0026ldquo;pay what you want\u0026rdquo; basis, and in any case, the web version will always be fully available online.\nPre-ordering / financially supporting the book. Due to my unfortunate citizenship and place of birth, you can\u0026rsquo;t — that is, until I find a way that at the same time complies with international sanctions, doesn\u0026rsquo;t sponsor the war, and won\u0026rsquo;t put me in prison for tax evasion.\nSo, don\u0026rsquo;t bother. If you want to support this book, just share it and help fix typos — that would be enough.\nTranslations. The website has a separate functionality for creating and managing translations — and I\u0026rsquo;ve already been contacted by some nice people willing to translate the book into Italian and Chinese (and I will personally translate at least some of it into my native Russian).\nHowever, as the book is still evolving, it is probably not the best idea to start translating it at least until Part I is finished. That said, you are very much encouraged to make translations of any articles and publish them in your blogs — just send me the link so that we can merge it back when centralized translation starts.\n\u0026ldquo;Translating\u0026rdquo; the Russian version. The articles hosted at ru.algorithmica.org/cs/ are not about advanced performance engineering but mostly about classical computer science algorithms — without discussing how to speed them up beyond asymptotic complexity. Most of the information there is not unique and already exists in English on some other places on the internet: for example, the similar-spirited cp-algorithms.com.\nTeaching performance engineering in colleges. One of my goals for writing this book is to change the way computer science — algorithm design, to be more precise — is taught in colleges. Let me elaborate on that.\nThere are two highly impactful textbooks on which most computer science courses are built. Both are undoubtedly outstanding, but one of them is 50 years old, and the other is 30 years old, and computers have changed a lot since then. Asymptotic complexity is not the sole deciding factor anymore. In modern practical algorithm design, you choose the approach that makes better use of different types of parallelism available in the hardware over the one that theoretically does fewer raw operations on galaxy-scale inputs.\nAnd yet, the computer science curricula in most colleges completely ignore this shift. Although there are some great courses that aim to correct that — such as \u0026ldquo;Performance Engineering of Software Systems\u0026rdquo; from MIT, \u0026ldquo;Programming Parallel Computers\u0026rdquo; from Aalto University, and some non-academic ones like Denis Bakhvalov\u0026rsquo;s \u0026ldquo;Performance Ninja\u0026rdquo; — most computer science graduates still treat modern hardware like something from the 1990s.\nWhat I really want to achieve is that performance engineering becomes taught right after introduction to algorithms. Writing the first comprehensive textbook on the subject is a large part of it, and this is why I rush to finish it by the summer so that the colleges can pick it up in the next academic year. But creating a new course requires more than that: you need a balanced curriculum, course infrastructure, lecture slides, lab assignments… so for some time after finishing the main book, I will be working on course materials and tools for teaching performance engineering — and I\u0026rsquo;m looking forward to collaborating with other people who want to make it a reality as well.\nPart I: Performance EngineeringThe first part covers the basics of computer architecture and optimization of single-threaded algorithms.\nIt walks through the main CPU optimization topics such as caching, SIMD, and pipelining, and provides brief examples in C++, followed by large case studies where we usually achieve a significant speedup over some STL algorithm or data structure.\nPlanned table of contents:\n0. Preface 1. Complexity Models 1.1. Modern Hardware 1.2. Programming Languages 1.3. Models of Computation 1.4. When to Optimize 2. Computer Architecture 1.1. Instruction Set Architectures 1.2. Assembly Language 1.3. Loops and Conditionals 1.4. Functions and Recursion 1.5. Indirect Branching 1.6. Machine Code Layout 1.7. System Calls 1.8. Virtualization 3. Instruction-Level Parallelism 3.1. Pipeline Hazards 3.2. The Cost of Branching 3.3. Branchless Programming 3.4. Instruction Tables 3.5. Instruction Scheduling 3.6. Throughput Computing 3.7. Theoretical Performance Limits 4. Compilation 4.1. Stages of Compilation 4.2. Flags and Targets 4.3. Situational Optimizations 4.4. Contract Programming 4.5. Non-Zero-Cost Abstractions 4.6. Compile-Time Computation 4.7. Arithmetic Optimizations 4.8. What Compilers Can and Can\u0026#39;t Do 5. Profiling 5.1. Instrumentation 5.2. Statistical Profiling 5.3. Program Simulation 5.4. Machine Code Analyzers 5.5. Benchmarking 5.6. Getting Accurate Results 6. Arithmetic 6.1. Floating-Point Numbers 6.2. Interval Arithmetic 6.3. Newton\u0026#39;s Method 6.4. Fast Inverse Square Root 6.5. Integers 6.6. Integer Division 6.7. Bit Manipulation (6.8. Data Compression) 7. Number Theory 7.1. Modular Inverse 7.2. Montgomery Multiplication (7.3. Finite Fields) (7.4. Error Correction) 7.5. Cryptography 7.6. Hashing 7.7. Random Number Generation 8. External Memory 8.1. Memory Hierarchy 8.2. Virtual Memory 8.3. External Memory Model 8.4. External Sorting 8.5. List Ranking 8.6. Eviction Policies 8.7. Cache-Oblivious Algorithms 8.8. Spacial and Temporal Locality (8.9. B-Trees) (8.10. Sublinear Algorithms) (9.13. Memory Management) 9. RAM \u0026amp; CPU Caches 9.1. Memory Bandwidth 9.2. Memory Latency 9.3. Cache Lines 9.4. Memory Sharing 9.5. Memory-Level Parallelism 9.6. Prefetching 9.7. Alignment and Packing 9.8. Pointer Alternatives 9.9. Cache Associativity 9.10. Memory Paging 9.11. AoS and SoA 10. SIMD Parallelism 10.1. Intrinsics and Vector Types 10.2. Moving Data 10.3. Reductions 10.4. Masking and Blending 10.5. In-Register Shuffles 10.6. Auto-Vectorization and SPMD 11. Algorithm Case Studies 11.1. Binary GCD (11.2. Prime Number Sieves) 11.3. Integer Factorization 11.4. Logistic Regression 11.5. Big Integers \u0026amp; Karatsuba Algorithm 11.6. Fast Fourier Transform 11.7. Number-Theoretic Transform 11.8. Argmin with SIMD 11.9. Prefix Sum with SIMD 11.10. Reading Decimal Integers 11.11. Writing Decimal Integers (11.12. Reading and Writing Floats) (11.13. String Searching) 11.14. Sorting 11.15. Matrix Multiplication 12. Data Structure Case Studies 12.1. Binary Search 12.2. Static B-Trees (12.3. Search Trees) 12.4. Segment Trees (12.5. Tries) (12.6. Range Minimum Query) 12.7. Hash Tables (12.8. Bitmaps) (12.9. Probabilistic Filters) Among the cool things that we will speed up:\n2x faster GCD (compared to std::gcd) 8-15x faster binary search (compared to std::lower_bound) 5-10x faster segment trees (compared to Fenwick trees) 5x faster hash tables (compared to std::unordered_map) 2x faster popcount (compared to repeatedly calling popcnt) 35x faster parsing series of integers (compared to scanf) ?x faster sorting (compared to std::sort) 2x faster sum (compared to std::accumulate) 2-3x faster prefix sum (compared to naive implementation) 10x faster argmin (compared to naive implementation) 10x faster array searching (compared to std::find) 15x faster search tree (compared to std::set) 100x faster matrix multiplication (compared to \u0026ldquo;for-for-for\u0026rdquo;) optimal word-size integer factorization (~0.4ms per 60-bit integer) optimal Karatsuba Algorithm optimal FFT Volume: 450-600 pages\nRelease date: Q3 2022\nPart II: Parallel AlgorithmsConcurrency, models of parallelism, context switching, green threads, concurrent runtimes, cache coherence, synchronization primitives, OpenMP, reductions, scans, list ranking, graph algorithms, lock-free data structures, heterogeneous computing, CUDA, kernels, warps, blocks, matrix multiplication, sorting.\nVolume: 150-200 pages\nRelease date: 2023-2024?\nPart III: Distributed Computing Metworking, message passing, actor model, communication-constrained algorithms, distributed primitives, all-reduce, MapReduce, stream processing, query planning, storage, sharding, compression, distributed databases, consistency, reliability, scheduling, workflow engines, cloud computing.\nRelease date: ??? (more likely to be completed than not)\nPart IV: Software \u0026amp; Hardware LLVM IR, compiler optimizations \u0026amp; back-end, interpreters, JIT-compilation, Cython, JAX, Numba, Julia, OpenCL, DPC++, oneAPI, XLA, (basic) Verilog, FPGAs, ASICs, TPUs and other AI accelerators.\nRelease date: ??? (less likely to be completed than not)\nAcknowledgementsThe book is largely based on blog posts, research papers, conference talks, and other work authored by a lot of people:\nAgner Fog Daniel Lemire Andrei Alexandrescu Chandler Carruth Wojciech Muła Malte Skarupke Travis Downs Brendan Gregg Andreas Abel Jakob Kogler Igor Ostrovsky Steven Pigeon Denis Bakhvalov Paul Khuong Pat Morin Victor Eijkhout Robert van de Geijn Edmond Chow Peter Cordes Geoff Langdale Matt Kulukundis Georg Sauthoff Danila Kutenin Ivica Bogosavljević Matt Pharr Jan Wassenberg Marshall Lochbaum Pavel Zemtsov Gustavo Duarte Nyaan Nayuki Konstantin InstLatX64 ridiculous_fish Z boson Creel Disclaimer: Technology ChoicesThe examples in this book use C++, GCC, x86-64, CUDA, and Spark, although the underlying principles conveyed are not specific to them.\nTo clear my conscience, I\u0026rsquo;m not happy with any of these choices: these technologies just happen to be the most widespread and stable at the moment and thus more helpful to the reader. I would have respectively picked C / Rust / Carbon?, LLVM, arm, OpenCL, and Dask; maybe there will be a 2nd edition in which some of the tech stack is changed.\n","id":50,"path":"/hugo-page/hpc/","title":"Algorithms for Modern Hardware"},{"content":"If you are reading this chapter sequentially from the beginning, you might be wondering: why would I introduce integer arithmetic after floating-point one? Isn\u0026rsquo;t it supposed to be easier?\nTrue: plain integer representations are simpler. But, counterintuitively, their simplicity allows for more possibilities for operations to be expressed in terms of others. And if floating-point representations are so unwieldy that most of their operations are implemented in hardware, efficiently manipulating integers requires much more creative use of the instruction set.\n#Binary FormatsUnsigned integers are just natural numbers written in binary:\n$$ \\begin{aligned} 5_{10} \u0026amp;= 101_2 = 4 + 1 \\ 42_{10} \u0026amp;= 101010_2 = 32 + 8 + 2 \\ 256_{10} \u0026amp;= 100000000_2 = 2^8 \\end{aligned} $$\nWhen the result of an operation can\u0026rsquo;t fit into the word size (e.g., is more or equal to $2^{32}$ for 32-bit unsigned integers), it overflows by leaving only the lowest 32 bits of the result. Similarly, if the result is a negative value, it underflows by adding it to $2^{32}$, so that it always stays in the $[0, 2^{32})$ range.\nThis is equivalent to performing all operations modulo a power of two:\n$$ \\begin{aligned} 256 \u0026amp;\\equiv 0 \\pmod {2^8} \\ 2021 \u0026amp;\\equiv 229 \\pmod {2^8} \\ -42 \\equiv 256 - 42 \u0026amp;\\equiv 214 \\pmod {2^8} \\end{aligned} $$\nIn either case, it raises a special flag which you can check, but usually when people explicitly use unsigned integers, they are expecting this behavior.\n#Signed IntegersSigned integers support storing negative values by dedicating the highest bit to represent the sign of the number, in a similar fashion as floating-point numbers do. This halves the range of representable non-negative numbers: the maximum possible 32-bit integer is now $(2^{31}-1)$ and not $(2^{32}-1)$. But the encoding of negative values is not quite the same as for floating-point numbers.\nComputer engineers are even lazier than programmers — and this is not only motivated by the instinctive desire for simplification, but also by saving transistor space. This can be achieved by reusing circuitry that you already have for other operations, which is what they aimed for when designing the signed integer format:\nFor an $n$-bit signed integer type, the encodings of all numbers in the $[0, 2^{n-1})$ range remain the same as their unsigned binary representations. All numbers in the $[-2^{n-1}, 0)$ range are encoded sequentially right after the \u0026ldquo;positive\u0026rdquo; range — that is, starting with $(-2^{n - 1})$ that has code $(2^{n-1})$ and ending with $(-1)$ that has code $(2^n - 1)$. One way to look at this is that all negative numbers are just encoded as if they were subtracted from $2^n$ — an operation known as two\u0026rsquo;s complement:\n$$ \\begin{aligned} -x \u0026amp;= 2^{32} - x \\ \u0026amp;= \\bar{x} + 1 \\end{aligned} $$\nHere $\\bar{x}$ represents bitwise negation, which can be also thought of as subtracting $x$ from $(2^n - 1)$.\nAs an exercise, here are some facts about signed integers:\nAll positive numbers and zero remain the same as their binary notation. All negative numbers have the highest bit set to one. There are more negative numbers than positive numbers (exactly by one — because of zero). For int, if you add $1$ to $(2^{31}-1)$, the result will be $-2^{31}$, represented as 10000000 (for exposition purposes, we will only write 8 bits instead of 32). Knowing a binary notation of a positive number x, you can get the binary notation of -x as ~x + 1. -1 is represented as ~1 + 1 = 11111110 + 00000001 = 11111111. -42 is represented as ~42 + 1 = 11010101 + 00000001 = 11010110. The number -1 = 11111111 is followed by 0 = -1 + 1 = 11111111 + 00000001 = 00000000. The main advantage of this encoding is that you don\u0026rsquo;t have to do anything to convert unsigned integers to signed ones (except maybe check for overflow), and you can reuse the same circuitry for most operations, possibly only flipping the sign bit for comparisons and such.\nThat said, you need to be careful with signed integer overflows. Even though they almost always overflow the same way as unsigned integers, programming languages usually consider the possibility of overflow as undefined behavior. If you need to overflow integer variables, convert them to unsigned integers: it\u0026rsquo;s free anyway.\nExercise. What is the only integer value for which std::abs produces a wrong result? What will this result be?\n#Integer TypesIntegers come in different sizes, but all function roughly the same.\nBits Bytes Signed C type Unsigned C type Assembly 8 1 signed char1 unsigned char byte 16 2 short unsigned short word 32 4 int unsigned int dword 64 8 long long unsigned long long qword The bits of an integer are simply stored sequentially. The only ambiguity here is the order in which to store them — left to right or right to left — called endianness. Depending on the architecture, the format can be either:\nLittle-endian, which lists lower bits first. For example, $42_{10}$ will be stored as $010101$. Big-endian, which lists higher bits first. All previous examples in this article follow it. This seems like an important architecture aspect, but in most cases, it doesn\u0026rsquo;t make a difference: just pick one style and stick with it. But in some cases it does:\nLittle-endian has the advantage that you can cast a value to a smaller type (e.g., long long to int) by just loading fewer bytes, which in most cases means doing nothing — thanks to register aliasing, eax refers to the first 4 bytes of rax, so conversion is essentially free. It is also easier to read values in a variety of type sizes — while on big-endian architectures, loading an int from a long long array would require shifting the pointer by 2 bytes. Big-endian has the advantage that higher bytes are loaded first, which in theory can make highest-to-lowest routines such as comparisons and printing faster. You can also perform certain checks such as finding out whether a number is negative by only loading its first byte. Big-endian is also more \u0026ldquo;natural\u0026rdquo; — this is how we write binary numbers on paper — but the advantage of having faster type conversions outweights it. For this reason, little-endian is used by default on most hardware, although some CPUs are \u0026ldquo;bi-endian\u0026rdquo; and can be configured to switch modes on demand.\n#128-bit IntegersSometimes we need to multiply two 64-bit integers to get a 128-bit integer — usually to serve as a temporary value and be reduced modulo a 64-bit integer right away.\nThere are no 128-bit registers to hold the result of such multiplication, so the mul instruction, in addition to the normal mul r r form where it multiplies the values in registers and keeps the lower half of the result, has another mul r mode, where it multiplies whatever is stored in the rax register by its operand, and writes the result into two registers — the lower 64 bits of the result will go into rax, and the higher 64 bits go into rdx:\n; input: 64-bit integers a and b, stored in rsi and rdi ; output: 128-bit product a * b, stored in rax (lower 64-bit) and rdx (higher 64-bit) mov rax, rdi mov r8, rdx imul rsi Some compilers have a separate type supporting this operation. In GCC and Clang it is available as __int128:\nvoid prod(int64_t a, int64_t b, __int128 *c) { *c = a * (__int128) b; } Its typical use case is to immediately extract either the lower or the higher part of the multiplication and forget about it:\n__int128_t x = 1; int64_t hi = x \u0026gt;\u0026gt; 64; int64_t lo = (int64_t) x; // will be just truncated For all purposes other than multiplication, 128-bit integers are just bundled as two registers. This makes it too weird to have a full-fledged 128-bit type, so the support for it is limited, other than for basic arithmetic operations. For example:\n__int128_t add(__int128_t a, __int128_t b) { return a + b; } is compiled into:\nadd: mov rax, rdi add rax, rdx ; this sets the carry flag in case of an overflow adc rsi, rcx ; +1 if the carry flag is set mov rdx, rsi ret Other platforms provide similar mechanisms for dealing with longer-than-word multiplication. For example, Arm has mulhi and mullo instructions, returning lower and higher parts of the multiplication, and x86 SIMD extensions have similar 32-bit instructions.\nNote that char, unsigned char, and signed char are technically three distinct types. The C standard leaves it up to the implementation whether the plain char is signed or unsigned (on most compilers, it is signed).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":51,"path":"/hugo-page/hpc/arithmetic/integer/","title":"Integer Numbers"},{"content":"In this section, we will apply external sorting and joining to solve a problem that seems useless on the surface but is actually a key primitive used in a large number of external memory and parallel algorithms.\nProblem. Given a singly-linked list, compute the rank of each element, equal to its distance from the last element.\nExample input and output for the list ranking problem This problem can be trivially solved in the RAM model: you just traverse the entire list with a counter. But this pointer jumping wouldn\u0026rsquo;t work well in the external memory setting because the list nodes are stored arbitrarily, and in the worst case, reading each new node may require reading a new block.\n#AlgorithmConsider a slightly more general version of the problem. Now, each element has a weight $w_i$, and for each element, we need to compute the sum of the weights of all its preceding elements instead of just its rank. To solve the initial problem, we can just set all weights equal to 1.\nThe main idea of the algorithm is to remove some fraction of elements, recursively solve the problem, and then use these weight-ranks to reconstruct the answer for the initial problem — which is the tricky part.\nConsider some three consecutive elements $x$, $y$ and $z$. Assume that we deleted $y$ and solved the problem for the remaining list, which included $x$ and $z$, and now we need to restore the answer for the original triplet. The weight of $x$ would be correct as it is, but we need to calculate the answer for $y$ and adjust it for $z$, namely:\n$w_y\u0026rsquo; = w_y + w_x$ $w_z\u0026rsquo; = w_z + w_y + w_x$ Now, we can just delete, say, the first element, solve the problem recursively, and recalculate weights for the original array. But, unfortunately, it would work in quadratic time, because to make the update, we would need to know where its neighbors are, and since we can\u0026rsquo;t hold the entire array in memory, we would need to scan it each time.\nTherefore, on each step, we want to remove as many elements as possible. But we also have a constraint: we can\u0026rsquo;t remove two consecutive elements because then merging results wouldn\u0026rsquo;t be that simple.\nIdeally, we want to split our list into even and odd elements, but doing this is not simpler than the initial problem. One workaround is to choose the elements at random: toss a coin for each element, and then remove all \u0026ldquo;heads\u0026rdquo; after which a \u0026ldquo;tail\u0026rdquo; follows. This way no two consecutive elements will ever be selected, and on average we get rid of ¼ of the current list. The arithmetic complexity of this solution would still be linear, because\n$$ T(N) = T\\left(\\frac{3}{4} N\\right) = O(N) $$\nThe only tricky part here is how to implement the merge step in external memory. To do it efficiently, we need to maintain our list in the following form:\nList of tuples $(i, j)$ indicating that element $j$ follows after element $i$ List of tuples $(i, w_i)$ indicating that element $i$ currently has weight $w_i$ A list of deleted elements Now, to restore the answer after randomly deleting some elements and recursively solving the smaller problem, we need to iterate over all lists using three pointers looking for deleted elements. and for each such element, we will write $(j, w_i)$ to a separate table, which would signify that before the recursive step we need to add $w_i$ to $j$. We can then join this new table with initial weights, add these additional weights to them.\nAfter coming back from the recursion, we need to update weights for the deleted elements, which we can do with the same technique, iterating over reversed connections instead of direct ones.\nI/O complexity of this algorithm with therefore be the same as joining, namely $SORT(N) = O\\left(\\frac{N}{B} \\log_{\\frac{M}{B}} \\frac{N}{M} \\right)$.\n#ApplicationsList ranking is especially useful in graph algorithms.\nFor example, we can obtain the Euler tour of a tree in external memory by constructing a linked list from the tree that corresponds to its Euler tour and then applying the list ranking algorithm — the ranks of each node will be the same as its index $tin_v$ in the Euler tour. To construct this list, we need to:\nsplit each undirected edge into two directed ones; duplicate the parent node for each up-edge (because list nodes can only have one incoming edge, but we visit some vertices multiple times); route each such node either to the \u0026ldquo;next sibling,\u0026rdquo; if it has one, or otherwise to its own parent; and then finally break the resulting cycle at the root. This general technique is called tree contraction, and it serves as the basis for a large number of tree algorithms.\nThe same approach can be applied to parallel algorithms, and we will cover that much more deeply in part II.\n","id":52,"path":"/hugo-page/hpc/external-memory/list-ranking/","title":"List Ranking"},{"content":"Memory requests can overlap in time: while you wait for a read request to complete, you can send a few others, which will be executed concurrently with it. This is the main reason why linear iteration is so much faster than pointer jumping: the CPU knows which memory locations it needs to fetch next and sends memory requests far ahead of time.\nThe number of concurrent memory operations is large but limited, and it is different for different types of memory. When designing algorithms and especially data structures, you may want to know this number, as it limits the amount of parallelism your computation can achieve.\nTo find this limit theoretically for a specific memory type, you can multiply its latency (time to fetch a cache line) by its bandwidth (number of cache lines fetched per second), which gives you the average number of memory operations in progress:\nThe latency of the L1/L2 caches is small, so there is no need for a long pipeline of pending requests, but larger memory types can sustain up to 25-40 concurrent read operations.\n#Direct ExperimentLet\u0026rsquo;s try to measure available memory parallelism more directly by modifying our pointer chasing benchmark so that we loop around $D$ separate cycles in parallel instead of just one:\nconst int M = N / D; int p[M], q[D][M]; for (int d = 0; d \u0026lt; D; d++) { iota(p, p + M, 0); random_shuffle(p, p + M); k[d] = p[M - 1]; for (int i = 0; i \u0026lt; M; i++) k[d] = q[d][k[d]] = p[i]; } for (int i = 0; i \u0026lt; M; i++) for (int d = 0; d \u0026lt; D; d++) k[d] = q[d][k[d]]; Fixing the sum of the cycle lengths constant at a few select sizes and trying different $D$, we get slightly different results:\nThe L2 cache run is limited by ~6 concurrent operations, as predicted, but larger memory types all max out between 13 and 17. You can\u0026rsquo;t make use of more memory lanes as there is a conflict over logical registers. When the number of lanes is fewer than the number of registers, you can issue just one read instruction per lane:\ndec edx movsx rdi, DWORD PTR q[0+rdi*4] movsx rsi, DWORD PTR q[1048576+rsi*4] movsx rcx, DWORD PTR q[2097152+rcx*4] movsx rax, DWORD PTR q[3145728+rax*4] jne .L9 But when it is over ~15, you have to use temporary memory storage:\nmov edx, DWORD PTR q[0+rdx*4] mov DWORD PTR [rbp-128+rax*4], edx You don\u0026rsquo;t always get to the maximum possible level of memory parallelism, but for most applications, a dozen concurrent requests are more than enough.\n","id":53,"path":"/hugo-page/hpc/cpu-cache/mlp/","title":"Memory-Level Parallelism"},{"content":"Staring at the source code or its assembly is a popular, but not the most effective way of finding performance issues. When the performance doesn\u0026rsquo;t meet your expectations, you can identify the root cause much faster using one of the special program analysis tools collectively called profilers.\nThere are many different types of profilers. I like to think about them by analogy of how physicists and other natural scientists approach studying small things, picking the right tool depending on the required level of precision:\nWhen objects are on a micrometer scale, they use optical microscopes. When objects are on a nanometer scale, and light no longer interacts with them, they use electron microscopes. When objects are smaller than that (e.g., the insides of an atom), they resort to theories and assumptions about how things work (and test these assumptions using intricate and indirect experiments). Similarly, there are three main profiling techniques, each operating by its own principles, having distinct areas of applicability, and allowing for different levels of precision:\nInstrumentation lets you time the program as a whole or by parts and count specific events you are interested in. Statistical profiling lets you go down to the assembly level and track various hardware events such as branch mispredictions or cache misses, which are critical for performance. Program simulation lets you go down to the individual cycle level and look into what is happening inside the CPU on each cycle when it is executing a small assembly snippet. Practical algorithm design can be very much considered an empirical field too. We largely rely on the same experimental methods, although this is not because we don\u0026rsquo;t know some of the fundamental secrets of nature but mostly because modern computers are just too complex to analyze — besides, this is also true that we, regular software engineers, can\u0026rsquo;t know some of the details because of IP protection from hardware companies (in fact, considering that the most accurate x86 instruction tables are reverse-engineered, there is a reason to believe that Intel doesn\u0026rsquo;t know these details themselves).\nIn this chapter, we will study these three key profiling methods, as well as some time-tested practices for managing computational experiments involving performance evaluation.\n","id":54,"path":"/hugo-page/hpc/profiling/","title":"Profiling"},{"content":"As we repeatedly demonstrate throughout this book, knowing darker corners of the instruction set can be very fruitful, especially in the case of CISC platforms like x86, which currently has somewhere between 1000 and 4000 distinct instructions, depending on how you count.\nMost of these instructions are related to arithmetic, and using them all efficiently to optimize arithmetic operations requires a great deal of both knowledge, skill, and creativity. Therefore, in this chapter, we will discuss number representations and their use in numerical algorithms.\n","id":55,"path":"/hugo-page/hpc/arithmetic/","title":"Arithmetic"},{"content":"Most good software engineering practices in one way or another address the issue of making development cycles faster: you want to compile your software faster (build systems), catch bugs as soon as possible (static analysis, continuous integration), release as soon as the new version is ready (continuous deployment), and react to user feedback without much delay (agile development).\nPerformance engineering is not different. If you do it correctly, it should also resemble a cycle:\nRun the program and collect metrics. Figure out where the bottleneck is. Remove the bottleneck and go to step 1. In this section, we will talk about benchmarking and discuss some practical techniques that make this cycle shorter and help you iterate faster. Most of the advice comes from working on this book, so you can find many real examples of described setups in the code repository for this book.\n#Benchmarking Inside C++There are several approaches to writing benchmarking code. Perhaps the most popular one is to include several same-language implementations you want to compare in one file, separately invoke them from the main function, and calculate all the metrics you want in the same source file.\nThe disadvantage of this method is that you need to write a lot of boilerplate code and duplicate it for each implementation, but it can be partially neutralized with metaprogramming. For example, when you are benchmarking multiple gcd implementations, you can reduce benchmarking code considerably with this higher-order function:\nconst int N = 1e6, T = 1e9 / N; int a[N], b[N]; void timeit(int (*f)(int, int)) { clock_t start = clock(); int checksum = 0; for (int t = 0; t \u0026lt; T; t++) for (int i = 0; i \u0026lt; n; i++) checksum ^= f(a[i], b[i]); float seconds = float(clock() - start) / CLOCKS_PER_SEC; printf(\u0026#34;checksum: %d\\n\u0026#34;, checksum); printf(\u0026#34;%.2f ns per call\\n\u0026#34;, 1e9 * seconds / N / T); } int main() { for (int i = 0; i \u0026lt; N; i++) a[i] = rand(), b[i] = rand(); timeit(std::gcd); timeit(my_gcd); timeit(my_another_gcd); // ... return 0; } This is a very low-overhead method that lets you run more experiments and get more accurate results from them. You still have to perform some repeated actions, but they can be largely automated with frameworks, Google benchmark library being the most popular choice for C++. Some programming languages also have handy built-in tools for benchmarking: special mention here goes to Python\u0026rsquo;s timeit function and Julia\u0026rsquo;s @benchmark macro.\nAlthough efficient in terms of execution speed, C and C++ are not the most productive languages, especially when it comes to analytics. When your algorithm depends on some parameters such as the input size, and you need to collect more than just one data point from each implementation, you really want to integrate your benchmarking code with the outside environment and analyze the results using something else.\n#Splitting Up ImplementationsOne way to improve modularity and reusability is to separate all testing and analytics code from the actual implementation of the algorithm, and also make it so that different versions are implemented in separate files, but have the same interface.\nIn C/C++, you can do this by creating a single header file (e.g., gcd.hh) with a function interface and all its benchmarking code in main:\nint gcd(int a, int b); // to be implemented // for data structures, you also need to create a setup function // (unless the same preprocessing step for all versions would suffice) int main() { const int N = 1e6, T = 1e9 / N; int a[N], b[N]; // careful: local arrays are allocated on the stack and may cause stack overflow // for large arrays, allocate with \u0026#34;new\u0026#34; or create a global array for (int i = 0; i \u0026lt; N; i++) a[i] = rand(), b[i] = rand(); int checksum = 0; clock_t start = clock(); for (int t = 0; t \u0026lt; T; t++) for (int i = 0; i \u0026lt; n; i++) checksum += gcd(a[i], b[i]); float seconds = float(clock() - start) / CLOCKS_PER_SEC; printf(\u0026#34;%d\\n\u0026#34;, checksum); printf(\u0026#34;%.2f ns per call\\n\u0026#34;, 1e9 * seconds / N / T); return 0; } Then you create many implementation files for each algorithm version (e.g., v1.cc, v2.cc, and so on, or some meaningful names if applicable) that all include that single header file:\n#include \u0026#34;gcd.hh\u0026#34; int gcd(int a, int b) { if (b == 0) return a; else return gcd(b, a % b); } The whole purpose of doing this is to be able to benchmark a specific algorithm version from the command line without touching any source code files. For this purpose, you may also want to expose any parameters that it may have — for example, by parsing them from the command line arguments:\nint main(int argc, char* argv[]) { int N = (argc \u0026gt; 1 ? atoi(argv[1]) : 1e6); const int T = 1e9 / N; // ... } Another way to do it is to use C-style global defines and then pass them with the -D N=... flag during compilation:\n#ifndef N #define N 1000000 #endif const int T = 1e9 / N; This way you can make use of compile-time constants, which may be very beneficial for the performance of some algorithms, at the expense of having to re-build the program each time you want to change the parameter, which considerably increases the time you need to collect metrics across a range of parameter values.\n#Makefiles Splitting up source files allows you to speed up compilation using a caching build system such as Make.\nI usually carry a version of this Makefile across my projects:\ncompile = g++ -std=c++17 -O3 -march=native -Wall %: %.cc gcd.hh $(compile) $\u0026lt; -o $@ %.s: %.cc gcd.hh $(compile) -S -fverbose-asm $\u0026lt; -o $@ %.run: % @./$\u0026lt; .PHONY: %.run You can now compile example.cc with make example, and automatically run it with make example.run.\nYou can also add scripts for calculating statistics in the Makefile, or incorporate it with perf stat calls to make profiling automatic.\n#Jupyter NotebooksTo speed up high-level analytics, you can create a Jupyter notebook where you put all your scripts and do all the plots.\nIt is convenient to add a wrapper for benchmarking an implementation, which just returns a scalar result:\ndef bench(source, n=2**20): !make -s {source} if _exit_code != 0: raise Exception(\u0026#34;Compilation failed\u0026#34;) res = !./{source} {n} {q} duration = float(res[0].split()[0]) return duration Then you can use it to write clean analytics code:\nns = list(int(1.17**k) for k in range(30, 60)) baseline = [bench(\u0026#39;std_lower_bound\u0026#39;, n=n) for n in ns] results = [bench(\u0026#39;my_binary_search\u0026#39;, n=n) for n in ns] # plotting relative speedup for different array sizes import matplotlib.pyplot as plt plt.plot(ns, [x / y for x, y in zip(baseline, results)]) plt.show() Once established, this workflow makes you iterate much faster and focus on optimizing the algorithm itself.\n","id":56,"path":"/hugo-page/hpc/profiling/benchmarking/","title":"Benchmarking"},{"content":"In \u0026ldquo;safe\u0026rdquo; languages like Java and Rust, you normally have well-defined behavior for every possible operation and every possible input. There are some things that are under-defined, like the order of keys in a hash table or the growth factor of an std::vector, but these are usually some minor details that are left up to implementation for potential performance gains in the future.\nIn contrast, C and C++ take the concept of undefined behavior to another level. Certain operations don\u0026rsquo;t cause an error during compilation or runtime but are just not allowed — in the sense of there being a contract between the programmer and the compiler, that in case of undefined behavior, the compiler is legally allowed to do literally anything, including blowing up your monitor or formatting your hard drive. But compiler engineers are not interested in doing that. Instead, undefined behavior is used to guarantee a lack of corner cases and help optimization.\n#Why Undefined Behavior ExistsThere are two major groups of actions that cause undefined behavior:\nOperations that are almost certainly unintentional bugs, like dividing by zero, dereferencing a null pointer, or reading from uninitialized memory. You want to catch these as soon as possible during testing, so crashing or having some non-deterministic behavior is better than having them always do a fixed fallback action such as returning zero.\nYou can compile and run a program with sanitizers to catch undefined behavior early. In GCC and Clang, you can use the -fsanitize=undefined flag, and some operations that are notorious for causing UB will be instrumented to detect it at runtime.\nOperations that have slightly different observable behavior on different platforms. For example, the result of left-shifting an integer by more than 31 bits is undefined, because the instruction that does it is implemented differently on Arm and x86 CPUs. If you standardize one specific behavior, then all programs compiled for the other platform will have to spend a few more cycles checking for that edge case, so it is best to leave it undefined.\nSometimes, when there is a legitimate use case for some platform-specific behavior, instead of declaring it undefined, it can be left implementation-defined. For example, the result of right-shifting a negative integer depends on the platform: it either shifts in zeros or ones (e.g., right-shifting 11010110 = -42 by one may mean either 01101011 = 107 or 11101011 = -21, both use cases being realistic).\nDesignating something as undefined instead of implementation-defined behavior also helps compilers in optimization. Consider the case of signed integer overflow. On almost all architectures, signed integers overflow the same way as unsigned ones, with INT_MAX + 1 == INT_MIN, and yet, this is undefined behavior according to the C++ standard. This is very much intentional: if you disallow signed integer overflow, then (x + 1) \u0026gt; x is guaranteed to be always true for int, but not for unsigned int, because (x + 1) may overflow. For signed types, this lets compilers optimize such checks away.\nAs a more naturally occurring example, consider the case of a loop with an integer control variable. Modern C++ and languages like Rust encourage programmers to use an unsigned integer (size_t / usize), while C programmers stubbornly keep using int. To understand why, consider the following for loop:\nfor (unsigned int i = 0; i \u0026lt; n; i++) { // ... } How many times does this loop execute? There are technically two valid answers: $n$ and infinity, the second being the case if $n$ exceeds $2^{32}$ so that $i$ keeps resetting to zero every $2^{32}$ iterations. While the former is probably the one assumed by the programmer, to comply with the language spec, the compiler still has to insert additional runtime checks and consider the two cases, which should be optimized differently. Meanwhile, the int version would make exactly $n$ iterations because the very possibility of a signed overflow is defined out of existence.\n#Removing Corner CasesThe \u0026ldquo;safe\u0026rdquo; programming style usually involves making a lot of runtime checks, but they do not have to come at the cost of performance.\nFor example, Rust famously uses bounds checking when indexing arrays and other random access structures. In C++ STL, vector and array have an \u0026ldquo;unsafe\u0026rdquo; [] operator and a \u0026ldquo;safe\u0026rdquo; .at() method that goes something like this:\nT at(size_t k) { if (k \u0026gt;= size()) throw std::out_of_range(\u0026#34;Array index exceeds its size\u0026#34;); return _memory[k]; } Interestingly, these checks are rarely actually executed during runtime because the compiler can often prove — during compile time — that each access will be within bounds. For example, when iterating in a for loop from 1 to the array size and indexing $i$-th element on each step, nothing illegal can possibly happen, so the bounds checks can be safely optimized away.\n#AssumptionsWhen the compiler can\u0026rsquo;t prove the inexistence of corner cases, but you can, this additional information can be provided using the mechanism of undefined behavior.\nClang has a helpful __builtin_assume function where you can put a statement that is guaranteed to be true, and the compiler will use this assumption in optimization. In GCC, you can do the same with __builtin_unreachable:\nvoid assume(bool pred) { if (!pred) __builtin_unreachable(); } For instance, you can put assume(k \u0026lt; vector.size()) before at in the example above, and then the bounds check will be optimized away.\nIt is also quite useful to combine assume with assert and static_assert to find bugs: you can use the same function to check preconditions in the debug build and then use them to improve performance in the production build.\n#ArithmeticCorner cases are something you should keep in mind, especially when optimizing arithmetic.\nFor floating-point arithmetic, this is less of a concern because you can just disable strict standard compliance with the -ffast-math flag (which is also included in -Ofast). You almost have to do it anyway because otherwise, the compiler can\u0026rsquo;t do anything but execute arithmetic operations in the same order as in the source code without any optimizations.\nFor integer arithmetic, this is different because the results always have to be exact. Consider the case of division by 2:\nunsigned div_unsigned(unsigned x) { return x / 2; } A widely known optimization is to replace it with a single right shift (x \u0026gt;\u0026gt; 1):\nshr eax This is certainly correct for all positive numbers, but what about the general case?\nint div_signed(int x) { return x / 2; } If x is negative, then simply shifting doesn\u0026rsquo;t work — regardless of whether shifting is done in zeros or sign bits:\nIf we shift in zeros, we get a non-negative result (the sign bit is zero). If we shift in sign bits, then rounding will happen towards negative infinity instead of zero (-5 / 2 will be equal to -3 instead of -2)1. So, for the general case, we have to insert some crutches to make it work:\nmov ebx, eax shr ebx, 31 ; extract the sign bit add eax, ebx ; add 1 to the value if it is negative to ensure rounding towards zero sar eax ; this one shifts in sign bits When only the positive case is what was intended, we can also use the assume mechanism to eliminate the possibility of negative x and avoid handling this corner case:\nint div_assume(int x) { assume(x \u0026gt;= 0); return x / 2; } Although in this particular case, perhaps the best syntax to express that we only expect non-negative numbers is to use an unsigned integer type.\nBecause of nuances like this, it is often beneficial to expand the algebra in intermediate functions and manually simplify arithmetic yourself rather than relying on the compiler to do it.\n#Memory AliasingCompilers are quite bad at optimizing operations that involve memory reads and writes. This is because they often don\u0026rsquo;t have enough context for the optimization to be correct.\nConsider the following example:\nvoid add(int *a, int *b, int n) { for (int i = 0; i \u0026lt; n; i++) a[i] += b[i]; } Since each iteration of this loop is independent, it can be executed in parallel and vectorized. But is it, technically?\nThere may be a problem if the arrays a and b intersect. Consider the case when b == a + 1, that is, if b is just a memory view of a starting from its second element. In this case, the next iteration depends on the previous one, and the only correct solution is to execute the loop sequentially. The compiler has to check for such possibilities even if the programmer knows they can\u0026rsquo;t happen.\nThis is why we have const and restrict keywords. The first one enforces that we won\u0026rsquo;t modify memory with the pointer variable, and the second is a way to tell the compiler that the memory is guaranteed to not be aliased.\nvoid add(int * __restrict__ a, const int * __restrict__ b, int n) { for (int i = 0; i \u0026lt; n; i++) a[i] += b[i]; } These keywords are also a good idea to use by themselves for the purpose of self-documenting.\n#C++ ContractsContract programming is an underused but very powerful technique.\nThere is a late-stage proposal to add design-by-contract into the C++ standard in the form of contract attributes, which are functionally equivalent to our hand-made, compiler-specific assume:\nT at(size_t k) [[ expects: k \u0026lt; n ]] { return _memory[k]; } There are 3 types of attributes — expects, ensures, and assert — respectively used for specifying pre- and post-conditions in functions and general assertions that can be put anywhere in the program.\nUnfortunately, this exciting new feature is not yet finally standardized, let alone implemented in a major C++ compiler. But maybe, in a few years, we will be able to write code like this:\nbool is_power_of_two(int m) { return m \u0026gt; 0 \u0026amp;\u0026amp; (m \u0026amp; (m - 1) == 0); } int mod_power_of_two(int x, int m) [[ expects: x \u0026gt;= 0 ]] [[ expects: is_power_of_two(m) ]] [[ ensures r: r \u0026gt;= 0 \u0026amp;\u0026amp; r \u0026lt; m ]] { int r = x \u0026amp; (m - 1); [[ assert: r = x % m ]]; return r; } Some forms of contract programming are also available in other performance-oriented languages such as Rust and D.\nA general and language-agnostic advice is to always inspect the assembly that the compiler produced, and if it is not what you were hoping for, try to think about corner cases that may be limiting the compiler from optimizing it.\nFun fact: in Python, integer-dividing a negative number for some reason floors the result, so that -5 // 2 = -3 and equivalent to -5 \u0026gt;\u0026gt; 1 = -3. I doubt that Guido van Rossum had this optimization in mind when initially designing the language, but, theoretically, a JIT-compiled Python program with many divisions by two may be faster than an analogous C++ program.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":57,"path":"/hugo-page/hpc/compilation/contracts/","title":"Contract Programming"},{"content":"You can control the I/O operations of your program manually, but most of the time people just rely on automatic bufferization and caching, either due to laziness or because of the computing environment limitations.\nBut automatic caching comes with its own challenges. When a program runs out of working memory to store its intermediate data, it needs to get rid of one block to make space for a new one. A concrete rule for deciding which data to retain in the cache in case of conflicts is called an eviction policy.\nThis rule can be arbitrary, but there are several popular choices:\nFirst in first out (FIFO): simply evict the earliest added block, without any regard to how often it was accessed before (the same way as a FIFO queue). Least recently used (LRU): evict the block that has not been accessed for the longest period of time. Last in first out (LIFO) and most recently used (MRU): the opposite of the previous two. It seems harmful to delete the hottest blocks, but there are scenarios where these policies are optimal, such as repeatedly looping around a file in a cycle. Least-frequently used (LFU): counts how often each block has been requested and discards the one used least often. Some variations also account for changing access patterns over time, such as using a time window to only consider the last $n$ accesses or using exponential averaging to give recent accesses more weight. Random replacement (RR): discard a block randomly. The advantage is that it does not need to maintain any data structures with block information. There is a natural trade-off between the accuracy of eviction policies and the additional overhead due to the complexity of their implementations. For a CPU cache, you need a simple policy that can be easily implemented in hardware with next-to-zero latency, while in more slow-paced and plannable settings such as Netflix deciding in which data centers to store their movies or Google Drive optimizing where to store user data, it makes sense to use more complex policies, possibly involving some machine learning to predict when the data is going to be accessed next.\n#Optimal CachingApart from the aforementioned strategies, there is also the theoretical optimal policy, denoted as $OPT$ or $MIN$, which determines, for a given sequence of queries, which blocks should be retained to minimize the total number of cache misses.\nThese decisions can be made using a simple greedy approach called Bélády algorithm: we can just keep the latest-to-be-used block, and it can be shown by contradiction that doing so is always one of the optimal solutions. The downside of this method is that you either need to have these queries in advance or somehow be able to predict the future.\nThe good thing is that, in terms of asymptotic complexity, it doesn\u0026rsquo;t really matter which particular method is used. Sleator \u0026amp; Tarjan showed that in most cases, the performance of popular policies such as $LRU$ differs from $OPT$ just by a constant factor.\nTheorem. Let $LRU_M$ and $OPT_M$ denote the number of blocks a computer with $M$ internal memory would need to access while executing the same algorithm following the least recently used cache replacement policy and the theoretical minimum respectively. Then:\n$$ LRU_M \\leq 2 \\cdot OPT_{M/2} $$\nThe main idea of the proof is to consider the worst case scenario. For LRU it would be the repeating series of $\\frac{M}{B}$ distinct blocks: each block is new and so LRU has 100% cache misses. Meanwhile, $OPT_{M/2}$ would be able to cache half of them (but not more, because it only has half the memory). Thus $LRU_M$ needs to fetch double the number of blocks that $OPT_{M/2}$ does, which is basically what is expressed in the inequality, and anything better for $LRU$ would only weaken it.\nDimmed are the blocks cached by OPT (but not cached by LRU) This is a very relieving result. It means that, at least in terms of asymptotic I/O complexity, you can just assume that the eviction policy is either LRU or OPT — whichever is easier for you — do complexity analysis with it, and the result you get will normally transfer to any other reasonable cache replacement policy.\n#Implementing Caching This is not always a trivial task to find the right block to evict in a reasonable time. While CPU caches are implemented in hardware (usually as some variation of LRU), higher-level eviction policies have to rely on software to store certain statistics about the blocks and maintain data structures on top of them to speed up the process.\nLet\u0026rsquo;s think about what we need to implement an LRU cache. Assume we are storing some moderately large objects — say, we need to develop a cache for a database, there both the requests and replies are medium-sized strings in some SQL dialect, so the overhead of our structure is small but non-negligible.\nFirst of all, we need a hash table to find the data itself. Since we are working with large variable-length strings, it makes sense to use the hash of the query as the key and a pointer to a heap-allocated result string as the value.\nTo implement the LRU logic, the simplest approach would be to create a queue where we put the current time and IDs/keys of objects when we access them, and also store when each object was accessed the last time (not necessarily as a timestamp — any increasing counter will suffice).\nNow, when we need to free up space, we can find the least recently used object by popping elements from the front of the queue. We can\u0026rsquo;t just delete them, because it may be that they were accessed again since their record was added to the queue. So we need to check if the time of when we put them in queue matches the time of when they were last accessed, and only then free up the memory.\nThe only remaining issue here is that we add an entry to the queue each time a block is accessed, and only remove entries when we have a cache miss and start popping them off from the front until we have a match. This may lead to the queue overflowing, and to mitigate this, instead of adding an entry and forgetting about it, we can move it to the end of the queue on a cache hit right away.\nTo support this, we need to implement the queue over a doubly-linked list and store a pointer to the block\u0026rsquo;s node in the queue in the hash table. Then, when we have a cache hit, we follow the pointer and remove the node from the linked list in constant time, and add a newer node to the end of the queue. This way, at any point in time, there would be exactly as many nodes in the queue as we have objects, and the memory overhead will be guaranteed to be constant per cache entry.\nAs an exercise, try to think about ways to implement other caching strategies.\n","id":58,"path":"/hugo-page/hpc/external-memory/policies/","title":"Eviction Policies"},{"content":"Masking lets you apply operations to only a subset of vector elements. It is a very effective and frequently used data manipulation technique, but in many cases, you need to perform more advanced operations that involve permuting values inside a vector register instead of just blending them with other vectors.\nThe problem is that adding a separate element-shuffling instruction for each possible use case in hardware is unfeasible. What we can do though is to add just one general permutation instruction that takes the indices of a permutation and produces these indices using precomputed lookup tables.\nThis general idea is perhaps too abstract, so let\u0026rsquo;s jump straight to the examples.\n#Shuffles and PopcountPopulation count, also known as the Hamming weight, is the count of 1 bits in a binary string.\nIt is a frequently used operation, so there is a separate instruction on x86 that computes the population count of a word:\nconst int N = (1\u0026lt;\u0026lt;12); int a[N]; int popcnt() { int res = 0; for (int i = 0; i \u0026lt; N; i++) res += __builtin_popcount(a[i]); return res; } It also supports 64-bit integers, improving the total throughput twofold:\nint popcnt_ll() { long long *b = (long long*) a; int res = 0; for (int i = 0; i \u0026lt; N / 2; i++) res += __builtin_popcountl(b[i]); return res; } The only two instructions required are load-fused popcount and addition. They both have a high throughput, so the code processes about $8+8=16$ bytes per cycle as it is limited by the decode width of 4 on this CPU.\nThese instructions were added to x86 CPUs around 2008 with SSE4. Let\u0026rsquo;s temporarily go back in time before vectorization even became a thing and try to implement popcount by other means.\nThe naive way is to go through the binary string bit by bit:\n__attribute__ (( optimize(\u0026#34;no-tree-vectorize\u0026#34;) )) int popcnt() { int res = 0; for (int i = 0; i \u0026lt; N; i++) for (int l = 0; l \u0026lt; 32; l++) res += (a[i] \u0026gt;\u0026gt; l \u0026amp; 1); return res; } As anticipated, it works just slightly faster than ⅛-th of a byte per cycle — at around 0.2.\nWe can try to process in bytes instead of individual bits by precomputing a small 256-element lookup table that contains the population counts of individual bytes and then query it while iterating over raw bytes of the array:\nstruct Precalc { alignas(64) char counts[256]; constexpr Precalc() : counts{} { for (int m = 0; m \u0026lt; 256; m++) for (int i = 0; i \u0026lt; 8; i++) counts[m] += (m \u0026gt;\u0026gt; i \u0026amp; 1); } }; constexpr Precalc P; int popcnt() { auto b = (unsigned char*) a; // careful: plain \u0026#34;char\u0026#34; is signed int res = 0; for (int i = 0; i \u0026lt; 4 * N; i++) res += P.counts[b[i]]; return res; } It now processes around 2 bytes per cycle, rising to ~2.7 if we switch to 16-bit words (unsigned short).\nThis solution is still very slow compared to the popcnt instruction, but now it can be vectorized. Instead of trying to speed it up through gather instructions, we will go for another approach: make the lookup table small enough to fit inside a register and then use a special pshufb instruction to look up its values in parallel.\nThe original pshufb introduced in 128-bit SSE3 takes two registers: the lookup table containing 16 byte values and a vector of 16 4-bit indices (0 to 15), specifying which bytes to pick for each position. In 256-bit AVX2, instead of a 32-byte lookup table with awkward 5-bit indices, we have an instruction that independently the same shuffling operation over two 128-bit lanes.\nSo, for our use case, we create a 16-byte lookup table with population counts for each nibble (half-byte), repeated twice:\nconst reg lookup = _mm256_setr_epi8( /* 0 */ 0, /* 1 */ 1, /* 2 */ 1, /* 3 */ 2, /* 4 */ 1, /* 5 */ 2, /* 6 */ 2, /* 7 */ 3, /* 8 */ 1, /* 9 */ 2, /* a */ 2, /* b */ 3, /* c */ 2, /* d */ 3, /* e */ 3, /* f */ 4, /* 0 */ 0, /* 1 */ 1, /* 2 */ 1, /* 3 */ 2, /* 4 */ 1, /* 5 */ 2, /* 6 */ 2, /* 7 */ 3, /* 8 */ 1, /* 9 */ 2, /* a */ 2, /* b */ 3, /* c */ 2, /* d */ 3, /* e */ 3, /* f */ 4 ); Now, to compute the population count of a vector, we split each of its bytes into the lower and higher nibbles and then use this lookup table to retrieve their counts. The only thing left is to carefully sum them up:\nconst reg low_mask = _mm256_set1_epi8(0x0f); int popcnt() { int k = 0; reg t = _mm256_setzero_si256(); for (; k + 15 \u0026lt; N; k += 15) { reg s = _mm256_setzero_si256(); for (int i = 0; i \u0026lt; 15; i += 8) { reg x = _mm256_load_si256( (reg*) \u0026amp;a[k + i] ); reg l = _mm256_and_si256(x, low_mask); reg h = _mm256_and_si256(_mm256_srli_epi16(x, 4), low_mask); reg pl = _mm256_shuffle_epi8(lookup, l); reg ph = _mm256_shuffle_epi8(lookup, h); s = _mm256_add_epi8(s, pl); s = _mm256_add_epi8(s, ph); } t = _mm256_add_epi64(t, _mm256_sad_epu8(s, _mm256_setzero_si256())); } int res = hsum(t); while (k \u0026lt; N) res += __builtin_popcount(a[k++]); return res; } This code processes around 30 bytes per cycle. Theoretically, the inner loop could do 32, but we have to stop it every 15 iterations because the 8-bit counters can overflow.\nThe pshufb instruction is so instrumental in some SIMD algorithms that Wojciech Muła — the guy who came up with this algorithm — took it as his Twitter handle. You can calculate population counts even faster: check out his GitHub repository with different vectorized popcount implementations and his recent paper for a detailed explanation of the state-of-the-art.\n#Permutations and Lookup TablesOur last major example in this chapter is the filter. It is a very important data processing primitive that takes an array as input and writes out only the elements that satisfy a given predicate (in their original order).\nIn a single-threaded scalar case, it is trivially implemented by maintaining a counter that is incremented on each write:\nint a[N], b[N]; int filter() { int k = 0; for (int i = 0; i \u0026lt; N; i++) if (a[i] \u0026lt; P) b[k++] = a[i]; return k; } To vectorize it, we will use the _mm256_permutevar8x32_epi32 intrinsic. It takes a vector of values and individually selects them with a vector of indices. Despite the name, it doesn\u0026rsquo;t permute values but just copies them to form a new vector: duplicates in the result are allowed.\nThe general idea of our algorithm is as follows:\ncalculate the predicate on a vector of data — in this case, this means performing the comparisons to get the mask; use the movemask instruction to get a scalar 8-bit mask; use this mask to index a lookup table that returns a permutation moving the elements that satisfy the predicate to the beginning of the vector (in their original order); use the _mm256_permutevar8x32_epi32 intrinsic to permute the values; write the whole permuted vector to the buffer — it may have some trailing garbage, but its prefix is correct; calculate the population count of the scalar mask and move the buffer pointer by that number. First, we need to precompute the permutations:\nstruct Precalc { alignas(64) int permutation[256][8]; constexpr Precalc() : permutation{} { for (int m = 0; m \u0026lt; 256; m++) { int k = 0; for (int i = 0; i \u0026lt; 8; i++) if (m \u0026gt;\u0026gt; i \u0026amp; 1) permutation[m][k++] = i; } } }; constexpr Precalc T; Then we can implement the algorithm itself:\nconst reg p = _mm256_set1_epi32(P); int filter() { int k = 0; for (int i = 0; i \u0026lt; N; i += 8) { reg x = _mm256_load_si256( (reg*) \u0026amp;a[i] ); reg m = _mm256_cmpgt_epi32(p, x); int mask = _mm256_movemask_ps((__m256) m); reg permutation = _mm256_load_si256( (reg*) \u0026amp;T.permutation[mask] ); x = _mm256_permutevar8x32_epi32(x, permutation); _mm256_storeu_si256((reg*) \u0026amp;b[k], x); k += __builtin_popcount(mask); } return k; } The vectorized version takes some work to implement, but it is 6-7x faster than the scalar one (the speedup is slightly less for either low or high values of P as the branch becomes predictable).\nThe loop performance is still relatively low — taking 4 CPU cycles per iteration — because, on this particular CPU (Zen 2), movemask, permute, and store have low throughput and all have to go through the same execution port (P2). On most other x86 CPUs, you can expect it to be ~2x faster.\nFiltering can also be implemented considerably faster on AVX-512: it has a special \u0026ldquo;compress\u0026rdquo; instruction that takes a vector of data and a mask and writes its unmasked elements contiguously. It makes a huge difference in algorithms that rely on various filtering subroutines, such as quicksort.\n","id":59,"path":"/hugo-page/hpc/simd/shuffling/","title":"In-Register Shuffles"},{"content":"Compared to other arithmetic operations, division works very poorly on x86 and computers in general. Both floating-point and integer division is notoriously hard to implement in hardware. The circuitry takes a lot of space in the ALU, the computation has a lot of stages, and as the result, div and its siblings routinely take 10-20 cycles to complete, with latency being slightly less on smaller data type sizes.\n#Division and Modulo in x86Since nobody wants to duplicate all this mess for a separate modulo operation, the div instruction serves both purposes. To perform a 32-bit integer division, you need to put the dividend specifically in the eax register and call div with the divisor as its sole operand. After this, the quotient will be stored in eax and the remainder will be stored in edx.\nThe only caveat is that the dividend actually needs to be stored in two registers, eax and edx: this mechanism enables 64-by-32 or even 128-by-64 division, similar to how 128-bit multiplication works. When performing the usual 32-by-32 signed division, we need to sign-extend eax to 64 bits and store its higher part in edx:\ndiv(int, int): mov eax, edi cdq idiv esi ret For unsigned division, you can just set edx to zero so that it doesn\u0026rsquo;t interfere:\ndiv(unsigned, unsigned): mov eax, edi xor edx, edx div esi ret An in both cases, in addition to the quotient in eax, you can also access the remainder as edx:\nmod(unsigned, unsigned): mov eax, edi xor edx, edx div esi mov eax, edx ret You can also divide 128-bit integer (stored in rdx:rax) by a 64-bit integer:\ndiv(u128, u64): ; a = rdi + rsi, b = rdx mov rcx, rdx mov rax, rdi mov rdx, rsi div edx ret The high part of the dividend should be less than the divisor, otherwise an overflow occurs. Because of this constraint, it is hard to get compilers to produce this code by themselves: if you divide a 128-bit integer type by a 64-bit integer, the compiler will bubble-wrap it with additional checks which may actually be unnecessary.\n#Division by ConstantsInteger division is painfully slow, even when fully implemented in hardware, but it can be avoided in certain cases if the divisor is constant. A well-known example is the division by a power of two, which can be replaced by a one-cycle binary shift: the binary GCD algorithm is a delightful showcase of this technique.\nIn the general case, there are several clever tricks that replace division with multiplication at the cost of a bit of precomputation. All these tricks are based on the following idea. Consider the task of dividing one floating-point number $x$ by another floating-point number $y$, when $y$ is known in advance. What we can do is to calculate a constant\n$$ d \\approx y^{-1} $$\nand then, during runtime, we will calculate\n$$ x / y = x \\cdot y^{-1} \\approx x \\cdot d $$\nThe result of $\\frac{1}{y}$ will be at most $\\epsilon$ off, and the multiplication $x \\cdot d$ will only add another $\\epsilon$ and therefore will be at most $2 \\epsilon + \\epsilon^2 = O(\\epsilon)$ off, which is tolerable for the floating-point case.\n#Barrett ReductionHow to generalize this trick for integers? Calculating int d = 1 / y doesn\u0026rsquo;t seem to work, because it will just be zero. The best thing we can do is to express it as\n$$ d = \\frac{m}{2^s} $$\nand then find a \u0026ldquo;magic\u0026rdquo; number $m$ and a binary shift $s$ such that x / y == (x * m) \u0026gt;\u0026gt; s for all x within range.\n$$ \\lfloor x / y \\rfloor = \\lfloor x \\cdot y^{-1} \\rfloor = \\lfloor x \\cdot d \\rfloor = \\lfloor x \\cdot \\frac{m}{2^s} \\rfloor $$\nIt can be shown that such a pair always exists, and compilers actually perform an optimization like that by themselves. Every time they encounter a division by a constant, they replace it with a multiplication and a binary shift. Here is the generated assembly for dividing an unsigned long long by $(10^9 + 7)$:\n; input (rdi): x ; output (rax): x mod (m=1e9+7) mov rax, rdi movabs rdx, -8543223828751151131 ; load magic constant into a register mul rdx ; perform multiplication mov rax, rdx shr rax, 29 ; binary shift of the result This technique is called Barrett reduction, and it\u0026rsquo;s called \u0026ldquo;reduction\u0026rdquo; because it is mostly used for modulo operations, which can be replaced with a single division, multiplication and subtraction by the virtue of this formula:\n$$ r = x - \\lfloor x / y \\rfloor \\cdot y $$\nThis method requires some precomputation, including performing one actual division. Therefore, this is only beneficial when you perform not just one but a few divisions, all with the same constant divisor.\n#Why It WorksIt is not very clear why such $m$ and $s$ always exist, let alone how to find them. But given a fixed $s$, intuition tells us that $m$ should be as close to $2^s/y$ as possible for $2^s$ to cancel out. So there are two natural choices: $\\lfloor 2^s/y \\rfloor$ and $\\lceil 2^s/y \\rceil$. The first one doesn\u0026rsquo;t work, because if you substitute\n$$ \\Bigl \\lfloor \\frac{x \\cdot \\lfloor 2^s/y \\rfloor}{2^s} \\Bigr \\rfloor $$\nthen for any integer $\\frac{x}{y}$ where $y$ is not even, the result will be strictly less than the truth. This only leaves the other case, $m = \\lceil 2^s/y \\rceil$. Now, let\u0026rsquo;s try to derive the lower and upper bounds for the result of the computation:\n$$ \\lfloor x / y \\rfloor = \\Bigl \\lfloor \\frac{x \\cdot m}{2^s} \\Bigr \\rfloor = \\Bigl \\lfloor \\frac{x \\cdot \\lceil 2^s /y \\rceil}{2^s} \\Bigr \\rfloor $$\nLet\u0026rsquo;s start with the bounds for $m$:\n$$ 2^s / y \\le \\lceil 2^s / y \\rceil \u0026lt; 2^s / y + 1 $$\nAnd now for the whole expression:\n$$ x / y - 1 \u0026lt; \\Bigl \\lfloor \\frac{x \\cdot \\lceil 2^s /y \\rceil}{2^s} \\Bigr \\rfloor \u0026lt; x / y + x / 2^s $$\nWe can see that the result falls somewhere in a range of size $(1 + \\frac{x}{2^s})$, and if this range always has exactly one integer for all possible $x / y$, then the algorithm is guaranteed to give the right answer. Turns out, we can always set $s$ to be high enough to achieve it.\nWhat will be the worst case here? How to pick $x$ and $y$ so that the $(x/y - 1, x/y + x / 2^s)$ range contains two integers? We can see that integer ratios don\u0026rsquo;t work because the left border is not included, and assuming $x/2^s \u0026lt; 1$, only $x/y$ itself will be in the range. The worst case is actually the $x/y$ that comes closest to $1$ without exceeding it. For $n$-bit integers, that is the second-largest possible integer divided by the first-largest:\n$$ \\begin{aligned} x = 2^n - 2 \\ y = 2^n - 1 \\end{aligned} $$\nIn this case, the lower bound will be $(\\frac{2^n-2}{2^n-1} - 1)$ and the upper bound will be $(\\frac{2^n-2}{2^n-1} + \\frac{2^n-2}{2^s})$. The left border is as close to a whole number as possible, and the size of the whole range is the second largest possible. And here is the punchline: if $s \\ge n$, then the only integer contained in this range is $1$, and so the algorithm will always return it.\n#Lemire ReductionBarrett reduction is a bit complicated, and also generates a length instruction sequence for modulo because it is computed indirectly. There is a new (2019) method, which is simpler and actually faster for modulo in some cases. It doesn\u0026rsquo;t have a conventional name yet, but I am going to refer to it as Lemire reduction.\nHere is the main idea. Consider the floating-point representation of some integer fraction:\n$$ \\frac{179}{6} = 11101.1101010101\\ldots = 29\\tfrac{5}{6} \\approx 29.83 $$\nHow can we \u0026ldquo;dissect\u0026rdquo; it to get the parts we need?\nTo get the integer part (29), we can just floor or truncate it before the dot. To get the fractional part (⅚), we can just take what is after the dots. To get the remainder (5), we can multiply the fractional part by the divisor. Now, for 32-bit integers, we can set $s = 64$ and look at the computation that we do in the multiply-and-shift scheme:\n$$ \\lfloor x / y \\rfloor = \\Bigl \\lfloor \\frac{x \\cdot m}{2^s} \\Bigr \\rfloor = \\Bigl \\lfloor \\frac{x \\cdot \\lceil 2^s /y \\rceil}{2^s} \\Bigr \\rfloor $$\nWhat we really do here is we multiply $x$ by a floating-point constant ($x \\cdot m$) and then truncate the result $(\\lfloor \\frac{\\cdot}{2^s} \\rfloor)$.\nWhat if we took not the highest bits but the lowest? This would correspond to the fractional part — and if we multiply it back by $y$ and truncate the result, this will be exactly the remainder:\n$$ r = \\Bigl \\lfloor \\frac{ (x \\cdot \\lceil 2^s /y \\rceil \\bmod 2^s) \\cdot y }{2^s} \\Bigr \\rfloor $$\nThis works perfectly because what we do here can be interpreted as just three chained floating-point multiplications with the total relative error of $O(\\epsilon)$. Since $\\epsilon = O(\\frac{1}{2^s})$ and $s = 2n$, the error will always be less than one, and hence the result will be exact.\nuint32_t y; uint64_t m = uint64_t(-1) / y + 1; // ceil(2^64 / y) uint32_t mod(uint32_t x) { uint64_t lowbits = m * x; return ((__uint128_t) lowbits * y) \u0026gt;\u0026gt; 64; } uint32_t div(uint32_t x) { return ((__uint128_t) m * x) \u0026gt;\u0026gt; 64; } We can also check divisibility of $x$ by $y$ with just one multiplication using the fact that the remainder of division is zero if and only if the fractional part (the lower 64 bits of $m \\cdot x$) does not exceed $m$ (otherwise, it would become a nonzero number when multiplied back by $y$ and right-shifted by 64).\nbool is_divisible(uint32_t x) { return m * x \u0026lt; m; } The only downside of this method is that it needs integer types four times the original size to perform the multiplication, while other reduction methods can work with just the double.\nThere is also a way to compute 64x64 modulo by carefully manipulating the halves of intermediate results; the implementation is left as an exercise to the reader.\n#Further ReadingCheck out libdivide and GMP for more general implementations of optimized integer division.\nIt is also worth reading Hacker\u0026rsquo;s Delight, which has a whole chapter dedicated to integer division.\n","id":60,"path":"/hugo-page/hpc/arithmetic/division/","title":"Integer Division"},{"content":"Taking advantage of the free concurrency available in memory hardware, it can be beneficial to prefetch data that is likely to be accessed next if its location can be predicted. This is easy to do when there are no data of control hazards in the pipeline and the CPU can just run ahead of the instruction stream and execute memory operations out of order.\nBut sometimes the memory locations aren\u0026rsquo;t in the instruction stream, and yet they can still be predicted with high probability. In these cases, they can be prefetched by other means:\nExplicitly, by separately reading the next data word or any of the bytes in the same cache line, so that it is lifted in the cache hierarchy. Implicitly, by using simple access patterns such as linear iteration, which are detectable by the memory hardware that can start prefetching automatically. Hiding memory latency is crucial for achieving performance, so in this section, we will look into prefetching techniques.\n#Hardware PrefetchingLet\u0026rsquo;s modify the pointer chasing benchmark to show the effect of hardware prefetching. Now, we generate our permutation in a way that makes the CPU request consecutive cache lines when iterating over the permutation, but still accessing the elements inside a cache line in random order:\nint p[15], q[N]; iota(p, p + 15, 1); for (int i = 0; i + 16 \u0026lt; N; i += 16) { random_shuffle(p, p + 15); int k = i; for (int j = 0; j \u0026lt; 15; j++) k = q[k] = i + p[j]; q[k] = i + 16; } There is no point in making a graph because it would be just flat: the latency is 3ns regardless of the array size. Even though the instruction scheduler still can\u0026rsquo;t tell what we are going to fetch next, the memory prefetcher can detect a pattern just by looking at the memory accesses and start loading the next cache line ahead of time, mitigating the latency.\nHardware prefetching is smart enough for most use cases, but it only detects simple patterns. You can iterate forward and backward over multiple arrays in parallel, perhaps with small-to-medium strides, but that\u0026rsquo;s about it. For anything more complex, the prefetcher won\u0026rsquo;t figure out what\u0026rsquo;s happening, and we need to help it out ourselves.\n#Software PrefetchingThe simplest way to do software prefetching is to load any byte in the cache line with the mov or any other memory instruction, but CPUs have a separate prefetch instruction that lifts a cache line without doing anything with it. This instruction isn\u0026rsquo;t a part of the C or C++ standard, but is available in most compilers as the __builtin_prefetch intrinsic:\n__builtin_prefetch(\u0026amp;a[k]); It\u0026rsquo;s quite hard to come up with a simple example when it can be useful. To make the pointer chasing benchmark benefit from software prefetching, we need to construct a permutation that at the same time loops around the whole array, can\u0026rsquo;t be predicted by hardware prefetcher, and has easily computable next addresses.\nLuckily, the linear congruential generator has the property that if the modulus $n$ is a prime number, then the period of the generator will be exactly $n$. So we get all the properties we need if we use a permutation generated by the LCG with the current index as its state:\nconst int n = find_prime(N); // largest prime not exceeding N for (int i = 0; i \u0026lt; n; i++) q[i] = (2 * i + 1) % n; When we run it, the performance matches a normal random permutation. But now we get the ability to peek ahead:\nint k = 0; for (int t = 0; t \u0026lt; K; t++) { for (int i = 0; i \u0026lt; n; i++) { __builtin_prefetch(\u0026amp;q[(2 * k + 1) % n]); k = q[k]; } } There is some overhead to computing the next address, but for arrays large enough, it is almost two times faster:\nInterestingly, we can prefetch more than just one element ahead, making use of this pattern in the LCG function:\n$$ \\begin{aligned} f(x) \u0026amp;= 2 \\cdot x + 1 \\ f^2(x) \u0026amp;= 4 \\cdot x + 2 + 1 \\ f^3(x) \u0026amp;= 8 \\cdot x + 4 + 2 + 1 \\ \u0026amp;\\ldots \\ f^k(x) \u0026amp;= 2^k \\cdot x + (2^k - 1) \\end{aligned} $$\nHence, to load the D-th element ahead, we can do this:\n__builtin_prefetch(\u0026amp;q[((1 \u0026lt;\u0026lt; D) * k + (1 \u0026lt;\u0026lt; D) - 1) % n]); If we execute this request on every iteration, we will be simultaneously prefetching D elements ahead on average, increasing the throughput by D times. Ignoring some issues such as the integer overflow when D is too large, we can reduce the average latency arbitrarily close to the cost of computing the next index (which, in this case, is dominated by the modulo operation).\nNote that this is an artificial example, and you actually fail more often than not when trying to insert software prefetching into practical programs. This is largely because you need to issue a separate memory instruction that may compete for resources with the others. At the same time, hardware prefetching is 100% harmless as it only activates when the memory and cache buses are not busy.\nYou can also specify a specific level of cache the data needs to be brought to when doing software prefetching — when you aren\u0026rsquo;t sure if you will be using it and don\u0026rsquo;t want to kick out what is already in the L1 cache. You can use it with the _mm_prefetch intrinsic, which takes an integer value as the second parameter, specifying the cache level. This is useful in combination with non-temporal loads and stores.\n","id":61,"path":"/hugo-page/hpc/cpu-cache/prefetching/","title":"Prefetching"},{"content":"Computing the minimum of an array is easily vectorizable, as it is not different from any other reduction: in AVX2, you just need to use a convenient _mm256_min_epi32 intrinsic as the inner operation. It computes the minimum of two 8-element vectors in one cycle — even faster than in the scalar case, which requires at least a comparison and a conditional move.\nFinding the index of that minimum element (argmin) is much harder, but it is still possible to vectorize very efficiently. In this section, we design an algorithm that computes the argmin (almost) at the speed of computing the minimum and ~15x faster than the naive scalar approach.\n#Scalar BaselineFor our benchmark, we create an array of random 32-bit integers, and then repeatedly try to find the index of the minimum among them (the first one if it isn\u0026rsquo;t unique):\nconst int N = (1 \u0026lt;\u0026lt; 16); alignas(32) int a[N]; for (int i = 0; i \u0026lt; N; i++) a[i] = rand(); For the sake of exposition, we assume that $N$ is a power of two, and run all our experiments for $N=2^{13}$ so that the memory bandwidth is not a concern.\nTo implement argmin in the scalar case, we just need to maintain the index instead of the minimum value:\nint argmin(int *a, int n) { int k = 0; for (int i = 0; i \u0026lt; n; i++) if (a[i] \u0026lt; a[k]) k = i; return k; } It works at around 1.5 GFLOPS — meaning $1.5 \\cdot 10^9$ values per second processed on average, or about 0.75 values per cycle (the CPU is clocked at 2GHz).\nLet\u0026rsquo;s compare it to std::min_element:\nint argmin(int *a, int n) { int k = std::min_element(a, a + n) - a; return k; } The version from GCC gives ~0.28 GFLOPS — apparently, the compiler couldn\u0026rsquo;t pierce through all the abstractions. Another reminder to never use STL.\n#Vector of IndicesThe problem with vectorizing the scalar implementation is that there is a dependency between consequent iterations. When we optimized array sum, we faced the same problem, and we solved it by splitting the array into 8 slices, each representing a subset of its indices with the same remainder modulo 8. We can apply the same trick here, except that we also have to take array indices into account.\nWhen we have the consecutive elements and their indices in vectors, we can process them in parallel using predication:\ntypedef __m256i reg; int argmin(int *a, int n) { // indices on the current iteration reg cur = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7); // the current minimum for each slice reg min = _mm256_set1_epi32(INT_MAX); // its index (argmin) for each slice reg idx = _mm256_setzero_si256(); for (int i = 0; i \u0026lt; n; i += 8) { // load a new SIMD block reg x = _mm256_load_si256((reg*) \u0026amp;a[i]); // find the slices where the minimum is updated reg mask = _mm256_cmpgt_epi32(min, x); // update the indices idx = _mm256_blendv_epi8(idx, cur, mask); // update the minimum (can also similarly use a \u0026#34;blend\u0026#34; here, but min is faster) min = _mm256_min_epi32(x, min); // update the current indices const reg eight = _mm256_set1_epi32(8); cur = _mm256_add_epi32(cur, eight); // // can also use a \u0026#34;blend\u0026#34; here, but min is faster } // find the argmin in the \u0026#34;min\u0026#34; register and return its real index int min_arr[8], idx_arr[8]; _mm256_storeu_si256((reg*) min_arr, min); _mm256_storeu_si256((reg*) idx_arr, idx); int k = 0, m = min_arr[0]; for (int i = 1; i \u0026lt; 8; i++) if (min_arr[i] \u0026lt; m) m = min_arr[k = i]; return idx_arr[k]; } It works at around 8-8.5 GFLOPS. There is still some inter-dependency between the iterations, so we can optimize it by considering more than 8 elements per iteration and taking advantage of the instruction-level parallelism.\nThis would help performance a lot, but not enough to match the speed of computing the minimum (~24 GFLOPS) because there is another bottleneck. On each iteration, we need a load-fused comparison, a load-fused minimum, a blend, and an addition — that is 4 instructions in total to process 8 elements. Since the decode width of this CPU (Zen 2) is just 4, the performance will still be limited by 8 × 2 = 16 GFLOPS even if we somehow got rid of all the other bottlenecks.\nInstead, we will switch to another approach that requires fewer instructions per element.\n#Branches Aren\u0026rsquo;t ScaryWhen we run the scalar version, how often do we update the minimum?\nIntuition tells us that, if all the values are drawn independently at random, then the event when the next element is less than all the previous ones shouldn\u0026rsquo;t be frequent. More precisely, it equals the reciprocal of the number of processed elements. Therefore, the expected number of times the a[i] \u0026lt; a[k] condition is satisfied equals the sum of the harmonic series:\n$$ \\frac{1}{2} + \\frac{1}{3} + \\frac{1}{4} + \\ldots + \\frac{1}{n} = O(\\ln(n)) $$\nSo the minimum is updated around 5 times for a hundred-element array, 7 for a thousand-element, and just 14 for a million-element array — which isn\u0026rsquo;t large at all when looked at as a fraction of all is-new-minimum checks.\nThe compiler probably couldn\u0026rsquo;t figure it out on its own, so let\u0026rsquo;s explicitly provide this information:\nint argmin(int *a, int n) { int k = 0; for (int i = 0; i \u0026lt; n; i++) if (a[i] \u0026lt; a[k]) [[unlikely]] k = i; return k; } The compiler optimized the machine code layout, and the CPU is now able to execute the loop at around 2 GFLOPS — a slight but sizeable improvement from 1.5 GFLOPS of the non-hinted loop.\nHere is the idea: if we are only updating the minimum a dozen or so times during the entire computation, we can ditch all the vector-blending and index updating and just maintain the minimum and regularly check if it has changed. Inside this check, we can use however slow method of updating the argmin we want because it will only be called a few times.\nTo implement it with SIMD, all we need to do on each iteration is a vector load, a comparison, and a test-if-zero:\nint argmin(int *a, int n) { int min = INT_MAX, idx = 0; reg p = _mm256_set1_epi32(min); for (int i = 0; i \u0026lt; n; i += 8) { reg y = _mm256_load_si256((reg*) \u0026amp;a[i]); reg mask = _mm256_cmpgt_epi32(p, y); if (!_mm256_testz_si256(mask, mask)) { [[unlikely]] for (int j = i; j \u0026lt; i + 8; j++) if (a[j] \u0026lt; min) min = a[idx = j]; p = _mm256_set1_epi32(min); } } return idx; } It already performs at ~8.5 GFLOPS, but now the loop is bottlenecked by the testz instruction which only has a throughput of one. The solution is to load two consecutive SIMD blocks and use the minimum of them so that the testz effectively processes 16 elements in one go:\nint argmin(int *a, int n) { int min = INT_MAX, idx = 0; reg p = _mm256_set1_epi32(min); for (int i = 0; i \u0026lt; n; i += 16) { reg y1 = _mm256_load_si256((reg*) \u0026amp;a[i]); reg y2 = _mm256_load_si256((reg*) \u0026amp;a[i + 8]); reg y = _mm256_min_epi32(y1, y2); reg mask = _mm256_cmpgt_epi32(p, y); if (!_mm256_testz_si256(mask, mask)) { [[unlikely]] for (int j = i; j \u0026lt; i + 16; j++) if (a[j] \u0026lt; min) min = a[idx = j]; p = _mm256_set1_epi32(min); } } return idx; } This version works in ~10 GFLOPS. To remove the other obstacles, we can do two things:\nIncrease the block size to 32 elements to allow for more instruction-level parallelism. Optimize the local argmin: instead of calculating its exact location, we can just save the index of the block and then come back at the end and find it just once. This lets us only compute the minimum on each positive check and broadcast it to a vector, which is simpler and much faster. With these two optimizations implemented, the performance increases to a whopping ~22 GFLOPS:\nint argmin(int *a, int n) { int min = INT_MAX, idx = 0; reg p = _mm256_set1_epi32(min); for (int i = 0; i \u0026lt; n; i += 32) { reg y1 = _mm256_load_si256((reg*) \u0026amp;a[i]); reg y2 = _mm256_load_si256((reg*) \u0026amp;a[i + 8]); reg y3 = _mm256_load_si256((reg*) \u0026amp;a[i + 16]); reg y4 = _mm256_load_si256((reg*) \u0026amp;a[i + 24]); y1 = _mm256_min_epi32(y1, y2); y3 = _mm256_min_epi32(y3, y4); y1 = _mm256_min_epi32(y1, y3); reg mask = _mm256_cmpgt_epi32(p, y1); if (!_mm256_testz_si256(mask, mask)) { [[unlikely]] idx = i; for (int j = i; j \u0026lt; i + 32; j++) min = (a[j] \u0026lt; min ? a[j] : min); p = _mm256_set1_epi32(min); } } for (int i = idx; i \u0026lt; idx + 31; i++) if (a[i] == min) return i; return idx + 31; } This is almost as high as it can get as just computing the minimum itself works at around 24-25 GFLOPS.\nThe only problem of all these branch-happy SIMD implementations is that they rely on the minimum being updated very infrequently. This is true for random input distributions, but not in the worst case. If we fill the array with a sequence of decreasing numbers, the performance of the last implementation drops to about 2.7 GFLOPS — almost 10 times as slow (although still faster than the scalar code because we only calculate the minimum on each block).\nOne way to fix this is to do the same thing that the quicksort-like randomized algorithms do: just shuffle the input yourself and iterate over the array in random order. This lets you avoid this worst-case penalty, but it is tricky to implement due to RNG- and memory-related issues. There is a simpler solution.\n#Find the Minimum, Then Find the IndexWe know how to calculate the minimum of an array fast and how to find an element in an array fast — so why don\u0026rsquo;t we just separately compute the minimum and then find it?\nint argmin(int *a, int n) { int needle = min(a, n); int idx = find(a, n, needle); return idx; } If we implement the two subroutines optimally (check the linked articles), the performance will be ~18 GFLOPS for random arrays and ~12 GFLOPS for decreasing arrays — which makes sense as we are expected to read the array 1.5 and 2 times respectively. This isn\u0026rsquo;t that bad by itself — at least we avoid the 10x worst-case performance penalty — but the problem is that this penalized performance also translates to larger arrays, when we are bottlenecked by the memory bandwidth rather than compute.\nLuckily, we already know how to fix it. We can split the array into blocks of fixed size $B$ and compute the minima on these blocks while also maintaining the global minimum. When the minimum on a new block is lower than the global minimum, we update it and also remember the block number of where the global minimum currently is. After we\u0026rsquo;ve processed the entire array, we just return to that block and scan through its $B$ elements to find the argmin.\nThis way we only process $(N + B)$ elements and don\u0026rsquo;t have to sacrifice neither ½ nor ⅓ of the performance:\nconst int B = 256; // returns the minimum and its first block pair\u0026lt;int, int\u0026gt; approx_argmin(int *a, int n) { int res = INT_MAX, idx = 0; for (int i = 0; i \u0026lt; n; i += B) { int val = min(a + i, B); if (val \u0026lt; res) { res = val; idx = i; } } return {res, idx}; } int argmin(int *a, int n) { auto [needle, base] = approx_argmin(a, n); int idx = find(a + base, B, needle); return base + idx; } This results for the final implementation are ~22 and ~19 GFLOPS for random and decreasing arrays respectively.\nThe full implementation, including both min() and find(), is about 100 lines long. Take a look if you want, although it is still far from being production-grade.\n#SummaryHere are the results combined for all implementations:\nalgorithm rand decr reason for the performance difference ----------- ----- ----- ------------------------------------------------------------- std 0.28 0.28 scalar 1.54 1.89 efficient branch prediction + hinted 1.95 0.75 wrong hint index 8.17 8.12 simd 8.51 1.65 scalar-based argmin on each iteration + ilp 10.22 1.74 ^ same + optimized 22.44 2.70 ^ same, but faster because there are less inter-dependencies min+find 18.21 12.92 find() has to scan the entire array + blocked 22.23 19.29 we still have an optional horizontal minimum every B elements Take these results with a grain of salt: the measurements are quite noisy, they were done for just for two input distributions, for a specific array size ($N=2^{13}$, the size of the L1 cache), for a specific architecture (Zen 2), and for a specific and slightly outdated compiler (GCC 9.3) — the compiler optimizations were also very fragile to little changes in the benchmarking code.\nThere are also still some minor things to optimize, but the potential improvement is less than 10% so I didn\u0026rsquo;t bother. One day I may pluck up the courage, optimize the algorithm to the theoretical limit, handle the non-divisible-by-block-size array sizes and non-aligned memory cases, and then re-run the benchmarks properly on many architectures, with p-values and such. In case someone does it before me, please ping me back.\n#AcknowledgementsThe first, index-based SIMD algorithm was originally designed by Wojciech Muła in 2018.\nThanks to Zach Wegner for pointing out that the performance of the Muła\u0026rsquo;s algorithm is improved when implemented manually using intrinsics (I originally used the GCC vector types).\nAfter publication, I\u0026rsquo;ve discovered that Marshall Lochbaum, the creator of BQN, designed a very similar algorithm while he was working on Dyalog APL in 2019. Pay more attention to the world of array programming languages!\n","id":62,"path":"/hugo-page/hpc/algorithms/argmin/","title":"Argmin with SIMD"},{"content":"In the context of the external memory model, there are two types of efficient algorithms:\nCache-aware algorithms that are efficient for known $B$ and $M$. Cache-oblivious algorithms that are efficient for any $B$ and $M$. For example, external merge sorting is a cache-aware, but not cache-oblivious algorithm: we need to know the memory characteristics of the system, namely the ratio of available memory to the block size, to find the right $k$ to perform $k$-way merge sort.\nCache-oblivious algorithms are interesting because they automatically become optimal for all memory levels in the cache hierarchy, and not just the one for which they were specifically tuned. In this article, we consider some of their applications in matrix calculations.\n#Matrix TranspositionAssume we have a square matrix $A$ of size $N \\times N$, and we need to transpose it. The naive by-definition approach would go something like this:\nfor (int i = 0; i \u0026lt; n; i++) for (int j = 0; j \u0026lt; i; j++) swap(a[j * N + i], a[i * N + j]); Here we used a single pointer to the beginning of the memory region instead of a 2d array to be more explicit about its memory operations.\nThe I/O complexity of this code is $O(N^2)$ because the writes are not sequential. If you try to swap the iteration variables, it will be the other way around, but the result is going to be the same.\n#AlgorithmThe cache-oblivious algorithm relies on the following block matrix identity:\n$$ \\begin{pmatrix} A \u0026amp; B \\ C \u0026amp; D \\end{pmatrix}^T= \\begin{pmatrix} A^T \u0026amp; C^T \\ B^T \u0026amp; D^T \\end{pmatrix} $$\nIt lets us solve the problem recursively using a divide-and-conquer approach:\nDivide the input matrix into 4 smaller matrices. Transpose each one recursively. Combine results by swapping the corner result matrices. Implementing D\u0026amp;C on matrices is a bit more complex than on arrays, but the main idea is the same. Instead of copying submatrices explicitly, we want to use \u0026ldquo;views\u0026rdquo; into them, and also switch to the naive method when the data starts fitting in the L1 cache (or pick something small like $32 \\times 32$ if you don\u0026rsquo;t know it in advance). We also need to carefully handle the case when we have odd $n$ and thus can\u0026rsquo;t split the matrix into 4 equal submatrices.\nvoid transpose(int *a, int n, int N) { if (n \u0026lt;= 32) { for (int i = 0; i \u0026lt; n; i++) for (int j = 0; j \u0026lt; i; j++) swap(a[i * N + j], a[j * N + i]); } else { int k = n / 2; transpose(a, k, N); transpose(a + k, k, N); transpose(a + k * N, k, N); transpose(a + k * N + k, k, N); for (int i = 0; i \u0026lt; k; i++) for (int j = 0; j \u0026lt; k; j++) swap(a[i * N + (j + k)], a[(i + k) * N + j]); if (n \u0026amp; 1) for (int i = 0; i \u0026lt; n - 1; i++) swap(a[i * N + n - 1], a[(n - 1) * N + i]); } } The I/O complexity of the algorithm is $O(\\frac{N^2}{B})$, as we only need to touch roughly half the memory blocks during each merge stage, meaning that on each stage our problem becomes smaller.\nAdapting this code for the general case of non-square matrices is left as an exercise to the reader.\n#Matrix MultiplicationNext, let\u0026rsquo;s consider something slightly more complex: matrix multiplication.\n$$ C_{ij} = \\sum_k A_{ik} B_{kj} $$\nThe naive algorithm just translates its definition into code:\n// don\u0026#39;t forget to initialize c[][] with zeroes for (int i = 0; i \u0026lt; n; i++) for (int j = 0; j \u0026lt; n; j++) for (int k = 0; k \u0026lt; n; k++) c[i * n + j] += a[i * n + k] * b[k * n + j]; It needs to access $O(N^3)$ blocks in total as each scalar multiplication needs a separate block read.\nOne well-known optimization is to transpose $B$ first:\nfor (int i = 0; i \u0026lt; n; i++) for (int j = 0; j \u0026lt; i; j++) swap(b[j][i], b[i][j]) // ^ or use our faster transpose from before for (int i = 0; i \u0026lt; n; i++) for (int j = 0; j \u0026lt; n; j++) for (int k = 0; k \u0026lt; n; k++) c[i * n + j] += a[i * n + k] * b[j * n + k]; // \u0026lt;- note the indices Whether the transpose is done naively or with the cache-oblivious method we previously developed, the matrix multiplication with one of the matrices transposed would work in $O(N^3/B + N^2)$ as all memory accesses are now sequential.\nIt seems like we can\u0026rsquo;t do better, but it turns out we can.\n#AlgorithmCache-oblivious matrix multiplication relies on essentially the same trick as the transposition. We need to divide the data until it fits into lowest cache (i.e., $N^2 \\leq M$). For matrix multiplication, this equates to using this formula:\n$$ \\begin{pmatrix} A_{11} \u0026amp; A_{12} \\ A_{21} \u0026amp; A_{22} \\ \\end{pmatrix} \\begin{pmatrix} B_{11} \u0026amp; B_{12} \\ B_{21} \u0026amp; B_{22} \\ \\end{pmatrix} = \\begin{pmatrix} A_{11} B_{11} + A_{12} B_{21} \u0026amp; A_{11} B_{12} + A_{12} B_{22}\\ A_{21} B_{11} + A_{22} B_{21} \u0026amp; A_{21} B_{12} + A_{22} B_{22}\\ \\end{pmatrix} $$\nIt is slightly harder to implement though because we now have a total of 8 recursive matrix multiplications:\nvoid matmul(const float *a, const float *b, float *c, int n, int N) { if (n \u0026lt;= 32) { for (int i = 0; i \u0026lt; n; i++) for (int j = 0; j \u0026lt; n; j++) for (int k = 0; k \u0026lt; n; k++) c[i * N + j] += a[i * N + k] * b[k * N + j]; } else { int k = n / 2; // c11 = a11 b11 + a12 b21 matmul(a, b, c, k, N); matmul(a + k, b + k * N, c, k, N); // c12 = a11 b12 + a12 b22 matmul(a, b + k, c + k, k, N); matmul(a + k, b + k * N + k, c + k, k, N); // c21 = a21 b11 + a22 b21 matmul(a + k * N, b, c + k * N, k, N); matmul(a + k * N + k, b + k * N, c + k * N, k, N); // c22 = a21 b12 + a22 b22 mul(a + k * N, b + k, c + k * N + k, k, N); mul(a + k * N + k, b + k * N + k, c + k * N + k, k, N); if (n \u0026amp; 1) { for (int i = 0; i \u0026lt; n; i++) for (int j = 0; j \u0026lt; n; j++) for (int k = (i \u0026lt; n - 1 \u0026amp;\u0026amp; j \u0026lt; n - 1) ? n - 1 : 0; k \u0026lt; n; k++) c[i * N + j] += a[i * N + k] * b[k * N + j]; } } } Because there are many other factors in play here, we are not going to benchmark this implementation, and instead just do its theoretical performance analysis in external memory model.\n#AnalysisArithmetic complexity of the algorithm remains is the same, because the recurrence\n$$ T(N) = 8 \\cdot T(N/2) + \\Theta(N^2) $$\nis solved by $T(N) = \\Theta(N^3)$.\nIt doesn\u0026rsquo;t seem like we \u0026ldquo;conquered\u0026rdquo; anything yet, but let\u0026rsquo;s think about its I/O complexity:\n$$ T(N) = \\begin{cases} O(\\frac{N^2}{B}) \u0026amp; N \\leq \\sqrt M \u0026amp; \\text{(we only need to read it)} \\ 8 \\cdot T(N/2) + O(\\frac{N^2}{B}) \u0026amp; \\text{otherwise} \\end{cases} $$\nThe recurrence is dominated by $O((\\frac{N}{\\sqrt M})^3)$ base cases, meaning that the total complexity is\n$$ T(N) = O\\left(\\frac{(\\sqrt{M})^2}{B} \\cdot \\left(\\frac{N}{\\sqrt M}\\right)^3\\right) = O\\left(\\frac{N^3}{B\\sqrt{M}}\\right) $$\nThis is better than just $O(\\frac{N^3}{B})$, and by quite a lot.\n#Strassen AlgorithmIn a spirit similar to the Karatsuba algorithm, matrix multiplication can be decomposed in a way that involves 7 matrix multiplications of size $\\frac{n}{2}$, and the master theorem tells us that such divide-and-conquer algorithm would work in $O(n^{\\log_2 7}) \\approx O(n^{2.81})$ time and a similar asymptotic in the external memory model.\nThis technique, known as the Strassen algorithm, similarly splits each matrix into 4:\n$$ \\begin{pmatrix} C_{11} \u0026amp; C_{12} \\ C_{21} \u0026amp; C_{22} \\ \\end{pmatrix} =\\begin{pmatrix} A_{11} \u0026amp; A_{12} \\ A_{21} \u0026amp; A_{22} \\ \\end{pmatrix} \\begin{pmatrix} B_{11} \u0026amp; B_{12} \\ B_{21} \u0026amp; B_{22} \\ \\end{pmatrix} $$\nBut then it computes intermediate products of the $\\frac{N}{2} \\times \\frac{N}{2}$ matrices and combines them to get matrix $C$:\n$$ \\begin{aligned} M_1 \u0026amp;= (A_{11} + A_{22})(B_{11} + B_{22}) \u0026amp; C_{11} \u0026amp;= M_1 + M_4 - M_5 + M_7 \\ M_2 \u0026amp;= (A_{21} + A_{22}) B_{11} \u0026amp; C_{12} \u0026amp;= M_3 + M_5 \\ M_3 \u0026amp;= A_{11} (B_{21} - B_{22}) \u0026amp; C_{21} \u0026amp;= M_2 + M_4 \\ M_4 \u0026amp;= A_{22} (B_{21} - B_{11}) \u0026amp; C_{22} \u0026amp;= M_1 - M_2 + M_3 + M_6 \\ M_5 \u0026amp;= (A_{11} + A_{12}) B_{22} \\ M_6 \u0026amp;= (A_{21} - A_{11}) (B_{11} + B_{12}) \\ M_7 \u0026amp;= (A_{12} - A_{22}) (B_{21} + B_{22}) \\end{aligned} $$\nYou can verify these formulas with simple substitution if you feel like it.\nAs far as I know, none of the mainstream optimized linear algebra libraries use the Strassen algorithm, although there are some prototype implementations that are efficient for matrices larger than 2000 or so.\nThis technique can and actually has been extended multiple times to reduce the asymptotic even further by considering more submatrix products. As of 2020, current world record is $O(n^{2.3728596})$. Whether you can multiply matrices in $O(n^2)$ or at least $O(n^2 \\log^k n)$ time is an open problem.\n#Further ReadingFor a solid theoretical viewpoint, consider reading Cache-Oblivious Algorithms and Data Structures by Erik Demaine.\n","id":63,"path":"/hugo-page/hpc/external-memory/oblivious/","title":"Cache-Oblivious Algorithms"},{"content":"In 1940, a British mathematician G. H. Hardy published a famous essay titled \u0026ldquo;A Mathematician\u0026rsquo;s Apology\u0026rdquo; discussing the notion that mathematics should be pursued for its own sake rather than for the sake of its applications.\nSimilar to mathematics, the various fields of computer science also form a spectrum, with mathematical logic and computability theory on one end and web programming and application development on the other. I assume that you, the reader, is more on the applied side: this book was written to show that there are way too few people working on practical algorithm design instead of theoretical computer science — and since you got to Chapter 7, you probably also believe in that statement.\nBut, regardless of the personal views on the matter, one can see where Hardy is coming from. Being 62 years old at the date of writing, he witnessed the devastation caused by the First and the ongoing Second World War — which was greatly amplified by the weaponization of science. As a number theorist, Hardy finds calm working in a \u0026ldquo;useless\u0026rdquo; field and not having to face any moral dilemmas, writing:\nNo one has yet discovered any warlike purpose to be served by the theory of numbers or relativity, and it seems unlikely that anyone will do so for many years.\nIronically, this statement was proved very wrong just 5 years later with the development of the atomic bomb, which would not have been possible without the understanding of relativity, and the inception of computer-era cryptography, which extensively builds on number theory — the computational aspect of which is the main topic of this chapter.\n","id":64,"path":"/hugo-page/hpc/number-theory/","title":"Number Theory"},{"content":"The fact that the memory is partitioned into 64B cache lines makes it difficult to operate on data words that cross a cache line boundary. When you need to retrieve some primitive type, such as a 32-bit integer, you really want to have it located on a single cache line — both because retrieving two cache lines requires more memory bandwidth and stitching the results in hardware requires precious transistor space.\nThis aspect heavily influences algorithm designs and how compilers choose the memory layout of data structures.\n#Aligned AllocationBy default, when you allocate an array of some primitive type, you are guaranteed that the addresses of all elements are a multiple of their size, which ensures that they only span a single cache line. For example, you are guaranteed the address of the first and every other element of an int array is a multiple of 4 bytes (sizeof int).\nSometimes you need to ensure that this minimum alignment is higher. For example, many SIMD applications read and write data in blocks of 32 bytes, and it is crucial for performance that these 32 bytes belong to the same cache line. In such cases, you can use the alignas specifier when defining a static array variable:\nalignas(32) float a[n]; To allocate a memory-aligned array dynamically, you can use std::aligned_alloc, which takes the alignment value and the size of an array in bytes and returns a pointer to the allocated memory — just like the new operator does:\nvoid *a = std::aligned_alloc(32, 4 * n); You can also align memory to sizes larger than the cache line. The only restriction is that the size parameter must be an integral multiple of alignment.\nYou can also use the alignas specifier when defining a struct:\nstruct alignas(64) Data { // ... }; Whenever an instance of Data is allocated, it will be at the beginning of a cache line. The downside is that the effective size of the structure will be rounded up to the nearest multiple of 64 bytes. This has to be done so that, e.g., when allocating an array of Data, not just the first element is properly aligned.\n#Structure AlignmentThis issue becomes more complicated when we need to allocate a group of non-uniform elements, which is the case for structures. Instead of playing Tetris trying to rearrange the members of a struct so that each of them is within a single cache line — which isn\u0026rsquo;t always possible as the structure itself doesn\u0026rsquo;t have to be placed on the start of a cache line — most C/C++ compilers also rely on the mechanism of memory alignment.\nStructure alignment similarly ensures that the address of all its member primitive types (char, int, float*, etc) are multiples of their size, which automatically guarantees that each of them only spans one cache line. It achieves that by:\npadding, if necessary, each structure member with a variable number of blank bytes to satisfy the alignment requirement of the next member; setting the alignment requirement of the structure itself to the maximum of the alignment requirements of its member types, so that when an array of the structure type is allocated or it is used as a member type in another structure, the alignment requirements of all its primitive types are satisfied. For better understanding, consider the following toy example:\nstruct Data { char a; short b; int c; char d; }; When stored succinctly, this structure needs a total of $1 + 2 + 4 + 1 = 8$ bytes per instance, but even assuming that the whole structure has the alignment of 4 bytes (its largest member, int), only a will be fine, while b, c and d are not size-aligned and potentially cross a cache line boundary.\nTo fix this, the compiler inserts some unnamed members so that each next member gets the right minimum alignment:\nstruct Data { char a; // 1 byte char x[1]; // 1 byte for the following \u0026#34;short\u0026#34; to be aligned on a 2-byte boundary short b; // 2 bytes int c; // 4 bytes (largest member, setting the alignment of the whole structure) char d; // 1 byte char y[3]; // 3 bytes to make total size of the structure 12 bytes (divisible by 4) }; // sizeof(Data) = 12 // alignof(Data) = alignof(int) = sizeof(int) = 4 This potentially wastes space but saves a lot of CPU cycles. This trade-off is mostly beneficial, so structure alignment is enabled by default in most compilers.\n#Optimizing Member OrderPadding is only inserted before a not-yet-aligned member or at the end of the structure. By changing the ordering of members in a structure, it is possible to change the required number of padding bytes and the total size of the structure.\nIn the previous example, we could reorder the structure members like this:\nstruct Data { int c; short b; char a; char d; }; Now, each of them is aligned without any padding, and the size of the structure is just 8 bytes. It seems stupid that the size of a structure and consequently its performance depends on the order of definition of its members, but this is required for binary compatibility.\nAs a rule of thumb, place your type definitions from largest data types to smallest — this greedy algorithm is guaranteed to work unless you have some weird non-power-of-two type sizes such as the 10-byte long double1.\n#Structure PackingIf you know what you are doing, you can disable structure padding and pack your data as tight as possible.\nYou have to ask the compiler to do it, as such functionality is not a part of neither C nor C++ standard yet. In GCC and Clang, this is done with the packed attribute:\nstruct __attribute__ ((packed)) Data { long long a; bool b; }; This makes the instances of Data take just 9 bytes instead of the 16 required by alignment, at the cost of possibly fetching two cache lines to reads its elements.\n#Bit fieldsYou can also use packing along with bit fields, which allow you to explicitly fix the size of a member in bits:\nstruct __attribute__ ((packed)) Data { char a; // 1 byte int b : 24; // 3 bytes }; This structure takes 4 bytes when packed and 8 bytes when padded. The number of bits a member has doesn\u0026rsquo;t have to be a multiple of 8, and neither does the total structure size. In an array of Data, the neighboring elements will be \u0026ldquo;merged\u0026rdquo; in the case of a non-whole number of bytes. It also allows you to set a width that exceeds the base type, which acts as padding — although it throws a warning in the process.\nThis feature is not so widespread because CPUs don\u0026rsquo;t have 3-byte arithmetic or things like that and has to do some inefficient byte-by-byte conversion during loading:\nint load(char *p) { char x = p[0], y = p[1], z = p[2]; return (x \u0026lt;\u0026lt; 16) + (y \u0026lt;\u0026lt; 8) + z; } The overhead is even larger when there is a non-whole byte — it needs to be handled with a shift and an and-mask.\nThis procedure can be optimized by loading a 4-byte int and then using a mask to discard its highest bits.\nint load(int *p) { int x = *p; return x \u0026amp; ((1\u0026lt;\u0026lt;24) - 1); } Compilers usually don\u0026rsquo;t do that because it\u0026rsquo;s technically not legal: that 4th byte may be on a memory page that you don\u0026rsquo;t own, so the operating system won\u0026rsquo;t let you load it even if you are going to discard it right away.\nThe 80-bit long double takes at least 10 bytes, but the exact format is up to the compiler — for example, it may pad it to 12 or 16 bytes to minimize alignment issues (64-bit GCC and Clang use 16 bytes by default; you can override this by specifying one of -mlong-double-64/80/128 or -m96/128bit-long-double options).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":65,"path":"/hugo-page/hpc/cpu-cache/alignment/","title":"Alignment and Packing"},{"content":"How long does it take to add two numbers together? Being one of the most frequently used instructions, add by itself only takes one cycle to execute. So, if the data is already loaded into registers, it takes one just cycle.\nBut in the general case (*c = *a + *b), we need to fetch its operands from memory first:\nmov eax, DWORD PTR [rsi] add eax, DWORD PTR [rdi] mov DWORD PTR [rdx], eax When you fetch anything from memory, there is always some latency before the data arrives. Moreover, the request doesn\u0026rsquo;t go directly to its ultimate storage location, but it first goes through a complex system of address translation units and caching layers designed to both help in memory management and reduce latency.\nTherefore, the only correct answer to this question is \u0026ldquo;it depends\u0026rdquo; — primarily on where the operands are stored:\nIf the data is stored in the main memory (RAM), it will take around ~100ns, or about 200 cycles, to fetch it, and then another 200 cycles to write it back. If it was accessed recently, it is probably cached and will take less than that to fetch, depending on how long ago it was accessed — it could be ~50 cycles for the slowest layer of cache and around 4-5 cycles for the fastest. But it could also be stored on some type of external memory such as a hard drive, and in this case, it will take around 5ms, or roughly $10^7$ cycles (!) to access it. Such a high variance of memory performance is caused by the fact that memory hardware doesn\u0026rsquo;t follow the same laws of silicon scaling as CPU chips do. Memory is still improving through other means, but if 50 years ago memory timings were roughly on the same scale with the instruction latencies, nowadays they lag far behind.\nTo be less of a limiting factor, modern memory systems are becoming increasingly hierarchical, where the higher layers trade off some of their capacity for reduced latency. As these characteristics may change in the orders of magnitude between the layers — especially in the case of external memory types — it became crucial for many memory-intensive algorithms to optimize their I/O operations before anything else.\nThis prompted the creation of a new cost model, called the external memory model, whose only primitive operations are block reads and writes, and everything else has zero cost as long as it only involves data stored in a limited-sized local memory. It spawned an exciting new field of external memory algorithms, which we will study in this chapter.\n","id":66,"path":"/hugo-page/hpc/external-memory/","title":"External Memory"},{"content":"When compilers can infer that a certain variable does not depend on any user-provided data, they can compute its value during compile time and turn it into a constant by embedding it into the generated machine code.\nThis optimization helps performance a lot, but it is not a part of the C++ standard, so compilers don\u0026rsquo;t have to do that. When a compile-time computation is either hard to implement or time-intensive, a compiler may pass on that opportunity.\n#Constant ExpressionsFor a more reliable solution, in modern C++ you can mark a function as constexpr; if it is called by passing constants its value is guaranteed to be computed during compile time:\nconstexpr int fibonacci(int n) { if (n \u0026lt;= 2) return 1; return fibonacci(n - 1) + fibonacci(n - 2); } static_assert(fibonacci(10) == 55); These functions have some restrictions like that they only call other constexpr functions and can\u0026rsquo;t do memory allocation, but otherwise, they are executed \u0026ldquo;as is.\u0026rdquo;\nNote that while constexpr functions don\u0026rsquo;t cost anything during run time, they still increase compilation time, so at least remotely care about their efficiency and don\u0026rsquo;t put something NP-complete in them:\nconstexpr int fibonacci(int n) { int a = 1, b = 1; while (n--) { int c = a + b; a = b; b = c; } return b; } There used to be many more limitations in earlier C++ standards, like you could not use any sort of state inside them and had to rely on recursion, so the whole process felt more like Haskell programming rather than C++. Since C++17, you can even compute static arrays using the imperative style, which is useful for precomputing lookup tables:\nstruct Precalc { int isqrt[1000]; constexpr Precalc() : isqrt{} { for (int i = 0; i \u0026lt; 1000; i++) isqrt[i] = int(sqrt(i)); } }; constexpr Precalc P; static_assert(P.isqrt[42] == 6); Note that when you call constexpr functions while passing non-constants, the compiler may or may not compute them during compile time:\nfor (int i = 0; i \u0026lt; 100; i++) cout \u0026lt;\u0026lt; fibonacci(i) \u0026lt;\u0026lt; endl; In this example, even though technically we perform a constant number of iterations and call fibonacci with parameters known at compile time, they are technically not compile-time constants. It\u0026rsquo;s up to the compiler whether to optimize this loop or not — and for heavy computations, it often chooses not to.\n","id":67,"path":"/hugo-page/hpc/compilation/precalc/","title":"Precomputation"},{"content":"The prefix sum, also known as cumulative sum, inclusive scan, or simply scan, is a sequence of numbers $b_i$ generated from another sequence $a_i$ using the following rule:\n$$ \\begin{aligned} b_0 \u0026amp;= a_0 \\ b_1 \u0026amp;= a_0 + a_1 \\ b_2 \u0026amp;= a_0 + a_1 + a_2 \\ \u0026amp;\\ldots \\end{aligned} $$\nIn other words, the $k$-th element of the output sequence is the sum of the first $k$ elements of the input sequence.\nPrefix sum is a very important primitive in many algorithms, especially in the context of parallel algorithms, where its computation scales almost perfectly with the number of processors. Unfortunately, it is much harder to speed up with SIMD parallelism on a single CPU core, but we will try it nonetheless — and derive an algorithm that is ~2.5x faster than the baseline scalar implementation.\n#BaselineFor our baseline, we could just invoke std::partial_sum from the STL, but for clarity, we will implement it manually. We create an array of integers and then sequentially add the previous element to the current one:\nvoid prefix(int *a, int n) { for (int i = 1; i \u0026lt; n; i++) a[i] += a[i - 1]; } It seems like we need two reads, an add, and a write on each iteration, but of course, the compiler optimizes the extra read away and uses a register as the accumulator:\nloop: add edx, DWORD PTR [rax] mov DWORD PTR [rax-4], edx add rax, 4 cmp rax, rcx jne loop After unrolling the loop, just two instructions effectively remain: the fused read-add and the write-back of the result. Theoretically, these should work at 2 GFLOPS (1 element per CPU cycle, by the virtue of superscalar processing), but since the memory system has to constantly switch between reading and writing, the actual performance is between 1.2 and 1.6 GFLOPS, depending on the array size.\n#VectorizationOne way to implement a parallel prefix sum algorithm is to split the array into small blocks, independently calculate local prefix sums on them, and then do a second pass where we adjust the computed values in each block by adding the sum of all previous elements to them.\nThis allows processing each block in parallel — both during the computation of the local prefix sums and the accumulation phase — so you usually split the array into as many blocks as you have processors. But since we are only allowed to use one CPU core, and non-sequential memory access in SIMD doesn\u0026rsquo;t work well, we are not going to do that. Instead, we will use a fixed block size equal to the size of a SIMD lane and calculate prefix sums within a register.\nNow, to compute these prefix sums locally, we are going to use another parallel prefix sum method that is generally inefficient (the total work is $O(n \\log n)$ instead of linear) but is good enough for the case when the data is already in a SIMD register. The idea is to perform $\\log n$ iterations where on $k$-th iteration, we add $a_{i - 2^k}$ to $a_i$ for all applicable $i$:\nfor (int l = 0; l \u0026lt; logn; l++) // (atomically and in parallel): for (int i = (1 \u0026lt;\u0026lt; l); i \u0026lt; n; i++) a[i] += a[i - (1 \u0026lt;\u0026lt; l)]; We can prove that this algorithm works by induction: if on $k$-th iteration every element $a_i$ is equal to the sum of the $(i - 2^k, i]$ segment of the original array, then after adding $a_{i - 2^k}$ to it, it will be equal to the sum of $(i - 2^{k+1}, i]$. After $O(\\log n)$ iterations, the array will turn into its prefix sum.\nTo implement it in SIMD, we could use permutations to place $i$-th element against $(i-2^k)$-th, but they are too slow. Instead, we will use the sll (\u0026ldquo;shift lanes left\u0026rdquo;) instruction that does exactly that and also replaces the unmatched elements with zeros:\ntypedef __m128i v4i; v4i prefix(v4i x) { // x = 1, 2, 3, 4 x = _mm_add_epi32(x, _mm_slli_si128(x, 4)); // x = 1, 2, 3, 4 // + 0, 1, 2, 3 // = 1, 3, 5, 7 x = _mm_add_epi32(x, _mm_slli_si128(x, 8)); // x = 1, 3, 5, 7 // + 0, 0, 1, 3 // = 1, 3, 6, 10 return x; } Unfortunately, the 256-bit version of this instruction performs this byte shift independently within two 128-bit lanes, which is typical to AVX:\ntypedef __m256i v8i; v8i prefix(v8i x) { // x = 1, 2, 3, 4, 5, 6, 7, 8 x = _mm256_add_epi32(x, _mm256_slli_si256(x, 4)); x = _mm256_add_epi32(x, _mm256_slli_si256(x, 8)); x = _mm256_add_epi32(x, _mm256_slli_si256(x, 16)); // \u0026lt;- this does nothing // x = 1, 3, 6, 10, 5, 11, 18, 26 return x; } We still can use it to compute 4-element prefix sums twice as fast, but we\u0026rsquo;ll have to switch to 128-bit SSE when accumulating. Let\u0026rsquo;s write a handy function that computes a local prefix sum end-to-end:\nvoid prefix(int *p) { v8i x = _mm256_load_si256((v8i*) p); x = _mm256_add_epi32(x, _mm256_slli_si256(x, 4)); x = _mm256_add_epi32(x, _mm256_slli_si256(x, 8)); _mm256_store_si256((v8i*) p, x); } Now, for the accumulate phase, we will create another handy function that similarly takes the pointer to a 4-element block and also the 4-element vector of the previous prefix sum. The job of this function is to add this prefix sum vector to the block and update it so that it can be passed on to the next block (by broadcasting the last element of the block before the addition):\nv4i accumulate(int *p, v4i s) { v4i d = (v4i) _mm_broadcast_ss((float*) \u0026amp;p[3]); v4i x = _mm_load_si128((v4i*) p); x = _mm_add_epi32(s, x); _mm_store_si128((v4i*) p, x); return _mm_add_epi32(s, d); } With prefix and accumulate implemented, the only thing left is to glue together our two-pass algorithm:\nvoid prefix(int *a, int n) { for (int i = 0; i \u0026lt; n; i += 8) prefix(\u0026amp;a[i]); v4i s = _mm_setzero_si128(); for (int i = 4; i \u0026lt; n; i += 4) s = accumulate(\u0026amp;a[i], s); } The algorithm already performs slightly more than twice as fast as the scalar implementation but becomes slower for large arrays that fall out of the L3 cache — roughly at half the two-way RAM bandwidth as we are reading the entire array twice.\nAnother interesting data point: if we only execute the prefix phase, the performance would be ~8.1 GFLOPS. The accumulate phase is slightly slower at ~5.8 GFLOPS. Sanity check: the total performance should be $\\frac{1}{ \\frac{1}{5.8} + \\frac{1}{8.1} } \\approx 3.4$.\n#BlockingSo, we have a memory bandwidth problem for large arrays. We can avoid re-fetching the entire array from RAM if we split it into blocks that fit in the cache and process them separately. All we need to pass to the next block is the sum of the previous ones, so we can design a local_prefix function with an interface similar to accumulate:\nconst int B = 4096; // \u0026lt;- ideally should be slightly less or equal to the L1 cache v4i local_prefix(int *a, v4i s) { for (int i = 0; i \u0026lt; B; i += 8) prefix(\u0026amp;a[i]); for (int i = 0; i \u0026lt; B; i += 4) s = accumulate(\u0026amp;a[i], s); return s; } void prefix(int *a, int n) { v4i s = _mm_setzero_si128(); for (int i = 0; i \u0026lt; n; i += B) s = local_prefix(a + i, s); } (We have to make sure that $N$ is a multiple of $B$, but we are going to ignore such implementation details for now.)\nThe blocked version performs considerably better, and not just for when the array is in the RAM:\nThe speedup in the RAM case compared to the non-blocked implementation is only ~1.5 and not 2. This is because the memory controller is sitting idle while we iterate over the cached block for the second time instead of fetching the next one — the hardware prefetcher isn\u0026rsquo;t advanced enough to detect this pattern.\n#Continuous LoadsThere are several ways to solve this under-utilization problem. The obvious one is to use software prefetching to explicitly request the next block while we are still processing the current one.\nIt is better to add prefetching to the accumulate phase because it is slower and less memory-intensive than prefix:\nv4i accumulate(int *p, v4i s) { __builtin_prefetch(p + B); // \u0026lt;-- prefetch the next block // ... return s; } The performance slightly decreases for in-cache arrays, but approaches closer to 2 GFLOPS for the in-RAM ones:\nAnother approach is to do interleaving of the two phases. Instead of separating and alternating between them in large blocks, we can execute the two phases concurrently, with the accumulate phase lagging behind by a fixed number of iterations — similar to the CPU pipeline:\nconst int B = 64; // ^ small sizes cause pipeline stalls // large sizes cause cache system inefficiencies void prefix(int *a, int n) { v4i s = _mm_setzero_si128(); for (int i = 0; i \u0026lt; B; i += 8) prefix(\u0026amp;a[i]); for (int i = B; i \u0026lt; n; i += 8) { prefix(\u0026amp;a[i]); s = accumulate(\u0026amp;a[i - B], s); s = accumulate(\u0026amp;a[i - B + 4], s); } for (int i = n - B; i \u0026lt; n; i += 4) s = accumulate(\u0026amp;a[i], s); } This has more benefits: the loop progresses at a constant speed, reducing the pressure on the memory system, and the scheduler sees the instructions of both subroutines, allowing it to be more efficient at assigning instruction to execution ports — sort of like hyper-threading, but in code.\nFor these reasons, the performance improves even on small arrays:\nAnd finally, it doesn\u0026rsquo;t seem that we are bottlenecked by the memory read port or the decode width, so we can add prefetching for free, which improves the performance even more:\nThe total speedup we were able to achieve is between $\\frac{4.2}{1.5} \\approx 2.8$ for small arrays and $\\frac{2.1}{1.2} \\approx 1.75$ for large arrays.\nThe speedup may be higher for lower-precision data compared to the scalar code, as it is pretty much limited to executing one iteration per cycle regardless of the operand size, but it is still sort of \u0026ldquo;meh\u0026rdquo; when compared to some other SIMD-based algorithms. This is largely because there isn\u0026rsquo;t a full-register byte shift in AVX that would allow the accumulate stage to proceed twice as fast, let alone a dedicated prefix sum instruction.\n#Other Relevant WorkYou can read this paper from Columbia that focuses on the multi-core setting and AVX-512 (which sort of has a fast 512-bit register byte shift) and this StackOverflow question for a more general discussion.\nMost of what I\u0026rsquo;ve described in this article was already known. To the best of my knowledge, my contribution here is the interleaving technique, which is responsible for a modest ~20% performance increase. There probably are ways to improve it further, but not by a lot.\nThere is also this professor at CMU, Guy Blelloch, who advocated for a dedicated prefix sum hardware back in the 90s when vector processors were still a thing. Prefix sums are very important for parallel applications, and the hardware is becoming increasingly more parallel, so maybe, in the future, the CPU manufacturers will revitalize this idea and make prefix sum calculations slightly easier.\n","id":68,"path":"/hugo-page/hpc/algorithms/prefix/","title":"Prefix Sum with SIMD"},{"content":" To precisely assess the performance of an algorithm in terms of its memory operations, we need to take into account multiple characteristics of the cache system: the number of cache layers, the memory and block sizes of each layer, the exact strategy used for cache eviction by each layer, and sometimes even the details of the memory paging mechanism.\nAbstracting away from all these minor details helps a lot in the first stages of designing algorithms. Instead of calculating theoretical cache hit rates, it often makes more sense to reason about cache performance in more qualitative terms.\nIn this context, we can talk about the degree of cache reuse primarily in two ways:\nTemporal locality refers to the repeated access of the same data within a relatively small time period, such that the data likely remains cached between the requests. Spatial locality refers to the use of elements relatively close to each other in terms of their memory locations, such that they are likely fetched in the same memory block. In other words, temporal locality is when it is likely that this same memory location will soon be requested again, while spatial locality is when it is likely that a nearby location will be requested right after.\nIn this section, we will do some case studies to show how these high-level concepts can help in practical optimization.\n#Depth-First vs. Breadth-FirstConsider a divide-and-conquer algorithm such as merge sorting. There are two approaches to implementing it:\nWe can implement it recursively, or \u0026ldquo;depth-first,\u0026rdquo; the way it is normally implemented: sort the left half, sort the right half and then merge the results. We can implement it iteratively, or \u0026ldquo;breadth-first:\u0026rdquo; do the lowest \u0026ldquo;layer\u0026rdquo; first, looping through the entire dataset and comparing odd elements with even elements, then merge the first two elements with the second two elements, the third two elements with the fourth two elements and so on. It seems like the second approach is more cumbersome, but faster — because recursion is always slow, right?\nGenerally, recursion is indeed slow, but this is not the case for this and many similar divide-and-conquer algorithms. Although the iterative approach has the advantage of only doing sequential I/O, the recursive approach has much better temporal locality: when a segment fully fits into the cache, it stays there for all lower layers of recursion, resulting in better access times later on.\nIn fact, since we only need to split the array $O(\\log \\frac{N}{M})$ times until this happens, we would only need to read $O(\\frac{N}{B} \\log \\frac{N}{M})$ blocks in total, while in the iterative approach the entire array will be read from scratch $O(\\log N)$ times no matter what. This results in the speedup of $O(\\frac{\\log N}{\\log N - \\log M})$, which may be up to an order of magnitude.\nIn practice, there is still some overhead associated with the recursion, and for this reason, it makes sense to use hybrid algorithms where we don\u0026rsquo;t go all the way down to the base case and instead switch to the iterative code on the lower levels of recursion.\n#Dynamic ProgrammingSimilar reasoning can be applied to the implementations of dynamic programming algorithms but leading to the reverse result. Consider the classic knapsack problem: given $N$ items with positive integer costs $c_i$, pick a subset of items with the maximum total cost that does not exceed a given constant $W$.\nThe way to solve it is to introduce the state $f[n, w]$, which corresponds to the maximum total cost not exceeding $w$ that can be achieved using only the first $n$ items. These values can be computed in $O(1)$ time per entry if we consider either taking or not taking the $n$-th item and using the previous states of the dynamic to make the optimal decision.\nPython has a handy lru_cache decorator which can be used for implementing it with memoized recursion:\n@lru_cache def f(n, w): # check if we have no items to choose if n == 0: return 0 # check if we can\u0026#39;t pick the last item (note zero-based indexing) if c[n - 1] \u0026gt; w: return f(n - 1, w) # otherwise, we can either pick the last item or not return max(f(n - 1, w), c[n - 1] + f(n - 1, w - c[n - 1])) When computing $f[N, W]$, the recursion may visit up to $O(N \\cdot W)$ different states, which is asymptotically efficient, but rather slow in reality. Even after nullifying the overhead of Python recursion and all the hash table queries required for the LRU cache to work, it would still be slow because it does random I/O throughout most of the execution.\nWhat we can do instead is to create a two-dimensional array for the dynamic and replace the recursion with a nice nested loop like this:\nint f[N + 1][W + 1] = {0}; // this zero-fills the array for (int n = 1; n \u0026lt;= N; n++) for (int w = 0; w \u0026lt;= W; w++) f[n][w] = c[n - 1] \u0026gt; w ? f[n - 1][w] : max(f[n - 1][k], c[n - 1] + f[n - 1][w - c[n - 1]]); Notice that we are only using the previous layer of the dynamic to calculate the next one. This means that if we can store one layer in the cache, we would only need to write $O(\\frac{N \\cdot W}{B})$ blocks in external memory.\nMoreover, if we only need the answer, we don\u0026rsquo;t actually have to store the whole 2d array but only the last layer. This lets us use just $O(W)$ memory by maintaining a single array of $W$ values. To simplify the code, we can slightly change the dynamic to store a binary value: whether it is possible to get the sum of exactly $w$ using the items that we have already considered. This dynamic is even faster to compute:\nbool f[W + 1] = {0}; f[0] = 1; for (int n = 0; n \u0026lt; N; n++) for (int x = W - c[n]; x \u0026gt;= 0; x--) f[x + c[n]] |= f[x]; As a side note, now that it only uses simple bitwise operations, it can be optimized further by using a bitset:\nstd::bitset\u0026lt;W + 1\u0026gt; b; b[0] = 1; for (int n = 0; n \u0026lt; N; n++) b |= b \u0026lt;\u0026lt; c[n]; Surprisingly, there is still some room for improvement, and we will come back to this problem later.\n#Sparse TableSparse table is a static data structure that is often used for solving the static RMQ problem and computing any similar idempotent range reductions in general. It can be formally defined as a two-dimensional array of size $\\log n \\times n$:\n$$ t[k][i] = \\min { a_i, a_{i+1}, \\ldots, a_{i+2^k-1} } $$\nIn plain English: we store the minimum on each segment whose length is a power of two.\nSuch array can be used for calculating minima on arbitrary segments in constant time because for each segment we can always find two possibly overlapping segments whose sizes are the same power of two, the union of which gives the whole segment.\nThis means that we can just take the minimum of these two precomputed minimums as the answer:\nint rmq(int l, int r) { // half-interval [l; r) int t = __lg(r - l); return min(mn[t][l], mn[t][r - (1 \u0026lt;\u0026lt; t)]); } The __lg function is an intrinsic available in GCC that calculates the binary logarithm of a number rounded down. Internally it uses the clz (\u0026ldquo;count leading zeros\u0026rdquo;) instruction and subtracts this count from 32 (in case of a 32-bit integer), and thus takes just a few cycles.\nThe reason why I bring it up in this article is that there are multiple alternative ways it can be built, with different efficiencies in terms of memory operations. In general, a sparse table can be built in $O(n \\log n)$ time in dynamic programming fashion by iterating in the order of increasing $i$ or $k$ and applying the following identity:\n$$ t[k][i] = \\min(t[k-1][i], t[k-1][i+2^{k-1}]) $$\nNow, there are two design choices to make: whether the log-size $k$ should be the first or the second dimension, and whether to iterate over $k$ and then $i$ or the other way around. This means that there are $2×2=4$ ways to build it, and here is the optimal one:\nint mn[logn][maxn]; memcpy(mn[0], a, sizeof a); for (int l = 0; l \u0026lt; logn - 1; l++) for (int i = 0; i + (2 \u0026lt;\u0026lt; l) \u0026lt;= n; i++) mn[l + 1][i] = min(mn[l][i], mn[l][i + (1 \u0026lt;\u0026lt; l)]); This is the only combination of the memory layout and the iteration order that results in beautiful linear passes that work ~3x faster. As an exercise, consider the other three variants and think about why they are slower.\n#Array-of-Structs vs. Struct-of-ArraysSuppose that you want to implement a binary tree and store its fields in separate arrays like this:\nint left_child[maxn], right_child[maxn], key[maxn], size[maxn]; Such memory layout, when we store each field separately from others, is called struct-of-arrays (SoA). In most cases, when implementing tree operations, you access a node and then shortly after all or most of its internal data. If these fields are stored separately, this would mean that they are also located in different memory blocks. If some of the requested fields happen to be are cached while the others are not, you would still have to wait for the slowest of them to be fetched.\nIn contrast, if it was instead stored as an array-of-structs (AoS), you would need ~4 times fewer block reads as all the data of a node is stored in the same block and fetched at once:\nstruct Node { int left_child, right_child, key, size; }; Node t[maxn]; The AoS layout is usually preferred for data structures, but SoA still has good uses: while it is worse for searching, it is much better for linear scanning.\nThis difference in design is important in data processing applications. For example, databases can be either row- or column-oriented (also called columnar):\nRow-oriented storage formats are used when you need to search for a limited number of objects in a large dataset and/or fetch all or most of their fields. Examples: PostgreSQL, MongoDB. Columnar storage formats are used for big data processing and analytics, where you need to scan through everything anyway to calculate certain statistics. Examples: ClickHouse, Hbase. Columnar formats have the additional advantage that you can only read the fields that you need, as different fields are stored in separate external memory regions.\n","id":69,"path":"/hugo-page/hpc/external-memory/locality/","title":"Spatial and Temporal Locality"},{"content":"In the previous chapter, we studied computer memory from a theoretical standpoint, using the external memory model to estimate the performance of memory-bound algorithms.\nWhile the external memory model is more or less accurate for computations involving HDDs and network storage, where cost of arithmetic operations on in-memory values is negligible compared to external I/O operations, it is too imprecise for lower levels in the cache hierarchy, where the costs of these operations become comparable.\nTo perform more fine-grained optimization of in-memory algorithms, we have to start taking into account the many specific details of the CPU cache system. And instead of studying loads of boring Intel documents with dry specs and theoretically achievable limits, we will estimate these parameters experimentally by running numerous small benchmark programs with access patterns that resemble the ones that often occur in practical code.\nExperimental SetupAs before, I will be running all experiments on Ryzen 7 4700U, which is a \u0026ldquo;Zen 2\u0026rdquo; CPU with the following main cache-related specs:\n8 physical cores (without hyper-threading) clocked at 2GHz (and 4.1GHz in boost mode — which we disable); 256K of 8-way set associative L1 data cache or 32K per core; 4M of 8-way set associative L2 cache or 512K per core; 8M of 16-way set associative L3 cache, shared between 8 cores; 16GB (2x8G) of DDR4 RAM @ 2667MHz. You can compare it with your own hardware by running dmidecode -t cache or lshw -class memory on Linux or by installing CPU-Z on Windows. You can also find additional details about the CPU on WikiChip and 7-CPU. Not all conclusions will generalize to every CPU platform in existence.\nDue to difficulties in preventing the compiler from optimizing away unused values, the code snippets in this article are slightly simplified for exposition purposes. Check the code repository if you want to reproduce them yourself.\nAcknowledgementsThis chapter is inspired by \u0026ldquo;Gallery of Processor Cache Effects\u0026rdquo; by Igor Ostrovsky and \u0026ldquo;What Every Programmer Should Know About Memory\u0026rdquo; by Ulrich Drepper, both of which can serve as good accompanying readings.\n","id":70,"path":"/hugo-page/hpc/cpu-cache/","title":"RAM \u0026 CPU Caches"},{"content":"SIMD parallelism is most often used for embarrassingly parallel computations: the kinds where all you do is apply some elementwise function to all elements of an array and write it back somewhere else. In this setting, you don\u0026rsquo;t even need to know how SIMD works: the compiler is perfectly capable of optimizing such loops by itself — you just need to be aware that such optimization exists and that it usually yields a 5-10x speedup.\nDoing nothing and relying on auto-vectorization is actually the most popular way of using SIMD. In fact, in many cases, it even advised to stick with the plain scalar code for its simplicity and maintainability.\nBut often even the loops that seem straightforward to vectorize are not optimized because of some technical nuances. As in many other cases, the compiler may need some additional input from the programmer as he may know a bit more about the problem than what can be inferred from static analysis.\n#Potential ProblemsConsider the \u0026ldquo;a + b\u0026rdquo; example we started with:\nvoid sum(int *a, int *b, int *c, int n) { for (int i = 0; i \u0026lt; n; i++) c[i] = a[i] + b[i]; } Let\u0026rsquo;s step into a compiler\u0026rsquo;s shoes and think about what can go wrong when this loop is vectorized.\nArray size. If the array size is unknown beforehand, it may be that it is too small for vectorization to be beneficial in the first place. Even if it is sufficiently large, we need to insert an additional check for the remainder of the loop to process it scalar, which would cost us a branch.\nTo eliminate these runtime checks, use array sizes that are compile-time constants, and preferably pad arrays to the nearest multiple of the SIMD block size.\nMemory aliasing. Even when array size issues are out of the question, vectorizing this loop is not always technically correct. For example, the arrays a and c can intersect in a way that their beginnings differ by a single position — because who knows, maybe the programmer wanted to calculate the Fibonacci sequence through a convolution this way. In this case, the data in the SIMD blocks will intersect and the observed behavior will differ from the one in the scalar case.\nWhen the compiler can\u0026rsquo;t prove that the function may be used for intersecting arrays, it has to generate two implementation variants — a vectorized and a \u0026ldquo;safe\u0026rdquo; one — and insert runtime checks to choose between the two. To avoid them, we can tell the compiler that we are that no memory is aliased by adding the __restrict__ keyword:\nvoid add(int * __restrict__ a, const int * __restrict__ b, int n) { for (int i = 0; i \u0026lt; n; i++) a[i] += b[i]; } The other way, specific to SIMD, is the \u0026ldquo;ignore vector dependencies\u0026rdquo; pragma. It is a general way to inform the compiler that there are no dependencies between the loop iterations:\n#pragma GCC ivdep for (int i = 0; i \u0026lt; n; i++) // ... Alignment. The compiler also doesn\u0026rsquo;t know anything about the alignment of these arrays and has to either process some elements at the beginning of these arrays before starting the vectorized section or potentially lose some performance by using unaligned memory accesses.\nTo help the compiler eliminate this corner case, we can use the alignas specifier on static arrays and the std::assume_aligned function to mark pointers aligned.\nChecking if vectorization happened. In either case, it is useful to check if the compiler vectorized the loop the way you intended. You can either compiling it to assembly and look for blocks for instructions that start with a \u0026ldquo;v\u0026rdquo; or add the -fopt-info-vec-optimized compiler flag so that the compiler indicates where auto-vectorization is happening and what SIMD width is being used. If you swap optimized for missed or all, you may also get some reasoning behind why it is not happening in other places.\nThere are many other ways of telling the compiler exactly what we mean, but in especially complex cases — e.g., when there are a lot of branches or function calls inside the loop — it is easier to go one level of abstraction down and vectorize manually.\n#SPMDThere is a neat compromise between auto-vectorization and the manual use of SIMD intrinsics: \u0026ldquo;single program, multiple data\u0026rdquo; (SPMD). This is a model of computation in which the programmer writes what appears to be a regular serial program, but that is actually executed in parallel on the hardware.\nThe programming experience is largely the same, and there is still the fundamental limitation in that the computation must be data-parallel, but SPMD ensures that the vectorization will happen regardless of the compiler and the target CPU architecture. It also allows for the computation to be automatically parallelized across multiple cores and, in some cases, even offloaded to other types of parallel hardware.\nThere is support for SPMD is some modern languages (Julia), multiprocessing APIs (OpenMP), and specialized compilers (Intel ISPC), but it has seen the most success in the context of GPU programming where both problems and hardware are massively parallel.\nWe will cover this model of computation in much more depth in Part 2\n","id":71,"path":"/hugo-page/hpc/simd/auto-vectorization/","title":"Auto-Vectorization and SPMD"},{"content":"It is not an uncommon for there to be two library algorithm implementations, each maintaining its own benchmarking code, and each claiming to be faster than the other. This confuses everyone involved, especially the users, who have to somehow choose between the two.\nSituations like these are usually not caused by fraudulent actions by their authors; they just have different definitions of what \u0026ldquo;faster\u0026rdquo; means, and indeed, defining and using just one performance metric is often very problematic.\n#Measuring the Right ThingThere are many things that can introduce bias into benchmarks.\nDiffering datasets. There are many algorithms whose performance somehow depends on the dataset distribution. In order to define, for example, what the fastest sorting, shortest path, or binary search algorithms are, you have to fix the dataset on which the algorithm is run.\nThis sometimes applies even to algorithms that process a single piece of input. For example, it is not a good idea to feed GCD implementations sequential numbers because it makes branches very predictable:\n// don\u0026#39;t do this int checksum = 0; for (int a = 0; a \u0026lt; 1000; a++) for (int b = 0; b \u0026lt; 1000; b++) checksum ^= gcd(a, b); However, if we sample these same numbers randomly, branch prediction becomes much harder, and the benchmark takes longer time, despite processing the same input, but in altered order:\nint a[1000], b[1000]; for (int i = 0; i \u0026lt; 1000; i++) a[i] = rand() % 1000, b[i] = rand() % 1000; int checksum = 0; for (int t = 0; t \u0026lt; 1000; t++) for (int i = 0; i \u0026lt; 1000; i++) checksum += gcd(a[i], b[i]); Although the most logical choices for most cases is to just sample data uniformly at random, many real-world applications have distributions that are far from uniform, so you can\u0026rsquo;t pick just one. In general, a good benchmark should be application-specific, and use the dataset that is as representing of your real use case as possible.\nMultiple objectives. Some algorithm design problems have more than one key objective. For example, hash tables, in addition to being highly dependant on the distribution of keys, also need to carefully balance:\nmemory usage, latency of add query, latency of positive membership query, latency of negative membership query. The only way to choose between hash table implementations is to try and put multiple variants into the application.\nLatency vs Throughput. Another aspect that people often overlook is that the execution time can be defined in more than one way, even for a single query.\nWhen you write code like this:\nfor (int i = 0; i \u0026lt; N; i++) q[i] = rand(); int checksum = 0; for (int i = 0; i \u0026lt; N; i++) checksum ^= lower_bound(q[i]); and then time the whole thing and divide it by the number of iterations, you are actually measuring the throughput of the query — how many operations it can process per unit of time. This is usually less than the time it actually takes to process one operation separately because of interleaving.\nTo measure actual latency, you need to introduce a dependency between the invocations:\nfor (int i = 0; i \u0026lt; N; i++) checksum ^= lower_bound(checksum ^ q[i]); It usually makes the most difference in algorithms with possible pipeline stall issues, e.g., when comparing branchy and branch-free algorithms.\nCold cache. Another source of bias is the cold cache effect, when memory reads initially take longer time because the required data is not in cache yet.\nThis is solved by making a warm-up run before starting measurements:\n// warm-up run volatile checksum = 0; for (int i = 0; i \u0026lt; N; i++) checksum ^= lower_bound(q[i]); // actual run clock_t start = clock(); checksum = 0; for (int i = 0; i \u0026lt; N; i++) checksum ^= lower_bound(q[i]); It is also sometimes convenient to combine the warm-up run with answer validation, if it is more complicated than just computing some sort of checksum.\nOver-optimization. Sometimes the benchmark is outright erroneous because the compiler just optimized the benchmarked code away. To prevent the compiler from cutting corners, you need to add checksums and either print them somewhere or add the volatile qualifier, which also prevents any sort of interleaving of loop iterations.\nFor algorithms that only write data, you can use the __sync_synchronize() intrinsic to add a memory fence and prevent the compiler from accumulating updates.\n#Reducing Noise The issues we\u0026rsquo;ve described produce bias in measurements: they consistently give advantage to one algorithm over the other. There are other types of possible problems with benchmarking that result in either unpredictable skews or just completely random noise, thus increasing variance.\nThese types of issues are caused by side effects and some sort of external noise, mostly due to noisy neighbors and CPU frequency scaling:\nIf you benchmark a compute-bound algorithm, measure its performance in cycles using perf stat: this way it will be independent of clock frequency, fluctuations of which is usually the main source of noise. Otherwise, set core frequency to what you expect it to be and make sure nothing interferes with it. On Linux you can do it with cpupower (e.g., sudo cpupower frequency-set -g powersave to put it to minimum or sudo cpupower frequency-set -g ondemand to enable turbo boost). I use a convenient GNOME shell extension that has a separate button to do it. If applicable, turn hyper-threading off and attach jobs to specific cores. Make sure no other jobs are running on the system, turn off networking and try not to fiddle with the mouse. You can\u0026rsquo;t remove noises and biases completely. Even a program\u0026rsquo;s name can affect its speed: the executable\u0026rsquo;s name ends up in an environment variable, environment variables end up on the call stack, and so the length of the name affects stack alignment, which can result in data accesses slowing down due to crossing cache line or memory page boundaries.\nIt is important to account for the noise when guiding optimizations and especially when reporting results to someone else. Unless you are expecting a 2x kind of improvement, treat all microbenchmarks the same way as A/B testing.\nWhen you run a program on a laptop for under a second, a ±5% fluctuation in performance is completely normal. So, if you want to decide whether to revert or keep a potential +1% improvement, run it until you reach statistical significance, which you can determine by calculating variances and p-values.\n#Further ReadingInterested readers can explore this comprehensive list of experimental computer science resources by Dror Feitelson, perhaps starting with \u0026ldquo;Producing Wrong Data Without Doing Anything Obviously Wrong\u0026rdquo; by Todd Mytkowicz et al.\nYou can also watch this great talk by Emery Berger on how to do statistically sound performance evaluation.\n","id":72,"path":"/hugo-page/hpc/profiling/noise/","title":"Getting Accurate Results"},{"content":"Computer engineers like to mentally split the pipeline of a CPU into two parts: the front-end, where instructions are fetched from memory and decoded, and the back-end, where they are scheduled and finally executed. Typically, the performance is bottlenecked by the execution stage, and for this reason, most of our efforts in this book are going to be spent towards optimizing around the back-end.\nBut sometimes the reverse can happen when the front-end doesn\u0026rsquo;t feed instructions to the back-end fast enough to saturate it. This can happen for many reasons, all ultimately having something to do with how the machine code is laid out in memory, and affect performance in anecdotal ways, such as removing unused code, swapping \u0026ldquo;if\u0026rdquo; branches, or even changing the order of function declarations causing performance to either improve or deteriorate.\n#CPU Front-EndBefore the machine code gets transformed into instructions, and the CPU understands what the programmer wants, it first needs to go through two important stages that we are interested in: fetch and decode.\nDuring the fetch stage, the CPU simply loads a fixed-size chunk of bytes from the main memory, which contains the binary encodings of some number of instructions. This block size is typically 32 bytes on x86, although it may vary on different machines. An important nuance is that this block has to be aligned: the address of the chunk must be multiple of its size (32B, in our case).\nNext comes the decode stage: the CPU looks at this chunk of bytes, discards everything that comes before the instruction pointer, and splits the rest of them into instructions. Machine instructions are encoded using a variable number of bytes: something simple and very common like inc rax takes one byte, while some obscure instruction with encoded constants and behavior-modifying prefixes may take up to 15. So, from a 32-byte block, a variable number of instructions may be decoded, but no more than a certain machine-dependent limit called the decode width. On my CPU (a Zen 2), the decode width is 4, which means that on each cycle, up to 4 instructions can be decoded and passed to the next stage.\nThe stages work in a pipelined fashion: if the CPU can tell (or predict) which instruction block it needs next, then the fetch stage doesn\u0026rsquo;t wait for the last instruction in the current block to be decoded and loads the next one right away.\n#Code AlignmentOther things being equal, compilers typically prefer instructions with shorter machine code, because this way more instructions can fit in a single 32B fetch block, and also because it reduces the size of the binary. But sometimes the reverse is prefereable, due to the fact that the fetched instructions\u0026rsquo; blocks must be aligned.\nImagine that you need to execute an instruction sequence that starts on the last byte of a 32B-aligned block. You may be able to execute the first instruction without additional delay, but for the subsequent ones, you have to wait for one additional cycle to do another instruction fetch. If the code block was aligned on a 32B boundary, then up to 4 instructions could be decoded and then executed concurrently (unless they are extra long or interdependent).\nHaving this in mind, compilers often do a seemingly harmful optimization: they sometimes prefer instructions with longer machine codes, and even insert dummy instructions that do nothing1 in order to get key jump locations aligned on a suitable power-of-two boundary.\nIn GCC, you can use -falign-labels=n flag to specify a particular alignment policy, replacing -labels with -function, -loops, or -jumps if you want to be more selective. On -O2 and -O3 levels of optimization, it is enabled by default — without setting a particular alignment, in which case it uses a (usually reasonable) machine-dependent default value.\n#Instruction CacheThe instructions are stored and fetched using largely the same memory system as for the data, except maybe the lower layers of cache are replaced with a separate instruction cache (because you wouldn\u0026rsquo;t want a random data read to kick out the code that processes it).\nThe instruction cache is crucial in situations when you either:\ndon\u0026rsquo;t know what instructions you are going to execute next, and need to fetch the next block with low latency, or are executing a long sequence of verbose-but-quick-to-process instructions, and need high bandwidth. The memory system can therefore become the bottleneck for programs with large machine code. This consideration limits the applicability of the optimization techniques we\u0026rsquo;ve previously discussed:\nInlining functions is not always optimal, because it reduces code sharing and increases the binary size, requiring more instruction cache. Unrolling loops is only beneficial up to some extent, even if the number of iterations is known during compile time: at some point, the CPU would have to fetch both instructions and data from the main memory, in which case it will likely be bottlenecked by the memory bandwidth. Huge code alignments increase the binary size, again requiring more instruction cache. Spending one more cycle on fetch is a minor penalty compared to missing the cache and waiting for the instructions to be fetched from the main memory. Another aspect is that placing frequently used instruction sequences on the same cache lines and memory pages improves cache locality. To improve instruction cache utilization, you should group hot code with hot code and cold code with cold code, and remove dead (unused) code if possible. If you want to explore this idea further, check out Facebook\u0026rsquo;s Binary Optimization and Layout Tool, which was recently merged into LLVM.\n#Unequal BranchesSuppose that for some reason you need a helper function that calculates the length of an integer interval. It takes two arguments, $x$ and $y$, but for convenience, it may correspond to either $[x, y]$ or $[y, x]$, depending on which one is non-empty. In plain C, you would probably write something like this:\nint length(int x, int y) { if (x \u0026gt; y) return x - y; else return y - x; } In x86 assembly, there is a lot more variability to how you can implement it, noticeably impacting performance. Let\u0026rsquo;s start with trying to map this code directly into assembly:\nlength: cmp edi, esi jle less ; x \u0026gt; y sub edi, esi mov eax, edi done: ret less: ; x \u0026lt;= y sub esi, edi mov eax, esi jmp done While the initial C code seems very symmetrical, the assembly version isn\u0026rsquo;t. This results in an interesting quirk that one branch can be executed slightly faster than the other: if x \u0026gt; y, then the CPU can just execute the 5 instructions between cmp and ret, which, if the function is aligned, are all going to be fetched in one go; while in case of x \u0026lt;= y, two more jumps are required.\nIt may be reasonable to assume that the x \u0026gt; y case is unlikely (why would anyone calculate the length of an inverted interval?), more like an exception that mostly never happens. We can detect this case, and simply swap x and y:\nint length(int x, int y) { if (x \u0026gt; y) swap(x, y); return y - x; } The assembly would go like this, as it typically does for the if-without-else patterns:\nlength: cmp edi, esi jle normal ; if x \u0026lt;= y, no swap is needed, and we can skip the xchg xchg edi, esi normal: sub esi, edi mov eax, esi ret The total instruction length is 6 now, down from 8. But it is still not quite optimized for our assumed case: if we think that x \u0026gt; y never happens, then we are wasteful when loading the xchg edi, esi instruction that is never going to be executed. We can solve this by moving it outside the normal execution path:\nlength: cmp edi, esi jg swap normal: sub esi, edi mov eax, esi ret swap: xchg edi, esi jmp normal This technique is quite handy when handling exceptions cases in general, and in high-level code, you can give the compiler a hint that a certain branch is more likely than the other:\nint length(int x, int y) { if (x \u0026gt; y) [[unlikely]] swap(x, y); return y - x; } This optimization is only beneficial when you know that a branch is very rarely taken. When this is not the case, there are other aspects more important than the code layout, that compel compilers to avoid any branching at all — in this case by replacing it with a special \u0026ldquo;conditional move\u0026rdquo; instruction, roughly corresponding to the ternary expression (x \u0026gt; y ? y - x : x - y) or calling abs(x - y):\nlength: mov edx, edi mov eax, esi sub edx, esi sub eax, edi cmp edi, esi cmovg eax, edx ; \u0026#34;mov if edi \u0026gt; esi\u0026#34; ret Eliminating branches is an important topic, and we will spend much of the next chapter discussing it in more detail.\nSuch instructions are called no-op, or NOP instructions. On x86, the \u0026ldquo;official way\u0026rdquo; of doing nothing is xchg rax, rax (swap a register with itself): the CPU recognizes it and doesn\u0026rsquo;t spend extra cycles executing it, except for the decode stage. The nop shorthand maps to the same machine code.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":73,"path":"/hugo-page/hpc/architecture/layout/","title":"Machine Code Layout"},{"content":"In the pointer chasing benchmark, for simplicity, we didn\u0026rsquo;t use actual pointers, but integer indices relative to a base address:\nfor (int i = 0; i \u0026lt; N; i++) k = q[k]; The memory addressing operator on x86 is fused with the address computation, so the k = q[k] line folds into just a single terse instruction that also does multiplication by 4 and addition under the hood:\nmov rax, DWORD PTR q[0+rax*4] Although fully fused, these additional computations add some delay to memory operations. The latency of an L1 fetch is either 4 or 5 cycles — the latter being the case if we need to perform a complex computation of the address. For this reason, the permutation benchmark measures 3ns or 6 cycles per jump: 4+1 for the read and address computation and another one to move the result to the right register.\n#PointersWe can make our benchmark run slightly faster if we replace \u0026ldquo;fake pointers\u0026rdquo; — indices — with actual pointers.\nThere are some syntactical issues in getting \u0026ldquo;pointer to pointer to pointer…\u0026rdquo; constructions to work, so instead we will define a struct that just wraps a pointers to its own type — this is how most pointer chasing works anyway:\nstruct node { node* ptr; }; Now we randomly fill our array with pointers and chase them instead:\nnode* k = q + p[N - 1]; for (int i = 0; i \u0026lt; N; i++) k = k-\u0026gt;ptr = q + p[i]; for (int i = 0; i \u0026lt; N; i++) k = k-\u0026gt;ptr; This code now runs in 2ns / 4 cycles for arrays that fit in the L1 cache. Why not 4+1=5? Because Zen 2 has an interesting feature that allows zero-latency reuse of data accessed just by address, so the \u0026ldquo;move\u0026rdquo; here is transparent, resulting in whole two cycles saved.\nUnfortunately, there is a problem with it on 64-bit systems as the pointers become twice as large, making the array spill out of cache much sooner compared to using a 32-bit index. The latency-versus-size graph looks like if it was shifted by one power of two to the left — exactly like it should:\nThis problem is mitigated by switching to the 32-bit mode:\nYou need to go through some trouble getting 32-bit libs to get this running on a computer made in this century, but this shouldn\u0026rsquo;t pose other problems unless you need to interoperate with 64-bit software or access more than 4G of RAM\n#Bit FieldsThe fact that on larger problem sizes the performance is bottlenecked by memory rather than CPU lets us try something even more strange: we can use less than 4 bytes for storing indices. This can be done with bit fields:\nstruct __attribute__ ((packed)) node { int idx : 24; }; You don\u0026rsquo;t need to do anything else other than defining a structure for the bit field — the compiler handles the 3-byte integer all by itself:\nint k = p[N - 1]; for (int i = 0; i \u0026lt; N; i++) { k = q[k].idx = p[i]; for (int i = 0; i \u0026lt; N; i++) { k = q[k].idx; This code measures at 6.5ns for the L1 cache. There is some room for improvement as the default conversion procedure chosen by the compiler is suboptimal. We could manually load a 4-byte integer and truncate it ourselves (we also need to add one more element to the q array to ensure we own that extra one byte of memory):\nk = *((int*) (q + k)); k \u0026amp;= ((1\u0026lt;\u0026lt;24) - 1); It now runs in 4ns, and produces the following graph:\nIf you zoom close enough (the graph is an svg), you\u0026rsquo;ll see that the pointers win on very small arrays, then starting from around the L2-L3 cache boundary our custom bit fields take over, and for very large arrays it doesn\u0026rsquo;t matter because we never hit cache anyway.\nThis isn\u0026rsquo;t a kind of optimization that can give you a 5x improvement, but it\u0026rsquo;s still something to try when all the other resources are exhausted.\n","id":74,"path":"/hugo-page/hpc/cpu-cache/pointers/","title":"Pointer Alternatives"},{"content":"Consider the following little program, in which we calculate the sum of an integer array:\nconst int n = 1e5; int a[n], s = 0; int main() { for (int t = 0; t \u0026lt; 100000; t++) for (int i = 0; i \u0026lt; n; i++) s += a[i]; return 0; } If we compile it with plain g++ -O3 and run, it finishes in 2.43 seconds.\nNow, let\u0026rsquo;s add the following magic directive in the very beginning:\n#pragma GCC target(\u0026#34;avx2\u0026#34;) // ...the rest is the same as before When compiled and run in the same environment, it finishes in 1.24 seconds. This is almost twice as fast, and we didn\u0026rsquo;t change a single line of code or the optimization level.\nWhat happened here is we provided a little bit of info about the computer on which this code is supposed to be run. Specifically, we told the compiler that the target CPU supports an extension to the x86 instruction set called \u0026ldquo;AVX2.\u0026rdquo; AVX2 is one of the many so-called \u0026ldquo;SIMD extensions\u0026rdquo; for x86. These extensions include instructions that operate on special registers capable of holding 128, 256, or even 512 bits of data using the \u0026ldquo;single instruction, multiple data\u0026rdquo; (SIMD) approach. Instead of working with a single scalar value, SIMD instructions divide the data in registers into blocks of 8, 16, 32, or 64 bits and perform the same operation on them in parallel, yielding a proportional increase in performance1.\nThese extensions are relatively new, and their support in CPUs has been implemented gradually while maintaining backward compatibility2. Apart from adding more specialized instructions, the most important difference between them is the introduction of progressively wider registers.\nIn particular, AVX2 has instructions for working with 256-bit registers, while by default, GCC assumes that nothing past the 128-bit SSE2 is enabled. Hence, after telling the optimizer that it can use instructions that add 8 integers at once instead of 4, the performance was increased twofold.\nCompilers often do a good job rewriting simple loops with SIMD instructions, like in the case above. This optimization is called auto-vectorization, and it is the most popular way of using SIMD.\nThe problem is that it only works with certain types of loops, and even then it often yields suboptimal results. To understand its limitations, we need to get our hands dirty and explore this technology on a lower level, which is what we are going to do in this chapter.\nOn some CPUs, especially heavy SIMD instructions consume more energy and thus require downclocking to balance off the total power consumption, so the real-time speedup is not always proportional.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStarting with AVX512, backward compatibility is no longer maintained: there are many different \u0026ldquo;flavors\u0026rdquo; tailored to specific needs such as data compression, encryption, or machine learning.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":75,"path":"/hugo-page/hpc/simd/","title":"SIMD Parallelism"},{"content":"When you try to explain a complex concept, it is generally a good idea to give a very simple and minimal example illustrating it. This is why in this book you see about a dozen different ways of calculating the sum of an array, each highlighting a certain CPU feature.\nBut the main purpose of this book is not to learn computer architecture just for the sake of learning it, but to acquire real-world skills in software optimization. The next two chapters exist to help you achieve this goal, as they contain detailed case studies of various algorithms that are much harder to optimize than the sum of an array.\n","id":76,"path":"/hugo-page/hpc/algorithms/","title":"Algorithms Case Studies"},{"content":"Consider a strided incrementing loop over an array of size $N=2^{21}$ with a fixed step size of 256:\nfor (int i = 0; i \u0026lt; N; i += 256) a[i]++; And then this one, with the step size of 257:\nfor (int i = 0; i \u0026lt; N; i += 257) a[i]++; Which one will be faster to finish? There are several considerations that come to mind:\nAt first, you think that there shouldn\u0026rsquo;t be much difference, or maybe that the second loop is $\\frac{257}{256}$ times faster or so because it does fewer iterations in total. Then you recall that 256 is a nice round number, which may have something to do with SIMD or the memory system, so maybe the first one is faster. But the right answer is very counterintuitive: the second loop is faster — and by a factor of 10.\nThis isn\u0026rsquo;t just a single bad step size. The performance degrades for all indices that are multiples of large powers of two:\nThe array size is normalized so that the total number of iterations is constant There is no vectorization or anything, and the two loops produce the same assembly except for the step size. This effect is due only to the memory system, in particular to a feature called cache associativity, which is a peculiar artifact of how CPU caches are implemented in hardware.\n#Hardware CachesWhen we were studying the memory system theoretically, we discussed different ways one can implement cache eviction policies in software. One particular strategy we focused on was the least recently used (LRU) policy, which is simple and effective but still requires some non-trivial data manipulation.\nIn the context of hardware, such scheme is called fully associative cache: we have $M$ cells, each capable of holding a cache line corresponding to any of the $N$ total memory locations, and in case of contention, the one not accessed the longest gets kicked out and replaced with the new one.\nFully associative cache The problem with fully associative cache is that implementing the \u0026ldquo;find the oldest cache line among millions\u0026rdquo; operation is pretty hard to do in software and just unfeasible in hardware. You can make a fully associative cache that has 16 entries or so, but managing hundreds of cache lines already becomes either prohibitively expensive or so slow that it\u0026rsquo;s not worth it.\nWe can resort to another, much simpler approach: just map each block of 64 bytes in RAM to a single cache line which it can occupy. Say, if we have 4096 blocks in memory and 64 cache lines for them, then each cache line at any time stores the contents of one of $\\frac{4096}{64} = 64$ different blocks.\nDirect-mapped cache A direct-mapped cache is easy to implement doesn\u0026rsquo;t require storing any additional meta-information associated with a cache line except its tag (the actual memory location of a cached block). The disadvantage is that the entries can be kicked out too quickly — for example, when bouncing between two addresses that map to the same cache line — leading to lower overall cache utilization.\nFor that reason, we settle for something in-between direct-mapped and fully associative caches: the set-associative cache. It splits the address space into equal groups, which separately act as small fully-associative caches.\nSet-associative cache (2-way associative) Associativity is the size of these sets, or, in other words, how many different cache lines each data block can be mapped to. Higher associativity allows for more efficient utilization of cache but also increases the cost.\nFor example, on my CPU, the L3 cache is 16-way set-associative, and there are 4MB available to a single core. This means that there are in total $\\frac{2^{22}}{2^{6}} = 2^{16}$ cache lines, which are split into $\\frac{2^{16}}{16} = 2^{12}$ groups, each acting as a fully associative cache of their own $(\\frac{1}{2^{12}})$-th fraction of the RAM.\nMost other CPU caches are also set-associative, including the non-data ones such as the instruction cache and the TLB. The exceptions are small specialized caches that only house 64 or fewer entries — these are usually fully associative.\n#Address TranslationThere is only one ambiguity remaining: how exactly the cache line mapping is done.\nIf we implemented set-associative cache in software, we would compute some hash function of the memory block address and then use its value as the cache line index. In hardware, we can\u0026rsquo;t really do that because it is too slow: for example, for the L1 cache, the latency requirement is 4 or 5 cycles, and even taking a modulo takes around 10-15 cycles, let alone something more sophisticated.\nInstead, the hardware uses the lazy approach. It takes the memory address that needs to be accessed and splits it into three parts — from lower bits to higher:\noffset — the index of the word within a 64B cache line ($\\log_2 64 = 6$ bits); index — the index of the cache line set (the next $12$ bits as there are $2^{12}$ cache lines in the L3 cache); tag — the rest of the memory address, which is used to tell the memory blocks stored in the cache lines apart. In other words, all memory addresses with the same \u0026ldquo;middle\u0026rdquo; part map to the same set.\nAddress composition for a 64-entry 2-way set-associative cache This makes the cache system simpler and cheaper to implement but also susceptible to certain bad access patterns.\n#Pathological MappingsNow, where were we? Oh, yes: the reason why iteration with strides of 256 causes such a terrible slowdown.\nWhen we jump over 256 integers, the pointer always increments by $1024 = 2^{10}$, and the last 10 bits remain the same. Since the cache system uses the lower 6 bits for the offset and the next 12 for the cache line index, we are essentially using just $2^{12 - (10 - 6)} = 2^8$ different sets in the L3 cache instead of $2^{12}$, which has the effect of shrinking our L3 cache by a factor of $2^4 = 16$. The array stops fitting into the L3 cache ($N=2^{21}$) and spills into the order-of-magnitude slower RAM, which causes the performance to decrease.\nPerformance issues caused by cache associativity effects arise with remarkable frequency in algorithms because, for multiple reasons, programmers just love using powers of two when indexing arrays:\nIt is easier to calculate the address for multi-dimensional array accesses if the last dimension is a power of two, as it only requires a binary shift instead of a multiplication. It is easier to calculate modulo a power of two, as it can be done with a single bitwise and. It is convenient and often even necessary to use power-of-two problem sizes in divide-and-conquer algorithms. It is the smallest integer exponent, so using the sequence of increasing powers of two as problem sizes are a popular choice when benchmarking memory-bound algorithms. Also, more natural powers of ten are by transitivity divisible by a slightly lower power of two. This especially often applies to implicit data structures that use a fixed memory layout. For example, binary searching over arrays of size $2^{20}$ takes about ~360ns per query while searching over arrays of size $(2^{20} + 123)$ takes ~300ns. When the array size is a multiple of a large power of two, then the indices of the \u0026ldquo;hottest\u0026rdquo; elements, the ones we likely request on the first dozen or so iterations, will also be divisible by some large powers of two and map to the same cache line — kicking each other out and causing a ~20% performance decrease.\nLuckily, such issues are more of an anomaly rather than serious problems. The solution is usually simple: avoid iterating in powers of two, make the last dimensions of multi-dimensional arrays a slightly different size or use any other method to insert \u0026ldquo;holes\u0026rdquo; in the memory layout, or create some seemingly random bijection between the array indices and the locations where the data is actually stored.\n","id":77,"path":"/hugo-page/hpc/cpu-cache/associativity/","title":"Cache Associativity"},{"content":"Optimizing data structures is different from optimizing algorithms as data structure problems have more dimensions: you may be optimizing for throughput, for latency, for memory usage, or any combination of those — and this complexity blows up exponentially when you need to process multiple query types and consider multiple query distributions.\nThis makes simply defining benchmarks much harder, let alone the actual implementations. In this chapter, we will try to navigate all this complexity and learn how to design efficient data structures with extensive case studies.\nA brief review of the CPU cache system is strongly advised.\n","id":78,"path":"/hugo-page/hpc/data-structures/","title":"Data Structures Case Studies"},{"content":"Consider yet again the strided incrementing loop:\nconst int N = (1 \u0026lt;\u0026lt; 13); int a[D * N]; for (int i = 0; i \u0026lt; D * N; i += D) a[i] += 1; We change the stride $D$ and increase the array size proportionally so that the total number of iterations $N$ remains constant. As the total number of memory accesses also remains constant, for all $D \\geq 16$, we should be fetching exactly $N$ cache lines — or $64 \\cdot N = 2^6 \\cdot 2^{13} = 2^{19}$ bytes, to be exact. This precisely fits into the L2 cache, regardless of the step size, and the throughput graph should look flat.\nThis time, we consider a larger range of $D$ values, up to 1024. Starting from around 256, the graph is definitely not flat:\nThis anomaly is also due to the cache system, although the standard L1-L3 data caches have nothing to do with it. Virtual memory is at fault, in particular the translation lookaside buffer (TLB), which is a cache responsible for retrieving the physical addresses of the virtual memory pages.\nOn my CPU, there are two levels of TLB:\nThe L1 TLB has 64 entries, and if the page size is 4K, then it can handle $64 \\times 4K = 512K$ of active memory without going to the L2 TLB. The L2 TLB has 2048 entries, and it can handle $2048 \\times 4K = 8M$ of memory without going to the page table. How much memory is allocated when $D$ becomes equal to 256? You\u0026rsquo;ve guessed it: $8K \\times 256 \\times 4B = 8M$, exactly the limit of what the L2 TLB can handle. When $D$ gets larger than that, some requests start getting redirected to the main page table, which has a large latency and very limited throughput, which bottlenecks the whole computation.\n#Changing Page SizeThat 8MB of slowdown-free memory seems like a very tight restriction. While we can\u0026rsquo;t change the characteristics of the hardware to lift it, we can increase the page size, which would in turn reduce the pressure on the TLB capacity.\nModern operating systems allow us to set the page size both globally and for individual allocations. CPUs only support a defined set of page sizes — mine, for example, can use either 4K or 2M pages. Another typical page size is 1G — it is usually only relevant for server-grade hardware with hundreds of gigabytes of RAM. Anything over the default 4K is called huge pages on Linux and large pages on Windows.\nOn Linux, there is a special system file that governs the allocation of huge pages. Here is how to make the kernel give you huge pages on every allocation:\n$ echo always \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled Enabling huge pages globally like this isn\u0026rsquo;t always a good idea because it decreases memory granularity and raises the minimum memory that a process consumes — and some environments have more processes than free megabytes of memory. So, in addition to always and never, there is a third option in that file:\n$ cat /sys/kernel/mm/transparent_hugepage/enabled always [madvise] never madvise is a special system call that lets the program advise the kernel on whether to use huge pages or not, which can be used for allocating huge pages on-demand. If it is enabled, you can use it in C++ like this:\n#include \u0026lt;sys/mman.h\u0026gt; void *ptr = std::aligned_alloc(page_size, array_size); madvise(ptr, array_size, MADV_HUGEPAGE); You can only request a memory region to be allocated using huge pages if it has the corresponding alignment.\nWindows has similar functionality. Its memory API combines these two functions into one:\n#include \u0026#34;memoryapi.h\u0026#34; void *ptr = VirtualAlloc(NULL, array_size, MEM_RESERVE | MEM_COMMIT | MEM_LARGE_PAGES, PAGE_READWRITE); In both cases, array_size should be a multiple of page_size.\n#Impact of Huge PagesBoth variants of allocating huge pages immediately flatten the curve:\nEnabling huge pages also improves latency by up to 10-15% for arrays that don\u0026rsquo;t fit into the L2 cache:\nIn general, enabling huge pages is a good idea when you have any sort of sparse reads, as they usually slightly improve and (almost) never hurt performance.\nThat said, you shouldn\u0026rsquo;t rely on huge pages if possible, as they aren\u0026rsquo;t always available due to either hardware or computing environment restrictions. There are many other reasons why grouping data accesses spatially may be beneficial, which automatically solves the paging problem.\n","id":79,"path":"/hugo-page/hpc/cpu-cache/paging/","title":"Memory Paging"},{"content":"It is often beneficial to group together the data you need to fetch at the same time: preferably, on the same or, if that isn\u0026rsquo;t possible, neighboring cache lines. This improves the spatial locality of your memory accesses, positively impacting the performance of memory-bound algorithms.\nTo demonstrate the potential effect of doing this, we modify the pointer chasing benchmark so that the next pointer is computed using not one, but a variable number of fields ($D$).\n#ExperimentThe first approach will locate these fields together as the rows of a two-dimensional array. We will refer to this variant as array of structures (AoS):\nconst int M = N / D; // # of memory accesses int p[M], q[M][D]; iota(p, p + M, 0); random_shuffle(p, p + M); int k = p[M - 1]; for (int i = 0; i \u0026lt; M; i++) q[k][0] = p[i]; for (int j = 1; j \u0026lt; D; j++) q[i][0] ^= (q[j][i] = rand()); k = q[k][0]; } for (int i = 0; i \u0026lt; M; i++) { int x = 0; for (int j = 0; j \u0026lt; D; j++) x ^= q[k][j]; k = x; } And in the second approach, we will place them separately. The laziest way to do this is to transpose the two-dimensional array q and swap the indices in all its subsequent accesses:\nint q[D][M]; // ^--^ By analogy, we call this variant structure of arrays (SoA). Obviously, for large $D$\u0026rsquo;s, it performs much worse:\nThe performance of both variants grows linearly with $D$, but AoS needs to fetch up to 16 times fewer total cache lines as the data is stored sequentially. Even when $D=64$, the additional time it takes to process the other 63 values is less than the latency of the first fetch.\nYou can also see the spikes at the powers of two. AoS performs slightly better because it can compute horizontal xor-sum faster with SIMD. In contrast, SoA performs much worse, but this isn\u0026rsquo;t about $D$, but about $\\lfloor N / D \\rfloor$, the size of the second dimension, being a large power of two: this causes a pretty complicated cache associativity effect.\n#Temporary Storage ContentionAt first, it seems like there shouldn\u0026rsquo;t be any cache issues as $N=2^{23}$ and the array is just too big to fit into the L3 cache in the first place. The nuance is that to process a number of elements from different memory locations in parallel, you still need some space to store them temporarily. You can\u0026rsquo;t simply use registers as there aren\u0026rsquo;t enough of them, so they need to be stored in the cache even though in just a microsecond you won\u0026rsquo;t be needing them.\nTherefore, when N / D is a large power of two, and we are iterating over the array q[D][N / D] along the first index, some of the memory addresses we temporarily need will map to the same cache line — and as there isn\u0026rsquo;t enough space there, many of them will have to be re-fetched from the upper layers of the memory hierarchy.\nHere is another head-scratcher: if we enable huge pages, it expectedly makes the total latency 10-15% lower for most values of $D$, but for $D=64$, it makes things ten times worse:\nNote the logarithmic scale I doubt that even the engineers who design memory controllers can explain what\u0026rsquo;s happening right off the bat.\nIn short, the difference is because, unlike the L1/L2 caches that are private to each core, the L3 cache has to use physical memory addresses instead of virtual ones for synchronization between different cores sharing the cache.\nWhen we are using 4K memory pages, the virtual addresses get somewhat arbitrarily dispersed over the physical memory, which makes the cache associativity problem less severe: the physical addresses will have the same remainder modulo 4K bytes, and not N / D as for the virtual addresses. When we specifically require huge pages, this maximum alignment limit increases to 2M, and the cache lines receive much more contention.\nThis is the only example I know when enabling huge pages makes performance worse, let alone by a factor of ten.\n#Padded AoSAs long as we are fetching the same number of cache lines, it doesn\u0026rsquo;t matter where they are located, right? Let\u0026rsquo;s test it and switch to padded integers in the AoS code:\nstruct padded_int { int val; int padding[15]; }; const int M = N / D / 16; padded_int q[M][D]; Other than that, we are still calculating the xor-sum of $D$ padded integers. We fetch exactly $D$ cache lines, but this time sequentially. The running time shouldn\u0026rsquo;t be different from SoA, but this isn\u0026rsquo;t what happens:\nThe running time is about ⅓ lower for $D=63$, but this only applies to arrays that exceed the L3 cache. If we fix $D$ and change $N$, you can see that the padded version performs slightly worse on smaller arrays because there are less opportunities for random cache sharing:\nAs the performance on smaller arrays sizes is not affected, this clearly has something to do with how RAM works.\n#RAM-Specific TimingsFrom the performance analysis point of view, all data in RAM is physically stored in a two-dimensional array of tiny capacitor cells, which is split into rows and columns. To read or write any cell, you need to perform one, two, or three actions:\nRead the contents of a row in a row buffer, which temporarily discharges the capacitors. Read or write a specific cell in this buffer. Write the contents of a row buffer back into the capacitors so that the data is preserved and the row buffer can be used for other memory accesses. Here is the punchline: you don\u0026rsquo;t have to perform steps 1 and 3 between two memory accesses that correspond to the same row — you can just use the row buffer as a temporary cache. These three actions take roughly the same time, so this optimization makes long sequences of row-local accesses run thrice as fast compared to dispersed access patterns.\nThe size of the row differs depending on the hardware, but it is usually somewhere between 1024 and 8192 bytes. So even though the padded AoS benchmark places each element in a separate cache line, they are still very likely to be on the same RAM row, and the whole read sequence runs in roughly ⅓ of the time plus the latency of the first memory access.\n","id":80,"path":"/hugo-page/hpc/cpu-cache/aos-soa/","title":"AoS and SoA"},{"content":" In this case study, we will design and implement several algorithms for matrix multiplication.\nWe start with the naive \u0026ldquo;for-for-for\u0026rdquo; algorithm and incrementally improve it, eventually arriving at a version that is 50 times faster and matches the performance of BLAS libraries while being under 40 lines of C.\nAll implementations are compiled with GCC 13 and run on a Zen 2 CPU clocked at 2GHz.\n#BaselineThe result of multiplying an $l \\times n$ matrix $A$ by an $n \\times m$ matrix $B$ is defined as an $l \\times m$ matrix $C$ such that:\n$$ C_{ij} = \\sum_{k=1}^{n} A_{ik} \\cdot B_{kj} $$\nFor simplicity, we will only consider square matrices, where $l = m = n$.\nTo implement matrix multiplication, we can simply transfer this definition into code, but instead of two-dimensional arrays (aka matrices), we will be using one-dimensional arrays to be explicit about pointer arithmetic:\nvoid matmul(const float *a, const float *b, float *c, int n) { for (int i = 0; i \u0026lt; n; i++) for (int j = 0; j \u0026lt; n; j++) for (int k = 0; k \u0026lt; n; k++) c[i * n + j] += a[i * n + k] * b[k * n + j]; } For reasons that will become apparent later, we will only use matrix sizes that are multiples of $48$ for benchmarking, but the implementations remain correct for all others. We also use 32-bit floats specifically, although all implementations can be easily generalized to other data types and operations.\nCompiled with g++ -O3 -march=native -ffast-math -funroll-loops, the naive approach multiplies two matrices of size $n = 1920 = 48 \\times 40$ in ~16.7 seconds. To put it in perspective, this is approximately $\\frac{1920^3}{16.7 \\times 10^9} \\approx 0.42$ useful operations per nanosecond (GFLOPS), or roughly 5 CPU cycles per multiplication, which doesn\u0026rsquo;t look that good yet.\n#TranspositionIn general, when optimizing an algorithm that processes large quantities of data — and $1920^2 \\times 3 \\times 4 \\approx 42$ MB clearly is a large quantity as it can\u0026rsquo;t fit into any of the CPU caches — one should always start with memory before optimizing arithmetic, as it is much more likely to be the bottleneck.\nThe field $C_{ij}$ can be thought of as the dot product of row $i$ of matrix $A$ and column $j$ of matrix $B$. As we increment k in the inner loop above, we are reading the matrix a sequentially, but we are jumping over $n$ elements as we iterate over a column of b, which is not as fast as sequential iteration.\nOne well-known optimization that tackles this problem is to store matrix $B$ in column-major order — or, alternatively, to transpose it before the matrix multiplication. This requires $O(n^2)$ additional operations but ensures sequential reads in the innermost loop:\nvoid matmul(const float *a, const float *_b, float *c, int n) { float *b = new float[n * n]; for (int i = 0; i \u0026lt; n; i++) for (int j = 0; j \u0026lt; n; j++) b[i * n + j] = _b[j * n + i]; for (int i = 0; i \u0026lt; n; i++) for (int j = 0; j \u0026lt; n; j++) for (int k = 0; k \u0026lt; n; k++) c[i * n + j] += a[i * n + k] * b[j * n + k]; // \u0026lt;- note the indices } This code runs in ~12.4s, or about 30% faster.\nAs we will see in a bit, there are more important benefits to transposing it than just the sequential memory reads.\n#VectorizationNow that all we do is just sequentially read the elements of a and b, multiply them, and add the result to an accumulator variable, we can use SIMD instructions to speed it all up. It is pretty straightforward to implement using GCC vector types — we can memory-align matrix rows, pad them with zeros, and then compute the multiply-sum as we would normally compute any other reduction:\n// a vector of 256 / 32 = 8 floats typedef float vec __attribute__ (( vector_size(32) )); // a helper function that allocates n vectors and initializes them with zeros vec* alloc(int n) { vec* ptr = (vec*) std::aligned_alloc(32, 32 * n); memset(ptr, 0, 32 * n); return ptr; } void matmul(const float *_a, const float *_b, float *c, int n) { int nB = (n + 7) / 8; // number of 8-element vectors in a row (rounded up) vec *a = alloc(n * nB); vec *b = alloc(n * nB); // move both matrices to the aligned region for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { a[i * nB + j / 8][j % 8] = _a[i * n + j]; b[i * nB + j / 8][j % 8] = _b[j * n + i]; // \u0026lt;- b is still transposed } } for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { vec s{}; // initialize the accumulator with zeros // vertical summation for (int k = 0; k \u0026lt; nB; k++) s += a[i * nB + k] * b[j * nB + k]; // horizontal summation for (int k = 0; k \u0026lt; 8; k++) c[i * n + j] += s[k]; } } std::free(a); std::free(b); } The performance for $n = 1920$ is now around 2.3 GFLOPS — or another ~4 times higher compared to the transposed but not vectorized version.\nThis optimization looks neither too complex nor specific to matrix multiplication. Why can\u0026rsquo;t the compiler auto-vectorizee the inner loop by itself?\nIt actually can; the only thing preventing that is the possibility that c overlaps with either a or b. To rule it out, you can communicate to the compiler that you guarantee c is not aliased with anything by adding the __restrict__ keyword to it:\nvoid matmul(const float *a, const float *_b, float * __restrict__ c, int n) { // ... } Both manually and auto-vectorized implementations perform roughly the same.\n#Memory efficiencyWhat is interesting is that the implementation efficiency depends on the problem size.\nAt first, the performance (defined as the number of useful operations per second) increases as the overhead of the loop management and the horizontal reduction decreases. Then, at around $n=256$, it starts smoothly decreasing as the matrices stop fitting into the cache ($2 \\times 256^2 \\times 4 = 512$ KB is the size of the L2 cache), and the performance becomes bottlenecked by the memory bandwidth.\nIt is also interesting that the naive implementation is mostly on par with the non-vectorized transposed version — and even slightly better because it doesn\u0026rsquo;t need to perform a transposition.\nOne might think that there would be some general performance gain from doing sequential reads since we are fetching fewer cache lines, but this is not the case: fetching the first column of b indeed takes more time, but the next 15 column reads will be in the same cache lines as the first one, so they will be cached anyway — unless the matrix is so large that it can\u0026rsquo;t even fit n * cache_line_size bytes into the cache, which is not the case for any practical matrix sizes.\nInstead, the performance deteriorates on only a few specific matrix sizes due to the effects of cache associativity: when $n$ is a multiple of a large power of two, we are fetching the addresses of b that all likely map to the same cache line, which reduces the effective cache size. This explains the 30% performance dip for $n = 1920 = 2^7 \\times 3 \\times 5$, and you can see an even more noticeable one for $1536 = 2^9 \\times 3$: it is roughly 3 times slower than for $n=1535$.\nSo, counterintuitively, transposing the matrix doesn\u0026rsquo;t help with caching — and in the naive scalar implementation, we are not really bottlenecked by the memory bandwidth anyway. But our vectorized implementation certainly is, so let\u0026rsquo;s work on its I/O efficiency.\n#Register reuseUsing a Python-like notation to refer to submatrices, to compute the cell $C[x][y]$, we need to calculate the dot product of $A[x][:]$ and $B[:][y]$, which requires fetching $2n$ elements, even if we store $B$ in column-major order.\nTo compute $C[x:x+2][y:y+2]$, a $2 \\times 2$ submatrix of $C$, we would need two rows from $A$ and two columns from $B$, namely $A[x:x+2][:]$ and $B[:][y:y+2]$, containing $4n$ elements in total, to update four elements instead of one — which is $\\frac{2n / 1}{4n / 4} = 2$ times better in terms of I/O efficiency.\nTo avoid fetching data more than once, we need to iterate over these rows and columns in parallel and calculate all $2 \\times 2$ possible combinations of products. Here is a proof of concept:\nvoid kernel_2x2(int x, int y) { int c00 = 0, c01 = 0, c10 = 0, c11 = 0; for (int k = 0; k \u0026lt; n; k++) { // read rows int a0 = a[x][k]; int a1 = a[x + 1][k]; // read columns int b0 = b[k][y]; int b1 = b[k][y + 1]; // update all combinations c00 += a0 * b0; c01 += a0 * b1; c10 += a1 * b0; c11 += a1 * b1; } // write the results to C c[x][y] = c00; c[x][y + 1] = c01; c[x + 1][y] = c10; c[x + 1][y + 1] = c11; } We can now simply call this kernel on all 2x2 submatrices of $C$, but we won\u0026rsquo;t bother evaluating it: although this algorithm is better in terms of I/O operations, it would still not beat our SIMD-based implementation. Instead, we will extend this approach and develop a similar vectorized kernel right away.\n#Designing the kernelInstead of designing a kernel that computes an $h \\times w$ submatrix of $C$ from scratch, we will declare a function that updates it using columns from $l$ to $r$ of $A$ and rows from $l$ to $r$ of $B$. For now, this seems like an over-generalization, but this function interface will prove useful later.\nTo determine $h$ and $w$, we have several performance considerations:\nIn general, to compute an $h \\times w$ submatrix, we need to fetch $2 \\cdot n \\cdot (h + w)$ elements. To optimize the I/O efficiency, we want the $\\frac{h \\cdot w}{h + w}$ ratio to be high, which is achieved with large and square-ish submatrices. We want to use the FMA (\u0026ldquo;fused multiply-add\u0026rdquo;) instruction available on all modern x86 architectures. As you can guess from the name, it performs the c += a * b operation — which is the core of a dot product — on 8-element vectors in one go, which saves us from executing vector multiplication and addition separately. To achieve better utilization of this instruction, we want to make use of instruction-level parallelism. On Zen 2, the fma instruction has a latency of 5 and a throughput of 2, meaning that we need to concurrently execute at least $5 \\times 2 = 10$ of them to saturate its execution ports. We want to avoid register spill (move data to and from registers more than necessary), and we only have $16$ logical vector registers that we can use as accumulators (minus those that we need to hold temporary values). For these reasons, we settle on a $6 \\times 16$ kernel. This way, we process $96$ elements at once that are stored in $6 \\times 2 = 12$ vector registers. To update them efficiently, we use the following procedure:\n// update 6x16 submatrix C[x:x+6][y:y+16] // using A[x:x+6][l:r] and B[l:r][y:y+16] void kernel(float *a, vec *b, vec *c, int x, int y, int l, int r, int n) { vec t[6][2]{}; // will be zero-filled and stored in ymm registers for (int k = l; k \u0026lt; r; k++) { for (int i = 0; i \u0026lt; 6; i++) { // broadcast a[x + i][k] into a register vec alpha = vec{} + a[(x + i) * n + k]; // converts to a broadcast // multiply b[k][y:y+16] by it and update t[i][0] and t[i][1] for (int j = 0; j \u0026lt; 2; j++) t[i][j] += alpha * b[(k * n + y) / 8 + j]; // converts to an fma } } // write the results back to C for (int i = 0; i \u0026lt; 6; i++) for (int j = 0; j \u0026lt; 2; j++) c[((x + i) * n + y) / 8 + j] += t[i][j]; } We need t so that the compiler stores these elements in vector registers. We could just update their final destinations in c, but, unfortunately, the compiler re-writes them back to memory, causing a slowdown (wrapping everything in __restrict__ keywords doesn\u0026rsquo;t help).\nAfter unrolling these loops and hoisting b out of the i loop (b[(k * n + y) / 8 + j] does not depend on i and can be loaded once and reused in all 6 iterations), the compiler generates something more similar to this:\nfor (int k = l; k \u0026lt; r; k++) { __m256 b0 = _mm256_load_ps((__m256*) \u0026amp;b[k * n + y]; __m256 b1 = _mm256_load_ps((__m256*) \u0026amp;b[k * n + y + 8]; __m256 a0 = _mm256_broadcast_ps((__m128*) \u0026amp;a[x * n + k]); t00 = _mm256_fmadd_ps(a0, b0, t00); t01 = _mm256_fmadd_ps(a0, b1, t01); __m256 a1 = _mm256_broadcast_ps((__m128*) \u0026amp;a[(x + 1) * n + k]); t10 = _mm256_fmadd_ps(a1, b0, t10); t11 = _mm256_fmadd_ps(a1, b1, t11); // ... } We are using $12+3=15$ vector registers and a total of $6 \\times 3 + 2 = 20$ instructions to perform $16 \\times 6 = 96$ updates. Assuming that there are no other bottleneks, we should be hitting the throughput of _mm256_fmadd_ps.\nNote that this kernel is architecture-specific. If we didn\u0026rsquo;t have fma, or if its throughput/latency were different, or if the SIMD width was 128 or 512 bits, we would have made different design choices. Multi-platform BLAS implementations ship many kernels, each written in assembly by hand and optimized for a particular architecture.\nThe rest of the implementation is straightforward. Similar to the previous vectorized implementation, we just move the matrices to memory-aligned arrays and call the kernel instead of the innermost loop:\nvoid matmul(const float *_a, const float *_b, float *_c, int n) { // to simplify the implementation, we pad the height and width // so that they are divisible by 6 and 16 respectively int nx = (n + 5) / 6 * 6; int ny = (n + 15) / 16 * 16; float *a = alloc(nx * ny); float *b = alloc(nx * ny); float *c = alloc(nx * ny); for (int i = 0; i \u0026lt; n; i++) { memcpy(\u0026amp;a[i * ny], \u0026amp;_a[i * n], 4 * n); memcpy(\u0026amp;b[i * ny], \u0026amp;_b[i * n], 4 * n); // we don\u0026#39;t need to transpose b this time } for (int x = 0; x \u0026lt; nx; x += 6) for (int y = 0; y \u0026lt; ny; y += 16) kernel(a, (vec*) b, (vec*) c, x, y, 0, n, ny); for (int i = 0; i \u0026lt; n; i++) memcpy(\u0026amp;_c[i * n], \u0026amp;c[i * ny], 4 * n); std::free(a); std::free(b); std::free(c); } This improves the benchmark performance, but only by ~40%:\nThe speedup is much higher (2-3x) on smaller arrays, indicating that there is still a memory bandwidth problem:\nNow, if you\u0026rsquo;ve read the section on cache-oblivious algorithms, you know that one universal solution to these types of things is to split all matrices into four parts, perform eight recursive block matrix multiplications, and carefully combine the results together. This solution is okay in practice, but there is some overhead to recursion, and it also doesn\u0026rsquo;t allow us to fine-tune the algorithm, so instead, we will follow a different, simpler approach.\n#BlockingThe cache-aware alternative to the divide-and-conquer trick is cache blocking: splitting the data into blocks that can fit into the cache and processing them one by one. If we have more than one layer of cache, we can do hierarchical blocking: we first select a block of data that fits into the L3 cache, then we split it into blocks that fit into the L2 cache, and so on. This approach requires knowing the cache sizes in advance, but it is usually easier to implement and also faster in practice.\nCache blocking is less trivial to do with matrices than with arrays, but the general idea is this:\nSelect a submatrix of $B$ that fits into the L3 cache (say, a subset of its columns). Select a submatrix of $A$ that fits into the L2 cache (say, a subset of its rows). Select a submatrix of the previously selected submatrix of $B$ (a subset of its rows) that fits into the L1 cache. Update the relevant submatrix of $C$ using the kernel. Here is a good visualization by Jukka Suomela (it features many different approaches; you are interested in the last one).\nNote that the decision to start this process with matrix $B$ is not arbitrary. During the kernel execution, we are reading the elements of $A$ much slower than the elements of $B$: we fetch and broadcast just one element of $A$ and then multiply it with $16$ elements of $B$. Therefore, we want $B$ to be in the L1 cache while $A$ can stay in the L2 cache and not the other way around.\nThis sounds complicated, but we can implement it with just three more outer for loops, which are collectively called macro-kernel (and the highly optimized low-level function that updates a 6x16 submatrix is called micro-kernel):\nconst int s3 = 64; // how many columns of B to select const int s2 = 120; // how many rows of A to select const int s1 = 240; // how many rows of B to select for (int i3 = 0; i3 \u0026lt; ny; i3 += s3) // now we are working with b[:][i3:i3+s3] for (int i2 = 0; i2 \u0026lt; nx; i2 += s2) // now we are working with a[i2:i2+s2][:] for (int i1 = 0; i1 \u0026lt; ny; i1 += s1) // now we are working with b[i1:i1+s1][i3:i3+s3] // and we need to update c[i2:i2+s2][i3:i3+s3] with [l:r] = [i1:i1+s1] for (int x = i2; x \u0026lt; std::min(i2 + s2, nx); x += 6) for (int y = i3; y \u0026lt; std::min(i3 + s3, ny); y += 16) kernel(a, (vec*) b, (vec*) c, x, y, i1, std::min(i1 + s1, n), ny); Cache blocking completely removes the memory bottleneck:\nThe performance is no longer (significantly) affected by the problem size:\nNotice that the dip at $1536$ is still there: cache associativity still affects the performance. To mitigate this, we can adjust the step constants or insert holes into the layout, but we will not bother doing that for now.\n#OptimizationTo approach closer to the performance limit, we need a few more optimizations:\nRemove memory allocation and operate directly on the arrays that are passed to the function. Note that we don\u0026rsquo;t need to do anything with a as we are reading just one element at a time, and we can use an unaligned store for c as we only use it rarely, so our only concern is reading b. Get rid of the std::min so that the size parameters are (mostly) constant and can be embedded into the machine code by the compiler (which also lets it unroll the micro-kernel loop more efficiently and avoid runtime checks). Rewrite the micro-kernel by hand using 12 vector variables (the compiler seems to struggle with keeping them in registers and writes them first to a temporary memory location and only then to $C$). These optimizations are straightforward but quite tedious to implement, so we are not going to list the code here in the article. It also requires some more work to effectively support \u0026ldquo;weird\u0026rdquo; matrix sizes, which is why we only run benchmarks for sizes that are multiple of $48 = \\frac{6 \\cdot 16}{\\gcd(6, 16)}$.\nThese individually small improvements compound and result in another 50% improvement:\nWe are actually not that far from the theoretical performance limit — which can be calculated as the SIMD width times the fma instruction throughput times the clock frequency:\n$$ \\underbrace{8}{SIMD} \\cdot \\underbrace{2}{thr.} \\cdot \\underbrace{2 \\cdot 10^9}_{cycles/sec} = 32 ; GFLOPS ;; (3.2 \\cdot 10^{10}) $$\nIt is more representative to compare against some practical library, such as OpenBLAS. The laziest way to do it is to simply invoke matrix multiplication from NumPy. There may be some minor overhead due to Python, but it ends up reaching 80% of the theoretical limit, which seems plausible (a 20% overhead is okay: matrix multiplication is not the only thing that CPUs are made for).\nWe\u0026rsquo;ve reached ~93% of BLAS performance and ~75% of the theoretical performance limit, which is really great for what is essentially just 40 lines of C.\nInterestingly, the whole thing can be rolled into just one deeply nested for loop with a BLAS level of performance (assuming that we\u0026rsquo;re in 2050 and using GCC version 35, which finally stopped screwing up with register spilling):\nfor (int i3 = 0; i3 \u0026lt; n; i3 += s3) for (int i2 = 0; i2 \u0026lt; n; i2 += s2) for (int i1 = 0; i1 \u0026lt; n; i1 += s1) for (int x = i2; x \u0026lt; i2 + s2; x += 6) for (int y = i3; y \u0026lt; i3 + s3; y += 16) for (int k = i1; k \u0026lt; i1 + s1; k++) for (int i = 0; i \u0026lt; 6; i++) for (int j = 0; j \u0026lt; 2; j++) c[x * n / 8 + i * n / 8 + y / 8 + j] += (vec{} + a[x * n + i * n + k]) * b[n / 8 * k + y / 8 + j]; There is also an approach that performs asymptotically fewer arithmetic operations — the Strassen algorithm — but it has a large constant factor, and it is only efficient for very large matrices ($n \u0026gt; 4000$), where we typically have to use either multiprocessing or some approximate dimensionality-reducing methods anyway.\n#GeneralizationsFMA also supports 64-bit floating-point numbers, but it does not support integers: you need to perform addition and multiplication separately, which results in decreased performance. If you can guarantee that all intermediate results can be represented exactly as 32- or 64-bit floating-point numbers (which is often the case), it may be faster to just convert them to and from floats.\nThis approach can be also applied to some similar-looking computations. One example is the \u0026ldquo;min-plus matrix multiplication\u0026rdquo; defined as:\n$$ (A \\circ B){ij} = \\min{1 \\le k \\le n} (A_{ik} + B_{kj}) $$\nIt is also known as the \u0026ldquo;distance product\u0026rdquo; due to its graph interpretation: when applied to itself $(D \\circ D)$, the result is the matrix of shortest paths of length two between all pairs of vertices in a fully-connected weighted graph specified by the edge weight matrix $D$.\nA cool thing about the distance product is that if we iterate the process and calculate\n$$ D_2 = D \\circ D \\ D_4 = D_2 \\circ D_2 \\ D_8 = D_4 \\circ D_4 \\ \\ldots $$\n…we can find all-pairs shortest paths in $O(\\log n)$ steps:\nfor (int l = 0; l \u0026lt; logn; l++) for (int i = 0; i \u0026lt; n; i++) for (int j = 0; j \u0026lt; n; j++) for (int k = 0; k \u0026lt; n; k++) d[i][j] = min(d[i][j], d[i][k] + d[k][j]); This requires $O(n^3 \\log n)$ operations. If we do these two-edge relaxations in a particular order, we can do it with just one pass, which is known as the Floyd-Warshall algorithm:\nfor (int k = 0; k \u0026lt; n; k++) for (int i = 0; i \u0026lt; n; i++) for (int j = 0; j \u0026lt; n; j++) d[i][j] = min(d[i][j], d[i][k] + d[k][j]); Interestingly, similarly vectorizing the distance product and executing it $O(\\log n)$ times (or possibly fewer) in $O(n^3 \\log n)$ total operations is faster than naively executing the Floyd-Warshall algorithm in $O(n^3)$ operations, although not by a lot.\nAs an exercise, try to speed up this \u0026ldquo;for-for-for\u0026rdquo; computation. It is harder to do than in the matrix multiplication case because now there is a logical dependency between the iterations, and you need to perform updates in a particular order, but it is still possible to design a similar kernel and a block iteration order that achieves a 30-50x total speedup.\n#AcknowledgementsThe final algorithm was originally designed by Kazushige Goto, and it is the basis of GotoBLAS and OpenBLAS. The author himself describes it in more detail in \u0026ldquo;Anatomy of High-Performance Matrix Multiplication\u0026rdquo;.\nThe exposition style is inspired by the \u0026ldquo;Programming Parallel Computers\u0026rdquo; course by Jukka Suomela, which features a similar case study on speeding up the distance product.\n","id":81,"path":"/hugo-page/hpc/algorithms/matmul/","title":"Matrix Multiplication"},{"content":"Algorithmica is an open-access web book dedicated to the art and science of computing.\nIt is created by Sergey Slotin and the teachers and students of Tinkoff Generation — a nonprofit educational organization that trains about half of the finalists of the Russian Olympiad in Informatics.\nThe English version of the website is a work in progress; the only useful thing you can find here is the continuously updated draft of Algorithms for Modern Hardware. We are currently more focused on the Russian version, which hosts various course materials that we use ourselves.\nIf you spot an error, please create an issue on GitHub or, preferably, fix it right away (the pencil icon on the top-right).\n","id":82,"path":"/hugo-page/","title":"Algorithmica"},{"content":"","id":83,"path":"/hugo-page/categories/","title":"Categories"},{"content":"","id":84,"path":"/hugo-page/tags/","title":"Tags"}]