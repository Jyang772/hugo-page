<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Instruction-Level Parallelism on Algorithmica</title>
    <link>https://jyang772.github.io/hugo-page/hpc/pipelining/</link>
    <description>Recent content in Instruction-Level Parallelism on Algorithmica</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="https://jyang772.github.io/hugo-page/hpc/pipelining/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Pipeline Hazards</title>
      <link>https://jyang772.github.io/hugo-page/hpc/pipelining/hazards/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://jyang772.github.io/hugo-page/hpc/pipelining/hazards/</guid>
      <description>Pipelining lets you hide the latencies of instructions by running them concurrently, but also creates some potential obstacles of its own — characteristically called pipeline hazards, that is, situations when the next instruction cannot execute on the following clock cycle.&#xA;There are multiple ways this may happen:&#xA;A structural hazard happens when two or more instructions need the same part of CPU (e.g., an execution unit). A data hazard happens when you have to wait for an operand to be computed from some previous step.</description>
    </item>
    <item>
      <title>The Cost of Branching</title>
      <link>https://jyang772.github.io/hugo-page/hpc/pipelining/branching/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://jyang772.github.io/hugo-page/hpc/pipelining/branching/</guid>
      <description>When a CPU encounters a conditional jump or any other type of branching, it doesn&amp;rsquo;t just sit idle until its condition is computed — instead, it starts speculatively executing the branch that seems more likely to be taken immediately. During execution, the CPU computes statistics about branches taken on each instruction, and after some time, they start to predict them by recognizing common patterns.&#xA;For this reason, the true &amp;ldquo;cost&amp;rdquo; of a branch largely depends on how well it can be predicted by the CPU.</description>
    </item>
    <item>
      <title>Branchless Programming</title>
      <link>https://jyang772.github.io/hugo-page/hpc/pipelining/branchless/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://jyang772.github.io/hugo-page/hpc/pipelining/branchless/</guid>
      <description>As we established in the previous section, branches that can&amp;rsquo;t be effectively predicted by the CPU are expensive as they may cause a long pipeline stall to fetch new instructions after a branch mispredict. In this section, we discuss the means of removing branches in the first place.&#xA;#PredicationWe are going to continue the same case study we&amp;rsquo;ve started before — we create an array of random numbers and sum up all its elements below 50:</description>
    </item>
    <item>
      <title>Instruction Tables</title>
      <link>https://jyang772.github.io/hugo-page/hpc/pipelining/tables/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://jyang772.github.io/hugo-page/hpc/pipelining/tables/</guid>
      <description>Interleaving the stages of execution is a general idea in digital electronics, and it is applied not only in the main CPU pipeline, but also on the level of separate instructions and memory. Most execution units have their own little pipelines and can take another instruction just one or two cycles after the previous one.&#xA;In this context, it makes sense to use two different &amp;ldquo;costs&amp;rdquo; for instructions:&#xA;Latency: how many cycles are needed to receive the results of an instruction.</description>
    </item>
    <item>
      <title>Throughput Computing</title>
      <link>https://jyang772.github.io/hugo-page/hpc/pipelining/throughput/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://jyang772.github.io/hugo-page/hpc/pipelining/throughput/</guid>
      <description>Optimizing for latency is usually quite different from optimizing for throughput:&#xA;When optimizing data structure queries or small one-time or branchy algorithms, you need to look up the latencies of its instructions, mentally construct the execution graph of the computation, and then try to reorganize it so that the critical path is shorter. When optimizing hot loops and large-dataset algorithms, you need to look up the throughputs of their instructions, count how many times each one is used per iteration, determine which of them is the bottleneck, and then try to restructure the loop so that it is used less often.</description>
    </item>
  </channel>
</rss>
