<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>SIMD Parallelism on Algorithmica</title><link>http://jyang772.github.io/hugo-page/hpc/simd/</link><description>Recent content in SIMD Parallelism on Algorithmica</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="http://jyang772.github.io/hugo-page/hpc/simd/index.xml" rel="self" type="application/rss+xml"/><item><title>Intrinsics and Vector Types</title><link>http://jyang772.github.io/hugo-page/hpc/simd/intrinsics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://jyang772.github.io/hugo-page/hpc/simd/intrinsics/</guid><description>The most low-level way to use SIMD is to use the assembly vector instructions directly — they aren&amp;rsquo;t different from their scalar equivalents at all — but we are not going to do that. Instead, we will use intrinsic functions mapping to these instructions that are available in modern C/C++ compilers.
In this section, we will go through the basics of their syntax, and in the rest of this chapter, we will use them extensively to do things that are actually interesting.</description></item><item><title>Moving Data</title><link>http://jyang772.github.io/hugo-page/hpc/simd/moving/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://jyang772.github.io/hugo-page/hpc/simd/moving/</guid><description>If you took some time to study the reference, you may have noticed that there are essentially two major groups of vector operations:
Instructions that perform some elementwise operation (+, *, &amp;lt;, acos, etc.). Instructions that load, store, mask, shuffle, and generally move data around. While using the elementwise instructions is easy, the largest challenge with SIMD is getting the data in vector registers in the first place, with low enough overhead so that the whole endeavor is worthwhile.</description></item><item><title>Reductions</title><link>http://jyang772.github.io/hugo-page/hpc/simd/reduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://jyang772.github.io/hugo-page/hpc/simd/reduction/</guid><description>Reduction (also known as folding in functional programming) is the action of computing the value of some associative and commutative operation (i.e., $(a \circ b) \circ c = a \circ (b \circ c)$ and $a \circ b = b \circ a$) over a range of arbitrary elements.
The simplest example of reduction is calculating the sum an array:
int sum(int *a, int n) { int s = 0; for (int i = 0; i &amp;lt; n; i++) s += a[i]; return s; } The naive approach is not so straightforward to vectorize, because the state of the loop (sum $s$ on the current prefix) depends on the previous iteration.</description></item><item><title>Masking and Blending</title><link>http://jyang772.github.io/hugo-page/hpc/simd/masking/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://jyang772.github.io/hugo-page/hpc/simd/masking/</guid><description>One of the bigger challenges of SIMD programming is that its options for control flow are very limited — because the operations you apply to a vector are the same for all its elements.
This makes the problems that are usually trivially resolved with an if or any other type of branching much harder. With SIMD, they have to be dealt with by the means of various branchless programming techniques, which aren&amp;rsquo;t always that straightforward to apply.</description></item><item><title>In-Register Shuffles</title><link>http://jyang772.github.io/hugo-page/hpc/simd/shuffling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://jyang772.github.io/hugo-page/hpc/simd/shuffling/</guid><description>Masking lets you apply operations to only a subset of vector elements. It is a very effective and frequently used data manipulation technique, but in many cases, you need to perform more advanced operations that involve permuting values inside a vector register instead of just blending them with other vectors.
The problem is that adding a separate element-shuffling instruction for each possible use case in hardware is unfeasible. What we can do though is to add just one general permutation instruction that takes the indices of a permutation and produces these indices using precomputed lookup tables.</description></item><item><title>Auto-Vectorization and SPMD</title><link>http://jyang772.github.io/hugo-page/hpc/simd/auto-vectorization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://jyang772.github.io/hugo-page/hpc/simd/auto-vectorization/</guid><description>SIMD parallelism is most often used for embarrassingly parallel computations: the kinds where all you do is apply some elementwise function to all elements of an array and write it back somewhere else. In this setting, you don&amp;rsquo;t even need to know how SIMD works: the compiler is perfectly capable of optimizing such loops by itself — you just need to be aware that such optimization exists and that it usually yields a 5-10x speedup.</description></item></channel></rss>