<!doctype html><html lang=en-us><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-WBN59M8Y5S"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-WBN59M8Y5S")</script><script type=text/javascript>(function(e,t,n,s,o,i,a){e[o]=e[o]||function(){(e[o].a=e[o].a||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)})(window,document,"script","https://mc.yandex.ru/metrika/tag.js","ym"),ym(53961409,"init",{clickmap:!0,trackLinks:!0,accurateTrackBounce:!0,webvisor:!0})</script><noscript><div><img src=https://mc.yandex.ru/watch/53961409 style=position:absolute;left:-9999px alt></div></noscript><meta charset=utf-8><link rel=stylesheet href=/hugo-page/style.min.a3a4a7a8e8602aaa85b7cb3d655edde028ac80d73f2a97389e2cbcf995dd672d.css integrity="sha256-o6SnqOhgKqqFt8s9ZV7d4CisgNc/Kpc4niy8+ZXdZy0="><link rel=stylesheet href=/syntax.css id=syntax-theme><link rel=stylesheet type=text/css href=https://tikzjax.com/v1/fonts.css><script src=https://tikzjax.com/v1/tikzjax.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.9/lunr.min.js></script><script src=/scripts/lunr.stemmer.support.min.js></script><script src=/scripts/lunr.ru.min.js></script><script src=/scripts/lunr.multi.min.js></script><link rel=stylesheet id=theme><script>function toggleSidebar(){console.log("Toggling sidebar visibility");var e=document.getElementById("sidebar"),t=document.getElementById("wrapper");(e.classList.contains("sidebar-toggled")||window.getComputedStyle(e).display=="block")&&(e.classList.toggle("sidebar-hidden"),t.classList.toggle("sidebar-hidden")),e.classList.add("sidebar-toggled"),t.classList.add("sidebar-toggled")}function switchTheme(e){console.log("Changing theme:",e),document.getElementById("theme").href=e=="dark"?"/hugo-page/dark.min.b3ae1169831434b11b48de5b3e3210547eea6b7884c295ab0030cb973ea0dc49.css":"",document.getElementById("syntax-theme").href=e=="dark"?"/syntax-dark.css":"/syntax.css",localStorage.setItem("theme",e)}async function toggleSearch(){console.log("Toggling search");var e=document.getElementById("search");if(window.getComputedStyle(e).display=="none"?(e.style.display="block",window.scrollTo({top:0}),document.getElementById("search-bar").focus()):e.style.display="none",!index){console.log("Fetching index");const e=await fetch("/hugo-page/searchindex.json"),t=await e.json();index=lunr(function(){this.use(lunr.multiLanguage("en","ru")),this.field("title",{boost:5}),this.field("content",{boost:1}),t.forEach(function(e){this.add(e),articles.push(e)},this)}),console.log("Ready to search")}}var articles=[],index=void 0;function search(){var n,e=document.getElementById("search-bar").value,s=document.getElementById("search-results"),o=document.getElementById("search-count");if(e==""){s.innerHTML="",o.innerHTML="";return}n=index.search(e),o.innerHTML="Found <b>"+n.length+"</b> pages";let t="";for(const a in n){const i=articles[n[a].ref];t+='<li><a href="'+i.path+'">'+i.title+"</a> <p>";const s=i.content,o=80;if(s.includes(e)){const n=s.indexOf(e);n>o&&(t+="…"),t+=s.substring(n-o,n)+"<b>"+e+"</b>"+s.substring(n+e.length,n+e.length+o)}else t+=s.substring(0,o*2);t+="…</p></li>"}s.innerHTML=t}localStorage.getItem("theme")=="dark"&&switchTheme("dark"),window.addEventListener("load",function(){var e=document.getElementById("active-element");e&&e.scrollIntoView({block:"center"})}),window.addEventListener("scroll",function(){var e=document.getElementById("menu");window.scrollY<120?e.classList.remove("scrolled"):e.classList.add("scrolled")}),window.addEventListener("keydown",function(e){if(e.altKey)return;if(document.activeElement.tagName=="INPUT")return;e.key=="ArrowLeft"?document.getElementById("prev-article").click():e.key=="ArrowRight"&&document.getElementById("next-article").click()})</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})'></script><title>Moving Data - Algorithmica</title></head><body><nav id=sidebar><div class=title><a href=/>Algorithmica</a>
<span class=slash>/</span>
<a href=/hugo-page/hpc/ class=divisionAbbr>HPC</a></div><ul><li class=part>Performance Engineering</li><li><a href=/hugo-page/hpc/complexity/>Complexity Models</a></li><ol><li><a href=/hugo-page/hpc/complexity/hardware/>Modern Hardware</a></li><li><a href=/hugo-page/hpc/complexity/languages/>Programming Languages</a></li></ol><li><a href=/hugo-page/hpc/architecture/>Computer Architecture</a></li><ol><li><a href=/hugo-page/hpc/architecture/isa/>Instruction Set Architectures</a></li><li><a href=/hugo-page/hpc/architecture/assembly/>Assembly Language</a></li><li><a href=/hugo-page/hpc/architecture/loops/>Loops and Conditionals</a></li><li><a href=/hugo-page/hpc/architecture/functions/>Functions and Recursion</a></li><li><a href=/hugo-page/hpc/architecture/indirect/>Indirect Branching</a></li><li><a href=/hugo-page/hpc/architecture/layout/>Machine Code Layout</a></li></ol><li><a href=/hugo-page/hpc/pipelining/>Instruction-Level Parallelism</a></li><ol><li><a href=/hugo-page/hpc/pipelining/hazards/>Pipeline Hazards</a></li><li><a href=/hugo-page/hpc/pipelining/branching/>The Cost of Branching</a></li><li><a href=/hugo-page/hpc/pipelining/branchless/>Branchless Programming</a></li><li><a href=/hugo-page/hpc/pipelining/tables/>Instruction Tables</a></li><li><a href=/hugo-page/hpc/pipelining/throughput/>Throughput Computing</a></li></ol><li><a href=/hugo-page/hpc/compilation/>Compilation</a></li><ol><li><a href=/hugo-page/hpc/compilation/stages/>Stages of Compilation</a></li><li><a href=/hugo-page/hpc/compilation/flags/>Flags and Targets</a></li><li><a href=/hugo-page/hpc/compilation/situational/>Situational Optimizations</a></li><li><a href=/hugo-page/hpc/compilation/contracts/>Contract Programming</a></li><li><a href=/hugo-page/hpc/compilation/precalc/>Precomputation</a></li></ol><li><a href=/hugo-page/hpc/profiling/>Profiling</a></li><ol><li><a href=/hugo-page/hpc/profiling/instrumentation/>Instrumentation</a></li><li><a href=/hugo-page/hpc/profiling/events/>Statistical Profiling</a></li><li><a href=/hugo-page/hpc/profiling/simulation/>Program Simulation</a></li><li><a href=/hugo-page/hpc/profiling/mca/>Machine Code Analyzers</a></li><li><a href=/hugo-page/hpc/profiling/benchmarking/>Benchmarking</a></li><li><a href=/hugo-page/hpc/profiling/noise/>Getting Accurate Results</a></li></ol><li><a href=/hugo-page/hpc/arithmetic/>Arithmetic</a></li><ol><li><a href=/hugo-page/hpc/arithmetic/float/>Floating-Point Numbers</a></li><li><a href=/hugo-page/hpc/arithmetic/ieee-754/>IEEE 754 Floats</a></li><li><a href=/hugo-page/hpc/arithmetic/errors/>Rounding Errors</a></li><li><a href=/hugo-page/hpc/arithmetic/newton/>Newton's Method</a></li><li><a href=/hugo-page/hpc/arithmetic/rsqrt/>Fast Inverse Square Root</a></li><li><a href=/hugo-page/hpc/arithmetic/integer/>Integer Numbers</a></li><li><a href=/hugo-page/hpc/arithmetic/division/>Integer Division</a></li></ol><li><a href=/hugo-page/hpc/number-theory/>Number Theory</a></li><ol><li><a href=/hugo-page/hpc/number-theory/modular/>Modular Arithmetic</a></li><li><a href=/hugo-page/hpc/number-theory/exponentiation/>Binary Exponentiation</a></li><li><a href=/hugo-page/hpc/number-theory/euclid-extended/>Extended Euclidean Algorithm</a></li><li><a href=/hugo-page/hpc/number-theory/montgomery/>Montgomery Multiplication</a></li></ol><li><a href=/hugo-page/hpc/external-memory/>External Memory</a></li><ol><li><a href=/hugo-page/hpc/external-memory/hierarchy/>Memory Hierarchy</a></li><li><a href=/hugo-page/hpc/external-memory/virtual/>Virtual Memory</a></li><li><a href=/hugo-page/hpc/external-memory/model/>External Memory Model</a></li><li><a href=/hugo-page/hpc/external-memory/sorting/>External Sorting</a></li><li><a href=/hugo-page/hpc/external-memory/list-ranking/>List Ranking</a></li><li><a href=/hugo-page/hpc/external-memory/policies/>Eviction Policies</a></li><li><a href=/hugo-page/hpc/external-memory/oblivious/>Cache-Oblivious Algorithms</a></li><li><a href=/hugo-page/hpc/external-memory/locality/>Spatial and Temporal Locality</a></li></ol><li><a href=/hugo-page/hpc/cpu-cache/>RAM & CPU Caches</a></li><ol><li><a href=/hugo-page/hpc/cpu-cache/bandwidth/>Memory Bandwidth</a></li><li><a href=/hugo-page/hpc/cpu-cache/latency/>Memory Latency</a></li><li><a href=/hugo-page/hpc/cpu-cache/cache-lines/>Cache Lines</a></li><li><a href=/hugo-page/hpc/cpu-cache/sharing/>Memory Sharing</a></li><li><a href=/hugo-page/hpc/cpu-cache/mlp/>Memory-Level Parallelism</a></li><li><a href=/hugo-page/hpc/cpu-cache/prefetching/>Prefetching</a></li><li><a href=/hugo-page/hpc/cpu-cache/alignment/>Alignment and Packing</a></li><li><a href=/hugo-page/hpc/cpu-cache/pointers/>Pointer Alternatives</a></li><li><a href=/hugo-page/hpc/cpu-cache/associativity/>Cache Associativity</a></li><li><a href=/hugo-page/hpc/cpu-cache/paging/>Memory Paging</a></li><li><a href=/hugo-page/hpc/cpu-cache/aos-soa/>AoS and SoA</a></li></ol><li><a href=/hugo-page/hpc/simd/>SIMD Parallelism</a></li><ol><li><a href=/hugo-page/hpc/simd/intrinsics/>Intrinsics and Vector Types</a></li><li><a href=/hugo-page/hpc/simd/moving/ id=active-element>Moving Data</a></li><li><a href=/hugo-page/hpc/simd/reduction/>Reductions</a></li><li><a href=/hugo-page/hpc/simd/masking/>Masking and Blending</a></li><li><a href=/hugo-page/hpc/simd/shuffling/>In-Register Shuffles</a></li><li><a href=/hugo-page/hpc/simd/auto-vectorization/>Auto-Vectorization and SPMD</a></li></ol><li><a href=/hugo-page/hpc/algorithms/>Algorithms Case Studies</a></li><ol><li><a href=/hugo-page/hpc/algorithms/gcd/>Binary GCD</a></li><li><a href=/hugo-page/hpc/algorithms/factorization/>Integer Factorization</a></li><li><a href=/hugo-page/hpc/algorithms/argmin/>Argmin with SIMD</a></li><li><a href=/hugo-page/hpc/algorithms/prefix/>Prefix Sum with SIMD</a></li><li><a href=/hugo-page/hpc/algorithms/matmul/>Matrix Multiplication</a></li></ol><li><a href=/hugo-page/hpc/data-structures/>Data Structures Case Studies</a></li><ol><li><a href=/hugo-page/hpc/data-structures/binary-search/>Binary Search</a></li><li><a href=/hugo-page/hpc/data-structures/s-tree/>Static B-Trees</a></li><li><a href=/hugo-page/hpc/data-structures/b-tree/>Search Trees</a></li><li><a href=/hugo-page/hpc/data-structures/segment-trees/>Segment Trees</a></li></ol></ul></nav><div id=wrapper><menu id=menu><div class=left><a><img src=/icons/bars-solid.svg onclick=toggleSidebar() title='open table of contents'>
</a><a><img src=/icons/adjust-solid.svg style=position:relative;top:-1px onclick='switchTheme(localStorage.getItem("theme")=="dark"?"light":"dark")' title='dark theme'>
</a><a><img src=/icons/search-solid.svg onclick=toggleSearch() title=search></a></div><div class=title>Moving Data</div><div class=right><a onclick=window.print()><img src=/icons/print-solid.svg title=print>
</a><a href=https://prose.io/#algorithmica-org/algorithmica/edit/master//hpc%2fsimd%2fmoving.md><img src=/icons/edit-solid.svg title=edit style=width:18px;position:relative;right:-2px;top:-1px>
</a><a href=https://github.com/algorithmica-org/algorithmica/blob/master//hpc/simd/moving.md class=github-main><img src=/icons/github-brands.svg title='view on github'></a></div></menu><main><div id=search><input id=search-bar type=search placeholder='Search this book…' oninput=search()><div id=search-count></div><div id=search-results></div></div><header><h1>Moving Data</h1><div class=info></div></header><article><p>If you took some time to study <a href=https://software.intel.com/sites/landingpage/IntrinsicsGuide>the reference</a>, you may have noticed that there are essentially two major groups of vector operations:</p><ol><li>Instructions that perform some elementwise operation (<code>+</code>, <code>*</code>, <code>&lt;</code>, <code>acos</code>, etc.).</li><li>Instructions that load, store, mask, shuffle, and generally move data around.</li></ol><p>While using the elementwise instructions is easy, the largest challenge with SIMD is getting the data in vector registers in the first place, with low enough overhead so that the whole endeavor is worthwhile.</p><span class=anchor id=aligned-loads-and-stores></span><h3><a class=anchor-link href=http://jyang772.github.io/hugo-page/hpc/simd/moving/#aligned-loads-and-stores>#</a>Aligned Loads and Stores</h3><p>Operations of reading and writing the contents of a SIMD register into memory have two versions each: <code>load</code> / <code>loadu</code> and <code>store</code> / <code>storeu</code>. The letter &ldquo;u&rdquo; here stands for &ldquo;unaligned.&rdquo; The difference is that the former ones only work correctly when the read / written block fits inside a single <a href=/hpc/cpu-cache/cache-lines>cache line</a> (and crash otherwise), while the latter work either way, but with a slight performance penalty if the block crosses a cache line.</p><p>Sometimes, especially when the &ldquo;inner&rdquo; operation is very lightweight, the performance difference becomes significant (at least because you need to fetch two cache lines instead of one). As an extreme example, this way of adding two arrays together:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>3</span><span class=p>;</span> <span class=n>i</span> <span class=o>+</span> <span class=mi>7</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=n>i</span> <span class=o>+=</span> <span class=mi>8</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>__m256i</span> <span class=n>x</span> <span class=o>=</span> <span class=n>_mm256_loadu_si256</span><span class=p>((</span><span class=n>__m256i</span><span class=o>*</span><span class=p>)</span> <span class=o>&amp;</span><span class=n>a</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>    <span class=n>__m256i</span> <span class=n>y</span> <span class=o>=</span> <span class=n>_mm256_loadu_si256</span><span class=p>((</span><span class=n>__m256i</span><span class=o>*</span><span class=p>)</span> <span class=o>&amp;</span><span class=n>b</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>    <span class=n>__m256i</span> <span class=n>z</span> <span class=o>=</span> <span class=n>_mm256_add_epi32</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>_mm256_storeu_si256</span><span class=p>((</span><span class=n>__m256i</span><span class=o>*</span><span class=p>)</span> <span class=o>&amp;</span><span class=n>c</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>z</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>…is ~30% slower than its aligned version:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=n>i</span> <span class=o>+=</span> <span class=mi>8</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>__m256i</span> <span class=n>x</span> <span class=o>=</span> <span class=n>_mm256_load_si256</span><span class=p>((</span><span class=n>__m256i</span><span class=o>*</span><span class=p>)</span> <span class=o>&amp;</span><span class=n>a</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>    <span class=n>__m256i</span> <span class=n>y</span> <span class=o>=</span> <span class=n>_mm256_load_si256</span><span class=p>((</span><span class=n>__m256i</span><span class=o>*</span><span class=p>)</span> <span class=o>&amp;</span><span class=n>b</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>    <span class=n>__m256i</span> <span class=n>z</span> <span class=o>=</span> <span class=n>_mm256_add_epi32</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>_mm256_store_si256</span><span class=p>((</span><span class=n>__m256i</span><span class=o>*</span><span class=p>)</span> <span class=o>&amp;</span><span class=n>c</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>z</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>In the first version, assuming that arrays <code>a</code>, <code>b</code> and <code>c</code> are all 64-byte <em>aligned</em> (the addresses of their first elements are divisible by 64, and so they start at the beginning of a cache line), roughly half of reads and writes will be &ldquo;bad&rdquo; because they cross a cache line boundary.</p><p>Note that the performance difference is caused by the cache system and not by the instructions themselves. On most modern architectures, the <code>loadu</code> / <code>storeu</code> intrinsics should be equally as fast as <code>load</code> / <code>store</code> given that in both cases the blocks only span one cache line. The advantage of the latter is that they can act as free run time assertions that all reads and writes are aligned.</p><p>This makes it important to properly <a href=/hpc/cpu-cache/alignment>align</a> arrays and other data on allocation, and it is also one of the reasons why compilers can&rsquo;t always <a href=../auto-vectorization>auto-vectorize</a> efficiently. For most purposes, we only need to guarantee that any 32-byte SIMD block will not cross a cache line boundary, and we can specify this alignment with the <code>alignas</code> specifier:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=k>alignas</span><span class=p>(</span><span class=mi>32</span><span class=p>)</span> <span class=kt>float</span> <span class=n>a</span><span class=p>[</span><span class=n>n</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=n>i</span> <span class=o>+=</span> <span class=mi>8</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>__m256</span> <span class=n>x</span> <span class=o>=</span> <span class=n>_mm256_load_ps</span><span class=p>(</span><span class=o>&amp;</span><span class=n>a</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>    <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span></code></pre></div><p>The <a href=../intrinsics>built-in vector types</a> already have corresponding alignment requirements and assume aligned memory reads and writes — so you are always safe when allocating an array of <code>v8si</code>, but when converting it from <code>int*</code> you have to make sure it is aligned.</p><p>Similar to the scalar case, many arithmetic instructions take memory addresses as operands — <a href=../intrinsics>vector addition</a> is an example — although you can&rsquo;t explicitly use it as an intrinsic and have to rely on the compiler. There are also a few other instructions for reading a SIMD block from memory, notably the <a href=/hpc/cpu-cache/bandwidth#bypassing-the-cache>non-temporal</a> load and store operations that don&rsquo;t lift accessed data in the cache hierarchy.</p><span class=anchor id=register-aliasing></span><h3><a class=anchor-link href=http://jyang772.github.io/hugo-page/hpc/simd/moving/#register-aliasing>#</a>Register Aliasing</h3><p>The first SIMD extension, MMX, started quite small. It only used 64-bit vectors, which were conveniently aliased to the mantissa part of a <a href=/hpc/arithmetic/ieee-754>80-bit float</a> so that there is no need to introduce a separate set of registers. As the vector size grew with later extensions, the same <a href=/hpc/architecture/assembly#instructions-and-registers>register aliasing</a> mechanism used in general-purpose registers was adopted for the vector registers to maintain backward compatibility: <code>xmm0</code> is the first half (128 bits) of <code>ymm0</code>, <code>xmm1</code> is the first half of <code>ymm1</code>, and so on.</p><p>This feature, combined with the fact that the vector registers are located in the FPU, makes moving data between them and the general-purpose registers slightly complicated.</p><span class=anchor id=extract-and-insert></span><h3><a class=anchor-link href=http://jyang772.github.io/hugo-page/hpc/simd/moving/#extract-and-insert>#</a>Extract and Insert</h3><p>To <em>extract</em> a specific value from a vector, you can use <code>_mm256_extract_epi32</code> and similar intrinsics. It takes the index of the integer to be extracted as the second parameter and generates different instruction sequences depending on its value.</p><p>If you need to extract the first element, it generates the <code>vmovd</code> instruction (for <code>xmm0</code>, the first half of the vector):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-nasm data-lang=nasm><span class=line><span class=cl><span class=nf>vmovd</span> <span class=nb>eax</span><span class=p>,</span> <span class=nv>xmm0</span>
</span></span></code></pre></div><p>For other elements of an SSE vector, it generates possibly slightly slower <code>vpextrd</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-nasm data-lang=nasm><span class=line><span class=cl><span class=nf>vpextrd</span> <span class=nb>eax</span><span class=p>,</span> <span class=nv>xmm0</span><span class=p>,</span> <span class=mi>1</span>
</span></span></code></pre></div><p>To extract anything from the second half of an AVX vector, it first has to extract that second half, and then the scalar itself. For example, here is how it extracts the last (eighth) element,</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-nasm data-lang=nasm><span class=line><span class=cl><span class=nf>vextracti128</span> <span class=nv>xmm0</span><span class=p>,</span> <span class=nv>ymm0</span><span class=p>,</span> <span class=mh>0x1</span>
</span></span><span class=line><span class=cl><span class=nf>vpextrd</span>      <span class=nb>eax</span><span class=p>,</span> <span class=nv>xmm0</span><span class=p>,</span> <span class=mi>3</span>
</span></span></code></pre></div><p>There is a similar <code>_mm256_insert_epi32</code> intrinsic for overwriting specific elements:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-nasm data-lang=nasm><span class=line><span class=cl><span class=nf>mov</span>          <span class=nb>eax</span><span class=p>,</span> <span class=mi>42</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>; v = _mm256_insert_epi32(v, 42, 0);</span>
</span></span><span class=line><span class=cl><span class=nf>vpinsrd</span> <span class=nv>xmm2</span><span class=p>,</span> <span class=nv>xmm0</span><span class=p>,</span> <span class=nb>eax</span><span class=p>,</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=nf>vinserti128</span>     <span class=nv>ymm0</span><span class=p>,</span> <span class=nv>ymm0</span><span class=p>,</span> <span class=nv>xmm2</span><span class=p>,</span> <span class=mh>0x0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>; v = _mm256_insert_epi32(v, 42, 7);</span>
</span></span><span class=line><span class=cl><span class=nf>vextracti128</span> <span class=nv>xmm1</span><span class=p>,</span> <span class=nv>ymm0</span><span class=p>,</span> <span class=mh>0x1</span>
</span></span><span class=line><span class=cl><span class=nf>vpinsrd</span>      <span class=nv>xmm2</span><span class=p>,</span> <span class=nv>xmm1</span><span class=p>,</span> <span class=nb>eax</span><span class=p>,</span> <span class=mi>3</span>
</span></span><span class=line><span class=cl><span class=nf>vinserti128</span>  <span class=nv>ymm0</span><span class=p>,</span> <span class=nv>ymm0</span><span class=p>,</span> <span class=nv>xmm2</span><span class=p>,</span> <span class=mh>0x1</span>
</span></span></code></pre></div><p>Takeaway: moving scalar data to and from vector registers is slow, especially when this isn&rsquo;t the first element.</p><span class=anchor id=making-constants></span><h3><a class=anchor-link href=http://jyang772.github.io/hugo-page/hpc/simd/moving/#making-constants>#</a>Making Constants</h3><p>If you need to populate not just one element but the entire vector, you can use the <code>_mm256_setr_epi32</code> intrinsic:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>__m256</span> <span class=n>iota</span> <span class=o>=</span> <span class=n>_mm256_setr_epi32</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>6</span><span class=p>,</span> <span class=mi>7</span><span class=p>);</span>
</span></span></code></pre></div><p>The &ldquo;r&rdquo; here stands for &ldquo;reversed&rdquo; — from <a href=/hpc/arithmetic/integer#integer-types>the CPU point of view</a>, not for humans. There is also the <code>_mm256_set_epi32</code> (without &ldquo;r&rdquo;) that fills the values from the opposite direction. Both are mostly used to create compile-time constants that are then fetched into the register with a block load. If your use case is filling a vector with zeros, use the <code>_mm256_setzero_si256</code> instead: it <code>xor</code>-s the register with itself.</p><p>In built-in vector types, you can just use normal braced initialization:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>vec</span> <span class=n>zero</span> <span class=o>=</span> <span class=p>{};</span>
</span></span><span class=line><span class=cl><span class=n>vec</span> <span class=n>iota</span> <span class=o>=</span> <span class=p>{</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>6</span><span class=p>,</span> <span class=mi>7</span><span class=p>};</span>
</span></span></code></pre></div><span class=anchor id=broadcast></span><h3><a class=anchor-link href=http://jyang772.github.io/hugo-page/hpc/simd/moving/#broadcast>#</a>Broadcast</h3><p>Instead of modifying just one element, you can also <em>broadcast</em> a single value into all its positions:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-nasm data-lang=nasm><span class=line><span class=cl><span class=c1>; __m256i v = _mm256_set1_epi32(42);</span>
</span></span><span class=line><span class=cl><span class=nf>mov</span>          <span class=nb>eax</span><span class=p>,</span> <span class=mi>42</span>
</span></span><span class=line><span class=cl><span class=nf>vmovd</span>        <span class=nv>xmm0</span><span class=p>,</span> <span class=nb>eax</span>
</span></span><span class=line><span class=cl><span class=nf>vpbroadcastd</span> <span class=nv>ymm0</span><span class=p>,</span> <span class=nv>xmm0</span>
</span></span></code></pre></div><p>This is a frequently used operation, so you can also use a memory location:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-nasm data-lang=nasm><span class=line><span class=cl><span class=c1>; __m256 v = _mm256_broadcast_ss(&amp;a[i]);</span>
</span></span><span class=line><span class=cl><span class=nf>vbroadcastss</span> <span class=nv>ymm0</span><span class=p>,</span> <span class=kt>DWORD</span> <span class=nv>PTR</span> <span class=p>[</span><span class=nb>rdi</span><span class=p>]</span>
</span></span></code></pre></div><p>When using built-in vector types, you can create a zero vector and add a scalar to it:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>vec</span> <span class=n>v</span> <span class=o>=</span> <span class=mi>42</span> <span class=o>+</span> <span class=n>vec</span><span class=p>{};</span>
</span></span></code></pre></div><span class=anchor id=mapping-to-arrays></span><h3><a class=anchor-link href=http://jyang772.github.io/hugo-page/hpc/simd/moving/#mapping-to-arrays>#</a>Mapping to Arrays</h3><p>If you want to avoid all this complexity, you can just dump the vector in memory and read its values back as scalars:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=kt>void</span> <span class=nf>print</span><span class=p>(</span><span class=n>__m256i</span> <span class=n>v</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>auto</span> <span class=n>t</span> <span class=o>=</span> <span class=p>(</span><span class=kt>unsigned</span><span class=o>*</span><span class=p>)</span> <span class=o>&amp;</span><span class=n>v</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=mi>8</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>bitset</span><span class=o>&lt;</span><span class=mi>32</span><span class=o>&gt;</span><span class=p>(</span><span class=n>t</span><span class=p>[</span><span class=n>i</span><span class=p>])</span> <span class=o>&lt;&lt;</span> <span class=s>&#34; &#34;</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>This may not be fast or technically legal (the C++ standard doesn&rsquo;t specify what happens when you cast data like this), but it is simple, and I frequently use this code to print out the contents of a vector during debugging.</p><span class=anchor id=non-contiguous-load></span><h3><a class=anchor-link href=http://jyang772.github.io/hugo-page/hpc/simd/moving/#non-contiguous-load>#</a>Non-Contiguous Load</h3><p>Later SIMD extensions added special &ldquo;gather&rdquo; and &ldquo;scatter instructions that read/write data non-sequentially using arbitrary array indices. These don&rsquo;t work 8 times faster though and are usually limited by the memory rather than the CPU, but they are still helpful for certain applications such as sparse linear algebra.</p><p>Gather is available since AVX2, and various scatter instructions are available since AVX512.</p><p><figure><img src=../img/gather-scatter.png><figcaption></figcaption></figure></p><p>Let&rsquo;s see if they work faster than scalar reads. First, we create an array of size $N$ and $Q$ random read queries:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=kt>int</span> <span class=n>a</span><span class=p>[</span><span class=n>N</span><span class=p>],</span> <span class=n>q</span><span class=p>[</span><span class=n>Q</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>a</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>rand</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>Q</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>q</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>rand</span><span class=p>()</span> <span class=o>%</span> <span class=n>N</span><span class=p>;</span>
</span></span></code></pre></div><p>In the scalar code, we add the elements specified by the queries to a checksum one by one:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=kt>int</span> <span class=n>s</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>Q</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>s</span> <span class=o>+=</span> <span class=n>a</span><span class=p>[</span><span class=n>q</span><span class=p>[</span><span class=n>i</span><span class=p>]];</span>
</span></span></code></pre></div><p>And in the SIMD code, we use the <code>gather</code> instruction to do that for 8 different indexes in parallel:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>reg</span> <span class=n>s</span> <span class=o>=</span> <span class=n>_mm256_setzero_si256</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>Q</span><span class=p>;</span> <span class=n>i</span> <span class=o>+=</span> <span class=mi>8</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>reg</span> <span class=n>idx</span> <span class=o>=</span> <span class=n>_mm256_load_si256</span><span class=p>(</span> <span class=p>(</span><span class=n>reg</span><span class=o>*</span><span class=p>)</span> <span class=o>&amp;</span><span class=n>q</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>reg</span> <span class=n>x</span> <span class=o>=</span> <span class=n>_mm256_i32gather_epi32</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>idx</span><span class=p>,</span> <span class=mi>4</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>s</span> <span class=o>=</span> <span class=n>_mm256_add_epi32</span><span class=p>(</span><span class=n>s</span><span class=p>,</span> <span class=n>x</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>They perform roughly the same, except when the array fits into the L1 cache:</p><p><figure><img src=../img/gather.svg><figcaption></figcaption></figure></p><p>The purpose of <code>gather</code> and <code>scatter</code> is not to perform memory operations faster, but to get the data into registers to perform heavy computations on them. For anything costlier than just one addition, they are hugely favorable.</p><p>The lack of (fast) gather and scatter instructions makes SIMD programming on CPUs very different from proper parallel computing environments that support independent memory access. You have to always engineer around it and employ various ways of organizing your data sequentially so that it be loaded into registers.</p></article><div class=nextprev><div class=left><a href=http://jyang772.github.io/hugo-page/hpc/simd/intrinsics/ id=prev-article>← Intrinsics and Vector Types</a></div><div class=right><a href=http://jyang772.github.io/hugo-page/hpc/simd/reduction/ id=next-article>Reductions →</a></div></div></main><footer>Copyright 2021–2022 Sergey Slotin<br></footer></div></body></html>