<!doctype html><html lang=en-us><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-WBN59M8Y5S"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-WBN59M8Y5S")</script><script type=text/javascript>(function(e,t,n,s,o,i,a){e[o]=e[o]||function(){(e[o].a=e[o].a||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)})(window,document,"script","https://mc.yandex.ru/metrika/tag.js","ym"),ym(53961409,"init",{clickmap:!0,trackLinks:!0,accurateTrackBounce:!0,webvisor:!0})</script><noscript><div><img src=https://mc.yandex.ru/watch/53961409 style=position:absolute;left:-9999px alt></div></noscript><meta charset=utf-8><link rel=stylesheet href=/style.min.a3a4a7a8e8602aaa85b7cb3d655edde028ac80d73f2a97389e2cbcf995dd672d.css integrity="sha256-o6SnqOhgKqqFt8s9ZV7d4CisgNc/Kpc4niy8+ZXdZy0="><link rel=stylesheet href=/syntax.css id=syntax-theme><link rel=stylesheet type=text/css href=https://tikzjax.com/v1/fonts.css><script src=https://tikzjax.com/v1/tikzjax.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.9/lunr.min.js></script><script src=/scripts/lunr.stemmer.support.min.js></script><script src=/scripts/lunr.ru.min.js></script><script src=/scripts/lunr.multi.min.js></script><link rel=stylesheet id=theme><script>function toggleSidebar(){console.log("Toggling sidebar visibility");var e=document.getElementById("sidebar"),t=document.getElementById("wrapper");(e.classList.contains("sidebar-toggled")||window.getComputedStyle(e).display=="block")&&(e.classList.toggle("sidebar-hidden"),t.classList.toggle("sidebar-hidden")),e.classList.add("sidebar-toggled"),t.classList.add("sidebar-toggled")}function switchTheme(e){console.log("Changing theme:",e),document.getElementById("theme").href=e=="dark"?"/dark.min.b3ae1169831434b11b48de5b3e3210547eea6b7884c295ab0030cb973ea0dc49.css":"",document.getElementById("syntax-theme").href=e=="dark"?"/syntax-dark.css":"/syntax.css",localStorage.setItem("theme",e)}async function toggleSearch(){console.log("Toggling search");var e=document.getElementById("search");if(window.getComputedStyle(e).display=="none"?(e.style.display="block",window.scrollTo({top:0}),document.getElementById("search-bar").focus()):e.style.display="none",!index){console.log("Fetching index");const e=await fetch("/searchindex.json"),t=await e.json();index=lunr(function(){this.use(lunr.multiLanguage("en","ru")),this.field("title",{boost:5}),this.field("content",{boost:1}),t.forEach(function(e){this.add(e),articles.push(e)},this)}),console.log("Ready to search")}}var articles=[],index=void 0;function search(){var n,e=document.getElementById("search-bar").value,s=document.getElementById("search-results"),o=document.getElementById("search-count");if(e==""){s.innerHTML="",o.innerHTML="";return}n=index.search(e),o.innerHTML="Found <b>"+n.length+"</b> pages";let t="";for(const a in n){const i=articles[n[a].ref];t+='<li><a href="'+i.path+'">'+i.title+"</a> <p>";const s=i.content,o=80;if(s.includes(e)){const n=s.indexOf(e);n>o&&(t+="…"),t+=s.substring(n-o,n)+"<b>"+e+"</b>"+s.substring(n+e.length,n+e.length+o)}else t+=s.substring(0,o*2);t+="…</p></li>"}s.innerHTML=t}localStorage.getItem("theme")=="dark"&&switchTheme("dark"),window.addEventListener("load",function(){var e=document.getElementById("active-element");e&&e.scrollIntoView({block:"center"})}),window.addEventListener("scroll",function(){var e=document.getElementById("menu");window.scrollY<120?e.classList.remove("scrolled"):e.classList.add("scrolled")}),window.addEventListener("keydown",function(e){if(e.altKey)return;if(document.activeElement.tagName=="INPUT")return;e.key=="ArrowLeft"?document.getElementById("prev-article").click():e.key=="ArrowRight"&&document.getElementById("next-article").click()})</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})'></script><title>Fast Inverse Square Root - Algorithmica</title></head><body><nav id=sidebar><div class=title><a href=/>Algorithmica</a>
<span class=slash>/</span>
<a href=/hpc/ class=divisionAbbr>HPC</a></div><ul><li class=part>Performance Engineering</li><li><a href=/hpc/complexity/>Complexity Models</a></li><ol><li><a href=/hpc/complexity/hardware/>Modern Hardware</a></li><li><a href=/hpc/complexity/languages/>Programming Languages</a></li></ol><li><a href=/hpc/architecture/>Computer Architecture</a></li><ol><li><a href=/hpc/architecture/isa/>Instruction Set Architectures</a></li><li><a href=/hpc/architecture/assembly/>Assembly Language</a></li><li><a href=/hpc/architecture/loops/>Loops and Conditionals</a></li><li><a href=/hpc/architecture/functions/>Functions and Recursion</a></li><li><a href=/hpc/architecture/indirect/>Indirect Branching</a></li><li><a href=/hpc/architecture/layout/>Machine Code Layout</a></li></ol><li><a href=/hpc/pipelining/>Instruction-Level Parallelism</a></li><ol><li><a href=/hpc/pipelining/hazards/>Pipeline Hazards</a></li><li><a href=/hpc/pipelining/branching/>The Cost of Branching</a></li><li><a href=/hpc/pipelining/branchless/>Branchless Programming</a></li><li><a href=/hpc/pipelining/tables/>Instruction Tables</a></li><li><a href=/hpc/pipelining/throughput/>Throughput Computing</a></li></ol><li><a href=/hpc/compilation/>Compilation</a></li><ol><li><a href=/hpc/compilation/stages/>Stages of Compilation</a></li><li><a href=/hpc/compilation/flags/>Flags and Targets</a></li><li><a href=/hpc/compilation/situational/>Situational Optimizations</a></li><li><a href=/hpc/compilation/contracts/>Contract Programming</a></li><li><a href=/hpc/compilation/precalc/>Precomputation</a></li></ol><li><a href=/hpc/profiling/>Profiling</a></li><ol><li><a href=/hpc/profiling/instrumentation/>Instrumentation</a></li><li><a href=/hpc/profiling/events/>Statistical Profiling</a></li><li><a href=/hpc/profiling/simulation/>Program Simulation</a></li><li><a href=/hpc/profiling/mca/>Machine Code Analyzers</a></li><li><a href=/hpc/profiling/benchmarking/>Benchmarking</a></li><li><a href=/hpc/profiling/noise/>Getting Accurate Results</a></li></ol><li><a href=/hpc/arithmetic/>Arithmetic</a></li><ol><li><a href=/hpc/arithmetic/float/>Floating-Point Numbers</a></li><li><a href=/hpc/arithmetic/ieee-754/>IEEE 754 Floats</a></li><li><a href=/hpc/arithmetic/errors/>Rounding Errors</a></li><li><a href=/hpc/arithmetic/newton/>Newton's Method</a></li><li><a href=/hpc/arithmetic/rsqrt/ id=active-element>Fast Inverse Square Root</a></li><li><a href=/hpc/arithmetic/integer/>Integer Numbers</a></li><li><a href=/hpc/arithmetic/division/>Integer Division</a></li></ol><li><a href=/hpc/number-theory/>Number Theory</a></li><ol><li><a href=/hpc/number-theory/modular/>Modular Arithmetic</a></li><li><a href=/hpc/number-theory/exponentiation/>Binary Exponentiation</a></li><li><a href=/hpc/number-theory/euclid-extended/>Extended Euclidean Algorithm</a></li><li><a href=/hpc/number-theory/montgomery/>Montgomery Multiplication</a></li></ol><li><a href=/hpc/external-memory/>External Memory</a></li><ol><li><a href=/hpc/external-memory/hierarchy/>Memory Hierarchy</a></li><li><a href=/hpc/external-memory/virtual/>Virtual Memory</a></li><li><a href=/hpc/external-memory/model/>External Memory Model</a></li><li><a href=/hpc/external-memory/sorting/>External Sorting</a></li><li><a href=/hpc/external-memory/list-ranking/>List Ranking</a></li><li><a href=/hpc/external-memory/policies/>Eviction Policies</a></li><li><a href=/hpc/external-memory/oblivious/>Cache-Oblivious Algorithms</a></li><li><a href=/hpc/external-memory/locality/>Spatial and Temporal Locality</a></li></ol><li><a href=/hpc/cpu-cache/>RAM & CPU Caches</a></li><ol><li><a href=/hpc/cpu-cache/bandwidth/>Memory Bandwidth</a></li><li><a href=/hpc/cpu-cache/latency/>Memory Latency</a></li><li><a href=/hpc/cpu-cache/cache-lines/>Cache Lines</a></li><li><a href=/hpc/cpu-cache/sharing/>Memory Sharing</a></li><li><a href=/hpc/cpu-cache/mlp/>Memory-Level Parallelism</a></li><li><a href=/hpc/cpu-cache/prefetching/>Prefetching</a></li><li><a href=/hpc/cpu-cache/alignment/>Alignment and Packing</a></li><li><a href=/hpc/cpu-cache/pointers/>Pointer Alternatives</a></li><li><a href=/hpc/cpu-cache/associativity/>Cache Associativity</a></li><li><a href=/hpc/cpu-cache/paging/>Memory Paging</a></li><li><a href=/hpc/cpu-cache/aos-soa/>AoS and SoA</a></li></ol><li><a href=/hpc/simd/>SIMD Parallelism</a></li><ol><li><a href=/hpc/simd/intrinsics/>Intrinsics and Vector Types</a></li><li><a href=/hpc/simd/moving/>Moving Data</a></li><li><a href=/hpc/simd/reduction/>Reductions</a></li><li><a href=/hpc/simd/masking/>Masking and Blending</a></li><li><a href=/hpc/simd/shuffling/>In-Register Shuffles</a></li><li><a href=/hpc/simd/auto-vectorization/>Auto-Vectorization and SPMD</a></li></ol><li><a href=/hpc/algorithms/>Algorithms Case Studies</a></li><ol><li><a href=/hpc/algorithms/gcd/>Binary GCD</a></li><li><a href=/hpc/algorithms/factorization/>Integer Factorization</a></li><li><a href=/hpc/algorithms/argmin/>Argmin with SIMD</a></li><li><a href=/hpc/algorithms/prefix/>Prefix Sum with SIMD</a></li><li><a href=/hpc/algorithms/matmul/>Matrix Multiplication</a></li></ol><li><a href=/hpc/data-structures/>Data Structures Case Studies</a></li><ol><li><a href=/hpc/data-structures/binary-search/>Binary Search</a></li><li><a href=/hpc/data-structures/s-tree/>Static B-Trees</a></li><li><a href=/hpc/data-structures/b-tree/>Search Trees</a></li><li><a href=/hpc/data-structures/segment-trees/>Segment Trees</a></li></ol></ul></nav><div id=wrapper><menu id=menu><div class=left><a><img src=/icons/bars-solid.svg onclick=toggleSidebar() title='open table of contents'>
</a><a><img src=/icons/adjust-solid.svg style=position:relative;top:-1px onclick='switchTheme(localStorage.getItem("theme")=="dark"?"light":"dark")' title='dark theme'>
</a><a><img src=/icons/search-solid.svg onclick=toggleSearch() title=search></a></div><div class=title>Fast Inverse Square Root</div><div class=right><a onclick=window.print()><img src=/icons/print-solid.svg title=print>
</a><a href=https://prose.io/#algorithmica-org/algorithmica/edit/master//hpc%2farithmetic%2frsqrt.md><img src=/icons/edit-solid.svg title=edit style=width:18px;position:relative;right:-2px;top:-1px>
</a><a href=https://github.com/algorithmica-org/algorithmica/blob/master//hpc/arithmetic/rsqrt.md class=github-main><img src=/icons/github-brands.svg title='view on github'></a></div></menu><main><div id=search><input id=search-bar type=search placeholder='Search this book…' oninput=search()><div id=search-count></div><div id=search-results></div></div><header><h1>Fast Inverse Square Root</h1><div class=info></div></header><article>The inverse square root of a floating-point number $\frac{1}{\sqrt x}$ is used in calculating normalized vectors, which are in turn extensively used in various simulation scenarios such as computer graphics (e.g., to determine angles of incidence and reflection to simulate lighting).
$$
\hat{v} = \frac{\vec v}{\sqrt {v_x^2 + v_y^2 + v_z^2}}
$$<p>Calculating an inverse square root directly — by first calculating a square root and then dividing $1$ by it — is extremely slow because both of these operations are slow even though they are implemented in hardware.</p><p>But there is a surprisingly good approximation algorithm that takes advantage of the way floating-point numbers are stored in memory. In fact, it is so good that it has been <a href=https://www.felixcloutier.com/x86/rsqrtps>implemented in hardware</a>, so the algorithm is no longer relevant by itself for software engineers, but we are nonetheless going to walk through it for its intrinsic beauty and great educational value.</p><p>Apart from the method itself, quite interesting is the history of its creation. It is attributed to a game studio <em>id Software</em> that used it in their iconic 1999 game <em>Quake III Arena</em>, although apparently, it got there by a chain of &ldquo;I learned it from a guy who learned it from a guy&rdquo; that seems to end on William Kahan (the same one that is responsible for IEEE 754 and Kahan summation algorithm).</p><p>It became popular in game developing community around 2005 when they released the source code of the game. Here is <a href=https://github.com/id-Software/Quake-III-Arena/blob/master/code/game/q_math.c#L552>the relevant excerpt from it</a>, including the comments:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=kt>float</span> <span class=nf>Q_rsqrt</span><span class=p>(</span><span class=kt>float</span> <span class=n>number</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>long</span> <span class=n>i</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>float</span> <span class=n>x2</span><span class=p>,</span> <span class=n>y</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>float</span> <span class=n>threehalfs</span> <span class=o>=</span> <span class=mf>1.5F</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>x2</span> <span class=o>=</span> <span class=n>number</span> <span class=o>*</span> <span class=mf>0.5F</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span>  <span class=o>=</span> <span class=n>number</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>i</span>  <span class=o>=</span> <span class=o>*</span> <span class=p>(</span> <span class=kt>long</span> <span class=o>*</span> <span class=p>)</span> <span class=o>&amp;</span><span class=n>y</span><span class=p>;</span>                       <span class=c1>// evil floating point bit level hacking
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>i</span>  <span class=o>=</span> <span class=mh>0x5f3759df</span> <span class=o>-</span> <span class=p>(</span> <span class=n>i</span> <span class=o>&gt;&gt;</span> <span class=mi>1</span> <span class=p>);</span>               <span class=c1>// what the fuck? 
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>y</span>  <span class=o>=</span> <span class=o>*</span> <span class=p>(</span> <span class=kt>float</span> <span class=o>*</span> <span class=p>)</span> <span class=o>&amp;</span><span class=n>i</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span>  <span class=o>=</span> <span class=n>y</span> <span class=o>*</span> <span class=p>(</span> <span class=n>threehalfs</span> <span class=o>-</span> <span class=p>(</span> <span class=n>x2</span> <span class=o>*</span> <span class=n>y</span> <span class=o>*</span> <span class=n>y</span> <span class=p>)</span> <span class=p>);</span>   <span class=c1>// 1st iteration
</span></span></span><span class=line><span class=cl><span class=c1>//  y  = y * ( threehalfs - ( x2 * y * y ) );   // 2nd iteration, this can be removed
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>y</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>We will go through what it does step by step, but first, we need to take a small detour.</p><span class=anchor id=approximate-logarithm></span><h3><a class=anchor-link href=https://en.algorithmica.org/hpc/arithmetic/rsqrt/#approximate-logarithm>#</a>Approximate Logarithm</h3><p>Before computers (or at least affordable calculators) became an everyday thing, people computed multiplication and related operations using logarithm tables — by looking up the logarithms of $a$ and $b$, adding them, and then finding the inverse logarithm of the result.</p>$$
a \times b = 10^{\log a + \log b} = \log^{-1}(\log a + \log b)
$$
You can do the same trick when computing $\frac{1}{\sqrt x}$ using the identity:
$$
\log \frac{1}{\sqrt x} = - \frac{1}{2} \log x
$$<p>The fast inverse square root is based on this identity, and so it needs to calculate the logarithm of $x$ very quickly. Turns out, it can be approximated by just reinterpreting a 32-bit <code>float</code> as an integer.</p><p><a href=../float>Recall</a> that floating-point numbers sequentially store the sign bit (equal to zero for positive values, which is our case), exponent $e_x$ and mantissa $m_x$, which corresponds to</p>$$
x = 2^{e_x} \cdot (1 + m_x)
$$
Its logarithm is therefore
$$
\log_2 x = e_x + \log_2 (1 + m_x)
$$
Since $m_x \in [0, 1)$, the logarithm on the right-hand side can be approximated by
$$
\log_2 (1 + m_x) \approx m_x
$$
The approximation is exact at both ends of the intervals, but to account for the average case we need to shift it by a small constant $\sigma$, therefore
$$
\log_2 x = e_x + \log_2 (1 + m_x) \approx e_x + m_x + \sigma
$$
Now, having this approximation in mind and defining $L=2^{23}$ (the number of mantissa bits in a <code>float</code>) and $B=127$ (the exponent bias), when we reinterpret the bit-pattern of $x$ as an integer $I_x$, we essentially get
$$
\begin{aligned}
I_x &= L \cdot (e_x + B + m_x)
\\  &= L \cdot (e_x + m_x + \sigma +B-\sigma )
\\ &\approx L \cdot \log_2 (x) + L \cdot (B-\sigma )
\end{aligned}
$$<p>(Multiplying an integer by $L=2^{23}$ is equivalent to left-shifting it by 23.)</p><p>When you tune $\sigma$ to minimize the mean square error, this results in a surprisingly accurate approximation.</p><p><figure><img src=../img/approx.svg><figcaption>Reinterpreting a floating-point number $x$ as an integer (blue) compared to its scaled and shifted logarithm (gray)</figcaption></figure></p><p>Now, expressing the logarithm from the approximation, we get</p>$$
\log_2 x \approx \frac{I_x}{L} - (B - \sigma)
$$<p>Cool. Now, where were we? Oh, yes, we wanted to calculate the inverse square root.</p><span class=anchor id=approximating-the-result></span><h3><a class=anchor-link href=https://en.algorithmica.org/hpc/arithmetic/rsqrt/#approximating-the-result>#</a>Approximating the Result</h3><p>To calculate $y = \frac{1}{\sqrt x}$ using the identity $\log_2 y = - \frac{1}{2} \log_2 x$, we can plug it into our approximation formula and get</p>$$
\frac{I_y}{L} - (B - \sigma)
\approx
- \frac{1}{2} ( \frac{I_x}{L} - (B - \sigma) )
$$
Solving for $I_y$:
$$
I_y \approx \frac{3}{2} L (B - \sigma) - \frac{1}{2} I_x
$$<p>It turns out, we don&rsquo;t even need to calculate the logarithm in the first place: the formula above is just a constant minus half the integer reinterpretation of $x$. It is written in the code as:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>i</span> <span class=o>=</span> <span class=o>*</span> <span class=p>(</span> <span class=kt>long</span> <span class=o>*</span> <span class=p>)</span> <span class=o>&amp;</span><span class=n>y</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>i</span> <span class=o>=</span> <span class=mh>0x5f3759df</span> <span class=o>-</span> <span class=p>(</span> <span class=n>i</span> <span class=o>&gt;&gt;</span> <span class=mi>1</span> <span class=p>);</span>
</span></span></code></pre></div><p>We reinterpret <code>y</code> as an integer in the first line, and then it plug into the formula on the second, the first term of which is the magic number $\frac{3}{2} L (B - \sigma) = \mathtt{0x5F3759DF}$, while the second is calculated with a binary shift instead of division.</p><span class=anchor id=iterating-with-newtons-method></span><h3><a class=anchor-link href=https://en.algorithmica.org/hpc/arithmetic/rsqrt/#iterating-with-newtons-method>#</a>Iterating with Newton&rsquo;s Method</h3><p>What we have next is a couple hand-coded iterations of Newton&rsquo;s method with $f(y) = \frac{1}{y^2} - x$ and a very good initial value. Its update rule is</p>$$
f'(y) = - \frac{2}{y^3} \implies y_{i+1} = y_{i} (\frac{3}{2} - \frac{x}{2} y_i^2) = \frac{y_i (3 - x y_i^2)}{2}
$$<p>which is written in the code as</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>x2</span> <span class=o>=</span> <span class=n>number</span> <span class=o>*</span> <span class=mf>0.5F</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>y</span>  <span class=o>=</span> <span class=n>y</span> <span class=o>*</span> <span class=p>(</span> <span class=n>threehalfs</span> <span class=o>-</span> <span class=p>(</span> <span class=n>x2</span> <span class=o>*</span> <span class=n>y</span> <span class=o>*</span> <span class=n>y</span> <span class=p>)</span> <span class=p>);</span>
</span></span></code></pre></div><p>The initial approximation is so good that just one iteration was enough for game development purposes. It falls within 99.8% of the correct answer after just the first iteration and can be reiterated further to improve accuracy — which is what is done in the hardware: <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#ig_expand=3037,3009,5135,4870,4870,4872,4875,833,879,874,849,848,6715,4845,6046,3853,288,6570,6527,6527,90,7307,6385,5993&amp;text=rsqrt&amp;techs=AVX,AVX2">the x86 instruction</a> does a few of them and guarantees a relative error of no more than $1.5 \times 2^{-12}$.</p><span class=anchor id=further-reading></span><h3><a class=anchor-link href=https://en.algorithmica.org/hpc/arithmetic/rsqrt/#further-reading>#</a>Further Reading</h3><p><a href=https://en.wikipedia.org/wiki/Fast_inverse_square_root#Floating-point_representation>Wikipedia article on fast inverse square root</a>.</p></article><div class=nextprev><div class=left><a href=https://en.algorithmica.org/hpc/arithmetic/newton/ id=prev-article>← Newton's Method</a></div><div class=right><a href=https://en.algorithmica.org/hpc/arithmetic/integer/ id=next-article>Integer Numbers →</a></div></div></main><footer>Copyright 2021–2022 Sergey Slotin<br></footer></div></body></html>