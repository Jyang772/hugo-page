<!doctype html><html lang=en-us><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-WBN59M8Y5S"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-WBN59M8Y5S")</script><script type=text/javascript>(function(e,t,n,s,o,i,a){e[o]=e[o]||function(){(e[o].a=e[o].a||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)})(window,document,"script","https://mc.yandex.ru/metrika/tag.js","ym"),ym(53961409,"init",{clickmap:!0,trackLinks:!0,accurateTrackBounce:!0,webvisor:!0})</script><noscript><div><img src=https://mc.yandex.ru/watch/53961409 style=position:absolute;left:-9999px alt></div></noscript><meta charset=utf-8><link rel=stylesheet href=/style.min.a3a4a7a8e8602aaa85b7cb3d655edde028ac80d73f2a97389e2cbcf995dd672d.css integrity="sha256-o6SnqOhgKqqFt8s9ZV7d4CisgNc/Kpc4niy8+ZXdZy0="><link rel=stylesheet href=/syntax.css id=syntax-theme><link rel=stylesheet type=text/css href=https://tikzjax.com/v1/fonts.css><script src=https://tikzjax.com/v1/tikzjax.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.9/lunr.min.js></script><script src=/scripts/lunr.stemmer.support.min.js></script><script src=/scripts/lunr.ru.min.js></script><script src=/scripts/lunr.multi.min.js></script><link rel=stylesheet id=theme><script>function toggleSidebar(){console.log("Toggling sidebar visibility");var e=document.getElementById("sidebar"),t=document.getElementById("wrapper");(e.classList.contains("sidebar-toggled")||window.getComputedStyle(e).display=="block")&&(e.classList.toggle("sidebar-hidden"),t.classList.toggle("sidebar-hidden")),e.classList.add("sidebar-toggled"),t.classList.add("sidebar-toggled")}function switchTheme(e){console.log("Changing theme:",e),document.getElementById("theme").href=e=="dark"?"/dark.min.b3ae1169831434b11b48de5b3e3210547eea6b7884c295ab0030cb973ea0dc49.css":"",document.getElementById("syntax-theme").href=e=="dark"?"/syntax-dark.css":"/syntax.css",localStorage.setItem("theme",e)}async function toggleSearch(){console.log("Toggling search");var e=document.getElementById("search");if(window.getComputedStyle(e).display=="none"?(e.style.display="block",window.scrollTo({top:0}),document.getElementById("search-bar").focus()):e.style.display="none",!index){console.log("Fetching index");const e=await fetch("/searchindex.json"),t=await e.json();index=lunr(function(){this.use(lunr.multiLanguage("en","ru")),this.field("title",{boost:5}),this.field("content",{boost:1}),t.forEach(function(e){this.add(e),articles.push(e)},this)}),console.log("Ready to search")}}var articles=[],index=void 0;function search(){var n,e=document.getElementById("search-bar").value,s=document.getElementById("search-results"),o=document.getElementById("search-count");if(e==""){s.innerHTML="",o.innerHTML="";return}n=index.search(e),o.innerHTML="Found <b>"+n.length+"</b> pages";let t="";for(const a in n){const i=articles[n[a].ref];t+='<li><a href="'+i.path+'">'+i.title+"</a> <p>";const s=i.content,o=80;if(s.includes(e)){const n=s.indexOf(e);n>o&&(t+="…"),t+=s.substring(n-o,n)+"<b>"+e+"</b>"+s.substring(n+e.length,n+e.length+o)}else t+=s.substring(0,o*2);t+="…</p></li>"}s.innerHTML=t}localStorage.getItem("theme")=="dark"&&switchTheme("dark"),window.addEventListener("load",function(){var e=document.getElementById("active-element");e&&e.scrollIntoView({block:"center"})}),window.addEventListener("scroll",function(){var e=document.getElementById("menu");window.scrollY<120?e.classList.remove("scrolled"):e.classList.add("scrolled")}),window.addEventListener("keydown",function(e){if(e.altKey)return;if(document.activeElement.tagName=="INPUT")return;e.key=="ArrowLeft"?document.getElementById("prev-article").click():e.key=="ArrowRight"&&document.getElementById("next-article").click()})</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})'></script><title>Prefetching - Algorithmica</title></head><body><nav id=sidebar><div class=title><a href=/>Algorithmica</a>
<span class=slash>/</span>
<a href=/hpc/ class=divisionAbbr>HPC</a></div><ul><li class=part>Performance Engineering</li><li><a href=/hpc/complexity/>Complexity Models</a></li><ol><li><a href=/hpc/complexity/hardware/>Modern Hardware</a></li><li><a href=/hpc/complexity/languages/>Programming Languages</a></li></ol><li><a href=/hpc/architecture/>Computer Architecture</a></li><ol><li><a href=/hpc/architecture/isa/>Instruction Set Architectures</a></li><li><a href=/hpc/architecture/assembly/>Assembly Language</a></li><li><a href=/hpc/architecture/loops/>Loops and Conditionals</a></li><li><a href=/hpc/architecture/functions/>Functions and Recursion</a></li><li><a href=/hpc/architecture/indirect/>Indirect Branching</a></li><li><a href=/hpc/architecture/layout/>Machine Code Layout</a></li></ol><li><a href=/hpc/pipelining/>Instruction-Level Parallelism</a></li><ol><li><a href=/hpc/pipelining/hazards/>Pipeline Hazards</a></li><li><a href=/hpc/pipelining/branching/>The Cost of Branching</a></li><li><a href=/hpc/pipelining/branchless/>Branchless Programming</a></li><li><a href=/hpc/pipelining/tables/>Instruction Tables</a></li><li><a href=/hpc/pipelining/throughput/>Throughput Computing</a></li></ol><li><a href=/hpc/compilation/>Compilation</a></li><ol><li><a href=/hpc/compilation/stages/>Stages of Compilation</a></li><li><a href=/hpc/compilation/flags/>Flags and Targets</a></li><li><a href=/hpc/compilation/situational/>Situational Optimizations</a></li><li><a href=/hpc/compilation/contracts/>Contract Programming</a></li><li><a href=/hpc/compilation/precalc/>Precomputation</a></li></ol><li><a href=/hpc/profiling/>Profiling</a></li><ol><li><a href=/hpc/profiling/instrumentation/>Instrumentation</a></li><li><a href=/hpc/profiling/events/>Statistical Profiling</a></li><li><a href=/hpc/profiling/simulation/>Program Simulation</a></li><li><a href=/hpc/profiling/mca/>Machine Code Analyzers</a></li><li><a href=/hpc/profiling/benchmarking/>Benchmarking</a></li><li><a href=/hpc/profiling/noise/>Getting Accurate Results</a></li></ol><li><a href=/hpc/arithmetic/>Arithmetic</a></li><ol><li><a href=/hpc/arithmetic/float/>Floating-Point Numbers</a></li><li><a href=/hpc/arithmetic/ieee-754/>IEEE 754 Floats</a></li><li><a href=/hpc/arithmetic/errors/>Rounding Errors</a></li><li><a href=/hpc/arithmetic/newton/>Newton's Method</a></li><li><a href=/hpc/arithmetic/rsqrt/>Fast Inverse Square Root</a></li><li><a href=/hpc/arithmetic/integer/>Integer Numbers</a></li><li><a href=/hpc/arithmetic/division/>Integer Division</a></li></ol><li><a href=/hpc/number-theory/>Number Theory</a></li><ol><li><a href=/hpc/number-theory/modular/>Modular Arithmetic</a></li><li><a href=/hpc/number-theory/exponentiation/>Binary Exponentiation</a></li><li><a href=/hpc/number-theory/euclid-extended/>Extended Euclidean Algorithm</a></li><li><a href=/hpc/number-theory/montgomery/>Montgomery Multiplication</a></li></ol><li><a href=/hpc/external-memory/>External Memory</a></li><ol><li><a href=/hpc/external-memory/hierarchy/>Memory Hierarchy</a></li><li><a href=/hpc/external-memory/virtual/>Virtual Memory</a></li><li><a href=/hpc/external-memory/model/>External Memory Model</a></li><li><a href=/hpc/external-memory/sorting/>External Sorting</a></li><li><a href=/hpc/external-memory/list-ranking/>List Ranking</a></li><li><a href=/hpc/external-memory/policies/>Eviction Policies</a></li><li><a href=/hpc/external-memory/oblivious/>Cache-Oblivious Algorithms</a></li><li><a href=/hpc/external-memory/locality/>Spatial and Temporal Locality</a></li></ol><li><a href=/hpc/cpu-cache/>RAM & CPU Caches</a></li><ol><li><a href=/hpc/cpu-cache/bandwidth/>Memory Bandwidth</a></li><li><a href=/hpc/cpu-cache/latency/>Memory Latency</a></li><li><a href=/hpc/cpu-cache/cache-lines/>Cache Lines</a></li><li><a href=/hpc/cpu-cache/sharing/>Memory Sharing</a></li><li><a href=/hpc/cpu-cache/mlp/>Memory-Level Parallelism</a></li><li><a href=/hpc/cpu-cache/prefetching/ id=active-element>Prefetching</a></li><li><a href=/hpc/cpu-cache/alignment/>Alignment and Packing</a></li><li><a href=/hpc/cpu-cache/pointers/>Pointer Alternatives</a></li><li><a href=/hpc/cpu-cache/associativity/>Cache Associativity</a></li><li><a href=/hpc/cpu-cache/paging/>Memory Paging</a></li><li><a href=/hpc/cpu-cache/aos-soa/>AoS and SoA</a></li></ol><li><a href=/hpc/simd/>SIMD Parallelism</a></li><ol><li><a href=/hpc/simd/intrinsics/>Intrinsics and Vector Types</a></li><li><a href=/hpc/simd/moving/>Moving Data</a></li><li><a href=/hpc/simd/reduction/>Reductions</a></li><li><a href=/hpc/simd/masking/>Masking and Blending</a></li><li><a href=/hpc/simd/shuffling/>In-Register Shuffles</a></li><li><a href=/hpc/simd/auto-vectorization/>Auto-Vectorization and SPMD</a></li></ol><li><a href=/hpc/algorithms/>Algorithms Case Studies</a></li><ol><li><a href=/hpc/algorithms/gcd/>Binary GCD</a></li><li><a href=/hpc/algorithms/factorization/>Integer Factorization</a></li><li><a href=/hpc/algorithms/argmin/>Argmin with SIMD</a></li><li><a href=/hpc/algorithms/prefix/>Prefix Sum with SIMD</a></li><li><a href=/hpc/algorithms/matmul/>Matrix Multiplication</a></li></ol><li><a href=/hpc/data-structures/>Data Structures Case Studies</a></li><ol><li><a href=/hpc/data-structures/binary-search/>Binary Search</a></li><li><a href=/hpc/data-structures/s-tree/>Static B-Trees</a></li><li><a href=/hpc/data-structures/b-tree/>Search Trees</a></li><li><a href=/hpc/data-structures/segment-trees/>Segment Trees</a></li></ol></ul></nav><div id=wrapper><menu id=menu><div class=left><a><img src=/icons/bars-solid.svg onclick=toggleSidebar() title='open table of contents'>
</a><a><img src=/icons/adjust-solid.svg style=position:relative;top:-1px onclick='switchTheme(localStorage.getItem("theme")=="dark"?"light":"dark")' title='dark theme'>
</a><a><img src=/icons/search-solid.svg onclick=toggleSearch() title=search></a></div><div class=title>Prefetching</div><div class=right><a onclick=window.print()><img src=/icons/print-solid.svg title=print>
</a><a href=https://prose.io/#algorithmica-org/algorithmica/edit/master//hpc%2fcpu-cache%2fprefetching.md><img src=/icons/edit-solid.svg title=edit style=width:18px;position:relative;right:-2px;top:-1px>
</a><a href=https://github.com/algorithmica-org/algorithmica/blob/master//hpc/cpu-cache/prefetching.md class=github-main><img src=/icons/github-brands.svg title='view on github'></a></div></menu><main><div id=search><input id=search-bar type=search placeholder='Search this book…' oninput=search()><div id=search-count></div><div id=search-results></div></div><header><h1>Prefetching</h1><div class=info></div></header><article><p>Taking advantage of the <a href=../mlp>free concurrency</a> available in memory hardware, it can be beneficial to <em>prefetch</em> data that is likely to be accessed next if its location can be predicted. This is easy to do when there are no <a href=/hpc/pipelining/hazards>data of control hazards</a> in the pipeline and the CPU can just run ahead of the instruction stream and execute memory operations out of order.</p><p>But sometimes the memory locations aren&rsquo;t in the instruction stream, and yet they can still be predicted with high probability. In these cases, they can be prefetched by other means:</p><ul><li>Explicitly, by separately reading the next data word or any of the bytes in the same cache line, so that it is lifted in the cache hierarchy.</li><li>Implicitly, by using simple access patterns such as linear iteration, which are detectable by the memory hardware that can start prefetching automatically.</li></ul><p>Hiding memory latency is crucial for achieving performance, so in this section, we will look into prefetching techniques.</p><span class=anchor id=hardware-prefetching></span><h3><a class=anchor-link href=https://en.algorithmica.org/hpc/cpu-cache/prefetching/#hardware-prefetching>#</a>Hardware Prefetching</h3><p>Let&rsquo;s modify the <a href=../latency>pointer chasing</a> benchmark to show the effect of hardware prefetching. Now, we generate our permutation in a way that makes the CPU request consecutive cache lines when iterating over the permutation, but still accessing the elements inside a cache line in random order:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=kt>int</span> <span class=n>p</span><span class=p>[</span><span class=mi>15</span><span class=p>],</span> <span class=n>q</span><span class=p>[</span><span class=n>N</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>iota</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>p</span> <span class=o>+</span> <span class=mi>15</span><span class=p>,</span> <span class=mi>1</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>+</span> <span class=mi>16</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>i</span> <span class=o>+=</span> <span class=mi>16</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>random_shuffle</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>p</span> <span class=o>+</span> <span class=mi>15</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>k</span> <span class=o>=</span> <span class=n>i</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=mi>15</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>k</span> <span class=o>=</span> <span class=n>q</span><span class=p>[</span><span class=n>k</span><span class=p>]</span> <span class=o>=</span> <span class=n>i</span> <span class=o>+</span> <span class=n>p</span><span class=p>[</span><span class=n>j</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=n>q</span><span class=p>[</span><span class=n>k</span><span class=p>]</span> <span class=o>=</span> <span class=n>i</span> <span class=o>+</span> <span class=mi>16</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>There is no point in making a graph because it would be just flat: the latency is 3ns regardless of the array size. Even though the instruction scheduler still can&rsquo;t tell what we are going to fetch next, the memory prefetcher can detect a pattern just by looking at the memory accesses and start loading the next cache line ahead of time, mitigating the latency.</p><p>Hardware prefetching is smart enough for most use cases, but it only detects simple patterns. You can iterate forward and backward over multiple arrays in parallel, perhaps with small-to-medium strides, but that&rsquo;s about it. For anything more complex, the prefetcher won&rsquo;t figure out what&rsquo;s happening, and we need to help it out ourselves.</p><span class=anchor id=software-prefetching></span><h3><a class=anchor-link href=https://en.algorithmica.org/hpc/cpu-cache/prefetching/#software-prefetching>#</a>Software Prefetching</h3><p>The simplest way to do software prefetching is to load any byte in the cache line with the <code>mov</code> or any other memory instruction, but CPUs have a separate <code>prefetch</code> instruction that lifts a cache line without doing anything with it. This instruction isn&rsquo;t a part of the C or C++ standard, but is available in most compilers as the <code>__builtin_prefetch</code> intrinsic:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>__builtin_prefetch</span><span class=p>(</span><span class=o>&amp;</span><span class=n>a</span><span class=p>[</span><span class=n>k</span><span class=p>]);</span>
</span></span></code></pre></div><p>It&rsquo;s quite hard to come up with a <em>simple</em> example when it can be useful. To make the pointer chasing benchmark benefit from software prefetching, we need to construct a permutation that at the same time loops around the whole array, can&rsquo;t be predicted by hardware prefetcher, and has easily computable next addresses.</p><p>Luckily, the <a href=https://en.wikipedia.org/wiki/Linear_congruential_generator>linear congruential generator</a> has the property that if the modulus $n$ is a prime number, then the period of the generator will be exactly $n$. So we get all the properties we need if we use a permutation generated by the LCG with the current index as its state:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>const</span> <span class=kt>int</span> <span class=n>n</span> <span class=o>=</span> <span class=n>find_prime</span><span class=p>(</span><span class=n>N</span><span class=p>);</span> <span class=c1>// largest prime not exceeding N
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>q</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>%</span> <span class=n>n</span><span class=p>;</span>
</span></span></code></pre></div><p>When we run it, the performance matches a normal random permutation. But now we get the ability to peek ahead:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=kt>int</span> <span class=n>k</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>t</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>t</span> <span class=o>&lt;</span> <span class=n>K</span><span class=p>;</span> <span class=n>t</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>__builtin_prefetch</span><span class=p>(</span><span class=o>&amp;</span><span class=n>q</span><span class=p>[(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>k</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>%</span> <span class=n>n</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>        <span class=n>k</span> <span class=o>=</span> <span class=n>q</span><span class=p>[</span><span class=n>k</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>There is some overhead to computing the next address, but for arrays large enough, it is almost two times faster:</p><p><figure><img src=../img/sw-prefetch.svg><figcaption></figcaption></figure></p><p>Interestingly, we can prefetch more than just one element ahead, making use of this pattern in the LCG function:</p>$$
\begin{aligned}
f(x) &= 2 \cdot x + 1
\\ f^2(x) &= 4 \cdot x + 2 + 1
\\ f^3(x) &= 8 \cdot x + 4 + 2 + 1
\\ &\ldots
\\ f^k(x) &= 2^k \cdot x + (2^k - 1)
\end{aligned}
$$<p>Hence, to load the <code>D</code>-th element ahead, we can do this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>__builtin_prefetch</span><span class=p>(</span><span class=o>&amp;</span><span class=n>q</span><span class=p>[((</span><span class=mi>1</span> <span class=o>&lt;&lt;</span> <span class=n>D</span><span class=p>)</span> <span class=o>*</span> <span class=n>k</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span> <span class=o>&lt;&lt;</span> <span class=n>D</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span> <span class=o>%</span> <span class=n>n</span><span class=p>]);</span>
</span></span></code></pre></div><p>If we execute this request on every iteration, we will be simultaneously prefetching <code>D</code> elements ahead on average, increasing the throughput by <code>D</code> times. Ignoring some issues such as the integer overflow when <code>D</code> is too large, we can reduce the average latency arbitrarily close to the cost of computing the next index (which, in this case, is dominated by the <a href=/hpc/arithmetic/division>modulo operation</a>).</p><p><figure><img src=../img/sw-prefetch-others.svg><figcaption></figcaption></figure></p><p>Note that this is an artificial example, and you actually fail more often than not when trying to insert software prefetching into practical programs. This is largely because you need to issue a separate memory instruction that may compete for resources with the others. At the same time, hardware prefetching is 100% harmless as it only activates when the memory and cache buses are not busy.</p><p>You can also specify a specific level of cache the data needs to be brought to when doing software prefetching — when you aren&rsquo;t sure if you will be using it and don&rsquo;t want to kick out what is already in the L1 cache. You can use it with the <code>_mm_prefetch</code> intrinsic, which takes an integer value as the second parameter, specifying the cache level. This is useful in combination with <a href=../bandwidth#bypassing-the-cache>non-temporal loads and stores</a>.</p></article><div class=nextprev><div class=left><a href=https://en.algorithmica.org/hpc/cpu-cache/mlp/ id=prev-article>← Memory-Level Parallelism</a></div><div class=right><a href=https://en.algorithmica.org/hpc/cpu-cache/alignment/ id=next-article>Alignment and Packing →</a></div></div></main><footer>Copyright 2021–2022 Sergey Slotin<br></footer></div></body></html>