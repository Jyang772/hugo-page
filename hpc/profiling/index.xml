<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Profiling on Algorithmica</title>
    <link>https://en.algorithmica.org/hpc/profiling/</link>
    <description>Recent content in Profiling on Algorithmica</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="https://en.algorithmica.org/hpc/profiling/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Instrumentation</title>
      <link>https://en.algorithmica.org/hpc/profiling/instrumentation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://en.algorithmica.org/hpc/profiling/instrumentation/</guid>
      <description>Instrumentation is an overcomplicated term that means inserting timers and other tracking code into programs. The simplest example is using the time utility in Unix-like systems to measure the duration of execution for the whole program.&#xA;More generally, we want to know which parts of the program need optimization. There are tools shipped with compilers and IDEs that can time designated functions automatically, but it is more robust to do it by hand using any methods of interacting with time that the language provides:</description>
    </item>
    <item>
      <title>Statistical Profiling</title>
      <link>https://en.algorithmica.org/hpc/profiling/events/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://en.algorithmica.org/hpc/profiling/events/</guid>
      <description>Instrumentation is a rather tedious way of doing profiling, especially if you are interested in multiple small sections of the program. And even if it can be partially automated by the tooling, it still won&amp;rsquo;t help you gather some fine-grained statistics because of its inherent overhead.&#xA;Another, less invasive approach to profiling is to interrupt the execution of a program at random intervals and look where the instruction pointer is. The number of times the pointer stopped in each function&amp;rsquo;s block would be roughly proportional to the total time spent executing these functions.</description>
    </item>
    <item>
      <title>Program Simulation</title>
      <link>https://en.algorithmica.org/hpc/profiling/simulation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://en.algorithmica.org/hpc/profiling/simulation/</guid>
      <description>The last approach to profiling (or rather a group of them) is not to gather the data by actually running the program but to analyze what should happen by simulating it with specialized tools.&#xA;There are many subcategories of such profilers, differing in which aspect of computation is simulated. In this article, we are going to focus on caching and branch prediction, and use Cachegrind for that, which is a profiling-oriented part of Valgrind, a well-established tool for memory leak detection and memory debugging in general.</description>
    </item>
    <item>
      <title>Machine Code Analyzers</title>
      <link>https://en.algorithmica.org/hpc/profiling/mca/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://en.algorithmica.org/hpc/profiling/mca/</guid>
      <description>A machine code analyzer is a program that takes a small snippet of assembly code and simulates its execution on a particular microarchitecture using information available to compilers, and outputs the latency and throughput of the whole block, as well as cycle-perfect utilization of various resources within the CPU.&#xA;#Using llvm-mcaThere are many different machine code analyzers, but I personally prefer llvm-mca, which you can probably install via a package manager together with clang.</description>
    </item>
    <item>
      <title>Benchmarking</title>
      <link>https://en.algorithmica.org/hpc/profiling/benchmarking/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://en.algorithmica.org/hpc/profiling/benchmarking/</guid>
      <description>Most good software engineering practices in one way or another address the issue of making development cycles faster: you want to compile your software faster (build systems), catch bugs as soon as possible (static analysis, continuous integration), release as soon as the new version is ready (continuous deployment), and react to user feedback without much delay (agile development).&#xA;Performance engineering is not different. If you do it correctly, it should also resemble a cycle:</description>
    </item>
    <item>
      <title>Getting Accurate Results</title>
      <link>https://en.algorithmica.org/hpc/profiling/noise/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://en.algorithmica.org/hpc/profiling/noise/</guid>
      <description>It is not an uncommon for there to be two library algorithm implementations, each maintaining its own benchmarking code, and each claiming to be faster than the other. This confuses everyone involved, especially the users, who have to somehow choose between the two.&#xA;Situations like these are usually not caused by fraudulent actions by their authors; they just have different definitions of what &amp;ldquo;faster&amp;rdquo; means, and indeed, defining and using just one performance metric is often very problematic.</description>
    </item>
  </channel>
</rss>
