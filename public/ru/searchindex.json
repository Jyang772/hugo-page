[{"content":"Бинарное возведение в степень — приём, позволяющий возводить любое число в $n$-ую степень за $O(\\log n)$ умножений (вместо $n$ умножений при обычном подходе).\n#Основная идеяЗаметим, что для любого числа $a$ и чётного числа $n$ выполняется тождество:\n$$ a^n = (a^{n/2})^2 = a^{n/2} \\cdot a^{n/2} $$\nПолучается, что для любого чётного $n$ мы можем свести задачу к вдвое меньшей ($n\u0026rsquo; = \\frac{n}{2}$), потратив всего одну операцию умножения.\nЕсли же $n$ нечётно, то верно следующее:\n$$ a^n = a^{n-1} \\cdot a $$\nПолучается, для нечетных $n$ мы можем сделать сведение к задаче размера $(n-1)$, которая уже будет чётной.\nМы фактически нашли рекуррентную формулу для подсчета $a^n$:\n$$ a^n = f(a, n) = \\begin{cases} 1, \u0026amp;\u0026amp; n = 0 \\ f(a, \\frac{n}{2})^2, \u0026amp;\u0026amp; 2 \\mid n \\ f(a, n - 1) \\cdot a, \u0026amp;\u0026amp; 2 \\nmid n \\end{cases} $$\nТак как каждые два перехода $n$ гарантированно уменьшается хотя бы в два раза, всего будет не более $2 \\cdot \\log_2 n$ переходов, прежде чем мы придём к $n = 0$. Каждый переход требует ровно одно умножение, и таким образом, мы получили алгоритм, работающий за $O(\\log n)$ умножений.\n#РеализацияСразу напрашивается рекурсивная реализация этого алгоритма:\nint binpow(int a, int n) { if (n == 0) return 1; if (n % 2 == 1) return binpow(a, n - 1) * a; else { int b = binpow(a, n / 2); return b * b; } } Эта реализация рекурсивная, что работает долго. Попробуем «развернуть» рекурсию и получить итеративную.\nРассмотрим двоичное представление числа $n$. Результат $a^n$ можно представить как произведение $a$ в степенях каких-то степеней двоек. Например, если $n = 42 = 32 + 8 + 2$, то\n$$ a^{42} = a^{32+8+2} = a^{32} \\cdot a^8 \\cdot a^2 $$\nЧтобы посчитать это произведение итеративно, пройдемся по всем битам числа $n$, поддерживая две переменные: непосредственно результат и текущее значение $a^{2^k}$, где $k$ это номер текущей итерации. На каждом шаге будем домножать $a^{2^k}$ на текущий результат, если $k$-тый бит числа $n$ единичный, и в любом случае возводить её в квадрат, получая $a^{2^k \\cdot 2} = a^{2^{k+1}}$ для следующей итерации.\nint binpow(int a, int n) { int res = 1; while (n != 0) { if (n \u0026amp; 1) res = res * a; a = a * a; n \u0026gt;\u0026gt;= 1; } return res; } Стоит отметить, что во многих языках программирования бинарное возведение в степень уже реализовано. Но не в C++: функция pow из стандартной библиотеки реализована только для действительных чисел и использует приближенные методы, и поэтому не дает точных результатов для целочисленных аргументов.\n#ОбобщенияБинарное возведение в степень применимо не только к умножению чисел, а вообще к любым ассоциативным операциям — таким, для любых $a$, $b$ и $c$ выполняется:\n$$ (a \\circ b) \\circ c = a \\circ (b \\circ c) $$\nНаиболее очевидное обобщение — на остатки по модулю:\nconst int mod = 1e9 + 7; long long binpow(long long a, int n) { long long res = 1; while (n != 0) { if (n \u0026amp; 1) res = res * a % mod; a = a * a % mod; n \u0026gt;\u0026gt;= 1; } return res; } Вторым самым «популярным» обобщением является произведение матриц, про которое мы подробнее поговорим в следующей статье.\n#Геометрические прогрессииВозводить какие-то объекты в степень часто нужно в контексте комбинаторики, и помимо этого, иногда требуется считать суммы подобного вида:\n$$ S = 1 + a + a^2 + \\ldots a^{n-1} $$\nВ простом случае это сумма геометрической прогрессии, и алгебраически она равна $\\frac{1-a^n}{1-a}$, так что можно просто посчитать $a^n$ бинарным возведением в степень, а затем провести деление $(1-a^n)$ на $(1-a)$. Однако в общем случае так сделать нельзя — например, хотя бы потому, что не по всем модулям будет существовать обратное. Поэтому бинарное возведение в степень для таких формул нужно несколько модифицировать:\n$$ \\begin{aligned} f(a, n) \u0026amp;= 1 + a + a^2 + \\ldots a^{n-1} \\ \u0026amp;= (1 + a + \\ldots + a^{n/2}) + a^{n/2} \\cdot (1 + a + \\ldots a^{n/2}) \\ \u0026amp;= (1 + a^{n/2}) \\cdot f(a, n) \\end{aligned} $$\nВ самом алгоритме нужно помимо степеней $a$ поддерживать ещё и суммы всех степеней от $1$ до $a^k$.\n","id":0,"path":"/cs/algebra/binpow/","title":"Бинарное возведение в степень"},{"content":"Определение. Функция $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ называется линейной, если для неё выполнено:\n$f(x+y) = f(x) + f(y)$ $f(ax) = a f(x), ; a \\in \\mathbb{R}$ Например, линейными являются:\nФункция, которая «растягивает» вектор в $k$ раз: $f(x) = k x$. Функция, которая поворачивает вектор на плоскости на угол $\\theta$. Функция, которая проецирует трёхмерный вектор на какую-нибудь плоскость. Скалярное произведение $f(x, y) = x \\cdot y = \\sum x_ky_k$ также линейно по обоим параметрам. Из одних лишь двух пунктов в определении можно вывести много полезных свойств:\nСумма линейных функций является линейной функцией. Композиция линейных функций $f(g(x)) = (f \\circ g)(x)$ является линейной функцией. Сумма линейных функций коммутативна: $f+g = g+f$. Сумма линейных функций ассоциативна: $(f+g)+h = f+(g+h)$. Композиция линейных функций ассоциативна: $(f \\circ g) \\circ h = f \\circ (g \\circ h) = f \\circ g \\circ h$. Композиция в общем случае не коммутативна. Пример: $f(x) = (-x_2, x_1)$ — поворот точки на плоскости на прямой угол, $g(x) = (x_1, 0)$ — проекция на $Ox$. Почти для всех точек плоскости важен порядок этих двух операций. Линейная алгебра занимается изучением линейных функций.\n#МатрицыМожно показать, что любую линейную функцию $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ можно представить в следующем виде:\n$$ f(x) = \\begin{pmatrix} a_{11} \\cdot x_1 + a_{12} \\cdot x_2 + \\ldots + a_{1n} \\cdot x_n \\ a_{21} \\cdot x_1 + a_{22} \\cdot x_2 + \\ldots + a_{2n} \\cdot x_n \\ \\ldots \\ a_{m1} \\cdot x_1 + a_{m2} \\cdot x_2 + \\ldots + a_{mn} \\cdot x_n \\end{pmatrix} $$\nМатрицы представляют собой просто очень компактную запись этих коэффициентов $a_{ij}$.\n$$ A = \\begin{pmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\ldots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\ldots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\ldots \u0026amp; a_{mn} \\ \\end{pmatrix} $$\nКаждой линейной функции $f$ из $\\mathbb{R}^n$ в $\\mathbb{R}^m$ соответствует какая-то матрица $A$ размера $n \\times m$ и наоборот. Число $n$ равно количеству строк, а $m$ — количеству столбцов. Элемент на пересечении $i$-ой строки и $j$-го столбца будем обозначать $A_{ij}$. Не перепутайте.\n#Связь с векторамиЕсли вектор — это упорядоченный набор скаляров, то матрицу можно рассматривать как вектор векторов. Вектор, в частности, можно представить как матрицу, у которой одна из размерностей равна единице — тогда его называют вектор-столбец либо вектор-строка.\ntypedef vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; matrix; Ещё есть тензоры — ими называют все объекты ещё более высокого порядка: векторы матриц (трёхмерный тензор), матрицы матриц (четырёхмерный тензор) и векторы матриц матриц и так далее.\nУ тензоров есть своя интересная алгебра, но в контекстах, в которых с ними сталкивается обычный программист, никакая алгебра, как правило, не подразумевается, и этот термин используется лишь потому, что в словосочетании «многомерный массив» слишком много букв.\n#Матричное умножениеПусть линейной функции $f$ соответствует матрица $A$, а функции $g$ соответствует матрица $B$. Тогда композиции этих функций $h = f \\circ g$ будет соответствовать произведение $C$ матриц $A$ и $B$, определяемое следующим образом:\n$$ C = AB: ; C_{ij} = \\sum_{i=1}^{k} A_{ik} B_{kj} $$\nЧитатель может убедиться в этом, аккуратно расписав подстановку формул для $f$ на вход $g$.\nПри перемножении матриц руками удобно думать так: элемент на пересечении $i$-го столбца и $j$-той строки — это скалярное произведение $i$-той строки $A$ и $j$-того столбца $B$. Заметим, что это накладывает ограничение на размерности перемножаемых матриц: если первая матрица имеет размер $n \\times k$, то вторая должна иметь размер $k \\times m$, то есть «средние» размерности должны совпадать.\nИсходное выражение для $f(x)$ теперь можно компактно записать как $f(x) = Ax$ вместо $m$ уравнений с $n$ слагаемыми в каждом.\nНапишем функцию, реализующую матричное умножение:\nconst int n, k, m; matrix matmul(matrix a, matrix b) { matrix c(n, vector\u0026lt;int\u0026gt;(m, 0)); for (int i = 0; i \u0026lt; n; i++) for (int j = 0; j \u0026lt; m; j++) for (int t = 0; t \u0026lt; k; t++) c[i][j] += a[i][t] * b[t][j]; return c; } Такая реализация хоть и наиболее простая, но не оптимальная: мы на каждой итерации двигаем указатель для $B$ на $m$ шагов вперёд, что приводит к лишним загрузкам кэш-линий и не позволяет компилятору применить автовекторизацию. Однако это легко исправить, если перед всеми циклами транспонировать $B$, то есть поменять каждый её $(i, j)$-тый элемент на $(j, i)$-тый — такая реализация будет работать в 5-10 раз быстрее.\nСуществуют способы соптимизировать матричное умножение и сильно дальше — в 50-100 раз по сравнению с наивным — но они выходят далеко за рамки этой статьи. Также наука знает способы способы перемножать матрицы асимптотически быстрее чем $O(n^3)$, но на практике они становятся эффективными только на матрицах от нескольких тысяч элементов.\n#Свойства матрицК матрицам не нужно относиться как к табличкам, в которых стоят какие-то числа. Каждой матрице соответствует какая-то линейная функция, как-то преобразующая вектора. Центральными объектами линейной алгебры являются именно линейные функции, а не матрицы.\nБлагодаря этому взаимно однозначному соотношению все ранее упомянутые свойства линейных функций переносятся и на матрицы:\nСумма матриц $A$ и $B$ тоже является матрицей: $C = A+B: C_{ij} = A_{ij} + B_{ij}$. Сумма матриц коммутативна: $A+B = B+A$. Сумма матриц ассоциативна: $(A+B)+C = A+(B+C)$. Умножение матриц ассоциативно: $(AB)C = A(BC) = ABC$. Умножение матриц в общем случае не коммутативно. Матрицы не обязательно рассматривать только для действительных чисел — все эти свойства переносятся на произвольные поля: множества, для которых определены $*$ и $+$ с определенными ограничениями на операции.\nСамый популярный класс таких полей — остатки по простому модулю. В частном случае, когда $p = 2$, в поле будет всего два элемента — ноль и единица — а также xor в качестве сложения и and в качестве умножения. Это позволяет эффективно хранить матрицы в виде битовых последовательностей.\n#Примеры матрицМатрица «увеличь всё в два раза»:\n$$ \\begin{pmatrix} 2 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; 2 \u0026amp; 0 \\ 0 \u0026amp; 0 \u0026amp; 2 \\ \\end{pmatrix} $$\nМатрица «поменяй $x$ и $y$ местами»:\n$$ \\begin{pmatrix} 0 \u0026amp; 1 \\ 1 \u0026amp; 0 \\ \\end{pmatrix} $$\nМатрица поворота на угол $\\alpha$ на плоскости:\n$$ \\begin{pmatrix} \\cos \\alpha \u0026amp; -\\sin \\alpha \\ \\sin \\alpha \u0026amp; \\cos \\alpha \\ \\end{pmatrix} $$\nМатрица проецирования на плоскость $xy$ в трёхмерном пространстве:\n$$ \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; 1 \u0026amp; 0 \\ 0 \u0026amp; 0 \u0026amp; 0 \\ \\end{pmatrix} $$\nМатрица «ничего не делай», также известная как единичная матрица:\n$$ I_3 = \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; 1 \u0026amp; 0 \\ 0 \u0026amp; 0 \u0026amp; 1 \\ \\end{pmatrix} $$\nЕдиничную матрицу обычно обозначают как $I$ или $E$. На её главной диагонали всегда единицы, а вне — нули.\n","id":1,"path":"/cs/algebra/matrix/","title":"Матрицы"},{"content":"Когда мы говорили о линейных функциях и матрицах, все примеры были про геометрию. Так просто проще думать: представлять в уме повороты и растяжения векторов на плоскости легче, чем рассуждать о свойствах абстрактных n-мерных пространств. Однако мы программисты, и нас в основном интересуют задачи вне геометрии.\nБольшинство применений в контексте олимпиад опираются на ассоциативность матричного умножения:\n$$ ABC = (AB)C = A(BC) $$\nЕсли задачу можно свести к возведению матрицы $A$ в какую-то большую степень $n$ — то есть к домножению единичной матрицы $n$ раз на матрицу $A$ — то можно просто воспользоваться свойством ассоциативности и посчитать результат бинарным возведением в степень:\n$$ A^8 = A^{2^{2^{2}}} = ((AA)(AA))((AA)(AA)) $$\nТакой подход имеет множество применений в динамическом программировании, комбинаторике, теории вероятностей и математике в целом.\n#Линейные рекуррентыЗадача. Дано число $n \u0026lt; 10^{18}$. Требуется вычислить $n$-ное число Фибоначчи:\n$$ \\begin{aligned} f_0 \u0026amp;= 0 \\ f_1 \u0026amp;= 1 \\ f_{n} \u0026amp;= f_{n-1} + f_{n-2} \\end{aligned} $$\nЗаметим, что вычисление очередного числа Фибоначчи выражается как линейная функция от двух предыдущих чисел Фибоначчи — а именно, как их сумма.\nЭто означает, что мы можем построить матрицу $2 \\times 2$, которая будет соответствовать этому преобразованию: как по двум соседним числам Фибоначчи $(f_n, f_{n+1})$ вычислить следующее число, то есть перейти к паре $(f_{n+1}, f_{n+2})$.\n#$$ \\begin{pmatrix} f_{n+1} \\ f_{n+2} \\ \\end{pmatrix} #\\begin{pmatrix} 0 + f_{n+1} \\ f_{n} + f_{n+1} \\ \\end{pmatrix}\\begin{pmatrix} 0 \u0026amp; 1 \\ 1 \u0026amp; 1 \\ \\end{pmatrix} \\begin{pmatrix} f_{n} \\ f_{n+1} \\ \\end{pmatrix} $$\nОбозначим за $A$ эту матрицу перехода. Чтобы посчитать $n$-ное число Фибоначчи, нужно применить $n$ раз эту матрицу к вектору $(f_0, f_1) = (0, 1)$.\nТак как размер матрицы $A$ константный, её умножение саму на себя будет работать за $O(1)$. Значит, можно посчитать $A^n$ бинарным возведением в степень за $O(\\log n)$ операций, домножить на вектор $(0, 1)$ и взять первый элемент результата — он будет равен в точности $f_n$, что мы и искали.\nПримечание. Домножать на вектор явно даже не обязательно — если нам нужен только первый элемент, можно посмотреть на выражение для итогового результата и понять, что достаточно просто взять верхний правый элемент итоговой матрицы.\n#Общий случайВ общем случае, когда мы учитываем не $2$, а $k$ последних элементов, причём с разными весами, линейная рекуррента $f_n = a_1 f_{n-1} + a_2 f_{n-2} + \\ldots + a_k f_{n-k}$ имеет такую матрицу перехода:\n$$ \\begin{pmatrix} 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; \\ldots \u0026amp; 0 \\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 1 \\ a_k \u0026amp; a_{k-1} \u0026amp; a_{k-2} \u0026amp; \\ldots \u0026amp; a_1 \\ \\end{pmatrix} $$\nПри домножении на вектор $(f_n, f_{n+1}, \\ldots, f_{n+k-1})$ для последнего элемента получается в точности формула из определения, а все $(k-1)$ предыдущих просто перекопируются.\nОтметим, что рекуррентные последовательности задаются не только коэффициентами $a_i$, но и начальными значениями $(f_1, f_2, \\ldots, f_k)$.\nУпражнение. Для линейной рекурренты порядка $k$ известны её коэффициенты $a_i$ и $k$ пар вида $(i, f_i)$. Восстановите первые $k$ элементов последовательности.\n#Геометрическая прогрессияВ статье про бинарное возведение в степень мы обсуждали модификацию алгоритма для нахождения суммы геометрической прогрессии:\n$$ f(n) = 1 + a + a^2 + \\ldots + a^{n-1} $$\nМожно подойти к этой задаче с другой стороны. Заметим, что сумма первых $n$ степеней следующим образом выражается через сумму первых $(n-1)$ степеней:\n$$ f(n) = f(n-1) \\cdot a + 1 $$\nЭто почти линейная зависимость от предыдущего — только ещё добавляется неудобная константа. Можно применить следующий трюк: представить, что мы также зависим ещё и от $(n-2)$-го элемента, но положить его постоянно равным единице:\n#$$ \\begin{pmatrix} f_{n+1} \\ 1 \\ \\end{pmatrix} #\\begin{pmatrix} f_n \\cdot a + 1 \\ 0 + 1 \\ \\end{pmatrix}\\begin{pmatrix} a \u0026amp; 1 \\ 0 \u0026amp; 1 \\ \\end{pmatrix} \\begin{pmatrix} f_{n} \\ 1 \\ \\end{pmatrix} $$\nТогда эту матрицу можно аналогично возвести в степень $n$ и вернуть её правое верхнее поле.\n#Динамическое программированиеПереходы в некоторых динамиках иногда тоже можно выразить в терминах матричного умножения.\nЗадача. Дана ориентированный граф $G$ размера $n \u0026lt; 500$. Требуется найти количество путей из $s$ в $t$ через ровно $k \u0026lt; 10^{18}$ переходов.\nЕсли бы ограничение на $k$ было сильно меньше, мы бы ввели динамику $f_{i,v}$, равную количеству способов дойти до $v$-той вершины через ровно $i$ переходов. Пересчёт — перебор предпоследней вершины в пути:\n$$ f_{i,v} = \\sum_{u \\in g_v} f_{i-1,u} $$\nПерепишем эту динамику в терминах матрицы смежности, в ячейке $G_{uv}$ которой содержится число ребер между $u$ и $v$ (ровно поэтому она называется матрицей).\n$$ f_{i,v} = \\sum_u f_{i-1,u} \\cdot G_{uv} $$\nВыясняется, что вектор $f_{i}$ зависит от $f_{i-1}$ линейно: его $v$-тый элемент получается скалярным умножением вектора $f_{i-1}$ и $v$-того столбца матрицы смежности $G$. Значит, имеет место следующее равенство:\n$$ f_i = G \\cdot f_{i-1} $$\nПолучается, $f_k$ можно раскрыть как $f_k = f_0 \\cdot G \\cdot G \\cdot \\ldots \\cdot G$ и применить бинарное возведение в степень.\nПо этой формуле нам потребуется $O(\\log k)$ раз перемножить две матрицы размера $n \\times n$, что суммарно будет работать за $O(n^3 \\log k)$.\nconst int n; matrix binpow(matrix a, int p) { // создадим единичную матрицу matrix b(n, vector\u0026lt;int\u0026gt;(n, 0)); for (int i = 0; i \u0026lt; n; i++) b[i][i] = 1; while (p \u0026gt; 0) { if (p \u0026amp; 1) b = matmul(b, a); a = matmul(a, a); p \u0026gt;\u0026gt;= 1; } return b; } В получившейся матрице в ячейке $G_{ij}$ будет находиться количество способов дойти из $i$-той вершины в $j$-тую, используя ровно $k$ переходов. Ответом нужно просто вывести $G_{st}$.\n#Модификации задачиС небольшими изменениями этим методом можно решать много похожих задач:\nПрактически не меняя сам алгоритм, можно решить задачу «с какой вероятностью мы попадём из вершины $s$ в вершину $t$», если вместо матрицы смежности даны вероятности, с которыми мы переходим из вершины в вершину в марковской цепи. Если нам не нужно количество способов, а только сам факт, можно ли дойти за ровно $k$ переходов, то можно обернуть матрицу в битсеты и сильно ускорить решение. Если нас спрашивают «за не более, чем $k$ переходов», то вместо $k$-той степени матрицы мы можем вышеописанным методом посчитать сумму геометрической прогрессии. Альтернативно, можно для каждой вершины добавить вторую, в которую будет вести ребро из изначальной, а единственное исходящее ребро — это петля в саму себя. Эту технику можно применить и к другим динамикам, где нужно посчитать количество способов что-то сделать — иногда очень неочевидными способами.\nНапример, можно решить такую задачу: найти количество строк длины $k \\approx 10^{18}$, не содержащих данные маленькие запрещённые подстроки. Для этого нужно построить граф «легальных» переходов в Ахо-Корасике, возвести его матрицу смежности в $k$-тую степень и просуммировать в нём первую строчку.\nВ некоторых изощрённых случаях в матричном умножении вместо умножения и сложения нужно использовать другие операции, которые ведут себя как умножение и сложение. Пример задачи: «найти путь от $s$ до $t$ с минимальным весом ребра, использующий ровно $k$ переходов»; здесь нужно возводить в $(k-1)$-ую степень матрицу весов графа, и вместо и сложения, и умножения использовать минимум из двух весов.\nТакже в контексте нахождения кратчайших путей известен «distance product»:\n$$ C_{ij} = \\min_k { a_{ik} + b_{kj} } $$\nВозводя матрицу весов в $k$-тую степень мы получаем, соответственно, минимальное расстояние от $a$ до $b$, используя не более $k$ переходов. В частности, при $k=n$ мы за $O(n^3 \\log n)$ операций получим матрицу кратчайших расстояний между всеми парами вершины, хотя для этого есть и более прямые методы.\n","id":2,"path":"/cs/algebra/matmul/","title":"Задачи на умножение матриц"},{"content":"Часто бывает полезно преобразовать последовательность чисел либо каких-то других объектов в промежуток последовательных целых чисел — например, чтобы использовать её элементы как индексы в массиве либо какой-нибудь другой структуре.\nЭта задача эквивалентна нумерации элементов множества, что можно сделать за $O(n)$ через хеш-таблицу:\nvector\u0026lt;int\u0026gt; compress(vector\u0026lt;int\u0026gt; a) { unordered_map\u0026lt;int, int\u0026gt; m; for (int \u0026amp;x : a) { if (m.count(x)) x = m[x]; else m[x] = m.size(); } return a; } Элементам будут присвоены номера в порядке их первого вхождения в последовательность. Если нужно сохранить порядок, присвоив меньшим элементам меньшие номера, то задача становится чуть сложнее, и её можно решить разными способами.\nКак вариант, можно отсортировать массив, а затем два раза пройтись по нему с хэш-таблицей — в первый раз заполняя её, а во второй раз сжимая сам массив:\nvector\u0026lt;int\u0026gt; compress(vector\u0026lt;int\u0026gt; a) { vector\u0026lt;int\u0026gt; b = a; sort(b.begin(), b.end()); unordered_map\u0026lt;int, int\u0026gt; m; for (int x : b) if (!m.count(x)) m[x] = m.size(); for (int \u0026amp;x : a) x = m[x]; return a; } Также можно выкинуть из отсортированного массива дупликаты (за линейное время), а затем использовать его для нахождения индекса каждого элемента исходного массива бинарным поиском:\nvector\u0026lt;int\u0026gt; compress(vector\u0026lt;int\u0026gt; a) { vector\u0026lt;int\u0026gt; b = a; sort(b.begin(), b.end()); b.erase(unique(b.begin(), b.end()), b.end()); for (int \u0026amp;x : a) x = int(lower_bound(b.begin(), b.end(), x) - b.begin()); return a; } Оба подхода работают за $O(n \\log n)$. Используйте тот, который больше нравится.\n","id":3,"path":"/cs/sequences/compression/","title":"Сжатие координат"},{"content":"В STL конкретная реализация бинарного дерева представлена структурой set, поддерживающей упорядоченное множество уникальных элементов.\n#Основные операцииСтруктуру set\u0026lt;T\u0026gt; можно объявить от любого типа, для которого реализован оператор сравнения — в частности, для пар и тюплов он реализован автоматически как лексикографическая сортировка.\nset\u0026lt;int\u0026gt; s; s.insert(3); // s = {3} s.insert(2); // s = {2, 3} s.size(); // вернет |s| = 2 s.insert(3); // 3 не будет добавлено ещё раз, так как уже присутствует в множестве s.size(); // |s| = 2 // присутствует ли в множестве элемент: s.count(3); // вернет 1 s.count(5); // вернет 0 s.erase(3); // s = {2} s.insert(6); // s = {2, 6} Так как set реализован как сбалансированное двоичное дерево поиска — а конкретно как красно-черное дерево — все операции с его элементами работают за $O(\\log n)$.\n#ИтераторыТакже set, как и все контейнеры STL, поддерживает итераторы.\nНачало set (наименьший элемент) можно получить через .begin(), конец — через .end(). Обратите внимание, что .end(), как и все итераторы, указывает на конец полуинтервала — то есть на несуществующий элемент, идущий после последнего.\nauto it = s.find(2); // возвращает итератор на элемент или `end`, если элемента нет ++it; // найти следующий элемент int x = *it; // x = 6 s.lower_bound(1); // вернет 2, так как это первый элемент \u0026gt;= 1 s.upper_bound(2); // вернет 6, так как это первый элемент \u0026gt; 2 auto it = s.upper_bound(10); if (it == s.end()) { // аккуратно: если разыменуете it, получите undefined behavior! } // вывод всех элементов сета в порядке возрастания с использованием итераторов for (auto it = s.begin(); it != s.end(); ++it) cout \u0026lt;\u0026lt; *it \u0026lt;\u0026lt; \u0026#34; \u0026#34;; // но для таких целей лучше использовать range-based for loop for (int x : s) cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; \u0026#34; \u0026#34;; Инкремент и декремент итераторов работает за логарифмические время.\n#Связанные структурыВ STL есть несколько структур со схожей функциональностью:\nmap ассоциирует с ключами значения и позволяет получать их как из бесконечных массивов: m[x] = y. multiset поддерживает дубликаты элементов. .count(x) у него возвращает количество элементов с заданным ключом, а не просто 0 или 1. Его можно реализовать как map, в котором в качестве значений хранятся количества элементов. multimap возвращает по ключу много различных значений вместо одного и позволяет итерироваться по ним (то же самое, что использовать). Его можно реализовать как map\u0026lt;A, vector\u0026lt;B\u0026gt;\u0026gt;. Также у всех этих контейнеров есть аналоги, работающие на хеш-таблицах вместо деревьев: unordered_set, unordered_map, unordered_multiset и unordered_multimap. В них поиск, удаление и вставка в среднем работают за константное время, но нет lower_bound и упорядоченного итерирования.\n","id":4,"path":"/cs/tree-structures/stl-trees/","title":"Деревья в STL"},{"content":"В этот раздел с течением времени будут переноситься статьи с алгокода, емакса и старой алгоритмики.\n","id":5,"path":"/cs/","title":"Алгоритмы"},{"content":"Рене Декарт (фр. René Descartes) — великий французский математик и философ XVII века.\nРене Декарт не является создателем декартова дерева, однако он является создателем декартовой системы координат, которую мы все знаем и любим.\nДекартово дерево же определяется и строится так:\nНанесём на плоскость набор из $n$ точек. Их $x$ зачем-то назовем ключом, а $y$ приоритетом. Выберем самую верхнюю точку (с наибольшим $y$, а если таких несколько — любую) и назовём её корнем. От всех вершин, лежащих слева (с меньшим $x$) от корня, рекурсивно запустим этот же процесс. Если слева была хоть одна вершина, то присоединим корень левой части в качестве левого сына текущего корня. Аналогично, запустимся от правой части и добавим корню правого сына. Заметим, что если все $y$ и $x$ различны, то дерево строится однозначно.\nЕсли нарисовать получившуюся структуру на плоскости, то получится действительно дерево — по традиции, корнем вверх:\nВершины дерева и их координатные представления Таким образом, декартово дерево — это одновременно бинарное дерево по $x$ и куча по $y$. Поэтому ему придумали много альтернативных названий:\nДерамида (дерево + пирамида) Дуча (дерево + куча) ПиВо (пирамида + дерево) КуРево (куча + дерево) Treap (tree + heap) По-английски его обычно называют cartesian или treap, а по-русски часто сокращают до «ДД».\n#Как бинарное дерево поискаС небольшими модификациями, декартово дерево умеет всё то же, что и любое бинарное дерево поиска, например:\nдобавить число $x$ в множество; определить, есть ли в множестве число $x$; найти первое число, не меньшее $x$ — аналогично lower_bound; найти количество чисел в промежутке $[l, r]$. Как и во всех сбалансированных деревьях поиска, все операции работают за высоту дерева: $O(\\log n)$.\n#Приоритеты и асимптотикаВ декартовом дереве логарифмическая высота дерева гарантируется не инвариантами и эвристиками, а теорией вероятностей: оказывается, что если все приоритеты ($y$) выбирать случайно, то средняя глубина вершины будет логарифмической. Поэтому ДД ещё называют рандомизированным деревом поиска.\nТеорема. Ожидание глубины вершины в декартовом дереве равно $O(\\log n)$.\nДоказательство. Введем функцию $a(x, y)$ равную единице, если $x$ является предком $y$, и нулем в противном случае. Такие функции называются индикаторами.\nГлубина вершины равна количеству её предков, и поэтому\n$$ d_i = \\sum_{j=1}^n a(j, i) $$\nЕё матожидание равно\n$$ E[d_i] = E[\\sum_{j \\neq i} a(j, i)] = \\sum_{j \\neq i} E[a(j, i)] = \\sum_{j \\neq i} E[a(j, i)] = \\sum_{j \\neq i} p(j, i) $$\nгде $p(x, y)$ это вероятность, что $a(x, y) = 1$, то есть что вершина $x$ это предок вершины $y$. Здесь мы воспользовались важным свойством линейности: матожидание суммы чего угодно равна сумме матожиданий этого чего угодно.\nТеперь осталось посчитать эти вероятности по всем возможным предкам и сложить. Но сначала нам понадобится вспомогательное утверждение.\nЛемма. Вершина $x$ является предком $y$, если у неё приоритет больше, чем у всех вершин из полуинтервала $(x, y]$ (без ограничения общности, будем считать, что $x \u0026lt; y$).\nНеобходимость. Если $x$ не самая высокая, то где-то между $x$ и $y$ есть вершина с большим приоритетом, чем у $x$. Она не может быть потомком $x$, а значит $x$ и $y$ будут разделены ей.\nДостаточность. Если справа от интервала будет какая-то вершина с большим приоритетом, то её левым сыном будет какая-то вершина, которая будет являться предком $x$. Таким образом, всё, что справа от $y$, ни на что влиять не будет.\nУ всех вершин на любом отрезке одинаковая вероятность иметь наибольший приоритет. Объединяя этот факт с результатом леммы, мы можем получить выражение для искомых вероятностей — чтобы вершина $x$ была предком $y$, её приоритет должен быть больше, чем у всех $|x - y|$ остальных на отрезке с $x$ по $y]$:\n$$ p(x, y) = \\frac{1}{|x-y|+1} $$\nТеперь, чтобы найти матожидание глубины, эти вероятности надо просуммировать:\n$$ E[d_i] = \\sum_{j \\neq i} p(j, i) = \\sum_{j \\neq i} \\frac{1}{|i-j|+1} = \\sum_{j \u0026lt; i} \\frac{1}{i - j} + \\sum_{j \u0026gt; i} \\frac{1}{j - i} \\leq 2 \\cdot \\sum_{k=2}^n \\frac{1}{k} = O(\\log n) $$\nПеред последним переходом мы получили сумму гармонического ряда.\nПримечательно, что ожидаемая глубина вершин зависит от их позиции: вершина из середины должна быть примерно в два раза глубже, чем крайняя.\nУпражнение. Выведите по аналогии с этим рассуждением асимптотику quicksort.\n#РеализацияДекартово дерево удобнее всего писать на указателях и структурах.\nСоздадим структуру Node, в которой будем хранить ключ, приоритет и указатели на левого и правого сына:\nstruct Node { int key, prior; Node *l = 0, *r = 0; Node(int key) : key(key), prior(rand()) {} }; Указателя на корень дерева достаточно для идентификации всего дерева. Поэтому, когда мы будем говорить «функция принимает два дерева» на самом деле будем иметь в виду указатели на их корни. К нулевому указателю же мы будем относиться, как к «пустому» дереву.\nОбъявим две вспомогательные функции, изменяющие структуру деревьев: одна будет разделять деревья, а другая объединять. Как мы увидим, через них можно легко выразить почти все функции, которые нам потом понадобятся.\n#MergeПринимает два дерева (два корня, $L$ и $R$), про которые известно, что в левом все вершины имеют меньший ключ, чем все в правом. Их нужно объединить в одно дерево так, чтобы ничего не сломалось: по ключам это всё ещё дерево, а по приоритетами — куча.\nСначала выберем, какая вершина будет корнем. Здесь всего два кандидата — левый корень $L$ или правый $R$ — просто возьмем тот, у кого приоритет больше.\nПусть, для однозначности, это был левый корень. Тогда левый сын корня итогового дерева должен быть левым сыном $L$. С правым сыном сложнее: возможно, его нужно смерджить с $R$. Поэтому рекурсивно сделаем merge(l-\u0026gt;r, r) и запишем результат в качестве правого сына.\nNode* merge(Node *l, Node *r) { if (!l) return r; if (!r) return l; if (l-\u0026gt;prior \u0026gt; r-\u0026gt;prior) { l-\u0026gt;r = merge(l-\u0026gt;r, r); return l; } else { r-\u0026gt;l = merge(l, r-\u0026gt;l); return r; } } #SplitПринимает дерево $P$ и ключ $x$, по которому его нужно разделить на два: $L$ должно иметь все ключи не больше $x$, а $R$ должно иметь все ключи больше $x$.\nВ этой функции мы сначала решим, в каком из деревьев должен быть корень, а потом рекурсивно разделим его правую или левую половину и присоединим, куда надо:\npair\u0026lt;Node*, Node*\u0026gt; split(Node *p, int x) { if (!p) return {0, 0}; if (p-\u0026gt;key \u0026lt;= x) { auto [l, r] = split(p-\u0026gt;r, x); p-\u0026gt;r = l; return {p, r}; } else { auto [l, r] = split(p-\u0026gt;l, x); p-\u0026gt;l = r; return {l, p}; } } #Пример: вставкаmerge и split сами по себе не очень полезные, но помогут написать всё остальное.\nНапример, чтобы добавить число $x$ в дерево, мы можем разрезать его по $x$ через split, создать новую вершину с одним числом $x$, и склеить через merge три получившихся дерева:\nNode *root = 0; void insert(int x) { auto [l, r] = split(root, x); Node *t = new Node(x); root = merge(l, merge(t, r)); } #Пример: модификация для суммы на отрезкеИногда нам нужно написать какие-то модификации для более продвинутых операций.\nНапример, нам может быть интересно иногда считать сумму чисел на отрезке. Для этого в вершине нужно хранить также своё число и сумму на своем «отрезке»:\nstruct Node { int val, sum; // ... }; При merge и split надо будет поддерживать эту сумму актуальной.\nВместо того, чтобы модифицировать и merge, и split под наши хотелки, напишем вспомогательную функцию upd, которую будем вызывать при обновлении детей вершины:\nint sum(Node* v) { return v ? v-\u0026gt;sum : 0; } // обращаться по пустому указателю нельзя -- выдаст ошибку void upd(Node* v) { v-\u0026gt;sum = sum(v-\u0026gt;l) + sum(v-\u0026gt;r) + v-\u0026gt;val; } В merge и split теперь можно просто вызывать upd перед тем, как вернуть вершину, и тогда ничего не сломается:\nNode* merge(Node *l, Node *r) { // ... if (...) { l-\u0026gt;r = merge(l-\u0026gt;r, r); upd(l); return l; } else { // ... } } pair\u0026lt;Node*, Node*\u0026gt; split(Node *p, int x) { // ... if (...) { // ... upd(p); return {p, r}; } else { // ... } } Тогда при запросе суммы нужно просто вырезать нужный отрезок и запросить эту сумму:\nint sum(int l, int r) { auto [T, R] = split(root, r); auto [L, M] = split(T, l); int res = sum(M); root = merge(L, merge(M, R)); return res; } #Небольшой рефакторингРеализация большинства операций всегда примерно одинаковая — вырезаем отрезок с $l$ по $r$, что-то с ним делаем и склеиваем обратно.\nДублирующийся код — это плохо. Давайте воспользуемся всей мощью C++ и определим функцию, которая принимает другую функцию, которая в свою очередь уже будет делать полезные вещи на нужном отрезке:\nint apply(int l, int r, auto f) { auto [T, R] = split(root, r); auto [L, M] = split(T, l); int res = f(M); root = merge(L, merge(M, R)); return res; } Применять её нужно так:\napply(l, r, sum); Для большинства операций удобно туда передать лямбду, если она ещё не была реализована для upd.\n","id":6,"path":"/cs/tree-structures/treap/","title":"Декартово дерево"},{"content":"Кратчайшие пути очень просто искать, если в графе нет циклов.\nВ этом случае задачу нахождения кратчайших расстояний от заданной вершины до всех остальных можно решить в стиле динамического программирования — кратчайшее расстояние до вершины $v$ можно найти через перебор всех потенциальных последних рёбер на кратчайшем пути:\n$$ d_v = \\min_{(u, v) \\in E} (d_u + w_{uv}) $$\nЧтобы посчитать эту формулу, нужно уметь для заданной вершины $v$ быстро проходиться по всем обратным ребрам, а также найти такой порядок обхода, что при подсчете $d_v$ все нужные $d_u$ уже были подсчитаны.\n#РеализацияКонкретно, транспонируем граф и переведем его в формат списков смежности — то есть для каждой вершины составим список ведущих в неё ребер с их весами:\nvector\u0026lt; pair\u0026lt;int, int\u0026gt; \u0026gt; t[N]; // откуда ведет ребро и его вес Заведем массив $d$, в котором мы для каждой вершины будем хранить расстояние от изначальной вершины — для удобства будем считать, что у неё нулевой номер. Для всех ячеек кроме первой этот массив изначально будет заполнен «бесконечностью» — некоторым числом, которое заведомо больше, чем любое правильное расстояние.\nvector\u0026lt;int\u0026gt; d(n, inf); d[0] = 0; Теперь, проходясь по вершинам графа в порядке какой-нибудь топологической сортировки — которую можно найти как отдельным обходом, так и как-нибудь неявно — будем релаксировать (обновлять более оптимальным значением) расстояние до $v$, рассматривая его обратные ребра по формуле выше:\nfor (int v = 1; u \u0026lt; n; u++) for (auto [u, w] : t[v]) d[v] = min(d[v], d[u] + w); Такой алгоритм очень простой и работает за $O(m)$.\nК несчастью, в общем случае, когда в графе есть циклы, он не работает: хотя релаксация d[v] = min(d[v], d[u] + w) всегда корректная и ничего не ломает для любых $v$ и $y$, ключевым предположением является то, что у нас есть порядок обхода, в котором все нужные предыдущие расстояния уже посчитаны, а задача нахождения такого обхода в общем случае не легче самой задачи нахождения кратчайшего пути.\n#Восстановление путейЧтобы получать не только расстояния, но ещё и научиться восстанавливать сами пути, мы можем для каждой вершины помимо расстояния хранить, из какой вершины произошло последнее релаксирование:\nvector\u0026lt;int\u0026gt; p(n); for (int v = 1; u \u0026lt; n; u++) { for (auto [u, w] : t[v]) { if (d[v] \u0026lt; d[u] + w) { d[v] = d[u] + w; p[v] = u; } } } Теперь для восстановления кратчайшего пути до $v$ нужно просто пройтись по предкам вершины $v$:\nwhile (v != 0) { cout \u0026lt;\u0026lt; v \u0026lt;\u0026lt; endl; v = p[v]; } Также часто в задачах бывает полезно рассматривать «каркас» графа: прогнать какой-нибудь алгоритм нахождения кратчайших путей и оставить в графе только те ребра, которые лежат на каком-то пути — то есть для которых выполняется $d_v = d_u + w_{uv}$. Такой подграф будет ацикличным, и с ним легко работать, например, аналогичным образом насчитывая разные динамики.\nУпражнение. Требуется за $O(m)$ найти количество кратчайший путей из $s$ в $t$ в ациклическом графе.\n","id":7,"path":"/cs/shortest-paths/acyclic-paths/","title":"Пути в ациклических графах"},{"content":"Жадными называют класс алгоритмов, заключающихся в принятии локально оптимальных решений на каждом этапе. Так как локально оптимальное решение вычислить гораздо проще, чем глобально оптимальное, такие алгоритмы обычно имеют хорошую асимптотику.\nВ некоторых случаях жадные алгоритмы приводят к оптимальным конечным решениям, а в других — нет. Придумать и особенно доказать корректность жадных алгоритмов часто бывает очень сложно.\nВ этой статье мы разберем несколько классических примеров задач, решающихся жадным алгоритмом.\n#МонеткиЗадача. Монетная система некоторого государства состоит из монет достоинством $a_1 = 1 \u0026lt; a_2 \u0026lt; a_3 \u0026lt; \\ldots \u0026lt; a_n$, причём каждый следующий номинал делится на предыдущий. Требуется выдать сумму $S$ наименьшим возможным количеством монет.\nЖадный алгоритм решения этой задачи заключается в том, что нужно сначала взять наибольшее количество монет достоинства $a_n$ (а именно, $\\lfloor \\frac{S}{a_n} \\rfloor$), затем для набора остатка взять наибольшее количество монет достоинства $a_{n-1}$ и так далее. Так как $a_1 = 1$, в конце мы всегда наберем нужную сумму.\na = [1, 2, 5, 10, 50, 100, 500, 1000, 2000, 5000] def solution(s): coins = [] for x in reversed(a): coins += [x] * (s // x) s %= x return coins Заметим, что без предположения, что номиналы монет делятся друг на друга, алгоритм не всегда выводит оптимальное решение, и в этом случае задачу остается решать динамическим программированием. Например, сумму в 24 рубля монетами в 1, 5 и 7 рублей жадный алгоритм разменивает как $(7 \\times 3 + 1 \\times 3)$, в то время как оптимальным будет $(7 \\times 2 + 5 \\times 2)$.\nКорректность. Докажем корректность жадного алгоритма от противного: пусть максимальная монета имеет достоинство $a_n \u0026lt; S$, но в оптимальном ответе её нет.\nПусть в оптимальном ответе монета с номиналом $a_i$ встретилась $b_i$ раз:\n$$ S = \\sum a_i \\cdot b_i $$\nЕсли $b_i \\geq \\frac{a_{i+1}}{a_i}$, то $\\frac{a_{i+1}}{a_i}$ монет можно заменить на одну монету достоинства $a_{i+1}$, а значит это не оптимальный ответ. Следовательно, $b_i \\leq \\frac{a_{i+1}}{a_i} - 1$.\nТеперь посчитаем, какой может быть максимальная сумма всех достоинств всех монет в оптимальном ответе, если мы не брали максимальные монеты:\n$$ \\begin{aligned} S \u0026amp;= a_1 \\cdot b_1 + a_2 \\cdot b_2 + \\ldots + a_{n-1} \\cdot b_{n-1} \\ \u0026amp;\\leq a_1 \\cdot (\\frac{a_2}{a_1} - 1) + a_2 \\cdot (\\frac{a_3}{a_2} - 1) + \\ldots + a_{n-1} \\cdot (\\frac{a_n}{a_{n-1}} - 1) \\ \u0026amp;= (a_2 - a_1) + (a_3 - a_2) + \\ldots + (a_n - a_{n-1}) \\ \u0026amp;= a_n - a_1 \u0026lt; a_n \\end{aligned} $$\nОтсюда делаем вывод, что если $S \\geq a_n$, то в оптимальный ответ всегда придется взять максимальную монету размера $a_k$, потому что все меньшие монеты просто не смогут оптимальном ответе давать так много. Аналогичным размышлением про наборы из меньших монет получаем такой же результат для всех $S \u0026lt; a_n$, откуда следует корректность жадного алгоритма.\nТакже этот алгоритм работает не только для делящихся друг на друга номинациях, но и для некоторых других. Такие монетные системы, где жадный алгоритм работает, называют каноническими.\nУпражнение. Верно ли, что алгоритм работает корректно для российской монетной системы (1, 2, 5…)?\n#Рюкзак с делимыми предметамиЗадача. Пусть есть рюкзак с вместимостью не более, чем $W$ грамм и $n$ предметов весом $w_i$ грамм и стоимостью $c_i$ за грамм. Мы умеем отрезать от любого предмета целое количество грамм. Требуется набрать рюкзак максимальной стоимости.\nОтсортируем предметы по убыванию «плотности ценности» $\\frac{c_i}{w_i}$ и будем брать их жадно в таком порядке. От последнего предмета, который не влезет полностью, возьмем часть.\nКорректность. Мысленно разделим все предметы на $w_i$ кусочков по 1 грамм, и при этом их ценность стала равна $\\frac{c_i}{w_i}$. Понятно, что из кусочков одинакового веса 1 грамм всегда оптимально просто взять кусочки с максимальной ценностью, что мы фактически и делаем в нашем алгоритме.\nИтоговая асимптотика $O(n \\log n)$ на сортировку.\n#Выбор заявокЗадача. Даны заявки на проведение занятий в некоторой аудитории. В каждой заявке указаны начало $l_i$ и конец $r_i$ занятия. Нужно из всех заявок оставить как можно больше таким образом, чтобы они не пересекались.\nЗдесь жадность становится не такой уже очевидной, потому что неясно, в каком порядке рассматривать заявки и как их «жадно» набирать.\nПосмотрим на первую по времени конца заявку. Заметим, что нам всегда выгодно включить её в оптимальный ответ — она заканчивается раньше всех остальных, а поэтому если в оптимальном ответе первая заявка какая-то другая, мы можем безболезненно заменить её на первую по времени конца, и при этом новых пересечений не появится, так как мы просто сдвинули самую первую заявку еще левее.\nПосле того, как мы выбрали первую по времени конца, уберем из рассмотрения все, которые с ней пересекаются, и так же выберем из оставшихся первую по времени конца, снова уберем все пересекающиеся и так далее, пока заявки не кончатся.\nПри реализации удобно не удалять отрезки явно, а просто отсортировать отрезки по времени конца и поддерживать время, когда кончается последнее взятое занятие.\nstruct activity { int l, r; }; int solve(vector\u0026lt;activity\u0026gt; activities) { sort(activities.begin(), activities.end(), [](activity a, activity b) { return a.r \u0026lt; b.r; }); int cnt = 0, last = 0; for (activity a : activities) if (a.l \u0026gt;= last) last = a.r, cnt++; return cnt; } Асимптотика $O(n \\log n)$ на сортировку.\n","id":8,"path":"/cs/combinatorial-optimization/greedy/","title":"Жадные алгоритмы"},{"content":"Перед тем, как научиться строить выпуклые оболочки, разберем несколько задач, которые можно решать с их помощью.\nВ этой статье сильно недостает иллюстраций, но мы надеемся на ваше воображение.\n#Локализация точкиПусть у нас есть выпуклый многоугольник $A_0 A_1 A_2 \\ldots A_n$ и поступают запросы определить, находится ли точка $P$ внутри него.\nРассмотрим его триангуляцию на треугольники $\\Delta A_0 A_1 A_2, \\Delta A_0 A_2 A_3, \\ldots, \\Delta A_0 A_{n-1} A_n$ и решим чуть более сложную задачу: определить, к в каком их треугольников она лежит (если она вообще лежит внутри многоугольника).\nРассмотрим вектора $\\overrightarrow{A_0 A_1}, \\overrightarrow{A_0 A_2}, \\ldots, \\overrightarrow{A_0 A_n}$ — диагонали, разбивающие многоугольник на треугольники. Заметим, что они отсортированы по углу — для определенности будем считать, что против часовой стрелки. Рассмотрим вектор $\\overrightarrow{A_0 P}$ и найдем в этой последовательности из $n-1$ векторов первый, который находится «справа» от $\\overrightarrow{A_0 P}$. Это можно сделать бинарным поиском и проверкой через знак векторного умножения.\nПусть мы нашли такое $1 \\leq i \u0026lt; n$, что вектор $\\overrightarrow{A_0 P}$ зажат между $\\overrightarrow{A_0 A_i}$ и $\\overrightarrow{A_0 A_{i+1}}$. Тогда просто проверим, попала ли точка $P$ в треугольник $\\Delta A_0 A_i A_{i+1}$ или нет. Если попала, то точка $P$ лежит внутри многоугольника и мы локализовали ее положение, иначе точка $P$ лежит снаружи многоугольника.\nКаждый такой запрос мы обрабатываем за время $O(\\log{n})$.\n#Поиск касательных, параллельных данной прямойПусть у нас есть выпуклый многоугольник и некоторая прямая. Мы хотим найти $2$ прямые, которые касаются многоугольника и при этом параллельны данной прямой.\nНайдем точки, в которых эти прямые будут касаться многоугольника. Рассмотрим вектор $v$ — направляющий вектор данной прямой. Рассмотрим вектора $\\overrightarrow{A_0 A_1}, \\overrightarrow{A_1 A_2}, \\ldots, \\overrightarrow{A_n A_0}$ — напомним, что вершины идут в порядке против часовой стрелки, а значит эти вектора будут отсортированы по полярному углу.\nТогда, если рассмотреть, между какими соседними позициями в этом порядке должны находиться вектора $v$ и $-v$, то эти позиции будут соответствовать тем вершинам, в которых касательные касаются многоугольника. Для того, чтобы найти эти позиции, достаточно сделать lower_bound по последовательности векторов.\n#Поиск касательных из точкиПусть у нас опять есть выпуклый многоугольник $A_0 A_1 \\ldots A_n$, но прямой нет, а есть некоторая точка $P$. Мы хотим найти $2$ точки $A_i$ и $A_j$, такие что прямые $P A_i$ и $P A_j$ это касательные к многоугольнику — или сказать, что точка $P$ лежит внутри многоугольника.\nПроводить проверку мы уже умеем за $O(\\log n)$, так что предположим, что точка лежит снаружи.\nРассмотрим такую же последовательность ориентированных векторов-ребер и разделим её на две половины: будем называть ребро $A_i A_{i+1}$ левым, если точка $A_{i+1}$ лежит слева от прямой $P A_i$, иначе правым.\nДальше мы можем запустить от левой половины бинпоиск, который будет искать первую вершину $A_i$, для которой $A_{i+1}$ лежит справа от вектора $P A_i$ — ровно этой вершины и будет касаться касательная. Бинпоиск в правой половине делается аналогично.\n#Пара наиболее отдаленных точекДан набор $n$ точек, и требуется найти среди них пару наиболее отдаленных.\nЗаметим, что оптимальная пара гарантированно лежит на выпуклой оболочке этого множества: если бы одна из точек лежала строго внутри выпуклой оболочки, то можно было бы вместо неё выбрать какую-то более отдаленную.\nПостроим выпуклую оболочку всех точек и пронумеруем её вершины в порядке обхода против часовой стрелки. Заметим, что если для $l$-той точки самой отдаленной была $r$-тая, то для $(l+1)$-ой и всех дальнейших никакая точка раньше $r$-той в обходе не может быть оптимальной.\nЗдесь возникает идея применить метод двух указателей: будем перебирать точку $l$ в оболочке и поддерживать оптимальную $r$ для неё, в цикле while проверяя, лучше ли получатся ответ для пары $l$ и $(r+1)$. Так как суммарно $r$ сделает не более $O(n)$ увеличений, алгоритм будет работать за $O(n)$ плюс время построения выпуклой оболочки.\n#Треугольник максимальной площадиДан набор $n$ точек, и требуется найти среди них треугольник с максимальной площадью.\nАналогично предыдущей задаче, все три точки гарантированно будут на выпуклой оболочке, так что сразу построим её.\nТеперь заметим, что для фиксированного основания $a$ и $b$, если мы увеличим его правую точку, то оптимальная третья точка $c$ будет иметь не меньший номер.\nЗначит, мы можем перебрать точку $a$, а затем с помощью метода двух указателей перебрать $b$, поддерживая оптимальный $c$. Суммарно алгоритм будет работать за $O(n^2)$.\n","id":9,"path":"/cs/convex-hulls/hull-applications/","title":"Применения выпуклых оболочек"},{"content":"Все целые числа можно записать в двоичной системе счисления:\n$$ \\begin{aligned} 5_{10} \u0026amp;= 101_2 = 4 + 1 \\ 42_{10} \u0026amp;= 101010_2 = 32 + 8 + 2 \\ 256_{10} \u0026amp;= 100000000_2 = 2^8 \\end{aligned} $$\nТак как на уровне схем почти вся логика бинарная, ровно такое представление и используется для хранения чисел в компьютерах: каждая целочисленная переменная указывает на какую-то ячейку из 8 (char), 16 (short), 32 (int) или 64 (long long) бит.\n#ЭндианностьЕдинственная неоднозначность в таком формате возникает с порядком хранения битов — также называемый эндианностью. Зависимости от архитектуры он может быть разным:\nПри схеме little-endian сначала идут младшие биты. Например, число $42_{10}$ будет храниться так: $010101$. При записи в формате big-endian сначала идут старшие биты. Все примеры из начала статьи даны в big-endian формате. Хотя big-endian более естественный для понимания — на бумаге мы ровно так обычно и записываем бинарные числа — по разным причинам на большинстве современных процессоров по умолчанию используется little endian.\nИными словами, «$i$-тый бит» означает «$i$-тый младший» или «$i$-тый справа», но на бумаге мы ничего не инвертируем и записываем двоичные числа стандартным образом.\n#Битовые операцииПомимо арифметических операций, с числами можно делать и битовые, которые интерпретируют их просто как последовательность битов.\n#СдвигиБитовую запись числа можно «сдвигать» влево (x \u0026lt;\u0026lt; y) или вправо (x \u0026gt;\u0026gt; y), что эквивалентно умножению или делению на степень двойки с округлением вниз.\nОбычное умножение и деление — не самые быстрые операции, однако все битовые сдвиги всегда работают ровно за один такт. Как следствие, умножение и деление на какую-то фиксированную степень двойки всегда работает быстро — даже если вы не используете сдвиги явно, компилятор скорее всего будет проводить подобную оптимизацию.\n#Побитовые операцииПомимо \u0026amp;\u0026amp;, || и !, существуют их побитовые версии, которые применяют соответствующую логическую операцию к целым последовательностям битов: \u0026amp;, |, ~.\nТакже помимо них есть ещё операция исключающего «или» (XOR), которая записывается как ^.\nНапример:\n$13$ \u0026amp; $7$ = $1101_2$ \u0026amp; $0111_2$ = $0101_2$ = $5$ $17$ | $10$ = $10001_2$ | $01010_2$ = $11011_2$ = $27$ $17$ ^ $9$ = $10001_2$ ^ $01001_2$ = $11000_2$ = $24$ Все побитовые операции тоже работают за один такт, вне зависимости от типа данных. Для больших не вмещающихся в один регистр битовых последовательностей существует битсет.\n#МаскиБинарные последовательности можно поставить в соответствие подмножествам какого-то фиксированного множества: если на $i$-той позиции стоит единица, то значит $i$-тый элемент входит множество, а иначе не входит.\nБитовые операции таким образом часто используются операций над множествами, представляемыми битовыми мазками — например, в задачах на полный перебор или динамическое программирование.\n#Выделить i-й бит числа(x \u0026gt;\u0026gt; i) \u0026amp; 1 или x \u0026amp; (1 \u0026lt;\u0026lt; i). Во втором случае результат будет либо 0, либо $2^i$.\nЭто часто используется для проверки, принадлежит ли $i$-тый элемент множеству:\nif ((num \u0026gt;\u0026gt; i) \u0026amp; 1) cout \u0026lt;\u0026lt; \u0026#34;YES\u0026#34;; else cout \u0026lt;\u0026lt; \u0026#34;NO\u0026#34;; Напомним, что нумерация идет с младших бит и начинается с нуля.\n#Получить число, состоящее из k единиц(1 \u0026lt;\u0026lt; k) - 1, также известное как $(2^k-1)$ и соответствующее маске полного множества.\n#Инвертировать все биты числаx = ~x\n#Добавить i-й элемент в множествоx |= (1 \u0026lt;\u0026lt; i)\n#Удалить $i$-й элемент из множестваx \u0026amp;= ~(1 \u0026lt;\u0026lt; i)\n#Удалить i-й элемент из множества, если он естьx ^= (1 \u0026lt;\u0026lt; i)\nТакже добавляет этот элемент, если его нет.\n#Знаковые числаЦелочисленные переменные делятся на два типа — знаковые (signed) и беззнаковые (unsigned).\nЕсли сложить две unsigned int переменные, сумма которых превосходит $2^{32}$, произойдет переполнение: сумму нельзя будет представить точно, и поэтому вместо неё результатом будут только нижние 32 бит. Все операции с беззнаковыми числами как бы проходят по модулю какой-то степени двойки.\nЗнаковые же типы нужны для хранения значений, которые могут быть и отрицательными. Для этого нужно выделить один бит для хранение знака — отрицательное ли число или нет — немного пожертвовав верхней границей представимых чисел: теперь самое большое представимое число это $2^{31}-1$, а не $2^{32}-1$.\nИнженеры, которые работают над процессорами, ещё более ленивые, чем программисты — это мотивировано не только стремлением к упрощению, но и экономией транзисторов. Поэтому когда в signed типах происходит переполнение, результат в битовом представлении считается так же, как и в случае с unsigned числами. Если мы хотим ничего не менять в плане того, как работают unsigned числа, представление отрицательных чисел должно быть таким, что число $-x$ как бы вычитается из большой степени двойки:\n$$ \\begin{aligned} -x = 2^{32} - x \\end{aligned} $$\nНесколько следствий:\nВсе неотрицательные числа записываются в точности как раньше. У всех отрицательных чисел самый большой бит будет единичным. Если прибавить к $2^{31}-1$ единицу, то результатом будет $-2^{31}$, представляемое как 10000000 (в целях изложения мы будем записывать 8 бит, хотя в int их 32). Зная двоичную запись положительного числа x, запись -x можно получить как ~x + 1. -1 записывается как ~1 + 1 = 11111110 + 00000001 = 11111111. -42 записывается как ~42 + 1 = 11010101 + 00000001 = 11010110. После -1 = 11111111 идет 0 = -1 + 1 = 11111111 + 00000001 = 00000000. Упражнение. Каких чисел больше: положительных или отрицательных?\nОсторожно. В стандарте C/C++ прописано, что переполнение знаковых переменных приводит к undefined behavior, поэтому полагаться на описанную логику переполнения нельзя, хотя равно это скорее всего и произойдет.\n#128-битные числаОбщих регистров размера больше 64 в процессорах нет, однако умножение и несколько смежных инструкций могут использовать два последовательных регистра как один большой. Это позволяет быстро перемножать два 64-битных числа и получать 128-битный результат, разделенный на нижние биты и верхние.\nЭто весьма специфичная операция, и поэтому в языках программирования нет полноценной поддержки 128-битных переменных. В C++ однако есть «костыль» — тип __int128_t — который фактически просто оборачивает пару из двух 64-битных регистров и поддерживает арифметические операции с ними.\n__int128_t x = 1; int64_t hi = x \u0026gt;\u0026gt; 64; // получить верхние 64 бит int64_t lo = (int64_t) x; // получить нижние 64 бит Базовые арифметические операции с ним чуть медленнее, его нельзя напрямую печатать, и деление и прочие сложные операции будут вызывать отдельную библиотеку и поэтому работать очень долго, однако в некоторых ситуациях он оказывается очень полезен.\n","id":10,"path":"/cs/arithmetic/bit-representation/","title":"Битовое представление чисел"},{"content":"Префиксное дерево или бор (англ. trie) — структура данных для компактного хранения строк, устроенная в виде дерева, где на рёбрах между вершинами записаны символы, а некоторые вершины помечены терминальными.\nГоворят, что префиксное дерево принимает строку $s$, если существует такая терминальная вершина $v$, что, если выписать подряд все буквы на путях от корня до $v$, то получится строка $s$.\nБор сам по себе можно использовать для разных задач:\nХранение строк: если есть много повторяющихся длинных префиксов, то бор может занимать гораздо меньше места, чем массив или set строк. Сортировка строк: по построенному бору можно пройтись dfs-ом и вывести все строки в лексикографическом порядке. Просто как множество строк: как мы увидим, в бор легко добавлять и удалять слова, а также делать проверки вхождения. С точки зрения теории автоматов, каждая вершина — это состояние, а все корректные односимвольные дополнения являются корректными переходами в автомате. Бор таким образом является автоматом, проверяющим вхождение слова в множество.\n#РеализацияПрефиксное дерево проще всего хранить в виде ссылающихся друг на друга вершин. В каждой вершине обычно хранится, является ли вершина терминальной, ссылки на детей, и какая дополнительная информация, зависящая от задачи — например, если мы хотим реализовать мультисет, можно хранить количество слов, заканчивающихся в вершине.\nДля латинского алфавита (в котором 26 строчных букв) изначально пустой бор можно реализовать так:\nconst int k = 26; struct Vertex { Vertex* to[k] = {0}; // нулевой указатель означает, что перехода нет bool terminal = 0; }; Vertex *root = new Vertex(); Чтобы добавить слово в бор, нужно пройтись от корня по символам слова. Если перехода по для очередного символа нет — создать его, иначе пройти по уже существующему. В конце текущее состояние нужно не забыть пометить терминальным.\nvoid add_string(string \u0026amp;s) { v = root; for (char c : s) { c -= \u0026#39;a\u0026#39;; // получаем число от 0 до 25 if (!v-\u0026gt;to[c]) v-\u0026gt;to[c] = new Vertex(); v = v-\u0026gt;to[c]; } v-\u0026gt;terminal = true; } Чтобы проверить, есть ли слово в боре, нужно так же пройти от корня по символам слова. Если в конце оказались в терминальной вершине — то слово есть. Если оказались в нетерминальной вершине или когда-нибудь потребовалось пройтись по несуществующей ссылке — то нет.\nbool find(string \u0026amp;s) { v = root; for (char c : s) { c -= \u0026#39;a\u0026#39;; if (!v-\u0026gt;to[c]) return false; v = v-\u0026gt;to[c]; } return v-\u0026gt;terminal; } Удалить слово можно «лениво»: просто дойти до него и убрать флаг терминальности.\nbool erase(string \u0026amp;s) { v = root; for (char c : s) v = v-\u0026gt;to[c - \u0026#39;a\u0026#39;]; v-\u0026gt;terminal = false; } В зависимости от задачи эти процедуры иногда следует изменить. Например, если мы хотим реализовать автодополнение — по запросу найти все слова с заданным префиксом — можно аккуратно удалять вершины, если они не ведут в терминальные вершины, и тогда при ответе на запрос можно просто пройтись dfs-ом из состояния-префикса, выводя ответ во всех терминальных вершинах.\n#Как хранить ссылкиИногда ограничения не позволяют хранить ссылки на детей просто в массиве.\nНапример, если алфавит большой — тогда нам не хватит ни времени, ни памяти инициализировать столько массивов, большинство из которых будут пустыми.\nПомимо массивов указателей, есть много других способов хранить отображение из символа в ссылку:\nрасширяющийся массив (std::vector), бинарное дерево (std::map), хеш-таблица (std::unordered_map). Чаще всего память является основным практическим соображением. Также на 64-битных системах может быть выгодно вместо new и указателей выделять всё на большом массиве или векторе и хранить 4-байтные индексы вершин вместо 8-байтных указателей на память.\n#Цифровой борВ некоторых задачах появляется идея хранить в боре числа, подобно строкам — такая структура называется цифровым бором.\nЧаще всего числа надо записывать в двоичной системе счисления, и тогда все очень просто, но в некоторых задачах требуется какая-то другая система счисления, а так, например, как сравнивать числа в лексикографическом порядке в десятичной системе счисления нельзя (2 \u0026gt; 11). Чтобы избавиться от этого момента, будем считать, что все числа меньше какой-то степени 10, и тогда будем просто дополнять число ведущими нулями: теперь 02 \u0026lt; 11.\nЗадача. Задано некоторое множество чисел, и требуется отвечать на три вида запросов:\nДобавить число $x$. Удалить число $x$. Найти число $y$ в массиве, у которого xor c $x$ максимален. Первые два вида запросов мы уже умеем делать в цифровом боре, а запрос третьего типа сложнее.\nЗаметим, что если существует $y$ из множества, такой что $y_i \\neq x_i$ (то есть они отличаются в $i$-ом бите), то взять его выгоднее, чем любое другое число, у которого префикс до $i$-того бита включительно равен префиксу $x$ — потому что xor такого числа с $x$ будет содержать только меньшие биты, чем $i$-тый, и соответственно не будет превосходить $2^i$, который мы уже можем получить с $y$.\nТогда при ответе на запрос третьего типа мы можем жадно спускаться по бору, каждый раз пытаясь пойти в ветку, у которой $i$-тый бит не равен $x_i$.\n","id":11,"path":"/cs/string-structures/trie/","title":"Префиксное дерево"},{"content":"Наибольшим общим делителем (англ. greatest common divisor) целых неотрицательных чисел $a$ и $b$ называется наибольшее число $x$, которое делит одновременно и $a$, и $b$.\n$$ \\gcd(a, b) = \\max_{k: ; k|a , \\land , k | b} k $$\nКогда оба числа равны нулю, результат не определён — подойдёт сколько угодно большое число. За исключением этого случая, верно следующее наблюдение: если одно из чисел равно нулю, то их $\\gcd$ равен второму числу.\n#Алгоритм нахожденияАлгоритм Евклида находит $\\gcd$ двух чисел $a$ и $b$ за $O(\\log \\min(a, b))$, основываясь на следующей несложной формуле:\n$$ \\gcd(a, b) = \\begin{cases} a, \u0026amp; b = 0 \\ \\gcd(b,, a - b), \u0026amp; b \u0026gt; 0 \\end{cases} $$\nЗдесь предполагается, что $a \u0026gt; b$.\nДокажем корректность этой формулы:\nЕсли $g = \\gcd(a, b)$ делит и $a$, и $b$, то их разность $(a-b)$ тоже будет делиться на $g$.\nНикакой больший делитель $d$ числа $b$ не может делить число $(a-b)$: если $d \u0026gt; g$, то $d$ не может делить $a$, а значит и не делит $(a - b)$.\nПрямая рекурсивная реализация:\nint gcd(int a, int b) { if (a \u0026lt; b) swap(a, b); if (b == 0) return a; else return gcd(b, a - b); } Этот алгоритм может работать долго — например, на паре $(10^9, 1)$ он сделает миллиард итераций.\nИдея дальнейшей оптимизации в том, чтобы вычитать из $a$ не одно $b$ за раз, а столько, чтобы в следующий раз $a$ и $b$ уже поменялись местами — чтобы новое $b$ стало меньше нового $a$. Простой способ этого достичь — просто вычесть $b$ из $a$ сразу максимально возможное число раз, то есть взять вместо нового $b$ остаток от деления $a$ на $b$:\n$$ \\gcd(a, b) = \\begin{cases} a, \u0026amp; b = 0 \\ \\gcd(b,, a \\bmod b), \u0026amp; b \u0026gt; 0 \\end{cases} $$\nРеализация:\nint gcd(int a, int b) { if (b == 0) return a; else return gcd(b, a % b); } Чуть более быстрая итеративная форма:\nint gcd(int a, int b) { while (b \u0026gt; 0) { a %= b; swap(a, b); } return a; } В современном C++ есть встроенная библиотечная функция gcd, которую рекомендуется использовать, не забывая про случай отрицательных чисел и $(0, 0)$.\nТакже помимо алгоритма Евклида существует в 2-3 раза более быстрый бинарный GCD.\n#Время работыМожно показать, что каждые две итерации меньшее число уменьшится хотя бы в два раза, а следовательно алгоритм работает за $O(\\log \\min (a, b))$. Эта оценка относится не только к худшему случаю, но и к среднему.\nВремя работы алгоритма на разных входных данных Примечательно, что худшие входные данные для алгоритма — это соседние числа Фибоначчи. На графике они видны как синие точки в пропорциях золотого сечения.\nТакже иногда полезно знать, что нахождение $\\gcd$ группы из $n$ чисел от $1$ до $A$ будет работать не за $O(n \\log A)$, а за $O(n + \\log A)$ — это несложно доказать по индукции.\n","id":12,"path":"/cs/modular/euclid/","title":"Алгоритм Евклида"},{"content":"Задача. Загадано целое число $x$ от $1$ до $100$, которое вам нужно отгадать какой-нибудь «данеткой»: например, вы можете спрашивать, больше ли число $x$ чем заданное, или четно ли оно. За сколько вопросов в худшем случае вы сможете найти число $x$?\nОдно из оптимальных решений имеет такую структуру: первым вопросом спрашиваем «больше ли число $x$, чем 50», и если ответ «да», то дальше спрашиваем «больше ли число $x$, чем 75», иначе «больше ли число $x$, чем 25», и повторяем дальше, каждый раз уменьшая отрезок возможных значений в два (или почти в два) раза.\nБолее структурно, если у нас есть отрезок поиска — изначально от $l=1$ до $r=100$ — то мы на каждой итерации выбираем «серединный» элемент $m = \\lfloor \\frac{l+r}{2} \\rfloor$, спрашиваем «$x\u0026gt;m?$», выполняем присвоения $l = m + 1$ или $r = m$ в зависимости от ответа, и повторяем процедуру с новыми границами, пока они не станут равными (что будет означать, что $l = r = x$).\nВот пример такой программы, которую можно запустить в консоли:\nl, r = 1, 100 while l \u0026lt; r: m = (l + r) // 2 resp = input(f\u0026#39;x \u0026gt; {m}? \u0026#39;) if resp == \u0026#39;yes\u0026#39;: l = m + 1 else: r = m print(f\u0026#39;x = {l}\u0026#39;) Если загадать $x=42$ и честно поотвечать на вопросы:\nx \u0026gt; 50? no x \u0026gt; 25? yes x \u0026gt; 38? yes x \u0026gt; 44? no x \u0026gt; 41? yes x \u0026gt; 43? no x \u0026gt; 42? no x = 42 Здесь нам потребовалось 7 вопросов, но в зависимости от загаданного числа иногда мы будем спрашивать ровно $7$ вопросов, а иногда $6$.\nВ общем случае, так как на каждой итерации длина отрезка поиска гарантированно уменьшится в два раза (возможно, с округлением вверх), то нам потребуется $O(\\log n)$ операций, а точнее либо $\\lceil \\log_2 n \\rceil$, либо $\\lfloor \\log_2 n \\rfloor$.\n#В структурах данныхПусть дан массив $a$, и требуется определить, есть ли в нём элемент $x$.\nОчевидно, в худшем случае этого элемента в массиве нет, но чтобы удостовериться в этом, нужно потратить $O(n)$ операций на просмотр всех элементов массива.\nОднако, если массив отсортирован, найти число $x$ в массиве можно и быстрее: можно аналогичным способом найти нижнюю грань (англ. lower bound) — самое малое число, не меньшее $x$ — и проверить, оказалось ли оно равно $x$.\nbool find(int x, int *a, int n) { int l = 0, r = n + 1; // отрезок поиска теперь полуинтервал: [l, r) while (l + 1 \u0026lt; r) { int m = (l + r) / 2; if (a[m] \u0026gt;= x) l = m; else r = m; } return (l \u0026lt; n \u0026amp;\u0026amp; a[l] == x); } Для стандартных контейнеров STL такая функция реализована:\nbool find(x, vector\u0026lt;int\u0026gt; a) { auto it = lower_bound(a.begin(), a.end(), x); return (it != a.end() \u0026amp;\u0026amp; *it == x); } Также в C++ есть функция upper_bound, которая находит первый элемент, который строго больше аргумента.\n#Проверка свойствПодобный метод обобщается не только на поиск элементов, но и на проверку каких-либо монотонных свойств, которые сначала выполняются, а потом не выполняются, или наоборот.\nПоиск нижней границы в массиве — частный случай: мы проверяем условие $(a[k] \\geq x)$ и хотим найти, начиная с какого $k$ оно выполняется.\nДругие примеры:\nНайти последнее число, равное $x$ в отсортированном массиве, или вывести, что таких чисел нет. Посчитать, сколько раз встречается число $x$ в отсортированном массиве. Дан массив чисел, первая часть состоит из нечетных чисел, а вторая из четных. Найти индекс, начиная с которого все числа четные. Все эти задачи решаются бинарным поиском за $O(\\log{n})$. Правда нужно понимать, что в чистом виде такую задачу решать двоичным поиском бессмысленно — ведь чтобы создать массив размера $n$, уже необходимо потратить $O(n)$ операций.\nПоэтому зачастую такие задачи сформулированы таким образом: дан отсортированный массив размера $n$. Нужно ответить на $m$ запросов вида «встречается ли число $x_i$ в массиве?». Подобные задача решается за $O(n + m\\log{n})$ — нужно сначала создать массив за $O(n)$ и $m$ раз запустить бинарный поиск.\n#Бинарный поиск с вещественными аргументамиУ нас все еще есть функция-предикат $f(x)$, которая сначала равна нулю, а потом единице, и мы хотим найти это место, где она меняет значение. Но теперь аргумент функции — вещественное число.\nПример такой функции:\n$$ f(x) = \\begin{cases} 0, \u0026amp; x^2 \u0026lt; 2 \\ 1, \u0026amp; x^2 \\geq 2 \\end{cases} $$\nПри $x \u0026lt; \\sqrt 2$, $f(x) = 0$, а при сколько-либо большем значении $f(x) = 1$. Если мы научимся находить такое $x_0$, что $f(x_0) = 0$ и $f(x_0 + \\epsilon) = 1$ для какого-то малого $\\epsilon$, то мы научимся считать корень из двух — и любого другого положительного числа.\nМы можем так же применить здесь бинарный поиск, но увы, возникает проблема: действительные числа хранятся в компьютере неточно.\n# известный пример print(0.1 + 0.1 + 0.1) # = 0.30000000000000004 Тем более не сможем найти точное значение $\\sqrt 2$, потому что это бесконечная непериодическая дробь.\nТак что при реализации бинарного поиска с вещественными аргументами нужно действовать осторожно:\nНужно убедиться, что $f(l_0) = 0$ и $f(r_0) = 1$. Нужно либо останавливаться, когда $r - l \u0026lt; \\epsilon$, либо (лучше) делать фиксированное число итераций. Сколько итераций понадобится, чтобы получить $k$ правильных цифр? Так как двоичный поиск работает за двоичный логарифм, можно сказать, что на угадывание десятичного разряда числа потребуется примерно три шага бинпоиска ($\\log 10 \\approx 3$). Значит, если нам, например, нужно посчитать значение функции до шести знаков после запятой, то нам нужно ещё примерно 18 шагов уже после того, как расстояние между $l$ и $r$ достигло одного.\nЧтобы каждый раз об этом не думать, можно считать, что ста шагов бинпоиска всегда хватит для всех разумных целей:\nfloat sqrt(float x) { float l = 0, r = x; for (int i = 0; i \u0026lt; 100; i++) { float m = (l + r) / 2; if (m * m \u0026lt; x) l = m; else r = m; } return l; } На самом деле, так можно искать ноль любой непрерывной функции (мы сейчас искали ноль функции $x^2 - 2$), у которой известны значения аргумента, при которых она принимает значения меньше нуля и больше нуля.\n","id":13,"path":"/cs/interactive/binary-search/","title":"Бинарный поиск"},{"content":"В этой главе мы поговорим о том, как оценивать вычислительные ресурсы — такие как время и память — которые требуются для завершения алгоритмов.\n","id":14,"path":"/cs/complexity/","title":"Вычислительная сложность"},{"content":"В этой статье мы реализуем дерево отрезков для точечных изменений и суммы на отрезке. Наша первая реализация будет на указателях. Она не самая быстрая и компактная, но зато самая общая и понятная.\nЗамечание. Почти везде мы будем использовать полуинтервалы — обозначаемые как $[l, r)$ — вместо отрезков. Несмотря на контринтуитивность, это немного упростит код и вообще является хорошей практикой в программировании, подобно нумерации с нуля.\n#Хранение в памятиКаждая вершина дерева отрезков будет структурой, которая содержит ссылки на детей, свои границы $[l, r)$ и дополнительную нужную для задачи информацию — в нашем случае, сумму на отрезке.\nВершины будем создавать во время построения без какого-либо заданного порядка, поэтому ссылки на детей будут просто указателями на другую такую же структуру вершины-ребенка.\nstruct Segtree { int lb, rb; // левые и правые границы отрезков int s = 0; // сумма на текущем отрезке Segtree *l = 0, *r = 0; // указатели на детей (0 означает, что ребенка нет) Segtree(int lb, int rb) { /* конструктор: метод, вызывающийся при создании */ } void add(int k, int x) { /* отреагировать на a[k] += x */ } int sum(int lq, int rq) { /* вывести сумму [lq, rq) */ } }; В олимпиадных задачах запросы часто подаются в формате «строка 0 k x для прибавления и 1 l r для суммы». Их можно парсить так:\nint n, q; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; q; Segtree s(0, n); while (q--) { int t, x, y; cin \u0026gt;\u0026gt; t \u0026gt;\u0026gt; x \u0026gt;\u0026gt; y; if (t) s.add(x, y); else cout \u0026lt;\u0026lt; s.sum(x, y) \u0026lt;\u0026lt; endl; } Осталось собственно реализовать построение и запросы.\n#ПостроениеСтроить дерево отрезков можно рекурсивным конструктором, который создает детей, пока не доходит до листьев:\nSegtree(int lb, int rb) : lb(lb), rb(rb) { if (lb + 1 \u0026lt; rb) { // если не лист, создаем детей int t = (lb + rb) / 2; l = new Segtree(lb, t); r = new Segtree(t, rb); } } Если изначально массив не нулевой, то можно параллельно с проведением ссылок насчитывать суммы:\nSegtree(int lb, int rb) : lb(lb), rb(rb) { if (lb + 1 == rb) s = a[lb]; else { int t = (lb + rb) / 2; l = new Segtree(lb, t); r = new Segtree(t, rb); s = l-\u0026gt;s + r-\u0026gt;s; } } Альтернативно, можно либо потом отдельно пройтись по массиву и добавить каждое число по отдельности.\n#ИзменениеДля запроса прибавления будем рекурсивно спускаться вниз, пока не дойдем до листа, соответствующего элементу $k$, и на всех промежуточных вершинах прибавим $x$:\nvoid add(int k, int x) { s += x; // проверим, есть ли у нас дети: if (l) { // если k в левой половине if (k \u0026lt; l-\u0026gt;rb) l-\u0026gt;add(k, x); else r-\u0026gt;add(k, x); } } #СуммаДля суммы сложнее — нужно делать разбор случаев, как отрезок запроса пересекается с отрезком вершины:\nint sum(int lq, int rq) { if (lb \u0026gt;= lq \u0026amp;\u0026amp; rb \u0026lt;= rq) // если мы лежим полностью в отрезке запроса, вывести сумму return s; if (max(lb, lq) \u0026gt;= min(rb, rq)) // если мы не пересекаемся с отрезком запроса, вывести ноль return 0; // иначе всё сложно -- запускаемся от детей и пусть они там сами решают return l-\u0026gt;sum(lq, rq) + r-\u0026gt;sum(lq, rq); } #ОптимизацииДанная реализация простая и расширяемая, но весьма неэффективная по времени и памяти, хотя ей можно сдать примерно 90% задач на олимпиадах по программированию.\nИз относительно легких оптимизаций:\nМожно не хранить границы отрезка lb и rb, а пересчитывать их во время спуска. На 64-битных системах выгоднее использовать не new и указатели, а выделять вершины на массиве / векторе и использовать индексы относительно него: они будут весить 4 байта, а не 8. Однако основная причина, почему реализация на указателях такая медленная, в том, что нужно много раз ходить по ссылкам, чтобы получать данные нужных вершин. Оказывается, можно избавиться от ссылок вообще, если ввести однозначную нумерацию вершин и расположить их в таком порядке на массиве — подробнее об этом можно почитать у Емакса и на CodeForces.\n","id":15,"path":"/cs/segment-tree/pointers/","title":"Дерево отрезков на указателях"},{"content":"Разберем несколько учебных задач на динамику, где состоянием является префикс или непрерывный подотрезок какого-то массива.\n#КузнечикЕсть полоска $1 \\times n$. Кузнечик стоит на первой клетке, он может прыгать вперед на 1, 2 или 3 клетки. Сколько есть способов добраться от начальной клетки до последней?\nПопытаемся придумать рекуррентную формулу — то есть вывести, как ответ для $n$ зависит от ответа для меньших чисел.\nПусть $f[k]$ равно количеству способов добраться от 1 клетки до клетки номер $k$. Попытаемся руками посчитать несколько его первых значений:\n$f[1] = 1$ способ (стоять на месте) $f[2] = 1$ способ: $$ 1 \\rightarrow 2 $$ $f[3] = 2$ способа: $$ \\begin{aligned} 1 \\rightarrow 2 \\rightarrow 3 \\ 1 \\rightarrow 3 \\end{aligned} $$ $f[4] = 4$ способа: $$ \\begin{aligned} 1 \\rightarrow 2 \\rightarrow 3 \\rightarrow 4 \\ 1 \\rightarrow 3 \\rightarrow 4 \\ 1 \\rightarrow 2 \\rightarrow 4 \\ 1 \\rightarrow 4 \\end{aligned} $$ $f[5] = 7$ способов: $$ \\begin{aligned} 1 \\rightarrow 2 \\rightarrow 3 \\rightarrow 4 \\rightarrow 5 \\ 1 \\rightarrow 3 \\rightarrow 4 \\rightarrow 5 \\ 1 \\rightarrow 2 \\rightarrow 4 \\rightarrow 5 \\ 1 \\rightarrow 4 \\rightarrow 5 \\ 1 \\rightarrow 2 \\rightarrow 3 \\rightarrow 5 \\ 1 \\rightarrow 3 \\rightarrow 5 \\ 1 \\rightarrow 2 \\rightarrow 5 \\end{aligned} $$ Дальше становится сложнее. Но можно заметить закономерность. А можно и не заметить. В любом случае, когда мы придумаем формулу, с выписанными несколькими первыми значениями легко проверить, работает ли она или нет.\nДавайте подумаем, каким мог быть последний прыжок кузнечика в его пути до $n$-й клетки? Один из трёх вариантов:\n$(n - 1) \\rightarrow n$ $(n - 2) \\rightarrow n$ $(n - 3) \\rightarrow n$ То есть все пути до $n$ разбиваются на 3 группы, и причём мы знаем сколько путей в каждой группе: в первой из них ровно $f[n-1]$ путей — столько путей идут до $(n-1)$-й клетки, и дальше идет еще один прыжок; аналогично, во второй и третьей группах $f[n-2]$ и $f[n-3]$ путей соответственно.\nПросуммировав число путей во всех трёх группах, получаем\n$$ f[n] = f[n-3] + f[n-2] + f[n-1] $$\nОчень похоже на числа Фибоначчи, да? На самом деле, если бы кузнечик мог прыгать только на 1 или 2 клетки вперед, ответом было бы в точности $n$-ое число Фибоначчи.\nДля подсчета этой динамики можно аналогично завести массив и заполнять его проходом слева направо:\nf[0] = 1 f[1] = 1 f[2] = 2 for (int i = 3; i \u0026lt; n; i++) f[i] = f[i - 3] + f[i - 2] + f[i - 1]; Примечание. Такая последовательность называется числами трибоначчи.\nУпражнение. Определите количество последовательностей из нулей и единиц длины $n$, в которых нет трёх единиц подряд.\n#Кузнечик с препятствиямиНемного изменим задачу: теперь некоторые из клеток закрыты, то есть нам известно про какие-то конкретные клетки, что на них кузнечик прыгать не может.\nТогда задача все еще решается такой же рекурсивной формулой, только нужно убедиться, что $f[k] = 0$ для всех запрещенных $k$.\nТак как нам нужно добавить в реализацию такую проверку, немного отрефакторим код, чтобы не рассматривать прыжок каждого размера по отдельности, а также будем инициализировать только $f_1$:\n// a -- булевый массив, свободна ли i-ая клетка f[0] = a[0]; for (int i = 1; i \u0026lt; n; i++) for (int j = i; j \u0026gt; max(i - 3, 0); j--) // f[i] изначально заполнены нулями f[i] += f[j] * a[j]; Заметим, что такая реализация масштабируется на любую длину прыжка $k$. Асимптотика будет $O(n k)$.\n#Число подпалиндромовПалиндромом называется последовательность символов $a_i$ длины $n$, для которой выполняется $a_i = a_{n-i}$.\nТребуется для данной строки размера $n$ определить, сколько её подстрок являются палиндромами.\nЗаведем динамику $f[l, r]$, равную единице, если подстрока с $l$ по $r$ является палиндромом, и нулем в противном случае. Посчитав её, мы найдем все подстроки-палиндромы в исходной строке, и для ответа сможем просто просуммировать все ячейки этой динамики.\nОпределение палиндрома можно альтернативно сформулировать так: строка $s$ является палиндромом, если $s_0 = s_n$ и строка $s_{1\\ldots n - 1}$ является палиндромом. Такое определение и будем использовать для пересчета динамики:\n$$ f[l, r] = (s_l = s_r) \\land f[l + 1, r - 1] $$\nЗаведем двумерный массив для хранения динамики и пройдемся по базовым случаям: подстрокам размера 1, всегда являющимися палиндромами, и подстрокам размера 2, являющимися палиндромами если первый символ равен второму. Для остальных состояний будем пользоваться формулой выше.\nЕдинственный нюанс — порядок обхода. Нельзя просто проходиться фориком сначала в порядке увеличения $l$, а потом увеличения $r$, потому что пересчет зависит от динамик с большими $l$. Вместо этого будем итерироваться по размеру подстроки $d = r - l + 1$, а затем перебирать все подстроки с таким размером. Тогда для любой подстроки динамика для $f[l + 1][r - 1]$ уже будет посчитана.\nbool f[n][n]; for (int i = 0; i \u0026lt; n; i++) { f[i][i] = true; f[i][i + 1] = (s[i] == s[i + 1]); } for (int d = 3; d \u0026lt;= n; d++) for (int l = 0, r = l + d - 1; r \u0026lt; n; l++, r++) f[l][r] = (s[l] == s[r]) \u0026amp;\u0026amp; f[l + 1][r - 1]; int ans = 0; for (int l = 0; l \u0026lt; n; l++) for (int r = l; r \u0026lt; n; r++) ans += f[l][r]; Альтернативно, можно проходиться в порядке уменьшения $l$ и увеличения $r$. Также базовым состоянием для палиндромов четного размера можно выбрать «подстроки размера ноль», всегда являющиеся палиндромами, выполнив в цикле f[i][i - 1] = true — либо тогда уже просто инициализировать весь массив динамик единицами и начинать с $d=2$:\nmemset(f, 1, sizeof f); for (int l = n - 1; l \u0026gt;= 0; l--) for (int r = l + 1; r \u0026lt; n; r++) f[l][r] = (s[l] == s[r]) \u0026amp;\u0026amp; f[l + 1][r - 1]; Отметим, что у этой задачи есть более оптимальные решения, в том числе линейные.\n","id":16,"path":"/cs/general-dynamic/segments/","title":"Динамика по подотрезкам"},{"content":"В этой статье мы рассмотрим различные методы сведения задач на деревьях к задачам на отрезках, которые можно решать уже известными структурами данных.\nПеред прочтением рекомендуется вспомнить свойства массивов $tin$ и $tout$, получаемых обходом в глубину.\n#Запросы на поддеревьяхПервым важным свойством dfs является то, что $tin$-ы вершин любого поддеререва являются каким-то непрерывным отрезком.\nЭто свойство можно использовать для обработки разных запросов на поддеревьях, сводя их к запросам на подотрезках, которые уже можно решать стандартными методами — например, через дерево отрезков.\n#Сумма на поддеревеЗадача. Дано корневое дерево. Рядом с каждой вершиной записано число. Поступают два типа запросов: изменить какое-то из значений и найти сумму значений на поддереве вершины $v_i$.\nВыпишем все числа у вершин в один массив в позиции, соответствующие их $tin$-ам. Что такое «сумма на поддереве $v$» в терминах этого массива? Это просто сумма на подотрезке $[tin_v, tout_v)$.\nПостроим поверх массива дерево отрезков или любую другую структуру для динамической суммы, и для запросов первого типа будем отправлять структуре запрос изменения ячейки $a_{tin_v} = x$, а для второго будем делать запрос суммы на подотрезке $[tin_v, tout_v)$.\n#Запросы на уровняхПомимо $tin$ и $tout$ бывает полезно во время обхода считать глубину вершины. Будем в дальнейшем обозначать её за $depth_v$.\n#Level AncestorЗадача. Дано корневое дерево. Требуется отвечать на запросы нахождения $d_i$-того предка вершины $v_i$, то есть вершины-предка, находящейся на расстоянии $d_i$ от $v_i$.\nСоздадим $h$ векторов, где $h$ — высота дерева. Для каждой вершины, во время прохода в dfs, добавим её $tin$ в вектор, соответствующей её глубине. Получаем $h$ отсортированных векторов.\nТеперь заметим, что внутри одного вектора все отрезки поддеревьев вершин — $[tin_v, tout_v)$ — тоже не пересекаются, а значит ещё и отсортированы. Тогда для ответа на запрос мы можем просто взять $tin$ вершины-запроса, посмотреть на вектор нужного уровня и за $O(\\log n)$ сделать бинпоиск по нужному отрезку.\nТакже существует другой способ, требующий $O(1)$ времени на запрос, но $O(n \\log n)$ памяти на предподсчет — лестничная декомпозиция.\n#Поддеревья заданной глубиныЗадача. Дано корневое дерево. Рядом с каждой вершиной записано число. Поступают два типа запросов: изменить какое-то из значений и найти сумму значений на поддереве вершины $v_i$ среди вершин на расстоянии не более $k_i$ от неё.\nКогда используется одновременно и глубина, и структура дерева, обычно помогает взглянуть на задачу геометрически. Сопоставим каждой вершине точку $(tin_u, depth_u)$. Тогда как выглядят искомые множества вершин? Они соответствуют тем точкам, у которых первая координата лежит между $tin_v$ и $tout_v$, а вторая больше $depth_v$.\nСоответственно, запросы на таких поддеревьях можно свести к запросам на прямоугольниках, которые можно решать либо «в лоб» двумерными структурами, либо применить методы вроде сканирующей прямой.\n#Запросы на путяхЕсли даны запросы на путях в оффлайн, то почти всегда их можно решить каким-то предподсчетом.\n#Сумма на путиЗадача. Дано дерево. У каждого ребра есть какое-то число. Нужно отвечать на запросы нахождения суммы на пути.\nПредподсчитаем во время обхода в глубину массив $s$, где $s_v$ это сумма чисел на пути от корня до вершины $v$.\nЛюбой путь в дереве разбивается на один или два вертикальных пути. Найдем наибольшего общего предка $c = lca(a, b)$ и разобьём сумму как $(s_a + s_b - 2 \\cdot s_c)$.\n#Xor на путиЗадача. Дано дерево. У каждого ребра есть какое-то число. Нужно отвечать на запросы нахождения xor-суммы на пути.\nЗаметим, что в случае с xor-суммой не нужно даже искать эти пути — по модулю двойки слагаемое $2 \\cdot s_c$ «отменит» само себя. Достаточно просто вывести s[a] ^ s[b].\n#Число различных чисел на путиЗадача. Дано дерево. У каждого ребра есть какое-то число. Требуется отвечать на $q$ запросов нахождения числа различных значений на пути с $v$ по $u$.\nДля более сложных запросов в оффлайн почти всегда нужно использовать обобщение алгоритма Мо на деревья, заключающееся примерно в следующем.\nВыпишем эйлеров обход дерева: при каждом проходе по ребру выписываем номер ребра (номер нижней/верхней вершины). Заметим, что каждое ребро будет выписано дважды: при входе и выходе из нижней вершины. Получив запрос, определим «более раннюю» вершину $v$ (с меньшим $tin_v$) и рассмотрим подотрезок эйлерова обхода с $tin_v$ по $tin_u$. Какая-то подпоследовательность ребер на этом отрезке является кратчайшим путем между $v$ и $u$. А именно, это будут ровно те ребра, которые встречались на этом отрезке ровно один раз — все, которые встречались дважды, ведут в какие-то побочные поддеревья. Теперь рядом с номерами ребер в обходе выпишем ещё и соответствующие им числа. Тогда запрос нахождения числа различных значений на пути эквивалентен нахождению числа различных значений у ребер, встречающихся ровно один раз на подотрезке. Эту задачу уже можно решить обычным алгоритмом Мо.\n","id":17,"path":"/cs/trees/tree-queries/","title":"Запросы на деревьях"},{"content":"В этой статье мы докажем одну лемму, ключевую для алгоритма нахождения паросочетания в двудольном графе. Но перед этим потребуется ввести ещё несколько понятий.\nЦепью длины $k$ назовём некоторый простой путь (т.е. не содержащий повторяющихся вершин или рёбер), содержащий ровно $k$ рёбер.\nЧередующейся цепью относительно некоторого паросочетания назовём простой путь длины $k$ в котором рёбра поочередно принадлежат/не принадлежат паросочетанию.\nУвеличивающей цепью относительно некоторого паросочетания назовём чередующуюся цепь, у которой начальная и конечная вершины не принадлежат паросочетанию.\nЗдесь красными помечены вершины паросочетания, а в графе есть увеличивающая цепь: $1 \\to 8 \\to 4 \\to 6 \\to 3 \\to 7$ Как подсказывает название, с помощью увеличивающей цепи можно увеличить мощность паросочетания на единицу. Для этого можно взять такую цепь и провести чередование: убрать из паросочетания все рёбра, принадлежащие цепи, и вместо них добавить все остальные ребра цепи.\nВсего в увеличивающей цепи нечетное число рёбер, а первое и последнее были не в паросочетании. После чередования каждая затронутая вершина будет соединена ровно с одной из другой доли. Значит, после чередования паросочетание осталось корректным, а его мощность увеличилась ровно на единицу.\nВ примере выше добавятся синие рёбра $(1, 8)$, $(3, 7)$ и $(4, 6)$, а удалятся красные $(3, 6)$ и $(4, 8)$. С ребром $(2, 5)$ ничего не случится — оно не в увеличивающей цепи. Таким образом, размер паросочетания увеличится на единицу.\nРаз чередованием увеличивающей цепи можно увеличить мощность паросочетания на единицу, то у максимального паросочетания увеличивающих цепей быть не должно. Оказывается, обратное тоже верно.\n#Теорема БержаТеорема. Паросочетание без увеличивающих цепей является максимальным.\nДоказательство проведём от противного: пусть есть два паросочетания вершин $|A| \\leq |B|$, и для $A$ нет увеличивающих путей. Покажем, как найти этот путь и увеличить $A$ на единицу.\nРаскрасим ребра из паросочетания, соответствующего $A$ в красный цвет, $B$ — в синий, а ребра из обоих паросочетаний — в пурпурный. Рассмотрим теперь граф из только красных и синих ребер. Так как это фактически симметричная разность двух паросочетаний, у каждой вершины будет не более 2 смежных ребер. Значит, любая компонента связности в нём представляет собой либо путь, либо цикл, состоящий из чередующихся красных и синих ребер. В любом цикле будет равное число красных и синих рёбер, а так как всего синих рёбер больше, то должен существовать путь, начинающийся и оканчивающийся синим ребром — он и будет увеличивающей цепью для $A$, а значит $A$ не максимальное, и мы получили противоречие.\n","id":18,"path":"/cs/matching/berge/","title":"Лемма Бержа"},{"content":"Назовем подграф $T$ графа $G$ безопасным, если он является подграфом какого-то минимального остова.\nНазовем ребро безопасным, если при добавлении его в подграф $T$ получившийся подграф $T\u0026rsquo;$ тоже является безопасным, то есть подграфом какого-то минимального остова.\nРазрезом связного графа будем называть подмножество рёбер и вершин, образующих подграф, в котором есть ровно две компоненты связности. Ребро пересекает данный разрез, если при его добавлении граф снова становится связным.\nВсе алгоритмы для поиска минимального остова опираются на следующее утверждение:\nЛемма о безопасном ребре. Рассмотрим произвольный разрез какого-либо подграфа минимального остова. Тогда ребро минимального веса, пересекающее этот разрез, является безопасным.\nДоказательство: Рассмотрим какой-то минимальный остов, в котором этого ребра нет. Если его добавить, то образуется цикл, из которого можно удалить ребро не меньшего веса, получив ответ точно не хуже. Противоречие.\nПолучается, что для построения минимального остова мы можем действовать жадно: на каждом шаге добавлять ребро минимального веса, которое увеличивает уже построенную часть остова. Есть несколько способов это сделать, о которых мы поговорим дальше.\n#Следствия Если веса всех рёбер различны, то остов будет уникален. Минимальный остов является также и остовом с минимальным произведением весов рёбер (замените веса всех рёбер на их логарифмы). Минимальный остов является также и остовом с минимальным весом самого тяжелого ребра. Остовные деревья — частный случай матроидов. ","id":19,"path":"/cs/spanning-trees/safe-edge/","title":"Лемма о безопасном ребре"},{"content":"В C++ есть несколько способов объединить группу переменных фиксированного размера в одну переменную.\n#Массивы в CВ языке C есть три основных способа определить массив:\nint a[100]; int main() { int b[100]; int *c = new int[100]; del[] c; return 0; } Получившиеся переменные функционально идентичные, но немного отличаются:\nОпределенный глобально массив a будет лежать в заранее выделенной области памяти на протяжении всего времени исполнения программы. Все элементы изначально заполнены своим значением по умолчанию (для int, нулём). Определенный внутри функции массив b будет лежать на стеке — специальной области памяти для временных переменных — и будет удален сразу когда функция (или любой другой блок вроде тела цикла или if-а) завершится. Так как размер стека исполнения ограничен, большие массивы ($\u0026gt;10^6$) выделять так нельзя. Изначально он заполнен чем-то случайным, что лежало на тот момент в памяти — чтобы заполнить нулями, можно написать int x[100] = {}. Чтобы заполнить все элементы заданными значениями, можно написать int y[5] = {4, 8, 15, 23, 42}. Определенный через оператор new массив c выделен динамически. Он существует, пока его специально не удалили через оператор del[]. Он также заполнен тем, что на тот момент лежало в памяти. В отличие от предыдущих двух вариантов, он может быть любого размера, даже неизвестного заранее. Важно. В первых двух вариантах размер массива должен быть известной на момент компиляции константой. Компилятор GCC может скомпилировать выражение вида int a[n], и действительно выделится массив не-константного размера; IDE поэтому может и не подчеркнуть его, хотя это не является частью стандарта.\nВсе элементы массива хранятся последовательно в памяти, а сами переменные a, b и c на самом деле являются указателями на первый элемент массива. Скобочки — это просто «синтаксический сахар»:\na[k] \u0026lt;=\u0026gt; *(a + k) Для инициализации и копирования в C есть две полезные функции, memset и memcpy соответственно.\nПервая берет указатель «куда», указатель «откуда» и количество байт, которые нужно перекопировать:\nmemcpy(dest, src, sizeof src); Вторая берет указатель «куда» и один байт — значение, которое нужно раскопировать по всему массиву.\nmemset(arr, 0, sizeof arr); Важно. memset работает именно с сырыми байтами, а не типами вроде int или float. Поэтому через memset массивы целочисленных типов можно заполнять только «периодичными» значениями, вроде $0$ и $-1$ (отрицательная единица в двоичной записи выглядит как 111..111).\nТакже важно помнить, что последний аргумент в обоих функциях — это число байтов, а не количество элементов. В случае с массивами не-константного размера можно домножить размер типа на размер массива:\nmemcpy(dest, src, sizeof(int) * n) Здесь sizeof(int) = 4. Вместо просто четверки так пишут для самокомментируемости.\n#std::arrayВ C++11 добавили свой класс для массивов константного размера:\n// int a[3] = {1, 2, 3}; array\u0026lt;int, 3\u0026gt; a = {1, 2, 3}; Все операции с ним работают по аналогии с сишными. Основное отличие — он является контейнером STL, то есть у него есть итераторы и с ним работают все алгоритмы из стандартной библиотеки.\nsort(a.begin(), a.end()); С обычными массивами, впрочем, тоже — указатели автоматически приводятся к итераторам:\nsort(a, a + 3); Массивы из STL, как и обычные массивы константного размера, поддерживают итерирование:\nfor (int x : a) cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; endl; Также можно изменять элементы в массиве во время итерирования следующим синтаксисом:\nfor (int \u0026amp;x : a) x *= 2; В STL также есть более ошибкоустойчивая альтернатива memset — std::fill:\nfill(a.begin(), a.end(), 42); Она уже работает с полными типами, хотя и немного медленнее.\n#std::pair и std::tupleТип pair\u0026lt;T1, T2\u0026gt; хранит пару из переменных не обязательно одинаковых типов:\npair\u0026lt;int, int\u0026gt; interval = {0, 42}; pair\u0026lt;int, double\u0026gt; index_and_value = {7, 3.1415}; Первый элемент доступен через поле .first, а второй через .second.\nЕго обобщение, tuple, хранит кортеж из произвольного количества переменных:\ntuple\u0026lt;int, int, int\u0026gt; coords_xyz = {1, 2, 3}; Вместо .first, .second, .third и так далее с tuple нужно использовать индексы.\nПары и тюплы удобно возвращать из функций:\ntypedef tuple\u0026lt;double, double, double\u0026gt; point; point rotate(point p) { return {p[1], p[2], p[0]}; } Также по массивам из них удобно итерироваться:\npoint points[100]; for (auto [x, y, z] : points) cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; y \u0026lt;\u0026lt; z \u0026lt;\u0026lt; endl; #structОчень рекомендуется по возможности вместо пар и тюплов объявлять структуры.\nstruct point { double x, y, z; }; Вместо возни с .first и .second или индексами вы получаете именованные поля, а также возможность определять свои методы и перегружать операторы:\npoint::length() { return sqrt(x * x + y * y + z * z); } point operator+(point a, point b) { return {a.x + b.x, a.y + b.y, a.z + b.z}; } Единственный минус структур в том, что по для пар и тюплов будут определены функции сравнения и хеширования, и поэтому их можно сразу в таком виде класть в качестве ключа в структуры из STL вроде set или unordered_set, а для структур их нужно писать отдельно.\n","id":20,"path":"/cs/basic-structures/array/","title":"Массивы и кортежи"},{"content":"Метод Ньютона, также иногда называемый методом Ньютона-Рафсона, это простой и эффективный алгоритм для приближенного нахождения корней действительнозначных функций, то есть решения уравнений вида:\n$$ f(x) = 0 $$\nЕдинственные требования, накладываемые на функцию $f$ — что у неё есть хотя бы один корень и что она непрерывна и дифференцируема на интервале поиска.\n#Описание алгоритмаАлгоритм начинает с какого-то изначального приближения $x_0$ и затем итеративно строит лучшее решение, строя касательную к графику в точке $x = x_i$ и присваивая в качестве следующего приближения $x_{i+1}$ координату пересечения касательной с осью $x$. Интуиция в том, что если функция $f$ «хорошая», и $x_i$ уже достаточно близок к корню, то $x_{i+1}$ будет ещё ближе.\nЧтобы получить точку пересечения для $x_i$, нужно приравнять уравнение касательной к нулю:\n$$ 0 = f(x_i) + (x_{i+1} - x_i) f\u0026rsquo;(x_i) $$\nоткуда можно выразить\n$$ x_{i+1} = x_i - \\frac{f(x_i)}{f\u0026rsquo;(x_i)} $$\nМетод Ньютона крайне важен в вычислительной математике: в большинстве случаев именно он используется для нахождения численных решений уравнений.\n#Поиск квадратных корнейВ качестве конкретного примера рассмотрим задачу нахождения квадратных корней, которую можно переформулировать как решение следующего уравнения:\n$$ x = \\sqrt n \\iff x^2 = n \\iff f(x) = x^2 - n = 0 $$\nЕсли в методе Ньютона подставим $f(x) = x^2 - n$, мы получим следующее правило:\n$$ x_{i+1} = x_i - \\frac{x_i^2 - n}{2 x_i} = \\frac{x_i + n / x_i}{2} $$\nЕсли нам нужно посчитать корень с некоторой заданной точностью $\\epsilon$, можно на каждой итерации делать соответствующую проверку:\nconst double eps = 1e-9; double sqrt(double n) { double x = 1; while (abs(x * x - n) \u0026gt; eps) x = (x + n / x) / 2; return x; } Алгоритм успешно сходится к правильному ответу для многих функций, однако это происходит надежно и доказуемо только для определенного множества функций (например, выпуклых). Другой вопрос — как быстра эта сходимость, если она происходит.\n#Скорость сходимостиЗапустим метод Ньютона для поиска квадратного корня $2$, начиная с $x_0 = 1$, и посмотрим, сколько первых цифр оказались правильными после каждой итерации:\n1.0000000000000000000000000000000000000000000000000000000000000 1.5000000000000000000000000000000000000000000000000000000000000 1.4166666666666666666666666666666666666666666666666666666666675 1.4142156862745098039215686274509803921568627450980392156862745 1.4142135623746899106262955788901349101165596221157440445849057 1.4142135623730950488016896235025302436149819257761974284982890 1.4142135623730950488016887242096980785696718753772340015610125 1.4142135623730950488016887242096980785696718753769480731766796 Можно заметить, что число корректных цифр примерно удваивается после каждой итерации. Такая прекрасная скорость сходимости не просто совпадение.\nЧтобы оценить скорость сходимости численно, рассмотрим небольшую относительную ошибку $\\delta_i$ на $i$-ой итерации и посмотрим, насколько меньше станет ошибка $\\delta_{i+1}$ на следующей итерации.\n$$ |\\delta_i| = \\frac{|x_n - x|}{x} $$\nВ терминах относительных ошибок, мы можем выразить $x_i$ как $x \\cdot (1 + \\delta_i)$. Подставляя это выражение в формулу для следующей итерации и деля обе стороны на $x$ получаем\n$$ 1 + \\delta_{i+1} = \\frac{1}{2} (1 + \\delta_i + \\frac{1}{1 + \\delta_i}) = \\frac{1}{2} (1 + \\delta_i + 1 - \\delta_i + \\delta_i^2 + o(\\delta_i^2)) = 1 + \\frac{\\delta_i^2}{2} + o(\\delta_i^2) $$\nЗдесь мы разложили $(1 + \\delta_i)^{-1}$ в ряд Тейлора в точке $0$, используя предположение что ошибка $d_i$ мала: так как последовательность $x_i$ сходится к $x$, то $d_i \\ll 1$ для достаточно больших $n$.\nНаконец, выражая $\\delta_{i+1}$, получаем\n$$ \\delta_{i+1} = \\frac{\\delta_i^2}{2} + o(\\delta_i^2) $$\nчто означает, что относительная ошибка примерно возводится в квадрат и делится пополам на каждой итерации, когда мы уже близки к решению. Так как логарифм $(- \\log_{10} \\delta_i)$ примерно равен числу правильных значимых цифр числа $x_i$, возведение ошибки в квадрат соответствует удвоению значимых цифр ответа, что мы и наблюдали ранее.\nЭто свойство называется квадратичной сходимостью, и оно относится не только к нахождению квадратных корней. Оставляя формальное доказательство в качестве упражнения, можно показать, что в общем случае\n$$ |\\delta_{i+1}| = \\frac{|f\u0026rsquo;\u0026rsquo;(x_i)|}{2 \\cdot |f\u0026rsquo;(x_n)|} \\cdot \\delta_i^2 $$\nчто означает хотя бы квадратичную сходимость при нескольких дополнительных предположениях, а именно что $f\u0026rsquo;(x)$ не равна нулю и $f\u0026rsquo;\u0026rsquo;(x)$ непрерывна.\n","id":21,"path":"/cs/numerical/newton/","title":"Метод Ньютона"},{"content":"Эта статья — одна из серии. Рекомендуется сначала прочитать все предыдущие.\nПосмотрим на формулу пересчета динамики из базового решения:\n$$ f[i, j] = \\min_{k \u0026lt; i} {f[k, j-1] + (x_{i-1}-x_k)^2 } $$\nОбозначим за $opt[i, j]$ оптимальный $k$ для данного состояния — то есть аргминимум от выражения выше. Для однозначности, если оптимальный индекс не один, то выберем среди них самый правый.\nКонкретно в задаче покрытия точек отрезками можно заметить следующее:\n$$ opt[i + 1, j] \\leq opt[i, j] $$\nИнтуация такая: если нам нужно покрыть больший префикс точек, то начало последнего отрезка точно не будет раньше.\n#АлгоритмПусть мы уже знаем $opt[l, k]$ и $opt[r, k]$ и хотим посчитать $opt[i, k]$ для какого-то $i$ между $l$ и $r$. Тогда, воспользовавшись неравенством выше, мы можем сузить отрезок поиска оптимального индекса для $i$ со всего отрезка $[0, i - 1]$ до $[opt[l, k], opt[r, k]]$.\nБудем делать следующее: заведем рекурсивную функцию, которая считает динамики для отрезка $[l, r]$ на $k$-том слое, зная, что их $opt$ лежат между $l\u0026rsquo;$ и $r\u0026rsquo;$. Эта функция просто берет середину отрезка $[l, r]$ и линейным проходом считает ответ для неё, а затем рекурсивно запускается от половин, передавая в качестве границ $[l\u0026rsquo;, opt]$ и $[opt, r\u0026rsquo;]$ соответственно:\n// [ l, r] -- какие динамики на k-том слое посчитать // [_l, _r] -- где могут быть их ответы void solve(int l, int r, int _l, int _r, int k) { if (l \u0026gt; r) return; // отрезок пустой -- выходим int opt = _l, t = (l + r) / 2; // считаем ответ для f[t][k] for (int i = _l; i \u0026lt;= min(_r, t); i++) { int val = f[i + 1][k - 1] + cost(i, t - 1); if (val \u0026lt; f[t][k]) f[t][k] = val, opt = i; } solve(l, t - 1, _l, opt, k); solve(t + 1, r, opt, _r, k); } Затем последовательно вызовем эту функцию для каждого слоя:\nfor (int k = 1; k \u0026lt;= m; k++) solve(0, n - 1, 0, n - 1, k); Так как отрезок $[l, r]$ на каждом вызове уменьшается примерно в два раза, глубина рекурсии будет $O(\\log n)$. Так как отрезки поиска для всех элементов на одном «уровне» могут пересекаться разве что только по границам, то суммарно на каждом уровне поиск проверит $O(n)$ различных индексов. Соответственно, пересчет всего слоя займет $O(n \\log n)$ операций вместо $O(n^2)$ в базовом решении.\nТаким образом, мы улучшили асимптотику до $O(n \\cdot m \\cdot \\log n)$.\n","id":22,"path":"/cs/layer-optimizations/divide-and-conquer/","title":"Оптимизация через разделяй-и-властвуй"},{"content":"Лайфхак: пока вы не выучили все детерминированные строковые алгоритмы, научитесь пользоваться хешами.\nБудем считать, что строка — это последовательность чисел от $1$ до $m$ (размер алфавита). В C/C++ тип char это на самом деле тоже число (8-битное), поэтому можно вычитать из символов минимальный код и кастовать в число:\nint x = (int) (c - \u0026#39;a\u0026#39; + 1); Определим прямой полиномиальный хеш строки как значение следующего многочлена:\n$$ h_f = (s_0 + s_1 k + s_2 k^2 + \\ldots + s_n k^n) \\bmod p $$\nЗдесь $k$ — произвольное число больше размера алфавита, а $p$ — достаточно большой модуль (вообще говоря, не обязательно простой).\nЕго можно посчитать за линейное время, поддерживая переменную, равную $k$ в нужной степени:\nconst int k = 31, mod = 1e9+7; string s = \u0026#34;abacabadaba\u0026#34;; long long h = 0, m = 1; for (char c : s) { int x = (int) (c - \u0026#39;a\u0026#39; + 1); h = (h + m * x) % mod; m = (m * k) % mod; } Можем ещё определить обратный полиномиальный хеш:\n$$ h_b = (s_0 k^n + s_1 k^{n-1} + \\ldots + s_n) \\mod p $$\nЕго преимущество в том, что можно писать на одну строчку кода меньше:\nlong long h = 0; for (char c : s) { int x = (int) (c - \u0026#39;a\u0026#39; + 1); h = (h * k + x) % mod; } Автору проще думать об обычных многочленах, поэтому он будет везде использовать прямой полиномиальный хеш и обозначать его просто буквой $h$.\n#Зачем это нужно?Используя тот факт, что хеш — это значение многочлена, можно быстро пересчитывать хеш от результата выполнения многих строковых операций.\nНапример, если нужно посчитать хеш от конкатенации строк $a$ и $b$ (строку $b$ приписали в конец строки $a$), то можно просто хеш $b$ домножить на $k^{|a|}$ и сложить с хешом $a$:\n$$ h(ab) = h(a) + k^{|a|} \\cdot h(b) $$\nУдалить префикс строки можно так:\n$$ h(b) = \\frac{h(ab) - h(a)}{k^{|a|}} $$\nА суффикс — ещё проще:\n$$ h(a) = h(ab) - k^{|a|} \\cdot h(b) $$\nВ задачах нам часто понадобится домножать $k$ в какой-то степени, поэтому имеет смысл предпосчитать все нужные степени и сохранить в массиве:\nconst int maxn = 1e5+5; int p[maxn]; p[0] = 1; for (int i = 1; i \u0026lt; maxn; i++) p[i] = (1ll * p[i-1] * k) % mod; // аккуратно с переполнением Как это использовать в реальных задачах? Пусть нам надо отвечать на запросы проверки на равенство произвольных подстрок одной большой строки. Подсчитаем значение хеш-функции для каждого префикса:\nint h[maxn]; h[0] = 0; // h[k] -- хеш префикса длины k // будем считать, что s это уже последовательность int-ов for (int i = 0; i \u0026lt; n; i++) h[i+1] = (h[i] + p[i] * s[i]) % mod; Теперь с помощью этих префиксных хешей мы можем определить функцию, которая будет считать хеш на произвольном подотрезке:\n$$ h(s[l:r]) = \\frac{h_r-h_l}{k^l} $$\nДеление по модулю возможно делать только при некоторых k и mod (а именно — при взаимно простых). В любом случае, писать его долго, и мы это делать не хотим.\nДля нашей задачи не важно получать именно полиномиальный хеш — главное, чтобы наша функция возвращала одинаковый многочлен от одинаковых подстрок. Вместо приведения к нулевой степени приведём многочлен к какой-нибудь достаточно большой — например, к $n$-ной.\n$$ \\hat{h}(s[l:r]) = k^{n-l} (h_r-h_l) $$\nТак проще — теперь нужно домножать, а не делить:\nint hash_substring(int l, int r) { return (h[r+1] - h[l]) * p[n-l] % mod; } Теперь мы можем просто вызывать эту функцию от двух отрезков и сравнивать числовое значение, отвечая на запрос за $O(1)$.\nУпражнение. Напишите то же самое, но используя обратный полиномиальный хеш — этот способ тоже имеет право на существование, и местами он даже проще. Обратный хеш подстроки принято считать и использовать в стандартном виде из определения, поскольку там нет необходимости в делении.\nЛайфхак. Если взять обратный полиномиальный хеш короткой строки на небольшом алфавите с $k=10$, то числовое значение хеша строки будет наглядно соотноситься с самой строкой:\n$$ h(abacaba)=1213121 $$\nЭтим удобно пользоваться при дебаге.\n#Примеры задачКоличество разных подстрок. Посчитаем хеши от всех подстрок за $O(n^2)$ и добавим их все в std::set. Чтобы получить ответ, просто вызовем set.size().\nПоиск подстроки в строке. Можно посчитать хеши от шаблона (строки, которую ищем) и пройтись «окном» размера шаблона по тексту, поддерживая хеш текущей подстроки. Если хеш какой-то из этих подстрок совпал с хешом шаблона, то мы нашли нужную подстроку. Это называется алгоритмом Рабина-Карпа.\nСравнение подстрок на больше-меньше, а не только на равенство. У любых двух строк есть какой-то общий префикс (возможно, пустой). Сделаем бинпоиск по его длине, а дальше в обеих подстроках возьмём идущий за ним символ и сравним. Это будет работать за $O(\\log n)$.\nПалиндромность подстроки. Можно посчитать два массива — обратные хеши и прямые. Проверка на палиндром будет заключаться в сравнении значений hash_substring() на первом массиве и на втором.\nКоличество палиндромов. Можно перебрать центр палиндрома, а для каждого центра — бинпоиском его размер. Проверять подстроку на палиндромность мы уже умеем. Как и всегда в задачах на палиндромы, случаи четных и нечетных палиндромов нужно обрабатывать отдельно. Это будет работать за $O(n \\log n)$, хотя это можно решить и линейно.\n","id":23,"path":"/cs/hashing/polynomial/","title":"Полиномиальное хеширование"},{"content":"Определение. Префикс-функцией от строки $s$ называется массив $p$, где $p_i$ равно длине самого большого префикса строки $s_0 s_1 s_2 \\ldots s_i$, который также является и суффиксом $i$-того префика (не считая весь $i$-й префикс).\nНапример, самый большой префикс, который равен суффиксу для строки «aataataa» — это «aataa»; префикс-функция для этой строки равна $[0, 1, 0, 1, 2, 3, 4, 5]$.\nvector\u0026lt;int\u0026gt; slow_prefix_function(string s) { int n = (int) s.size(); vector\u0026lt;int\u0026gt; p(n, 0); for (int i = 1; i \u0026lt; n; i++) for (int len = 1; len \u0026lt;= i; len++) // если префикс длины len равен суффиксу длины len if (s.substr(0, len) == s.substr(i - len + 1, len)) p[i] = len; return p; } Этот алгоритм пока что работает за $O(n^3)$, но позже мы его ускорим.\n#Как это поможет решить исходную задачу?Давайте пока поверим, что мы умеем считать префикс-функцию за линейное от размера строки, и научимся с помощью нее искать подстроку в строке.\nСоединим подстроки $s$ и $t$ каким-нибудь символом, который не встречается ни там, ни там — обозначим пусть этот символ #. Посмотрим на префикс-функцию получившейся строки s#t.\nstring s = \u0026#34;choose\u0026#34;; string t = \u0026#34;choose life. choose a job. choose a career. choose a family. choose a fu...\u0026#34;; cout \u0026lt;\u0026lt; s + \u0026#34;#\u0026#34; + t \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; slow_prefix_function(s + \u0026#34;#\u0026#34; + t) \u0026lt;\u0026lt; endl; choose#choose life. choose a job. choose a career. choose a family. choose a fu... 0000000123456000000012345600000000123456000100000001234560000000000012345600000000 Видно, что все места, где значения равны 6 (длине $s$) — это концы вхождений $s$ в текст $t$.\nТакой алгоритм (посчитать префикс-функцию от s#t и посмотреть, в каких позициях она равна $|s|$) называется алгоритмом Кнута-Морриса-Пратта.\n#Как её быстро считатьРассмотрим ещё несколько примеров префикс-функций и попытаемся найти закономерности:\naaaaa 01234 abcdef 000000 abacabadava 00101230101 Можно заметить следующую особенность: $p_{i+1}$ максимум на единицу превосходит $p_i$.\nДоказательство. Если есть префикс, равный суффиксу строки $s_{:i+1}$, длины $p_{i+1}$, то, отбросив последний символ, можно получить правильный суффикс для строки $s_{:i}$, длина которого будет ровно на единицу меньше.\nПопытаемся решить задачу с помощью динамики: найдём формулу для $p_i$ через предыдущие значения.\nЗаметим, что $p_{i+1} = p_i + 1$ в том и только том случае, когда $s_{p_i} =s_{i+1}$. В этом случае мы можем просто обновить $p_{i+1}$ и пойти дальше.\nНапример, в строке $\\underbrace{aabaa}t\\overbrace{aabaa}$ выделен максимальный префикс, равный суффиксу: $p_{10} = 5$. Если следующий символ равен будет равен $t$, то $p_{11} = p_{10} + 1 = 6$.\nНо что происходит, когда $s_{p_i}\\neq s_{i+1}$? Пусть следующий символ в этом же примере равен не $t$, а $b$.\n$\\implies$ Длина префикса, равного суффиксу новой строки, будет точно меньше 5. $\\implies$ Помимо того, что искомый новый супрефикс является суффиксом «aabaab», он ещё является префиксом подстроки «aabaa». $\\implies$ Значит, следующий кандидат на проверку — это значение префикс-функции от «aabaa», то есть $p_4 = 2$, которое мы уже посчитали. $\\implies$ Если $s_2 = s_{11}$ (т. е. новый символ совпадает с идущим после префикса-кандидата), то $p_{11} = p_2 + 1 = 2 + 1 = 3$. В данном случае это действительно так (нужный префикс — «aab»). Но что делать, если, в общем случае, $p_{i+1} \\neq p_{p_i+1}$? Тогда мы проводим такое же рассуждение и получаем нового кандидата, меньшей длины — $p_{p_{p_i}}$. Если и этот не подошел — аналогично проверяем меньшего, пока этот индекс не станет нулевым.\nvector\u0026lt;int\u0026gt; prefix_function(string s) { int n = (int) s.size(); vector\u0026lt;int\u0026gt; p(n, 0); for (int i = 1; i \u0026lt; n; i++) { // префикс функция точно не больше этого значения + 1 int cur = p[i - 1]; // уменьшаем cur значение, пока новый символ не сматчится while (s[i] != s[cur] \u0026amp;\u0026amp; cur \u0026gt; 0) cur = p[cur - 1]; // здесь либо s[i] == s[cur], либо cur == 0 if (s[i] == s[cur]) p[i] = cur + 1; } return p; } Асимптотика. В худшем случае этот while может работать $O(n)$ раз за одну итерацию, но в среднем каждый while работает за $O(1)$.\nПрефикс-функция каждый шаг возрастает максимум на единицу и после каждой итерации while уменьшается хотя бы на единицу. Значит, суммарно операций будет не более $O(n)$.\n","id":24,"path":"/cs/string-searching/prefix-function/","title":"Префикс-функция"},{"content":"Определение. Префиксными суммами массива $[a_0, a_1, a_2, \\ldots, a_{n - 1}]$ называется массив $[s_0, s_1, s_2, \\ldots, s_n]$, определенный следующим образом:\n$$ \\begin{aligned} \u0026amp; s_0 = 0\\ \u0026amp; s_1 = a_0\\ \u0026amp; s_2 = a_0 + a_1\\ \u0026amp; s_3 = a_0 + a_1 + a_2\\ \u0026amp; \\ldots\\ \u0026amp; s_n = a_0 + a_1 + \\ldots + a_{n - 1} \\end{aligned} $$\nОбратите внимание, что в такой индексации\n$s_k$ равен сумме первых $k$ элементов массива $a$ не включая $a_k$, длина $s$ на единицу больше длины $a$, $s_0$ всегда равен нулю. Иногда префиксные суммы определяют включая правый конец и без нулевого элемента, то есть как $s_k = a_0 + a_1 + \\ldots + a_k$, но по той же причине, почему отрезки почти всегда менее удобны, чем полуинтервалы, мы всегда будем работать с «полуинтрвальными» префиксными суммами из определения.\nФормулу для $s_k$ можно записать рекуррентно как $s_{k+1} = s_k + a_k$, что сразу дает метод подсчета префиксных сумм за линейное время:\nint s[n + 1]; s[0] = 0; for (int i = 0; i \u0026lt; n; i++) s[i + 1] = s[i] + a[i]; Задача. Дан массив целых чисел, и приходят запросы вида «найти сумму на полуинтервале с позиции $l$ до позиции $r$». Нужно отвечать на запросы за $O(1)$.\nПредподсчитаем перед ответами на запросы массив префиксных сумм для исходного массива. Тогда если бы во всех запросах $l$ было равно нулю, то ответом на запрос была бы просто префиксная сумма $s_r$. Но как действовать, если $l \\neq 0$?\nВ префиксной сумме $s_r$ содержатся все нужные нам элементы, однако есть еще лишние, а именно $a_0, a_1, \\ldots, a_{l - 1}$. Заметим, что такая сумма в свою очередь равна уже посчитанной префиксной сумме $s_l$. Таким образом, выполнено тождество:\n$$ a_{l} + a_{l + 1} + \\ldots + a_{r - 1} = s_{r} - s_{l} $$\nДля ответа на запрос поиска суммы на произвольном полуинтервале нужно просто вычесть друг из друга две предподсчитанные префиксные суммы.\n#Другие операцииПодобный прием можно использовать не только для сложения, но и для других операций.\nЧто нам на самом деле здесь нужно было от сложения? Только то, что у сложения есть обратная операция — вычитание — с помощью которой можно по двум префиксам восстановить значение на отрезке, «отменив» $s_l$. Сумма обратима, но например минимум или максимум необратимы — по значениям минимумов на префиксах в общем случае невозможно получить значение минимума на отрезке (в чем несложно убедиться, рассмотрев случай, когда первый элемент массива минимальный, и все префиксные суммы будут ему равны, все зависимости от остальных значений).\nПомимо сложения, есть и другие операции, которые являются обратимыми:\nпобитовое исключающее «или», также известное как xor и обозначаемое $\\oplus$, сложение по модулю, умножение и умножение по модулю (обратное — деление). Помимо обычной суммой, самая популярная из них — xor, которая считается ещё проще:\n$$ a_l \\oplus a_{l + 1} \\oplus \\ldots \\oplus a_{r - 1} = s_r \\oplus s_l $$\n#Задачи на префиксные суммыЗадача. Дан массив целых чисел. Определите, есть ли в нём подотрезок заданной суммы за линейное время.\nЗадача. Даны два массива одинаковой длины. Найдите такой подотрезок, что сумма элементов первого массива совпадала с суммой элементов второго массива (на индексах этого отрезка).\nЗадача. Дано мультимножество из $n$ целых чисел. Найдите любое его подмножество, сумма чисел которого делится на $n$.\n","id":25,"path":"/cs/range-queries/prefix-sum/","title":"Префиксные суммы"},{"content":"Метод сканирующей прямой (англ. scanline) заключается в сортировке точек на координатной прямой либо каких-то абстрактных «событий» по какому-то признаку и последующему проходу по ним.\nОн часто используется для решения задач на структуры данных, когда все запросы известны заранее, а также в геометрии для нахождения объединений фигур.\n#Точка, покрытая наибольшим количеством отрезковЗадача. Дан набор из $n$ отрезков на прямой, заданных координатами начал и концов $[l_i, r_i]$. Требуется найти любую точку на прямой, покрытую наибольшим количеством отрезков.\nРассмотрим функцию $f(x)$, равную числу отрезков, покрывающих точку $x$. Понятно, что каждую точку этой функции мы проверить не можем.\nНазовем интересными те точки, в которых происходит смена количества отрезков, которыми она покрыта. Так как смена ответа может происходить только в интересной точке, то максимум достигается также в какой-то из интересных точек. Отсюда сразу следует решение за $O(n^2)$: просто перебрать все интересные точки (это будут концы заданных отрезков) и проверить для каждой по отдельности ответ.\nЭто решение можно улучшить. Отсортируем интересные точки по возрастанию координаты и пройдем по ним слева направо, поддерживая количество отрезков cnt, которые покрывают данную точку. Если в данной точке начинается отрезок, то надо увеличить cnt на единицу, а если заканчивается, то уменьшить. После этого пробуем обновить ответ на задачу текущим значением cnt.\nКак такое писать: нужно представить интересные точки в виде структур с полями «координата» и «тип» (начало / конец) и отсортировать со своим компаратором. Удобно начало отрезка обозначать +1, а конец -1, чтобы просто прибавлять к cnt это значение и не разбивать на случаи.\nЕдинственный нюанс — если координаты двух точек совпали, чтобы получить правильный ответ, сначала надо рассмотреть все начала отрезков, а только потом концы (чтобы при обновлении ответа в этой координате учлись и правые, и левые граничные отрезки).\nstruct event { int x, type; }; int scanline(vector\u0026lt;pair\u0026lt;int, int\u0026gt;\u0026gt; segments) { vector\u0026lt;event\u0026gt; events; for (auto [l, r] : segments) { events.push_back({l, 1}); events.push_back({r, -1}); } sort(events.begin(), events.end(), [](event a, event b) { return (a.x \u0026lt; b.x || (a.x == b.x \u0026amp;\u0026amp; a.type \u0026gt; b.type)); }); int cnt = 0, res = 0; for (event e : events) { cnt += e.type; res = max(res, cnt); } return res; } Такое решение работает за $O(n \\log n)$ на сортировку. Если координаты небольшие, то от логарифма можно избавиться, если создать vector событий для каждой различной координаты и просто итерироваться по всем целочисленным координатам и событиям в них. Также всегда хорошей идеей будет сжать координаты.\nРассмотрим теперь несколько смежных задач.\n#Длина объединения отрезковЗадача. Дан набор из $n$ отрезков на прямой, заданных координатами начал и концов $[l_i, r_i]$. Требуется найти суммарную длину их объединения.\nКак и в прошлой задаче, отсортируем все интересные точки и при проходе будем поддерживать число отрезков, покрывающих текущую точку. Если оно больше 0, то отрезок, который мы прошли с прошлой рассмотренной точки, принадлежит объединению, и его длину нужно прибавить к ответу:\nint cnt = 0, res = 0, prev = -inf; for (event e : events) { if (prev != -inf \u0026amp;\u0026amp; cnt \u0026gt; 0) res += e.x - prev; // весь отрезок [prev, e.x] покрыт cnt отрезками cnt += e.type; prev = e.x; } Время работы $O(n \\log n)$.\n#Скольким отрезкам принадлежит точкаПусть теперь надо для $q$ точек (не обязательно являющихся концами отрезков) ответить на вопрос: скольким отрезкам принадлежит данная точка?\nВоспользуемся следующим приемом: сразу считаем все запросы и сохраним их, чтобы потом ответить на все сразу. Добавим точки запросов как события с новым типом 0, который будет означать, что в этой точке надо ответить на запрос, и отдельным полем для номера запроса.\nТеперь аналогично отсортируем точки интереса и пройдем по ним слева направо, поддерживая cnt и отвечая на запросы, когда их встретим.\nstruct event { int x, type, idx; }; void scanline(vector\u0026lt;pair\u0026lt;int, int\u0026gt;\u0026gt; segments, vector\u0026lt;int\u0026gt; queries) { int q = (int) queries.size(); vector\u0026lt;int\u0026gt; ans(q) vector\u0026lt;event\u0026gt; events; for (auto [l, r] : segments) { events.push_back({l, 1}); events.push_back({r, -1}); } for (int i = 0; i \u0026lt; q; i++) events.push_back({queries[i], 0, i}); // при равенстве координат сначала идут добавления, потом запросы, потом удаления sort(events.begin(), events.end(), [](event a, event b) { return (a.x \u0026lt; b.x || (a.x == b.x \u0026amp;\u0026amp; a.type \u0026gt; b.type)); }); int cnt = 0; for (event e : events) { cnt += e.type; if (e.type == 0) ans[e.idx] = cnt; } } Асимптотика $O((n+q)\\log(n+q))$.\n#Количество пересекающихся отрезковЗадача. Дан набор из $n$ отрезков на прямой, заданных координатами начал и концов $[l_i, r_i]$. Требуется для каждого отрезка сказать, с каким количеством отрезков он пересекается (в частности, он может иметь одну общую точку или быть вложенным).\nВместо того, чтобы для каждого отрезка считать количество отрезков, с которыми он пересекается, посчитаем количество отрезков, с которыми он не пересекается, и вычтем это число из $(n-1)$.\nОтрезок $[l_1, r_1]$ не пересекается с другим отрезком $[l_2, r_2]$ только если $r_2 \u0026lt; l_1$ или $r_1 \u0026lt; l_2$. Посчитаем количество отрезков для каждого из этих случаев. Без ограничения общности, рассмотрим первый случай, то есть найдем для каждого отрезка количество отрезков, лежащих строго слева. Для этого нужно ввести два типа событий:\nКакой-то отрезок закончился в координате $r_i$. Какой-то отрезок с таким-то индексом начался в координате $l_j$. Теперь нам достаточно пройтись по этим событиям слева направо, поддерживая количество встреченных событий первого типа и вычитая его из ячейки ответа для событий второго типа.\nАналогично пройдемся справа налево, находя отрезки, лежащие строго справа. Итоговое время работы будет $O(n \\log n)$ на сортировку.\n#Сумма на прямоугольникеПерейдем к двумерному сканлайну.\nЗадача. Даны $n$ точек на плоскости. Требуется ответить на $m$ запросов количества точек на прямоугольнике.\nВо-первых, сожмем все координаты (и точек, и запросов): будем считать, что они все порядка $O(n + m)$.\nТеперь разобьём каждый запрос на два запроса суммы на префиксах: сумма на прямоугольнике $[x_1, x_2] \\times [y_1, y_2]$ равна сумме на прямоугольнике $[0, x_2] \\times [y_1, y_2]$ минус сумме на прямоугольнике $[0, x_1] \\times [y_1, y_2]$.\nСоздадим дерево отрезков для суммы и массив ans для ответов на запросы. Теперь будем проходиться в порядке увеличения по всем интересным $x$ — координатам точек и правых границ префиксных запросов — и обрабатывать события трёх типов:\nЕсли мы встретили точку, то добавляем единицу к ячейке $y$ в дереве отрезков. Если мы встретили «левый» запрос префиксной суммы, то посчитаем через дерево отрезков сумму на отрезке $[y_1, y_2]$ и вычтем его из ячейки ответа. Если мы встретили «правый» запрос, то аналогично прибавим сумму на $[y_1, y_2]$ к ячейке ответа. Таким образом, мы решим задачу в оффлайн за $O(n \\log n)$: сжатие координат / сортировка плюс $O(n)$ запросов к дереву отрезков (или любой другой структуре для динамической суммы).\nСумма на прямоугольнике как вспомогательная подзадача также часто используется в задачах на инверсии в перестановках и запросы на поддеревьях.\n#Площадь объединения прямоугольниковЗадача. Дано $n$ прямоугольников, требуется найти площадь их объединения.\nВдохновляясь предыдущим подходом, можно создать два типа событий:\nпрямоугольник с $y$-координатами от $y_1$ до $y_2$ начинается в точке $x_1$; прямоугольник с $y$-координатами от $y_1$ до $y_2$ заканчивается в точке $x_2$; и затем как-то пройтись по этим событиям в порядке увеличения $x$ и посчитать общую площадь подобно тому, как мы делали с одномерными отрезками.\nЕсли обобщать подход напрямую, то нам нужно завести массив размера $Y$ (максимальной $y$-координаты) и для каждой $y$-параллели поддерживать число прямоугольников, которые её покрывают, и каждый раз, когда $x$-координата события меняется, добавлять к ответу разницу $x$-координат старого и нового события, помноженную на число ненулевых элементов в массиве (точек на вертикальной сканирующей прямой, которые покрываются хотя бы одним прямоугольником).\nНо это в худшем случае работает за $O(nY)$, что достаточно долго. Чтобы получить более приятную асимптотику, заменим массив деревом отрезков, в узлах которого будет храниться минимум и число элементов с таким минимумом (изначально минимум 0 и таких элементов на всём массиве $Y$).\nЧтобы обновить ответ, нужно помножить разницу $x$-координат соседних событий на число ненулевых элементов, которое можно найти, вычтя из $Y$ количество минимумов-нулей на всём дереве. Одномерная задача пересчета этой информации при прибавлениях на отрезке остается упражнением читателю.\nТакой алгоритм работает за $O(n \\log n)$, если аккуратно сжать координаты, и за $O(n \\log Y)$, если этого не делать. Этот метод также обобщается на задачу нахождения площадей и других геометрических фигур.\n","id":26,"path":"/cs/decomposition/scanline/","title":"Сканирующая прямая"},{"content":"Наш первый подход будет заключаться в следующем: обозначим за $n$ длину массива и $n$ раз пройдёмся по нему слева направо, меняя два соседних элемента, если первый больше второго.\nКаждую итерацию максимальный элемент «всплывает» как пузырек к концу массива — отсюда и название.\nvoid bubble_sort(int *a, int n) { for (int k = 0; k \u0026lt; n; k++) for (int i = 0; i \u0026lt; n - 1; i++) // сравниваем элемент со следующим if (a[i] \u0026gt; a[i + 1]) // меняем местами, если следующий меньше swap(a[i], a[i + 1]); } int a[5] = {5, 2, 1, 3, 1}; bubble_sort(a, 5); for (int i = 0; i \u0026lt; 5; i++) cout \u0026lt;\u0026lt; a[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; Корректность. По индукции можно показать, что после $k$ шагов алгоритма сортировки пузырьком последние $k$ чисел всегда отсортированы, а значит алгоритм работает корректно.\nАсимптотика. Так как у нас два вложенных цикла, каждый из которых делает не более $O(n)$ итераций, внутри которых за $O(1)$ происходит сравнение и swap, суммарное время работы будет не более $O(n^2)$.\nУпражнение. Алгоритм можно немного (неасимптотически) ускорить. Как нужно изменить границы в двух for-циклах, чтобы не делать никаких лишних действий?\n","id":27,"path":"/cs/sorting/bubble/","title":"Сортировка пузырьком"},{"content":"Состояние любой структуры как-то лежит в памяти: в каких-то массивах, или в более общем случае, по каким-то определенным адресам в памяти. Для простоты, пусть у нас есть некоторый массив $a$ размера $n$, и нам нужно обрабатывать запросы присвоения и чтения, а также иногда откатывать изменения обратно.\n#Список измененийОдин из самых простых и общих способов поддерживать простой откат состояний — поддерживать лог изменений всех затронутых элементов.\nЕсли нужно только откатывать изменения до некоторого прошлого состояния (как \u0026ldquo;ctrl+z\u0026rdquo;), то можно просто создать стек, в котором будут храниться пары «какая ячейка изменилась, какое значение там было раньше», и для отката состояния пройтись по нему в обратном порядке, восстанавливая исходные значения.\nint a[N]; stack\u0026lt; pair\u0026lt;int, int\u0026gt; \u0026gt; s; void change(int k, int x) { s.push({k, a[k]}); a[k] = x; } void rollback() { while (!s.empty()) { auto [k, x] = s.top(); a[k] = x; s.pop(); } } Такой подход добавит небольшой константный оверхед на все операции, но позволит откатывать структуру — либо на какое-то на заданное изначальное состояние, либо на некоторое состояние в прошлом, если помимо позиции и значения хранить в стеке ещё и какую-то временную метку.\n#Массив версийНемного переформулируем задачу. Есть изначально нулевой массив $a$, и нужно поддерживать две операции:\nСделать присвоение элемента $a[k] = x$. Занулить массив. Задачу можно решить более элегантно. Заведем глобальный счетчик времени $t$ и изначально заполненный нулями массив версий $v$ такого же размера, как и $a$: в этом массиве будет храниться время, когда соответствующий элемент был в последний раз изменен.\nint t = 1; int a[N], v[N]; Условимся, что если версия элемента равна текущему времени $t$, то она корректная, иначе элемент равен нулю:\nint get(int k) { return (v[k] == t ? a[k] : 0); } Теперь, когда мы делаем присвоение элемента, мы обновляем его версию:\nvoid change(int k, int x) { a[k] = x; v[k] = t; } Когда мы зануляем массив, мы просто инкрементируем счетчик $t$ и всё:\nvoid rollback() { t++; } Такой подход проще кодится и имеет меньший оверхед, однако немного менее применим.\n#«Толстые» узлыПусть нам теперь нужно не откатывать изменения, а просто делать чтения каких-то элементов в прошлом. Список изменений нас не устраивает, потому что для каждого чтения в худшем случае потребуется просматривать $O(n)$ последних изменений.\nЗаведем список версий для каждого элемента, в конец которого будут складываться новые версии при каждом изменении, а также время этого изменения.\nint t = 0; vector\u0026lt; pair\u0026lt;int, int\u0026gt; \u0026gt; versions[N]; void change(int k, int x) { versions[k].push_back({t++, x}); } Тогда при чтении мы уже можем просматривать не все изменения, а только затрагивающие нужную ячейку. Но можно и ещё быстрее — элементы списка отсортированы по времени, а значит можно бинарным поиском найти последнее изменение, не превосходящее время из запроса:\nint get(int k, int v) { auto it = upper_bound(versions[k].begin(), versions[k].end(), v); return (--it)-\u0026gt;second; } Такое решение будет работать за $O(1)$ на изменение и $O(\\log n)$ на запрос.\n#Дерево измененийФормально предыдущими методами мы достигли персистентности, но в весьма ограниченном смысле: мы не можем быстро откатываться до произвольной версии и делать «форк» из неё, при этом сохраняя все версии из «альтернативного будущего».\nЭту проблему можно решить, храня радом с каждым изменением номер версии $p$, из которой произошел форк:\nstruct Node { int k, x, p; }; vector\u0026lt;Node\u0026gt; versions; void change(int k, int x, int v) { versions.push_back({k, x, v}); } Тогда чтобы найти значение ячейки $k$ в момент времени $v$, нужно откатиться по этому массиву\nint get(int k, int v) { int res = 0; for (int u = v; u \u0026gt; v; u = u = versions[u].p) if (versions[u].k == k) res = versions[u].x; return res; } Такое решение реализует полноценный персистентный массив, но в худшем случае работает за $O(n)$.\nДерево изменений можно ускорить разными способами, обменяв время на память. Например, можно в некоторых нодах сохранять чекпойнты — полные массивы, соответствующие состоянию массива в этот момент времени.\nПопулярным конкретным способом это сделать является рандомизированное сохранение чекпойнтов на каждом запросе чтения — с вероятностью, пропорциональной дополнительной работе, которую нужно совершить, чтобы получить массив из следующего чекпойнта.\n","id":28,"path":"/cs/persistent/persistent-array/","title":"Структуры с откатами"},{"content":"Отрезок, для которого указано, какой из его концов считается началом, а какой концом, называется вектором. Вектор на плоскости можно задать двумя числами — его координатами по горизонтали и вертикали.\nТочка $\\simeq$ вектор. Так как оба задаются просто парой чисел, можно считать их одним и тем же классом объектов и сопоставлять каждой точке её радиус-вектор — вектор из начала координат, ведущий в эту точку.\n#Как их хранитьСоздадим класс, который будет отвечать за все операции с векторами. В C++ есть два способа это сделать: через struct и через class. Их основное отличие в том, что по умолчанию в class все поля приватные — к ним нет прямого доступа снаружи. Это нужно для дополнительной защиты, чтобы в крупных проектах никто случайно ничего не поломал, но на олимпиадах это не очень актуально, поэтому объявим struct. По принятой в математике и физике нотации, назовём его r. Если хотите, можете назвать его point, pt, vec — как угодно.\nstruct r { int x, y; r() {} r(int x, int y) : x(x), y(y) {} }; Функция r внутри класса или структуры с таким же именем вызывается при инициализации объекта. Её называют конструктором, и её можно указывать разную для разных входных аргументов. Здесь r()вернёт точку с неопределенными (какие оказались в памяти в тот момент) координатами, а r(x, y) вернет точку с координатами $(x, y)$.\nВажным моментом является то, что мы выбрали целочисленный тип, int, для хранения координат. Если все входные координаты целые, то, как мы увидим, очень часто можно все операции тоже проводить в целых числах, что позволяет избежать многих проблем чисел с плавающей точкой.\n#Операции над векторамиДавайте напишем функцию, которая принимает вектор и что-то с ним делает. Например, считает длину:\ndouble len(r a) { return sqrt(a.x * a.x + a.y * a.y); } // или: double len(r a) { return hypot(a.x, a.y); } Это подход языка C. В C++ удобнее определить метод:\ndouble r::len() { return hypot(x, y); } // (альтернативно можно добавить функцию len() внутри самой структуры) Помимо чуть более чистой реализации, есть ещё разница в синтаксисе вызова: len(a) или a.len().\n#ОператорыВ C++ можно перегружать почти все стандартные операторы, например, +, -, * и т. д.\nПереопределим для будущих нужд + и -:\nr operator+(r a, r b) { return {a.x + b.x, a.y + b.y}; } r operator-(r a, r b) { return {a.x - b.x, a.y - b.y}; } Как вы думаете, как на самом деле работает cin \u0026gt;\u0026gt; x? Это тоже перегрузка оператора: \u0026gt;\u0026gt;. Делается это так:\nistream\u0026amp; operator\u0026gt;\u0026gt;(istream \u0026amp;in, r \u0026amp;p) { in \u0026gt;\u0026gt; p.x \u0026gt;\u0026gt; p.y; return in; } ostream\u0026amp; operator\u0026lt;\u0026lt;(ostream \u0026amp;out, r \u0026amp;p) { out \u0026lt;\u0026lt; p.x \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; p.y \u0026lt;\u0026lt; endl; return out; } #Углы и поворотыДля подсчета угла вектора относительно оси $x$ можно вспомнить тригонометрический круг и посчитать арктангенс от $\\frac{y}{x}$.\nВ C++ и Python есть функция atan2, которая делает это немного быстрее и точнее деления и арктангенса:\ndouble r::angle() { return atan2(y, x); } Вернется число от в промежутке $[-\\pi, +\\pi]$, в радианах. Для градусов нужно домножить на $\\frac{180}{\\pi}$.\nПоворот вектора на угол $\\alpha$ задаётся следующим матричным уравнением:\n#$$ Rot_{\\alpha}(\\overline{(x, y)}) #\\begin{pmatrix} \\cos(\\alpha) \u0026amp; -\\sin(\\alpha) \\ \\sin(\\alpha) \u0026amp; \\cos(\\alpha) \\end{pmatrix} \\begin{pmatrix} x \\ y \\end{pmatrix}\\begin{pmatrix} \\cos(\\alpha) \\cdot x - \\sin(\\alpha) \\cdot y \\ \\sin(\\alpha) \\cdot x + \\cos(\\alpha) \\cdot y \\end{pmatrix} $$\nВ частности, $Rot_{90^{\\circ}} (\\overline{(x, y)}) = \\overline{(-y,x)}$.\n#Для ноулайферов: комплексные числаМожно всё это делать используя std::complex.\n","id":29,"path":"/cs/geometry-basic/vectors/","title":"Точки и вектора"},{"content":"Чаще всего в задачах по программированию вершины графа пронумерованы числами от $0$ до $(n-1)$, чтобы было удобно обращаться к ним как к индексам в разных массивах.\nВ задачах графы чаще всего задаются списком всех ребер, которые нужно считать из ввода. В каком формате оптимально хранить граф? В зависимости от задачи есть несколько способов.\n#Список рёберИногда достаточно хранить просто список ребер, которые нам дают на вход. Однако часто такой подход оказывается неэффективным для определенных операций.\nНапример, если мы хотим проверить, существует ли ребро между вершинами $a$ и $b$, в наивном случае нам нужно пройтись по всему списку в поисках такого ребра.\nЧтобы избежать этого, можно задать этому списку какую-нибудь структуру. Например, можно отсортировать его сначала по первому элементу, а потом по второму, и тогда можно выполнять проверку бинарным поиском за $O(\\log m)$. Можно пойти дальше и обернуть его в какую-нибудь структуру, например в бинарное дерево или, ещё лучше, хеш-таблицу, но в любом случае возникают некоторые неэффективности.\n#Матрица смежностиМатрицей смежности называется двумерная матрица $G$ размера $n \\times n$, в ячейке $G_{ij}$ которой хранится единица, если существует ребро $(i, j)$, и ноль в противном случае.\nПри реализации обычно создается обычный двумерный массив типа bool. Если для каждой из $n$ вершин мы храним информацию, существует ли ребро в каждую из остальных вершин, то суммарно мы храним $n^2$ ячеек, и, следовательно, асимптотика по памяти — $O(n^2)$.\nВ случае графов с кратными ребрами, иногда вместо нулей и единиц в матрице смежности хранят кратность соответствующего ребра.\nЗаметим, что в случае простых неориентированных графов, матрица смежности будет симметричной и иметь нули на главной диагонали.\nМатрицы смежности очень эффективны для проверок — нужно считать и сравнить всего одну ячейку. Но что, если мы хотим для заданной вершины $v$ иметь возможность быстро пройтись по всем её соседям? Здесь ничего быстрее $O(n)$ не получится.\n#Список смежностиДавайте теперь для каждой из $n$ вершин хранить не булевый массив, а номера всех смежных с ней вершин. Для этого нам потребуется любая динамическая структура, например std::vector в C++.\n// если на вход идет матрица смежности vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; g(n); for (int i = 0; i \u0026lt; n; ++i){ for (int j = 0;j \u0026lt; n;++j){ cin \u0026gt;\u0026gt; a; if (a) g[i].push_back(j); } } // если на вход идет список ребер cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; m; vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; g(n); for (int i = 0; i \u0026lt; m; ++i){ cin \u0026gt;\u0026gt; v1 \u0026gt;\u0026gt; v2; g[v1].push_back(v2); g[v2].push_back(v1); } Здесь асимптотика по памяти и времени считывания - $O(n + m)$, так как мы храним суммарно $2m$ ребер и $n$ векторов.\nПлотные графы, имеющие большое количество ребер, следует хранить при помощи матриц смежности, а разреженные графы, имеющие малое количество ребер, оптимальнее хранить при помощи списков смежности.\n","id":30,"path":"/cs/graph-traversals/storing-graphs/","title":"Хранение графов"},{"content":"Задача сортировки массива заключается в том, чтобы расставить его элементы в определённом порядке — чаще всего по неубыванию: каждый элемент должен быть больше или равен предыдущему.\na = [5, 2, 1, 3, 1] a.sort() print(a) # [1, 1, 2, 3, 5] Хотя эффективные алгоритмы сортировки реализованы в стандартной библиотеке большинства языков, полезно знать, какие подходы существуют, потому что часто можно модифицировать их для решения других смежных задач. Все алгоритмы в этом разделе относительно несложные и являются хорошими упражнениями в оценке времени работы.\nПолезно вместе с описанием алгоритмов смотреть их визуализацию.\n","id":31,"path":"/cs/sorting/","title":"Сортировки"},{"content":"Дерево Фенвика или двоичное индексированное дерево (англ. binary indexed tree) — структура данных, которая на многих задачах заменяет собой дерево отрезков, но при этом работает в 3-4 раза быстрее, занимает минимально возможное количество памяти (столько же, сколько и массив той же длины), намного быстрее пишется и легче обобщается на большие размерности.\n#ОпределениеПусть дан массив $a$ длины $n$. Деревом Фенвика будем называть массив $t$ той же длины, объявленный следующим образом:\n$$ t_i = \\sum_{k=F(i)}^i a_k $$\nгде $F$ это какая-то функцию, для которой выполнено $F(i) \\leq i$. Конкретно её определим потом.\nЗапрос суммы. Когда нам нужна сумма на отрезке, мы будем сводить этот запрос к двум суммам на префиксе:\n$$ sum(l, r) = sum(r) - sum(l-1) $$\nОба этих запроса будем считать по формуле:\n$$ sum(k) = t_k + sum(F(k)-1) $$\nЗапрос обновления. Когда мы изменяем $k$-ю ячейку исходного массива, мы обновляем все $t_i$, в которых учтена эта ячейка.\n$F$ можно выбрать так, что и «спусков» при подсчете суммы, и интересных нам $t_i$ при обновлении будет будет $O(\\log n)$. Популярны две функции:\n$F_1(x) =$ x \u0026amp; (x + 1) $F_2(x) =$ x - (x \u0026amp; -x) + 1 Первый вариант описан на Викиконспектах и Емаксе и поэтому более известен. Второй, как мы дальше увидим, более простой для запоминания и кодинга, а так же более гибкий — например, там можно делать бинпоиск по префиксным суммам. Его мы и будем использовать.\nПримечание. Наверное, меньше четверти умеющих писать эту структуру полностью понимают, как она работает. Анализ действительно весьма сложный, поэтому мы приведём его в конце. Рекомендуется пока что абстрагироваться и принять на веру, что любой префикс разбивается на $O(\\log n)$ отрезков вида $[F(i), i]$, а также что любой элемент входит в не более $O(\\log n)$ таких отрезков.\nДерево Фенвика можно видеть как дерево отрезков, в котором выкинуты все «ненужные» узлы #РеализацияТак как $F(0) = 1 \u0026gt; 0$, то $[0, F(0)]$ не является корректным отрезком. Поэтому нам будет удобнее хранить массив в 1-индексации и не использовать $t_0$.\nint t[maxn]; // возвращает сумму на префиксе int sum (int r) { int res = 0; for (; r \u0026gt; 0; r -= r \u0026amp; -r) res += t[r]; return res; } int sum (int l, int r) { return sum(r) - sum(l-1); } // обновляет нужные t void add (int k, int x) { for (; k \u0026lt;= n; k += k \u0026amp; -k) t[k] += x; } Автор отмечает красивую симметрию в формулах r -= r \u0026amp; -r и k += k \u0026amp; -k, которой нет в «традиционной» версии.\n#Многомерный случай $k$-мерное дерево Фенвика пишется в $(k+1)$ строчку\nНужно добавить всего одну такую же строчку в sum, add, а также при подсчете суммы на прямоугольнике вместо двух запросов к префиксным суммам использовать четыре.\nsum перепишется следующим образом:\nint sum (int r1, int r2) { int res = 0; for (int i = r1; i \u0026gt; 0; i -= i \u0026amp; -i) for (int j = r2; j \u0026gt; 0; j -= j \u0026amp; -j) res += t[i][j]; return res; } В $k$-мерном случае, в соответствии с принципом включений-исключений, для запроса суммы нужно $2^k$ запросов суммы на префиксах.\nЕсли размерности больше, чем позволяет память, то можно вместо массива t использовать хеш-таблицу — так потенциально потребуется $O(q \\log^2 A)$ памяти ($A$ — максимальная координата), но это всё равно один из самых безболезненных способов решать достаточно простые задачи на двумерные структуры.\n#БинпоискОказывается, можно производить бинарный поиск (точнее, спуск) по префиксным суммам за $O(\\log n)$.\n// возвращает индекс, на котором сумма уже больше int lower_bound (int s) { int k = 0; for (int l = logn; l \u0026gt;= 0; l--) { if (k + (1\u0026lt;\u0026lt;l) \u0026lt;= n \u0026amp;\u0026amp; t[k + (1\u0026lt;\u0026lt;l)] \u0026lt; s) { k += (1\u0026lt;\u0026lt;l); s -= t[k]; } } return k; } Если знать, что $F(x)$ удаляет последний бит $x$, то принцип понятен: просто делаем спуск по бинарному дереву, как в ДО. Чем-то похоже на генерацию $k$-го лексикографического комбинаторного объекта: пытаемся увеличить следующий символ всегда, когда это возможно.\nОтметим, что в «традиционной» индексации такое делать нельзя.\n#Ограничения на операциюДерево Фенвика можно использовать, когда наша операция обратима, а также когда трюк с префиксными суммами работает. Это обычно простые операции типа суммы, xor, умножения по модулю (если гарантируется, что на этот модуль ничего не делится). Минимум и gcd, отложенные операции и персистентность прикрутить в общем случае уже не получится — тогда уже нужно писать дерево отрезков.\n#Почему это работаетИтак, мы выбрали вариант с $F(x)$ = x - (x \u0026amp; -x) + 1. Поймем, что означает x \u0026amp; -x.\nЛемма. x \u0026amp; -x возвращает последний единичный бит в двоичной записи x.\nДоказательство потребует знания, как в компьютерах хранятся целые числа. Чтобы процессор не сжигал лишние такты, проверяя знак числа при арифметических операциях, их хранят как бы по модулю $2^k$, а первый бит отвечает за знак (0 для положительных и 1 для отрицательных). Поэтому когда мы хотим узнать, как выглядит отрицательное число, нужно его вычесть из нуля: $-x = 0-x = 2^k-x$.\nКак будет выглядеть -x в битовой записи? Ответ можно мысленно разделить на три блока:\nПервые сколько-то (возможно, нисколько) нулей с конца числа x ими же в ответе и останутся. Потом, ровно на самом младшем единичном бите x, мы «займём» много единиц, так что весь префикс станет единицами. В ответе на этом месте точно будет единица. Потом отменятся ровно те биты из этого префикса, которые были единицами в исходном числе. Пример:\n$$ \\begin{aligned} +90 = 2+8+16+64 \u0026amp; = 0 , 10110_2 \\ -90 = 00000_2 - 10110_2 \u0026amp; = 1 , 01010_2 \\ \\implies (+90) \u0026amp; (-90) \u0026amp; = 0 , 00010_2 \\end{aligned} $$\nТеперь мы можем доказать нашу лемму. Когда мы сделаем \u0026amp;, в префиксе до младшего единичного бита все биты x и -x будут противоположными, младший единичный бит останется единичным, а на суффиксе все как было нулями, так и осталось. Следовательно, «выживет» только этот самый младший единичный бит, что мы и доказывали.\nСледствие 1. sum будет работать за логарифм, а точнее за количество единичных битов в записи $x$: на каждой итерации мы делаем x -= x \u0026amp; -x, то есть удаляем младший бит.\nСледствие 2. add тоже будет работать за логарифм: каждую итерацию количество нулей на конце $x$ увеличивается хотя бы на единицу.\nСледствие 3. (Почему дерево Фенвика — дерево.)\nДлина отрезка, соответствующего любому $t_i$ — степень двойки, причём начинается этот отрезок на индексе, кратном этой же степени двойки.\n$\\implies$ Множества элементов, учтённых в произвольных $t_i$ и $t_j$, либо не пересекаются, либо одно является подмножеством другого.\n$\\implies$ На $t_i$ можно ввести отношение вложенности.\nТо есть, если напрячь воображение, то $t$ можно рассматривать как лес деревьев. В частном случае, когда $n$ является степенью двойки, дерево будет одно.\nТеперь единственное, что осталось доказать ­— это корректность add. На самом деле, в add мы делаем ни что иное, как подъём от вершины до корня по всем предкам.\nКак для $x$ найти непосредственного родителя? Нужно найти минимальное число $y \u0026gt; x$, у которого $t_y$ будет включать $x$. Иными словами, должно выполняться y \u0026gt;= x \u0026gt; y - (y \u0026amp; -y).\nДальше читателю предлагается самостоятельно попялиться в пример, чтобы понять, что x + (x \u0026amp; -x) — минимальное такое число:\n$$ \\begin{aligned} x = 90 = 2+8+16+64 \u0026amp; = 10110_2 \\\\ y = 96 = 32 + 64 \u0026amp; = 11000_2 \\end{aligned} $$\n","id":32,"path":"/cs/range-queries/fenwick/","title":"Дерево Фенвика"},{"content":"Немного более простая для понимания альтернатива префикс-функции — z-функция.\nZ-функция от строки $s$ определяется как массив $z$, такой что $z_i$ равно длине максимальной подстроки, начинающейся с $i$-й позиции, которая равна префиксу $s$.\n$$ \\underbrace{aba}c\\overbrace{aba}daba \\hspace{1em} (z_4 = 3) $$\nНаивно её реализовать ещё проще:\nvector\u0026lt;int\u0026gt; slow_z_function(string s) { int n = (int) s.size(); vector\u0026lt;int\u0026gt; z(n, 0); // z[0] считается не определенным for (int i = 1; i \u0026lt; n; i++) // если мы не вышли за границу и следующие символы совпадают while (i + z[i] \u0026lt; n \u0026amp;\u0026amp; s[z[i]] == s[i + z[i]]) z[i]++; return z; } aaaaa 04321 abcdef 000000 abacabadaba 00103010301 Z-функцию можно использовать вместо префикс-функции в алгоритме Кнута-Морриса-Пратта — только теперь нужные позиции будут начинаться c $|s|$, а не заканчиваться. Осталось только научиться её искать за $O(n)$.\n#Как её быстро считатьБудем идти слева направо и хранить z-блок — самую правую подстроку, равную префиксу, которую мы успели обнаружить. Будем обозначать его границы как $l$ и $r$ включительно.\nПусть мы сейчас хотим найти $z_i$, а все предыдущие уже нашли. Новый $i$-й символ может лежать либо правее z-блока, либо внутри него:\nЕсли правее, то мы просто наивно перебором найдем $z_i$ (максимальный отрезок, начинающийся с $s_i$ и равный префиксу), и объявим его новым z-блоком. Если $i$-й элемент лежит внутри z-блока, то мы можем посмотреть на значение $z_{i-l}$ и использовать его, чтобы инициализировать $z_i$ чем-то, возможно, отличным от нуля. Если $z_{i-l}$ левее правой границы $z$-блока, то $z_i = z_{i-l}$ — больше $z_i$ быть не может. Если он упирается в границу, то «обрежем» его до неё и будем увеличивать на единичку. vector\u0026lt;int\u0026gt; z_function(string s) { int n = (int) s.size(); vector\u0026lt;int\u0026gt; z(n, 0); int l = 0, r = 0; for (int i = 1; i \u0026lt; n; i++) { // если мы уже видели этот символ if (i \u0026lt;= r) // то мы можем попробовать его инициализировать z[i - l], // но не дальше правой границы: там мы уже ничего не знаем z[i] = min(r - i + 1, z[i - l]); // дальше каждое успешное увеличение z[i] сдвинет z-блок на единицу while (i + z[i] \u0026lt; n \u0026amp;\u0026amp; s[z[i]] == s[i + z[i]]) z[i]++; // проверим, правее ли мы текущего z-блока if (i + z[i] - 1 \u0026gt; r) { r = i + z[i] - 1; l = i; } } return z; } Асимптотика. В алгоритме мы делаем столько же действий, сколько раз сдвигается правая граница z-блока — а это $O(n)$.\n#СравнениеВ целом они зет- и префикс-функции очень похожи, но алгоритм Кнута-Морриса-Пратта есть во всех классических учебниках по программированию, а про z-функцию почему-то мало кто знает кроме олимпиадных программистов.\nПро префикс-функцию важно ещё знать, что она онлайновая — достаточно считать следующий символ, и сразу можно узнать значение.\nУпражнение 1. Дан массив префикс-функции. Исходная строка не дана. Вычислите за $O(n)$ зет-функцию этой строки.\nУпражнение 2. Дан массив зет-функции. Исходная строка не дана. Вычислите за $O(n)$ префикс-функцию этой строки.\n","id":33,"path":"/cs/string-searching/z-function/","title":"Z-функция"},{"content":"Представим, что мы работаем журналистами в некотором авторитарном государстве, контролирующем СМИ, и в котором время от времени издаются законы, запрещающие упоминать определенные политические события или использовать определенные слова. Как эффективно реализовать подобную цензуру программно?\nБолее формально, пусть дан набор строк $s_1, s_2, \\ldots, s_m$ алфавита размера $k$ суммарной длины $n$, называемый словарем, и длинный текст $t$. Необходимо определить, есть ли в тексте хотя бы одно слово из словаря, и если есть, то на какой позиции.\nМы уже умеем решать это, например, за $O(n \\cdot |t|)$ независимо проверяя каждое слово на вхождение, или за $O(\\max |s_i| \\cdot |t|)$ хешируя все короткие подстроки. Но мы всё же хотим масштабируемое решение за $O(|t|)$, и сегодня мы построим автомат, который решит нашу задачу.\nАлгоритм Ахо-Корасик (названный по фамилиям создателей, Альфреда Ахо и Маргарет Корасик; в народе — «карасик») за $O(nk)$ времени и памяти строит префиксное дерево для этого набора строк, а затем по этому дереву строит автомат, который может использоваться в этой и многих других строковых задачах. Алгоритм был открыт в 1975-м году, и с тех пор получил широкое распространение в системных программах для потоковой обработки текстов — например, в утилите grep.\nАвтору этой статьи понадобилось по работе реализовать алгоритм Ахо-Корасик целых два раза, что на два раза больше, чем все остальные «олимпиадные» алгоритмы.\n#Основная идеяПостроим по словарю префиксное дерево. Предположим, что строки из словаря не являются подстроками друг друга — в этом случае большие можно выкинуть, но позже мы увидим, что это требование избыточно.\nДля решения задачи мы построим автомат, в котором после считывания каждого очередного символа будет поддерживаться состояние, соответствующее наибольшей строке, являющейся одновременно\nпрефиксом какой-то строки из словаря, и суффиксом считанного на данный момент текста. Этой информации о состоянии нам достаточно: если это состояние помечено терминальным, то суффикс совпадает с какой-то строкой из словаря, и мы выводим его. Иначе по определению мы нашли самый длинный принимаемый бором суффикс, но никакая его подстрока не является запрещенной, а значит в этой позиции никакое запрещенное слово не содержится.\nСложная часть — построить такой автомат, то есть по состоянию в префиксном дереве и новому символу быстро находить вершину, соответствующую наидлиннейшему входящему в бор суффиксу нового выписанного префикса текста. Для этого нам понадобятся несколько вспомогательных понятий.\n#Суффиксные ссылкиАнти-формализм. Чтобы не писать везде громоздкое «строка $s$, которая соответствуют вершине $v$», условимся дальше отождествлять вершину и соответствующую ей строку в боре.\nОпределение. Суффиксная ссылка $l(v)$ ведёт в вершину $u \\neq v$, которая соответствует наидлиннейшему принимаемому бором суффиксу $v$.\nОпределение. Автоматный переход $\\delta(v, c)$ ведёт в вершину, соответствующую максимальному принимаемому бором суффиксу строки $v + c$.\nНаблюдение. Если переход и так существует в боре (будем называть такой переход прямым), то автоматный переход будет вести туда же.\nАвтоматные переходы — это именно то, что нам и надо в задаче: они ведут ровно в те вершины, которые соответствуют самому длинному принимаемому суффиксу.\n#Как их считатьЗаметим следующую связь суффиксных ссылок и автоматных переходов:\n$l(s_{:n}) = \\delta(l(s_{:n-1}), s_n)$: если пойти в родителя, пройтись по его суффиксной ссылке, а затем по автоматному переходу по последнему символу, то мы получим собственную суффиксную ссылку.\nЕсли прямого перехода $v \\to_c u$ не существует, то $\\delta(v, c) = \\delta(l(v), c)$: если пойти по суффиксной ссылке, а затем по автоматному переходу по символу $c$, то получим собственный автоматных переход по этому символу.\nЕсть некоторые нюансы, связанные с корнем и его непосредственными детьми, но важная часть в том, что мы только что выразили $l$ и $\\delta$ для строки длины $n$ через $l$ и $\\delta$ для строк размера $(n-1)$ или меньше. Значит, суффиксные ссылки и автоматные переходы можно найти динамическим программированием по только что выписанным формулам.\n#РеализацияПо сравнению с префиксным деревом, нам помимо массива прямых переходов to и «терминальности» вершины нужно будет хранить некоторую дополнительную информацию:\nмассив автоматных переходов go такого же размера, как и to; суффиксную ссылку link; «родительский» символ pch, который нужен в формуле для суффиксной ссылки. Предполагая, что мы работаем с латинским алфавитом:\nconst int k = 26; struct Vertex { Vertex *to[k] = {0}, *go[k] = {0}; Vertex *link = 0, *p; int pch; Vertex (int _pch, Vertex *_p) { pch = _pch, p = _p; } }; Vertex *root = new Vertex(-1, 0); Добавление строки почти такое же:\nvoid add_string(string \u0026amp;s) { Vertex *v = root; for (char _c : s) { int c -= int(_c - \u0026#39;a\u0026#39;); if (!v-\u0026gt;to[c]) v-\u0026gt;to[c] = new Vertex(c, v); v = v-\u0026gt;to[c]; } } Подсчитывать динамики go и link будем через «ленивую динамику» — введём для них две функции, которые будут запоминать свой результат:\n// нам нужно объявить две функции, ссылающиеся друг на друга // в C/C++ для этого нужно сигнатуру хотя бы одной из них объявить перед другой Vertex* go(Vertex *v, int c); Vertex* link(Vertex *v) { if (!v-\u0026gt;link) { // для строк длины меньше двух суффиксная ссылка это корень if (v == root || v-\u0026gt;p == root) v-\u0026gt;link = root; else // в остальных случаях формула работает v-\u0026gt;link = go(link(v-\u0026gt;p), v-\u0026gt;pch); } return v-\u0026gt;link; } Vertex* go(Vertex *v, int c) { if (!v-\u0026gt;go[c]) { // если переход есть, то автоматный должен вести туда же if (v-\u0026gt;to[c]) v-\u0026gt;go[c] = v-\u0026gt;to[c]; // если перехода нет, но вершина корень, то нужно сделать петлю else if (v == root) v-\u0026gt;go[c] = root; else // в остальных случаях формула работает v-\u0026gt;go[c] = go(link(v), c); } return v-\u0026gt;go[c]; } Эффективнее и «чище» строить автомат через bfs — состояния более длинных строчек зависят только от менее длинных, так что можно пустить «волну» из корня — но так получается сложнее для понимания и реализации.\n","id":34,"path":"/cs/string-structures/aho-corasick/","title":"Алгоритм Ахо-Корасик"},{"content":"Одним из самых простых алгоритмов построения выпуклой оболочки является алгоритм Джарвиса.\nВыберем какую-нибудь точку $p_0$, которая гарантированно попадёт в выпуклую оболочку. Например, нижнюю, а если таких несколько, то самую левую из них.\nДальше будем действовать так: найдём самую «правую» точку от последней добавленной (то есть точку с минимальным полярным углом относительно неё) и добавим её в оболочку. Будем так итеративно добавлять точки, пока не «замкнёмся», то есть пока самой правой точкой не станет $p_0$.\nКорректность алгоритма легко доказывается по индукции:\nНа нулевом шаге мы выбрали точку, точно лежащую в выпуклой оболочке. На $i$-м шаге мы взяли такую точку, что все остальные лежат по левую сторону отрезка $(p_{i-1}, p_i)$, и поэтому точно не перекрывают точку $p_i$. Алгоритм Джарвиса также называют методом заворачивания подарка: мы как бы проходимся вокруг множества с оберткой, которая естественным образом заворачивается вокруг каждого следующего угла.\nДля простоты, будем считать, что все точки различны, имеют целочисленные координаты, а также что нет трёх точек на одной прямой.\nvector\u0026lt;r\u0026gt; jarvis(vector\u0026lt;r\u0026gt; points) { r p0 = points[0]; for (r p : points) if (p.x \u0026lt; p0.x || (p.x == p0.x \u0026amp;\u0026amp; p.y \u0026lt; p0.y)) p0 = p; vector\u0026lt;r\u0026gt; hull = {p0}; while (true) { r t = p0; // кандидат на следующую точку for (r p : points) // лучше никакие полярные углы не считать if ((p - p0) ^ (t - p0) \u0026gt; 0) t = p; if (t == p0) continue; else { p0 = t; hull.push_back(t); } } return hull; } Для каждой точки выпуклой оболочки (обозначим их количество за $h$) мы из всех оставшихся $O(n)$ точек будем искать оптимальную, что суммарно будет работать за $O(n h)$.\nВажно помнить, что асимптотика именно $O(nh)$, а не $O(n^2)$: существуют задачи, где оболочка маленькая, и это существенно.\nЗадача. Дано множество точек, по которому итеративно строят выпуклую оболочку и удаляют те точки, которые в неё попали. Для каждой точки требуется определить, после какой итерации она будет удалена. Асимптотика $O(n^2)$.\n","id":35,"path":"/cs/convex-hulls/jarvis/","title":"Алгоритм Джарвиса"},{"content":"Лемма Бержа говорит о том, что паросочетание без увеличивающих цепей является максимальным.\nЕё важное следствие состоит в том, что максимальное паросочетание можно строить инкрементально, на каждом шаге делая поиск увеличивающей цепи и проводя её чередование.\nАлгоритм Куна ровно в этом и заключается — начнем с пустого паросочетания и будем искать увеличивающие цепи, пока они ищутся.\nДля поиска увеличивающей цепи можно мысленно построить граф, в котором из правой доли в левую можно идти только по рёбрам паросочетания, а из левой в правую — по любым. Тогда можно запустить поиск любого пути из свободной вершины левой доли в какую-нибудь свободную вершину правой доли в измененном графе, и ровно такой путь и будет увеличивающим.\nЭто можно делать как угодно — для упражнения автор рекомендует явно строить такой граф, искать путь любимым алгоритмом поиска и явно проводить чередования — однако устоялась эффективная реализация в виде dfs на 20 строчек кода, приведённая ниже.\nconst int maxn; vector\u0026lt;int\u0026gt; g[maxn]; // будем хранить только рёбра из левой доли в правую int mt[maxn]; // с какой вершиной сматчена вершина правой доли (-1, если ни с какой) bool used[maxn]; // вспомогательный массив для поиска пути dfs-ом // dfs возвращает, можно ли найти путь из вершины v // в какую-нибудь вершину правой доли // если можно, то ещё и проводит чередование bool dfs(int v) { if (used[v]) return false; used[v] = true; for (int u : g[v]) { // если вершина свободна, то можно сразу с ней соединиться // если она занята, то с ней можно соединиться только тогда, // когда из её текущей пары можно найти какую-нибудь другую вершину if (mt[u] == -1 || dfs(mt[u])) { mt[u] = v; return true; } } return false; } Теперь, для нахождения самого паросочетания нужно просто запустить этот поиск от всех вершин левой доли, откатывая состояние вспомогательного массива used:\nmemset(mt, -1, sizeof mt); for (int i = 0; i \u0026lt; n; i++) { memset(used, 0, sizeof mt); if (dfs(i)) cnt++; } Алгоритм ровно $n$ раз ищет увеличивающий путь, каждый раз просматривая не более $m$ рёбер, а значит суммарно отработает за $O(nm)$.\n#ОптимизацииЧто примечательно, алгоритм можно не бояться запускать на ограничениях и побольше ($n, m \\approx 10^4$), если сделать некоторые неасимптотические оптимизации:\nПаросочетание можно жадно инициализировать — например, если просто заранее пройтись по вершинам левой доли и сматчить их со свободной вершиной правой, если она есть. Можно не заполнять нулями на каждой итерации массив used, а использовать следующий трюк: хранить в нём вместо булева флага версию последнего изменения, а конкретно — номер итерации, на которой это значение стало true. Если этот номер меньше текущего номера итерации, то мы можем воспринимать это значение как false. В каком-то смысле это позволяет эмулировать очищение массива за константу. Очень часто граф приходит из какой-то другой задачи, природа которой накладывает ограничения на его вид. Например, в задачах на решетках (когда есть двумерный массив, и соседние клетки связаны друг с другом) граф двудольный, но степень каждой вершины маленькая, и граф имеет очень специфичную структуру. На таких графах алгоритм Куша часто работает быстрее, чем ожидается из формулы $n \\times m$. Контрпримеры в таких задачах почти всегда возможно сгенерировать, но авторы редко заморачиваются. Подробнее про оптимизации можно почитать обсуждение здесь, но вообще говоря, увлекаться ускорением алгоритма Куна не стоит, потому что существует более асимптотически быстрый алгоритм. Задача нахождения максимального паросочетания — частный случай задачи о максимальном потоке, и если применить алгоритм Диница к двудольным графам с единичной пропускной способностью, то работать он будет за $O(m \\sqrt n)$.\n","id":36,"path":"/cs/matching/kuhn/","title":"Алгоритм Куна"},{"content":"Лемма о безопасном ребре говорит, что мы можем строить минимальный остов постепенно, добавляя по одному ребра, про которые мы точно знаем, что они минимальные для соединения какого-то разреза.\nОдин из подходов это использовать заключается в алгоритме Прима:\nИзначально остов — одна произвольная вершина. Пока минимальный остов не найден, выбирается ребро минимального веса, исходящее из какой-нибудь вершины текущего остова в вершину, которую мы ещё не добавили. Добавляем это ребро в остов и начинаем заново, пока остов не будет найден. Этот алгоритм очень похож на алгоритм Дейкстры, только мы выбираем следующую вершину с другой весовой функцией — вес минимального соединяющего ребра вместо суммарного расстояния до неё.\nСовсем наивная реализация за $O(nm)$ — каждый раз перебираем все рёбра:\nconst int maxn = 1e5, inf = 1e9; vector from, to, weight; bool used[maxn] // считать все рёбра в массивы used[0] = 1; for (int i = 0; i \u0026lt; n-1; i++) { int opt_w = inf, opt_from, opt_to; for (int j = 0; j \u0026lt; m; j++) if (opt_w \u0026gt; weight[j] \u0026amp;\u0026amp; used[from[j]] \u0026amp;\u0026amp; !used[to[j]]) opt_w = weight[j], opt_from = from[j], opt_to = to[j] used[opt_to] = 1; cout \u0026lt;\u0026lt; opt_from \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; opt_to \u0026lt;\u0026lt; endl; } Реализация за $O(n^2)$:\nconst int maxn = 1e5, inf = 1e9; bool used[maxn]; vector\u0026lt; pair\u0026lt;int, int\u0026gt; \u0026gt; g[maxn]; int min_edge[maxn] = {inf}, best_edge[maxn]; min_edge[0] = 0; // ... for (int i = 0; i \u0026lt; n; i++) { int v = -1; for (int u = 0; u \u0026lt; n; u++) if (!used[u] \u0026amp;\u0026amp; (v == -1 || min_edge[u] \u0026lt; min_edge[v])) v = u; used[v] = 1; if (v != 0) cout \u0026lt;\u0026lt; v \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; best_edge[v] \u0026lt;\u0026lt; endl; for (auto e : g[v]) { int u = e.first, w = e.second; if (w \u0026lt; min_edge[u]) { min_edge[u] = w; best_edge[u] = v; } } } Также, как в алгоритме Дейкстры, можно не делать линейный поиск оптимальной вершины, а поддерживать его в приоритетной очереди. Получается реализация за $O(m \\log n)$:\nset\u0026lt; pair\u0026lt;int, int\u0026gt; \u0026gt; q; int d[maxn]; while (q.size()) { v = q.begin()-\u0026gt;second; q.erase(q.begin()); for (auto e : g[v]) { int u = e.first, w = e.second; if (w \u0026lt; d[u]) { q.erase({d[u], u}); d[u] = w; q.insert({d[u], u}); } } } Про алгоритм за $O(n^2)$ тоже забывать не стоит — он работает лучше в случае плотных графов.\n","id":37,"path":"/cs/spanning-trees/prim/","title":"Алгоритм Прима"},{"content":"Часто бывает полезно оценить, сколько времени работает алгоритм. Конечно, можно его просто реализовать и запустить, но тут возникают проблемы:\nНа разных компьютерах время работы всегда будет отличаться. Не всегда заранее доступны именно те данные, на которых он в реальности будет запускаться. Иногда приходится оценивать алгоритмы, требующие часы или даже дни работы. Жизнь коротка, и если есть несколько вариантов алгоритмов, то хочется заранее предсказать, какой из них будет быстрее, чтобы реализовывать только его. Для этого, в качестве первого приближения, попробуем оценить число операций в алгоритме. Возникает вопрос: какие именно операции считать. Как один из вариантов — учитывать любые элементарные операции:\nарифметические операции с числами: +, -, *, / сравнение чисел: \u0026lt;, \u0026gt;, \u0026lt;=, \u0026gt;=, ==, != присваивание: a[0] = 3 При этом важно не просто считать строчки, а ещё учитывать, как реализованы некоторые отдельные вещи в самом языке. Например, в питоне срезы массива (array[3:10]) копируют этот массив, то есть этот срез работает за 7 элементарных действий. А swap, например, можно реализовать за 3 присваивания.\nУпражнение. Попробуйте посчитать точное число сравнений и присваиваний в сортировках пузырьком, выбором, вставками и подсчетом в худшем случае. Это должна быть какая-то формула, зависящая от $n$ — длины массива.\nЧтобы учесть вообще все элементарные операции, ещё надо посчитать, например, сколько раз прибавилась единичка внутри цикла for. А ещё, например, строчка n = len(array) — это тоже действие. Поэтому даже посчитав их, не сразу очевидно, какой из этих алгоритмов работает быстрее — сравнивать формулы сложно. Хочется придумать способ упростить эти формулы так, чтобы\nне нужно было учитывать информацию, не очень сильно влияющую на итоговое время; легко было оценивать время работы разных алгоритмов для больших чисел; легко было сравнивать алгоритмы на предмет того, какой из них лучше подходит для тех или иных входных данных. Для этого придумали О-нотацию — асимптотическое время работы вместо точного (часто его ещё называют просто асимптотикой).\nОпределение. Пусть $f(n)$ — это какая-то функция. Говорят, что функция $g(n) = O(f(n))$, если существует такие константы $c$ и $n_0$, что $g(n) \u0026lt; c \\cdot f(n)$ для всех $n \\geq n_0$.\nНапример:\n$\\frac{n}{3} = O(n)$ $\\frac{n(n-1)(n-2)}{6} = O(n^3)$ $1 + 2 + 3 + \\ldots + n = O(n^2)$ $1^2 + 2^2 + 3^2 + \\ldots + n^2 = O(n^3)$ $\\log_2{n} + 3 = O(\\log n)$ $179 = O(1)$ $10^{100} = O(1)$ В контексте анализа алгоритмов, фраза «алгоритм работает за $O(f(n))$ операций» означает, что начиная с какого-то $n$ он делает не более чем за $c \\cdot f(n)$ операций.\nВ таких обозначениях можно сказать, что\nсортировка пузырьком работает за $O(n^2)$; сортировка выбором работает за $O(n^2)$; сортировка вставками работает за $O(n^2)$; сортировка подсчетом работает за $O(n + m)$. Это обозначение удобно тем, что оно короткое и понятное, а также не зависит от умножения на константу или прибавления константы. Например, если алгоритм работает за $O(n^2)$, то это может значить, что он работает за $n^2$, за $n^2 + 3$, за $\\frac{n(n-1)}{2}$ или даже за $1000 \\cdot n^2 + 1$ действие. Главное — что функция ведет себя как $n^2$, то есть при увеличении $n$ она увеличивается как некоторая квадратичная функция: если увеличить $n$ в 10 раз, время работы программы увеличится приблизительно в 100 раз.\nОсновное преимущество О-нотации в том, что все рассуждения про то, сколько операций в swap или считать ли отдельно присваивания, сравнения и циклы — отпадают. Каков бы ни был ответ на эти вопросы, они меняют ответ лишь на константу, а значит асимптотическое время работы алгоритма не меняется.\nПервые три сортировки поэтому называют квадратичными — они работают за $O(n^2)$. Сортировка подсчетом может работать намного быстрее — она работает за $O(n + m)$, а если $m \\leq n$, то это вообще линейная функция $O(n)$.\n","id":38,"path":"/cs/complexity/asymptotic/","title":"Асимптотический анализ"},{"content":"Итератор — это объект, указывающий на элемент какого-то контейнера.\nИтератор является абстракцией над концепцией указателей. Если указатели работают только с последовательными данными (массивами), то итераторы работают с любыми контейнерами — например со связными списками или деревьями поиска — причём в единообразном синтаксисе, что сильно помогает разработчикам библиотек избегать дублирования кода.\n#СинтаксисЧтобы получить элемент, на который указывает итератор it, необходимо воспользоваться оператором разыменования: *it. Если нужно перейти к следующему элементу надо использовать инкремент: ++it (постфиксного инкремента у итераторов нет).\nУ всех контейнеров есть какой-то первый и последний элемент. Итератор на первый элемент можно получить через a.begin(), а через a.end() можно получить итератор на некий фиктивный элемент, следующий последним. Таким образом, если проходить от a.begin() до a.end() не включительно, ты мы пройдём по всем элементам контейнера.\nvector\u0026lt;int\u0026gt; a = {1, 2, 3, 4, 5}; // в типе итератора должна содержаться информация о контейнере // поэтому в случае вектора интов это \u0026#34;vector\u0026lt;int\u0026gt;::iterator\u0026#34; for (vector\u0026lt;int\u0026gt;::iterator it = a.begin(); it != a.end(); ++it) cout \u0026lt;\u0026lt; *it \u0026lt;\u0026lt; endl; // в современном C++ можно вместо него использовать \u0026#34;auto\u0026#34; for (auto it = a.begin(); it != a.end(); ++it) cout \u0026lt;\u0026lt; *it \u0026lt;\u0026lt; endl; // также если нужно пройтись по всему массиву, можно сжать до такого for (int x : a) cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; endl; // если нужно как-то изменить элемент, его можно передать по ссылке for (int \u0026amp;x : a) x *= 2; for (x : a) cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; endl; for (const int \u0026amp;x : a) cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; endl; // (также можно вместо int писать auto) // the initializer may be a braced-init-list for (int x : {1, 2, 3, 4, 5}) cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; endl; // the initializer may be an array int b[] = {1, 2, 3, 4, 5}; for (int x : b) cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; endl; array\u0026lt;int, 5\u0026gt; c = {1, 2, 3, 4, 5}; for (int x : c) cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; endl; #Категории итераторовИтераторы — очень важная часть интерфейса контейнера. Итераторы можно передавать в различные алгоритмы стандартной библиотеки, которым не обязательно знать про внутреннее устройство контейнера, но нужно знать про то, какие паттерны доступа к данным возможны.\nПоэтому в зависимости от внутренней структуры, итераторы контейнера могут отвечать к одному из нескольких абстрактных классов с разными уровнями гарантий:\ninput_iterator, поддерживающий только операции разыменования и инкремента — причём даже без гарантии, что после того, как был произведён инкремент, его предыдущие значения остались валидными. Как можно догадаться из названия, используется для потокового ввода.\nforward_iterator, помимо предыдущего гарантирующий что итераторы на какой-то конкретный элемент можно инкрементировать сколько угодно раз не опасаясь, что они исчезнут (что позволяет их использовать в алгоритмах, проходящих по данным несколько раз).\nbidirectional_iterator, помимо предыдущего поддерживающий возможность декремента (it--) — то есть перехода к предыдущему элементу.\nrandom_access_iterator, помимо предыдущего поддерживающий возможность переходить к элементу, который находится на каком-то расстоянии $k$ — it + k, it - k, it += k, it -= k — и находить расстояние между позициями, на которые указывают два итератора: например, выражение a - b вернет целое число — расстояние между двумя элементами коллекции, соответствующим итераторам a и b.\n#Алгоритмы из STLНапример, итераторы std::vector относятся к random_access_iterator, и если вызвать функцию lower_bound из стандартной библиотеки, то она произведет бинарный поиск по элементам (предполагая, что они отсортированы в порядке неубывания):\nvector\u0026lt;int\u0026gt; a = {1, 2, 3, 5, 8, 13}; // вернет 8 cout \u0026lt;\u0026lt; *lower_bound(a.begin(), a.end(), 7) \u0026lt;\u0026lt; endl; Функция lower_bound возвращает итератор на первый элемент, не меньший заданного. Также есть upper_bound, возвращающий первый элемент, строго больший (в случае с int это то же самое, что найти lower_bound от x + 1).\nЗная, что итераторы вектора поддерживают произвольный доступ, бинарный поиск будет работать за $O(n \\log n)$, но для других структур это, возможно, будет не так.\nТакже по этой причине часто имеет смысл вместо сишных массивов использовать std::array, который является точно таким же массивом фиксированной длины, но поддерживает итераторы и вместе с ними все алгоритмы STL:\narray\u0026lt;int, 3\u0026gt; a = {4, 2, 1, 3}; // вернет 1 cout \u0026lt;\u0026lt; *min_element(a.begin(), a.end()) \u0026lt;\u0026lt; endl; ","id":39,"path":"/cs/basic-structures/iterators/","title":"Итераторы"},{"content":"У алгоритмов, использующих хеширование, есть один неприятный недостаток: недетерминированность. Если мы сгенерируем бесконечное количество примеров, то когда-нибудь нам не повезет, и программа отработает неправильно. На CodeForces даже иногда случаются взломы решений, использующих хеширование — можно в оффлайне сгенерировать тест против конкретного решения. Есть даже гайды про то, как это делать.\nСобытие, когда два хеша совпали, а не должны, называется коллизией. Пусть мы решаем задачу определения количества различных подстрок — мы добавляем в set $O(n^2)$ различных случайных значений в промежутке $[0, m)$. Понятно, что если произойдет коллизия, то мы какую-то строку не учтем и получим WA. Насколько большим следует делать $m$, чтобы не бояться такого?\n#Выбор константПрактическое правило: если вам нужно хранить $n$ различных хешей, то безопасный модуль — это число порядка $10 \\cdot n^2$. Обоснование — см. парадокс дней рождений.\nНе всегда такой модуль можно выбрать один — если он будет слишком большой, то при умножении будут происходить переполнения. Вместо этого можно брать два или даже три модуля и считать много хешей параллельно.\nМожно также брать «модуль» $2^{64}$. У него есть несколько преимуществ:\nОн большой — второй модуль точно не понадобится. С ним ни о каких переполнениях заботиться не нужно — если все хранить в unsigned long long, процессор сам автоматически сделает эти взятия остатков при переполнении. С ним хеширование будет значительно быстрее — раз переполнение происходит на уровне процессора, можно не выполнять долгую операцию %. Всё с этим модулем было прекрасно, пока не придумали тест против него. Однако его добавляют далеко не на все контесты — имейте это в виду.\nВ выборе же $k$ ограничения не такие серьезные:\nОна должна быть чуть больше размера словаря — иначе можно изменить две соседние буквы и получить коллизию. Она должна быть взаимно проста с модулем — иначе в какой-то момент всё может занулиться. Главное — чтобы значения $k$ и модуля не знал человек, который генерирует тесты.\n#Парадокс дней рождений В группе, состоящей из 23 или более человек, вероятность совпадения дней рождения хотя бы у двух людей превышает 50%.\nБолее общее утверждение: в мультимножество нужно добавить $\\Theta(\\sqrt{n})$ случайных чисел от 1 до n, чтобы какие-то два совпали.\nПервое доказательство (для любителей матана). Пусть $f(n, d)$ это вероятность того, что в группе из $n$ человек ни у кого не совпали дни рождения. Будем считать, что дни рождения распределены независимо и равномерно в промежутке от $1$ до $d$.\n$$ f(n, d) = (1-\\frac{1}{d}) \\times (1-\\frac{2}{d}) \\times \u0026hellip; \\times (1-\\frac{n-1}{d}) $$\nПопытаемся оценить $f$:\n$$ \\begin{aligned} e^x \u0026amp; = 1 + x + \\frac{x^2}{2!} + \\ldots \u0026amp; \\text{(ряд Тейлора для экспоненты)} \\ \u0026amp; \\simeq 1 + x \u0026amp; \\text{(аппроксимация для $|x| \\ll 1$)} \\ e^{-\\frac{n}{d}} \u0026amp; \\simeq 1 - \\frac{n}{d} \u0026amp; \\text{(подставим $\\frac{n}{d} \\ll 1$)} \\ f(n, d) \u0026amp; \\simeq e^{-\\frac{1}{d}} \\times e^{-\\frac{2}{d}} \\times \\ldots \\times e^{-\\frac{n-1}{d}} \u0026amp; \\ \u0026amp; = e^{-\\frac{n(n-1)}{2d}} \u0026amp; \\ \u0026amp; \\simeq e^{-\\frac{n^2}{2d}} \u0026amp; \\ \\end{aligned} $$\nИз последнего выражения более-менее понятно, что вероятность $\\frac{1}{2}$ достигается при $n \\approx \\sqrt{d}$ и в этой точке изменяется очень быстро.\nВторое доказательство (для любителей теорвера). Введем $\\frac{n(n-1)}{2}$ индикаторов — по одному для каждой пары людей $(i, j)$ — каждый будет равен единице, если дни рождения совпали. Ожидание и вероятность каждого индикатора равна $\\frac{1}{d}$.\nОбозначим за $X$ число совпавших дней рождений. Его ожидание равно сумме ожиданий этих индикаторов, то есть $\\frac{n (n-1)}{2} \\cdot \\frac{1}{d}$.\nОтсюда понятно, что если $d = \\Theta(n^2)$, то ожидание равно константе, а если $d$ асимптотически больше или меньше, то $X$ стремится нулю или бесконечности соответственно.\nПримечание: формально, из этого явно не следует, что вероятности тоже стремятся к 0 и 1.\n#Бонус: «мета-задача»Дана произвольная строка, по которой известным только авторам задачи способом генерируется ответ yes/no. В задаче 100 тестов. У вас есть 20 попыток отослать решение. В качестве фидбэка вам доступны вердикты на каждом тесте. Вердиктов всего два: OK (ответ совпал) и WA. Попытки поделить на ноль, выделить терабайт памяти и подобное тоже считаются как WA.\n«Решите» задачу.\n","id":40,"path":"/cs/hashing/collision/","title":"Коллизии хешей"},{"content":"Некоторые линейные функции обратимы: например, поворот на угол. Другие — необратимы: например, проекция. Понятие обратимости можно продолжить и на матрицы.\nОпределение. Матрица $A$ является обратимой, если существует матрица $A^{-1}$ такая, что\n$$ A \\cdot A^{-1} = A^{-1} \\cdot A = I $$\nИз свойств матричного умножения следует, что для того, чтобы обратная матрица существовала, матрица $A$ должна быть квадратной, но этого не всегда достаточно.\n#Системы линейных уравненийПонятие обратимости матриц важно для решения линейных уравнений:\n$$ Ax = b \\leftrightarrow \\begin{cases} a_{11} x_1 + a_{12} x_2 + \\ldots + a_{1n} x_n = b_1 \\ a_{21} x_1 + a_{22} x_2 + \\ldots + a_{2n} x_n = b_2 \\ \\ldots \\ a_{n1} x_1 + a_{n2} x_2 + \\ldots + a_{nn} x_n = b_n \\end{cases} $$\nЕсли матрица $A$ обратима, то решение будет единственным, а именно\n$$ x = A^{-1} b $$\nВ противном случае будет либо ноль, либо бесконечное количество решений, в общем случае живущих на каком-то многомерном пространстве. Об этом удобно думать так: каждое условие (строчка-уравнение) задает какую-то гиперплоскость в $n$-мерном пространстве, и пересечения этих гиперплоскостей либо пустые, либо совпадают, либо дают какое-то подпространство меньшей размерности.\nРешение систем линейных уравнений — одна из основных задач вычислительной линейной алгебры, часто появляющаяся как подзадача в других алгоритмах.\n#Метод ГауссаМетод Гаусса (англ. Gaussian elimination) позволяет решать системы линейных уравнений, а также находить обратные к матрицам и решать смежные задачи.\nОн основывается на том факте, что с системами уравнений мы можем свободно делать следующие операции:\nпоменять два уравнения местами, домножить любое уравнение на ненулевую константу, вычесть из одного уравнения другое. При этом все вышеприведенные операции обратимы и не теряют никакую информацию, потому что все строки всегда будут нетривиальными линейными комбинациями друг друга, и любое изначальное уравнение всегда будет учтено с каким-то коэффициентом в каком-нибудь из уравнений.\nЦель алгоритма — используя эти три операции привести систему к виду\n$$ \\begin{cases} 1 \\cdot x_1 + 0 \\cdot x_2 + \\ldots + 0 \\cdot x_n = b_1' \\ 0 \\cdot x_1 + 1 \\cdot x_2 + \\ldots + 0 \\cdot x_n = b_2' \\ \\ldots \\ 0 \\cdot x_1 + 0 \\cdot x_2 + \\ldots + 1 \\cdot x_n = b_n' \\end{cases} $$\nи тогда решением будет просто $x = b\u0026rsquo;$.\n#АлгоритмБудем действовать в предположении, что решение существует и единственно.\nПрипишем вектор $b$ справа от матрицы $A$, и будем итеративно приводить получившуюся $n \\times (n+1)$ матрицу к требуемому виду. На шаге $i$, мы хотим сделать так, чтобы в ячейке $a_{ii}$ стояла единица, а во всех остальных ячейках $i$-того столбца стояли нули.\nЧтобы выполнить $i$-тый шаг:\nнайдем какой-нибудь ненулевой элемент на $i$-том столбце и поменяем его строку местами с $i$-той; поделим всю $i$-тую строку на элемент $a_{ii}$ (он ненулевой: мы его специально искали на предыдущем шаге); пройдемся по всем остальным строкам $j \\ne i$ и вычтем $i$-тую строку, помноженную на коэффициент $a_{ji}$. В результате после $i$-того шага элемент $a_{ii}$ будет единичным (потому что мы его поделили на самого себя), а во всех остальных ячейках $i$-того столбца стали нули (потому что мы в этих позициях вычли $a_{ij} \\cdot a_{ii} = a_{ij}$).\nТаким образом, через $n$ шагов мы приведем матрицу к нужному виду, и ответом будет последний, $(n+1)$-ый приписанный столбец.\n#РеализацияИз соображений вычислительной стабильности имеет смысл на каждом шаге брать не любой ненулевой элемент в качестве опорного, а самый большой — мы ведь на него делим, а в случае в числами с плавающей точкой могло получиться очень близкий к нулю число.\ntypedef vector\u0026lt; vector\u0026lt;double\u0026gt; \u0026gt; matrix; vector\u0026lt;double\u0026gt; gauss(matrix a) { int n = (int) a.size(); for (int i = 0; i \u0026lt; n; i++) { // находим опорный элемент int best = i; for (int j = i + 1; j \u0026lt; n; j++) if (abs(a[j][i]) \u0026gt; abs(a[best][i])) best = j; // меняем строчки местами swap(a[best], a[i]); // нормализуем i-тую строчку (слева нули, с ними ничего делать не надо) for (int j = n; j \u0026gt;= i; j--) a[i][j] /= a[i][i]; // зануляем все остальные ячейки в i-том столбце for (int j = 0; j \u0026lt; n; j++) if (j != i) // слева только нули, так что можно пройтись только по правой части for (int k = n; k \u0026gt;= i; k--) a[j][k] -= a[i][k] * a[j][i]; } vector\u0026lt;double\u0026gt; ans(n); for (int i = 0; i \u0026lt; n; i++) ans[i] = a[i][n]; return ans; } Алгоритм работает $O(n^3)$.\nТакже его можно обобщить не только на действительные числа, но и на другие поля — например, на остатки по простому модулю, только нужно использовать соответствующую процедуру для деления.\n#Булевы матрицыВ контексте олимпиад, в большинстве задач, где требуется решать системы линейных уравнений, на самом деле это нужно делать над полем $\\mathbb{Z}_2$, то есть по модулю 2.\nЗадача. Есть $n$ лампочек и $n$ переключателей: каждый активированный переключатель меняет состояние (включает или выключает) какого-то подмножества лампочек. Известно состояние всех лампочек, нужно восстановить состояние переключателей.\nНас по сути просят решить следующую систему:\n$$ \\begin{cases} a_{11} x_1 + a_{12} x_2 + \\ldots + a_{1n} x_n \\equiv b_1 \\pmod 2\\ a_{21} x_1 + a_{22} x_2 + \\ldots + a_{2n} x_n \\equiv b_2 \\pmod 2\\ \\ldots \\ a_{n1} x_1 + a_{n2} x_2 + \\ldots + a_{nn} x_n \\equiv b_n \\pmod 2 \\end{cases} $$\nЗдесь $x$ — состояния переключателей, $b$ — состояния лампочек, $A$ — информация о том, влияет ли переключатель на лампочку.\nВ таком случае можно значительно ускорить и упростить обычный метод Гаусса через битсет:\ntypedef bitset\u0026lt;maxn\u0026gt; t; typedef array\u0026lt;t, maxn\u0026gt; matrix; t gauss(matrix a) { for (int i = 0; i \u0026lt; n; i++) { int nonzero = i; for (int j = i + 1; j \u0026lt; n; j++) if (a[j][i]) nonzero = j; swap(a[nonzero], a[i]); for (int j = 0; j \u0026lt; n; j++) if (j != i \u0026amp;\u0026amp; a[j][i]) a[j] ^= a[i]; } t x; for (int i = 0; i \u0026lt; n; i++) x[i] = a[i][n] ^ a[i][i]; return x; } Обратим внимание, что как в этой, так и в предыдущей реализации предполагалось, что решение существует и единственное. Если это не так, то в какой-то момент мы не найдем ненулевой элемент в качестве опорного.\nВ этом случае можно дальше запустить алгоритм, игнорируя полностью нулевые столбцы, и тогда в конце будет какое-то количество нулевых сумм, равных каким-то $b_i\u0026rsquo;$. Если все эти $b_i\u0026rsquo;$ нулевые, то решение существует, в противном случае — нет.\nВ случае входных данных из «реального мира» можно добавить небольшой шум $\\epsilon_{ij}$ к коэффициентам: это гарантирует, что решение, причем уникальное, всегда будет существовать.\n#БазисыОпределение. Базисом множества векторов называется набор векторов, через линейную комбинацию которых единственным способом можно выразить все вектора этого множества и только их.\nБазисы есть не только в линейной алгебре. Например, ${1, x, x^2}$ является базисом всех квадратных трёхчленов. Или ${\\neg, \\land, \\lor}$ является базисом всех логических выражений (то есть всё можно выразить через И, ИЛИ и НЕ). В произвольном языке программирования можно выделить какой-то набор команд, через который можно будет написать всё что угодно, и он тоже в этом смысле будет базисом.\nОпределение. Система векторов ${a_1, a_2, \\dots, a_n}$ называется линейно зависимой, если существует такой набор действительных коэффициентов $(\\lambda_1, \\lambda_2, \\dots, \\lambda_n)$, что $\\lambda_i \\neq 0$ хотя бы для одного $i$ и\n$$ \\lambda_1 \\cdot a_1 + \\lambda_2 \\cdot a_2 + \\dots + \\lambda_n \\cdot a_n = 0 $$\nВ противном случае система называется линейно независимой.\nУтверждение. Базис $n$-мерного пространства — это линейно независимый набор из $n$ векторов.\nДоказательство. Пусть базис линейно зависим. Тогда вектор нулевой длины можно выразить хотя бы двумя способами, что противоречит определению.\nБазисы часто требуется искать или поддерживать в олимпиадных задачах — часто вида «набрать базис максимального веса, где каждый вектор сколько-то стоит». Для проверки, является ли набор векторов базисом, можно проверить, выражается ли как-то нетривиально с помощью них нулевой вектор — это можно сделать методом Гаусса.\nУпражнение. Дан массив из $10^5$ целых чисел от $0$ до $(2^{30}-1)$. Требуется найти количество различных подпоследовательностей этого массива, xor-сумма которых равна заданному числу $x$.\n","id":41,"path":"/cs/algebra/gauss/","title":"Линейные уравнения"},{"content":"Рандомизированные алгоритмы делятся на две категории, названные в честь известных игорных центров:\nАлгоритмы Лас-Вегас гарантированно находят правильный ответ, но требуют недетерминированное количество времени для работы. Например, Quicksort является таким алгоритмом. Алгоритмы Монте-Карло требуют детерминированное время для работы, но с какой-то вероятностью могут дать неправильный ответ. Например, тестирование на простоту почти всегда подразумевает какую-то вероятность ошибки. В контексте численных методов, алгоритмы Монте-Карло часто используются для подсчета некоторых величин, где высокая точность не нужна.\n#Подсчет площадейРассмотрим следующую задачу. Дана карта города (для простоты, предположим, что это единичный квадрат) и список координат сотовых вышек и их радиусов действия. Требуется посчитать площадь покрытия в этом городе, то есть доля точек города, имеющих хотя бы одну вышку в пределах радиуса её действия\nМожно перефразировать эту задачу как нахождение площади пересечения единичного квадрата и объединения кругов. Эта задача имеет точное, но очень сложное решение, которое требует подсчета всех «точек интереса» где какие-либо две фигуры пересекаются, прохождения по ним сканирующей прямой и подсчета кучи нетривиальных интегралов на каждом свободном от пересечений отрезке. Это решение настолько точное, насколько позволяет действительнозначная арифметика, однако медленное и очень неприятное в реализации для не-экспертов в вычислительной геометрии.\nВместо всего этого можно поступить следующим образом: взять несколько случайных точек на квадрате и для каждой из них независимо проверить, покрыта ли она каким-нибудь кругом с помощью простого предиката:\n$$ (x-x_i)^2 + (y-y_i)^2 \\leq r_i^2 $$\nТогда доля покрытых точек будет нашей оценкой ответа, и если мы взяли достаточно точек, то эта оценка будет близка к реальности.\nМожно найти произвольно точное приближение $\\pi$, если поставить единичный круг в угол квадрата Если у нас есть какое-то формальное требование точности нашего ответа — например, если нам нужно получить 3 правильных значимых знака хотя бы 99% времени — как много точек нам нужно проверить?\nМожно показать, что, чтобы получить $k$ правильных знаков с произвольно близкой к единице вероятностью, потребуется $n = \\Theta(10^{2k})$ точек.\nДля доказательства этого факта читатель перенаправляется в раздел теорвера.\n","id":42,"path":"/cs/numerical/monte-carlo/","title":"Методы Монте-Карло"},{"content":"Обычное декартово дерево — это структура для множеств, каждый элемент которых имеет какой-то ключ. Эти ключи задают на этом множестве порядок, и все запросы к ДД обычно как-то привязаны к этому порядку.\nНо что, если у нас есть запросы, которые этот порядок как-то нетривиально меняют? Например, если у нас есть массив, в котором нужно уметь\nвыводить сумму на произвольном отрезке, «переворачивать» произвольный отрезок, то есть переставлять элементы с $l$ по $r$ в обратном порядке, не меняя остальные. Если бы не было второй операции, мы бы просто использовали индекс элемента в качестве ключа, но с операцией переворота нет способа их быстро поддерживать актуальными.\nРешение такое: выкинем ключи, а вместо них будем поддерживать информацию, которая поможет неявно восстановить ключ, когда он нам будет нужен. А именно, будем хранить вместе с каждой вершиной размер её поддерева:\nstruct Node { int prior, size = 1; // ^ размер поддерева // ... }; Тогда ключ (позицию элемента) можно восстановить как число элементов, которые находятся слева от него — что можно пересчитывать во время спуска по дереву.\nРазмеры поддеревьев будем поддерживать по аналогии с суммой — напишем вспомогательную функцию, которую будем вызывать после каждого структурного изменения вершины.\nint size(Node *v) { return v ? v-\u0026gt;size : 0; } void upd(Node *v) { v-\u0026gt;size = 1 + size(v-\u0026gt;l) + size(v-\u0026gt;r); } Операция merge не меняется, так как нигде не использует ключи, а вот в split нужно использовать позицию корня вместо его ключа. Про split теперь удобнее думать как «вырежи первые k элементов»:\npair\u0026lt;Node*, Node*\u0026gt; split(Node *p, int k) { if (!p) return {0, 0}; if (size(p-\u0026gt;l) + 1 \u0026lt;= k) { auto [l, r] = split(p-\u0026gt;r, k - size(p-\u0026gt;l) - 1); // ^ правый сын не знает количество вершин слева от него p-\u0026gt;r = l; upd(p); return {p, r}; } else { auto [l, r] = split(p-\u0026gt;l, k); p-\u0026gt;l = r; upd(p); return {l, p}; } } Всё. Теперь у нас есть клёвая гибкая структура, которую можно резать как угодно, не опираясь на ключи.\n#Пример: ctrl+x, ctrl+vNode* ctrlx(int l, int r) { auto [T, R] = split(root, r); auto [L, M] = split(T, l); root = merge(L, R); return M; } void ctrlv(Node *v, int k) { auto [l, r] = split(root, k); root = merge(l, merge(v, r)); } #Пример: переворотВернемся к изначальной задаче: нужно за $O(\\log n)$ обрабатывать запросы переворота произвольных подстрок: значение $a_l$ поменять с $a_r$, $a_{l+1}$ поменять с $a_{r-1}$ и т. д.\nБудем хранить в каждой вершине флаг rev, который будет означать, что её подотрезок перевернут:\nstruct Node { bool rev; // ... }; Поступим по аналогии с техникой отложенных операций в дереве отрезков — когда мы когда-либо встретим такую вершину, мы поменяем местами ссылки на её детей, а им самим передадим эту метку:\nvoid push(node *v) { if (v-\u0026gt;rev) { swap(v-\u0026gt;l, v-\u0026gt;r); if (v-\u0026gt;l) v-\u0026gt;l-\u0026gt;rev ^= 1; if (v-\u0026gt;r) v-\u0026gt;r-\u0026gt;rev ^= 1; } v-\u0026gt;rev = 0; } Аналогично, эту функцию будем вызывать в начале merge и split.\nСаму функцию reverse реализуем так: вырезать нужный отрезок, поменять флаг.\nvoid reverse(int l, int r) { auto [T, R] = split(root, r); auto [L, M] = split(T, l); M-\u0026gt;rev ^= 1; root = merge(L, merge(M, R)); } ","id":43,"path":"/cs/tree-structures/implicit/","title":"Неявный ключ"},{"content":"Эта статья — одна из серии. Рекомендуется сначала прочитать все предыдущие.\nПредыдущий метод оптимизации опирался на тот факт, что $opt[i, j] \\leq opt[i, j + 1]$.\nАсимптотику можно ещё улучшить, заметив, что $opt$ монотонен также и по второму параметру:\n$$ opt[i - 1, j] \\leq opt[i, j] \\leq opt[i, j + 1] $$\nВ задаче про покрытие отрезками это выполняется примерно по той же причине: если нам доступно больше отрезков, то последний отрезок в оптимальном решении точно не будет длиннее, чем раньше.\n#АлгоритмВоспользуемся полученными неравенствами самым прямым образом: будем для каждого состояния просто перебирать все элементы от $opt[i - 1, j]$ до $opt[i, j + 1]$.\nЧтобы к тому моменту, когда мы дойдем до состояния $(i, j)$, границы $opt[i - 1, j]$ и $opt[i, j + 1]$ были уже посчитаны, мы будем проходиться в порядке увеличения $i$ и уменьшения $j$.\nfor (int i = 1; i \u0026lt;= n; i++) { for (int j = m; j \u0026gt;= 1; j--) { for (int k = opt[i - 1][j]; k \u0026lt;= opt[i][j + 1]; k++) { int val = f[i + 1][k-1] + cost(i, j); if (val \u0026lt; f[t][k]) f[t][k] = val, opt[i][j] = i; } } } Реализация получилась очень лаконичной: она всего на 3 строчки длиннее, чем базовое решение.\n#АсимптотикаВыясняется, что это работает быстро. Чтобы понять, почему, распишем количество элементов, которые мы просмотрим для каждого состояния, и просуммируем:\n$$ \\sum_{i, j} (opt[i, j+1] - opt[i-1, j] + 1) = nm + \\sum_{ij} (opt[i, j+1] - opt[i-1, j]) $$\nЗаметим, что все элементы, кроме граничных, учитываются в сумме ровно два раза — один раз с плюсом, другой с минусом — а значит их можно сократить. Граничных же элементов $O(n)$, и каждый из них порядка $O(n)$. Значит, итоговая асимптотика составит $O(n \\cdot m + n \\cdot n) = O(n^2)$.\n","id":44,"path":"/cs/layer-optimizations/knuth/","title":"Оптимизация Кнута"},{"content":"Иногда чтобы решить задачу полной персистентности от структуры не нужно, но нужно уметь хоть как-то откатываться до более ранних состояний.\nВ этой статье мы рассмотрим несколько общих подходов и их популярные применения.\n#Корневая эвристика по запросамРассмотрим стандартную задачу: есть массив размера $n$, и требуется выполнять $q$ запросов прибавления на отрезке и суммы на отрезке. (Про существование дерева отрезков и прочих деревьев временно забудем.)\nЕсли бы запросов обновления не было, мы бы решали эту задачу просто массивом префиксных сумм:\nint s[maxn]; // [0, r) s[0] = 0 for (int i = 0; i \u0026lt; n; i++) s[i+1] = s[i] + a[i]; Вернемся к исходной задаче. Разобьём все запросы изменения на блоки размера примерно $\\sqrt q$, и будем честно каждые $\\sqrt q$ запросов строить массив префиксных сумм за $O(n)$ с учетом всех предыдущих запросов.\nТеперь для ответа на каждый запрос суммы мы можем потратить $O(1)$ времени на запрос к префиксным суммам для предыдущего чекпойнта плюс $O(\\sqrt q)$ времени на «корректировку», просто линейно проходясь по всем ещё не учтенным запросам.\nconst int c = 300; // ~= sqrt(q) struct query { int l, r, x; }; vector\u0026lt;query\u0026gt; buffer; // запросы, не учтенные в префиксных суммах int sum(int l, int r) { int res = s[r + 1] - s[l]; for (query q : buffer) // пересечем запрос суммы со всеми запросами res += q.x * max(0, min(r, q.r) - max(l, q.l)); return res; } void rebuild() { vector\u0026lt;int\u0026gt; d(n, 0); // массив дельт for (query q : buffer) { d[q.l] += x; d[q.r + 1] -= x; } buffer.clear(); int delta = 0, running_sum = 0; for (int i = 1; i \u0026lt;= n; i++) { p[i] += running_sum; delta += d[i]; running_sum += delta; } } void upd(int l, int r, int x) { buffer.push_back({l, r, x}); if (buffer.size() == c) // буфер слишком большой; надо пересчитать префиксные суммы и очистить его rebuild(); } Такое решение будет работать за $O(n \\sqrt q + q \\sqrt q)$: каждые $\\sqrt q$ запросов мы перестраиваем префиксные суммы за $O(n)$, и каждый запрос суммы мы просматриваем $O(\\sqrt q)$ предыдущих.\n#Превращение статических структур в динамическиеПредыдущий метод можно применять не только к массиву префиксных сумм, но и к любым статическим структурам данных.\nЗадача. Требуется добавлять точки в выпуклую оболочку и находить касательные (то есть находить точку, которая максимизирует скалярное произведение $a_i x + b_i y$).\nМы можем так же каждые $\\sqrt n$ запросов перестраивать выпуклую оболочку, а при ответе на запрос касательной помимо кандидата из построенной выпуклой оболочки рассмотреть дополнительно $\\sqrt n$ скалярных произведений из точек из буфера.\nЭто будет работать за $O(n \\cdot (\\sqrt n + \\log n) + \\sqrt n \\cdot n) = O(n \\sqrt n)$, если поддерживать отсортированный список точек, чтобы перестраивать оболочку за линейное время.\nЗадача. Требуется в заранее известном порядке добавлять и удалять точки из выпуклой оболочки и находить касательные.\nЭто уже немного сложнее. Разобьём запросы на блоки явно, и будем обрабатывать их по отдельности. На каждом блоке построим выпуклую оболочку только тех точек, которые существуют на всём её блоке, а остальные запросы сохраним в буфер — чтобы найти такие точки, нужно посмотреть на $O(\\sqrt n)$ запросов наперед и отметить удаленные ребра в каком-нибудь массиве или хеш-таблице.\nТогда, при ответе на запрос касательной, помимо кандидата из построенной оболочки будем рассматривать все точки, которые существуют на момент данного запроса — найти все такие можно аналогично за $O(\\sqrt n)$, проанализировав историю текущего блока.\n#Структуры размера степеней двоекЧасто, если удалений в задаче нет или их можно как-то эмулировать, можно применить похожую, но более мощную технику: поддерживать $O(\\log n)$ структур (например, выпуклых оболочек) размеров степеней двоек, причём так, что нет двух структур одинакового размера.\nДля этого нужно при добавлении новой точки\nсоздать для неё новую структуру размера 1, где есть только одна новая точка; затем, если структура размера 1 уже существует, то объединить её с новой и создать структуру размера 2; затем, если структура размера 2 уже существует, то объединить её с новой и создать структуру размера 4 …и так далее, пока мы не найдем какую-то степень двойки, что структуры такого размера нет.\nОбщее число элементов $n$ однозначно задает, структуры какого размера будут в этом объединении: они будут соответствовать единицам в двоичной записи $n$, а значит, базовых структур всегда будет не более $O(\\log n)$.\nВ случае с выпуклыми оболочками, каждую структуру мы можем строить за её размер, если мерджить отсортированные массивы точек. Структуры размера 1 будем строить каждую операцию; структуры размера 2 мы будем строить каждые 2 операции; структуры размера 4 — каждые 4 операции, и так далее. На поддержание такого объединения статических структур мы амортизировано потратим $O(n \\log n)$ операций: по $O(n)$ для каждого размера.\nНа запрос же мы будем отвечать, передавая его во все $O(\\log n)$ базовых структур и объединяя ответы. Для выпуклых оболочек это суммарно займет $O(\\log^2 n)$ времени.\nПомимо выпуклых оболочек и огибающих, такой подход полезен, например, для обновления строковых автоматов и для динамического построения индекса для поиска.\n","id":45,"path":"/cs/decomposition/rollback/","title":"Откатывание состояний"},{"content":"Поиском в глубину (англ. depth-first search, DFS) или эйлеровым обходом называется рекурсивный алгоритм обхода корневого дерева или графа, начинающий в корневой вершине (в случае графа может быть выбрана произвольная вершина) и рекурсивно обходящий весь граф, посещая каждую вершину ровно один раз.\nconst int maxn = 1e5; bool used[maxn]; // тут будем отмечать посещенные вершины void dfs(int v) { used[v] = true; for (int u : g[v]) if (!used[u]) dfs(u); } Немного его модифицируем, а именно будем сохранять для каждой вершины, в какой момент мы в неё вошли и в какой вышли — соответствующие массивы будем называть $tin$ и $tout$.\nКак их заполнить: заведем таймер, отвечающий за «время» на текущем состоянии обхода, и будем инкрементировать его каждый раз, когда заходим в новую вершину:\nint tin[maxn], tout[maxn]; int t = 0; // таймер void dfs(int v) { tin[v] = t++; for (int u : g[v]) if (!used[u]) dfs(u); tout[v] = t; // иногда счетчик тут тоже увеличивают } У этих массивов много полезных свойств:\nВершина $u$ является предком $v$ $\\iff tin_v \\in [tin_u, tout_u)$. Эту проверку можно делать за константу. Два полуинтервала $[tin_v, tout_v)$ и $[tin_u, tout_u)$ либо не пересекаются, либо один вложен в другой. В массиве $tin$ есть все числа от 0 до $(n - 1)$, причём у каждой вершины свой номер. Размер поддерева вершины $v$ (включая саму вершину) равен $(tout_v - tin_v)$. Если ввести нумерацию вершин, соответствующую $tin$-ам, то индексы любого поддерева всегда будут каким-то промежутком в этой нумерации. Как мы дальше увидим, эти свойства очень полезны в других обходах и самых разных задачах на деревья.\n","id":46,"path":"/cs/graph-traversals/dfs/","title":"Поиск в глубину"},{"content":"Поиск в ширину (англ. breadth-first search) — один из основных алгоритмов на графах, позволяющий находить все кратчайшие пути от заданной вершины и решать многие другие задачи.\nПоиск в ширину также называют обходом — так же, как поиск в глубину и все другие обходы, он посещает все вершины графа по одному разу, только в другом порядке: по увеличению расстояния до начальной вершины.\n#Описание алгоритмаНа вход алгоритма подаётся невзвешенный граф и номер стартовой вершины $s$. Граф может быть как ориентированным, так и неориентированным — для алгоритма это не важно.\nОсновную идею алгоритма можно понимать как процесс «поджигания» графа: на нулевом шаге мы поджигаем вершину $s$, а на каждом следующем шаге огонь с каждой уже горящей вершины перекидывается на всех её соседей, в конечном счете поджигая весь граф.\nЕсли моделировать этот процесс, то за каждую итерацию алгоритма будет происходить расширение «кольца огня» в ширину на единицу. Номер шага, на котором вершина $v$ начинает гореть, в точности равен длине её минимального пути из вершины $s$.\nМоделировать это можно следующим образом. Создадим очередь, в которую будут помещаться горящие вершины, а также заведём булевый массив, в котором для каждой вершины будем отмечать, горит она или нет — или иными словами, была ли она уже посещена. Изначально в очередь помещается только вершина $s$, которая сразу помечается горящей.\nЗатем алгоритм представляет собой такой цикл: пока очередь не пуста, достать из её головы одну вершину $v$, просмотреть все рёбра, исходящие из этой вершины, и если какие-то из смежных вершин $u$ ещё не горят, поджечь их и поместить в конец очереди.\nВ итоге, когда очередь опустеет, мы по одному разу обойдём все достижимые из $s$ вершины, причём до каждой дойдём кратчайшим путём. Длины кратчайших путей можно посчитать, если завести для них отдельный массив $d$ и при добавлении в очередь пересчитывать по правилу $d_u = d_v + 1$. Также можно компактно сохранить дополнительную информацию для восстановления самих путей, заведя массив «предков», в котором для каждой вершины хранится номер вершины из которой мы в неё попали.\n#РеализацияЕсли мы всё равно поддерживаем массив расстояний, то отдельный булевый массив с метками горящих вершин можно не создавать, а вместо этого просто присвоить изначальное расстояние всех вершин некоторым некоторым специальным значением (например, -1), которое будет сигнализировать а том, что эту вершину мы ещё не просмотрели.\nvector\u0026lt;int\u0026gt; g[maxn]; void bfs(int s) { queue\u0026lt;int\u0026gt; q; q.push(s); vector\u0026lt;int\u0026gt; d(n, -1), p(n); d[s] = 0; while (!q.empty()) { int v = q.front(); q.pop(); for (int u : g[v]) { if (d[u] == -1) { q.push(u); d[u] = d[v] + 1; p[u] = v; } } } } Теперь, чтобы восстановить кратчайший путь до какой-то вершины $v$, это можно сделать через массив p:\nwhile (v != s) { cout \u0026lt;\u0026lt; v \u0026lt;\u0026lt; endl; v = p[v]; } Обратим внимание, что путь выведется в обратном порядке.\n#В неявных графахПоиск в ширину часто применяется для поиска кратчайшего пути в неявно заданных графах.\nВ качестве конкретного примера, пусть у нас есть булева матрица размера $n \\times n$, в которой помечено, свободна ли клетка с координатами $(x, y)$, и требуется найти кратчайший путь от $(x_s, y_t)$ до $(x_y, y_t)$ при условии, что за один шаг можно перемещаться в свободную соседнюю по вертикали или горизонтали клетку.\nПоиск в ширину можно использовать для нахождения выхода из лабиринта Можно сконвертировать граф в явный формат: как-нибудь пронумеровать все ячейки (например по формуле $x \\cdot n + y$) и для каждой посмотреть на всех её соседей, добавляя ребро в $(x \\pm 1, y \\pm 1)$, если соответствующая клетка свободна.\nТакой подход будет работать за оптимальное линейное время, однако с точки зрения реализации проще адаптировать не входные данные, а сам алгоритм обхода в глубину:\nbool a[N][N]; // свободна ли клетка (x, y) int d[N][N]; // кратчайшее расстояние до (x, y) // чтобы немного упростить код struct cell { int x, y; }; void bfs(cell s, cell t) { queue\u0026lt;cell\u0026gt; q; q.push(s); memset(d, -1, sizeof d); d[s.x][x.y] = 0; while (!q.empty()) { auto [x, y] = q.front(); q.pop(); // просматриваем всех соседей for (auto [dx, dy] : {{-1, 0}, {+1, 0}, {0, -1}, {0, +1}}) { // считаем координаты соседа int _x = x + dx, _y = y + dy; // и проверяем, что он свободен и не был посещен ранее if (a[_x][_y] \u0026amp;\u0026amp; d[_x][_y] == -1) { d[_x][_y] = d[x][y] + 1; q.push_back({_x, _y}); } } } } Перед запуском bfs следует убедиться, что не произойдет выход за пределы границ. Вместо того, чтобы добавлять проверки на _x \u0026lt; 0 || x \u0026gt;= n и т. п. при просмотре возможных соседей, удобно сделать следующий трюк: изначально создать матрицу a с размерностями на 2 больше, чем нужно, и на этапе чтения данных заполнять её в индексации не с нуля, а с единицы. Тогда границы матрицы всегда будут заполнены нулями (то есть помечены непроходимыми) и алгоритм никогда в них не зайдет.\n#Применения и обобщенияПомимо нахождения расстояний в невзвешенном графе, обход в ширину можно использовать для многих задач, в которых работает обход в глубину, например для поиска компонент связности или проверки на двудольность.\nМножественный BFS. Добавив в очередь изначально не одну, а несколько вершин, мы найдем для каждой вершины кратчайшее расстояние до одной из них.\nЭто полезно для задач, в которых нужно моделировать пожар, наводнение, извержение вулкана или подобные явления, в которых источник «волны» не один. Также так можно чуть быстрее находить кратчайший путь для конкретной пары вершин, запустив параллельно два обхода от каждой и остановив в тот момент, когда они встретятся.\nИгры. С помощью BFS можно найти решение какой-либо задачи / игры за наименьшее число ходов, если каждое состояние системы можно представить вершиной графа, а переходы из одного состояния в другое — рёбрами графа.\nВ качестве (нетривиального) примера: собрать кубик Рубика за наименьшее число ходов.\nКратчайшие циклы. Чтобы найти кратчайший цикл в ориентированном невзвешенном графе, можно произвести поиск в ширину из каждой вершины. Как только в процессе обхода мы пытаемся пойти из текущей вершины по какому-то ребру в уже посещённую вершину, то это означает, что мы нашли кратчайший цикл для данной вершины, и останавливаем обход. Среди всех таких найденных циклов (по одному от каждого запуска обхода) выбираем кратчайший.\nТакой алгоритм будет работать за $O(n^2)$: худшим случаем будет один большой цикл, в котором мы для каждой вершины пройдемся по всем остальным.\nРебра на кратчайшем пути. Мы можем за линейное время найти все рёбра, лежащие на каком-либо кратчайшем пути между заданной парой вершин $a$ и $b$. Для этого нужно запустить два поиска в ширину: из $a$ и из $b$.\nОбозначим через $d_a$ и $d_b$ массивы кратчайших расстояний, получившиеся в результате этих обходов. Тогда каждое ребро $(u,v)$ можно проверить критерием\n$$ d_a[u] + d_b[v] + 1 = d_a[b] $$\nАльтернативно, можно запустить один обход из $a$, и когда он дойдет до $b$, начать рекурсивно проходиться по всем обратным ребрам, ведущим в более близкие к $a$ вершины (то есть те, для которых $d[u] = d[v] - 1$), отдельно помечая их.\nАналогично можно найти все вершины на каком-либо кратчайшем пути.\n0-1 BFS. Если веса некоторых ребер могут быть нулевыми, то кратчайшие пути искать не сильно сложнее.\nКлючевое наблюдение: если от вершины $a$ до вершины $b$ можно дойти по пути, состоящему из нулевых рёбер, то кратчайшие расстояния от вершины $s$ до этих вершин совпадают.\nЕсли в нашем графе оставить только $0$-рёбра, то он распадётся на компоненты связности, в каждой из которых ответ одинаковый. Если теперь вернуть единичные рёбра и сказать, что эти рёбра соединяют не вершины, а компоненты связности, то мы сведём задачу к обычному BFS.\nПолучается, запустив обход, мы можем при обработке вершины $v$, у которой есть нулевые ребра в непосещенные вершины, сразу пройтись по ним и добавить все вершины нулевой компоненты, проставив им такое же расстояние, как и у $v$.\nЭто можно сделать и напрямую, запустив BFS внутри BFS, однако можно заметить, что достаточно при посещении вершины просто добавлять всех её непосещенных соседей по нулевым ребрам в голову очереди, чтобы обработать их раньше, чем те, которые там уже есть. Это легко сделать, если очередь заменить деком:\nvector\u0026lt;int\u0026gt; d(n, -1); d[s] = 0; deque\u0026lt;int\u0026gt; q; q.push_back(s); while (!q.empty()) { int v = q.front(); q.pop_front(); for (auto [u, w] : g[v]) { if (d[u] == -1) { d[u] = d[v] + w; if (w == 0) q.push_front(u); else q.push_back(u); } } } Также вместо дека можно завести две очереди: одну для нулевых ребер, а другую для единичных, в внутри цикла while сначала просматривать первую, а только потом, когда она станет пустой, вторую. Этот подход уже можно обобщить.\n1-k BFS. Теперь веса рёбер принимают значения от $1$ до некоторого небольшого $k$, и всё так же требуется найти кратчайшие расстояния от вершины $s$, но уже в плане суммарного веса.\nНаблюдение: максимальное кратчайшее расстояние в графе равно $(n - 1) \\cdot k$.\nЗаведём для каждого расстояния $d$ очередь $q_d$, в которой будут храниться вершины, находящиеся на расстоянии $d$ от $s$ — плюс, возможно, некоторые вершины, до которых мы уже нашли путь длины $d$ от $s$, но для которых возможно существует более короткий путь. Нам потребуется $O((n - 1) \\cdot k)$ очередей.\nПоложим изначально вершину $s$ в $q_0$, а дальше будем брать вершину из наименьшего непустого списка и класть всех её непосещенных соседей в очередь с номером $d_v + w$ и релаксировать $d_u$, не забывая при этом, что кратчайшее расстояние до неё на самом деле может быть и меньше.\nint d[maxn]; d[s] = 0; queue\u0026lt;int\u0026gt; q[maxd]; q[0].push_back(s); for (int dist = 0; dist \u0026lt; maxd; dist++) { while (!q[dist].empty()) { int v = q[dist].front(); q[dist].pop(); // если расстояние меньше и мы уже рассмотрели эту вершину, // но она всё ещё лежит в более верхней очереди if (d[v] \u0026gt; dist) continue; for (auto [u, w] : g[v]) { if (d[u] \u0026lt; d[v] + w) { d[u] = d[v] + w; q[d[u]].push(u); } } } } Сложность такого алгоритма будет $O(k n + m)$, поскольку каждую вершину мы можем прорелаксировать и добавить в другую очередь не более $k$ раз, а просматривать рёбра, исходящие из вершины мы будем только когда обработаем эту вершину в самый первый раз.\nНа самом деле, нам так много списков не нужно. Если каждое ребро имеет вес не более $k$, то в любой момент времени не более $k$ очередей будут непустыми. Мы можем завести только $k$ списков, и вместо добавления в $(d_v + w)$-ый список использовать $(d_v+w) \\bmod k$.\nЗаметим, что алгоритм также работает когда есть общее ограничение на длину пути, а не только на вес каждого ребра. Для более общего случая, когда веса ребер могут быть любыми неотрицательными, есть алгоритм Дейкстры, который мы разберем в следующей статье.\n","id":47,"path":"/cs/shortest-paths/bfs/","title":"Поиск в ширину"},{"content":"Просто для нахождения $\\gcd$ даже не нужно знать, как устроен алгоритм Евклида — он есть в компиляторе.\nРасширенный алгоритм Евклида находит, помимо $g = \\gcd(a, b)$, такие целые коэффициенты $x$ и $y$, что\n$$ a \\cdot x + b \\cdot y = g $$\nЗаметим, что решений бесконечно много: имея решение $(x, y)$, можно $x$ увеличить на $b$, а $y$ уменьшить на $a$, и равенство при этом не изменится.\n#Основная идеяАлгоритм тоже будет рекурсивный. Пусть мы посчитали нужные коэффициенты $x\u0026rsquo;$ и $y\u0026rsquo;$, когда рекурсивно считали $\\gcd(b, a \\bmod b)$. Иными словами, у нас есть решение $(x\u0026rsquo;, y\u0026rsquo;)$ для пары $(b, a \\bmod b)$:\n$$ b \\cdot x\u0026rsquo; + (a \\bmod b) \\cdot y\u0026rsquo; = g $$\nЧтобы получить решение для исходной пары, запишем выражение $(a \\bmod b)$ по определению как $(a - \\lfloor \\frac{a}{b} \\rfloor \\cdot b)$ и подставим в приведенное выше равенство:\n$$ b \\cdot x\u0026rsquo; + (a - \\Big \\lfloor \\frac{a}{b} \\Big \\rfloor \\cdot b) \\cdot y\u0026rsquo; = g $$\nТеперь выполним перегруппировку слагаемых (сгруппируем по исходным $a$ и $b$) и получим:\n$$ a \\cdot \\underbrace{y\u0026rsquo;}_x + b \\cdot \\underbrace{(x\u0026rsquo; - \\Big \\lfloor \\frac{a}{b} \\Big \\rfloor \\cdot y\u0026rsquo;)}_y = g $$\nСравнивая это с исходным выражением, получаем, что для исходных $x$ и $y$ подходят коэффициенты при $a$ и $b$.\n#Реализацияint gcd(int a, int b, int \u0026amp;x, int \u0026amp;y) { if (a == 0) { x = 0; y = 1; return b; } int x1, y1; int d = gcd(b % a, a, x1, y1); x = y1 - (b / a) * x1; y = x1; return d; } Эта рекурсивная функция по прежнему возвращает значение $\\gcd(a, b)$, но помимо этого записывает в переданные по ссылке переменные $x$ и $y$ искомые коэффициенты.\n#ПримененияЭта модификация алгоритма интересна, потому что с помощью неё можно искать обратный элемент по модулю: такой элемент $a^{-1}$, что $a \\cdot a^{1} \\equiv 1$ — что то же самое, что найти решение в целых числах:\n$$ a^{-1} \\cdot a + k \\cdot m = 1 $$\nТакже с помощью расширенного алгоритма Евклида можно решать линейные диофантовы уравнения — находить решения\n$$ a \\cdot x + b \\cdot y = c $$\nв целых числах. Для этого достаточно проверить, что $c$ делится на $g = \\gcd(a, b)$, и если это так, то $x$ и $y$ из алгоритма нужно просто домножить на $\\frac{c}{g}$.\n","id":48,"path":"/cs/modular/extended-euclid/","title":"Расширенный алгоритм Евклида"},{"content":"Помимо очевидных сложения, вычитания и умножения на константу, у векторов можно ввести и свои особенные операции, которые нам упростят жизнь.\n#Скалярное произведениеСкалярное произведение (англ. dot product) двух векторов равно произведению их длин и косинуса угла между ними. Для него справедлива следующая формула:\n$$ a \\cdot b = |a| \\cdot |b| \\cdot \\cos \\theta = x_a x_b + y_a y_b $$\nОна доказывается муторно и чисто технически, так что мы это делать не будем.\nГеометрически, она равна проекции вектора $b$ на вектор $a$, помноженный на длину $а$:\nПолезные свойства:\nСкалярное произведение симметрично ($a \\cdot b = b \\cdot a$). Перпендикулярные вектора должны иметь нулевое скалярное произведение. Если угол острый, то скалярное произведение положительное. Если угол тупой, то скалярное произведение отрицательное. Добавим в нашу реализацию отдельный оператор для него:\nint operator*(r a, r b) { return a.x * b.x + a.y * b.y; } #Векторное произведениеВекторное произведение (англ. cross product, также называется косым или псевдоскалярным) для двух векторов равно произведению их длин и синуса угла между ними — причём знак этого синуса зависит от порядка операндов. Оно тоже удобно выражается в координатах:\n$$ a \\times b = |a| \\cdot |b| \\cdot \\sin \\theta = x_a y_b - y_a x_b $$\nТак же, как и со скалярным произведением, доказательство координатной формулы оставляется упражнением читателю. Если кто-то захочет это сделать: это следует из линейности обоих произведений (что в свою очередь тоже нужно доказать) и разложения по базисным векторам $\\overline{(0, 1)}$ и $\\overline{(1, 0)}$.\nГеометрически, это ориентированная площадь параллелограмма, натянутого на вектора $a$ и $b$:\nЕго свойства:\nВекторное произведение антисимметрично: $a \\times b = - (b \\times a)$. Коллинеарные вектора имеют нулевое векторное произведение. Если $b$ «слева» от $a$, то векторное произведение положительное. Если $b$ «справа» от $a$, то векторное произведение отрицательное. Для него обычно тоже перегружают оператор — либо ^, либо %:\nint operator^(r a, r b) { return a.x*b.y - b.x*a.y; } Примечание. Вообще говоря, формально векторное произведение определяется не так. Оно определено как вектор такой же длины, но перпендикулярный обоим исходным векторам. Это имеет применение в трёхмерной геометрии и физике, но пока в нашем двумерном мире об этом думать не надо.\nСкалярное и векторное произведения тесно связаны с углами между векторами и могут использоваться для подсчета величин вроде ориентированных углов и площадей, которые обычно используются для разных проверок.\nКогда они уже реализованы, использовать произведения гораздо проще, чем опираться на алгебру. Например, можно легко вычислить угол между двумя векторами, подставив в знакомый нам atan2 векторное и скалярное произведение:\ndouble angle(r a, r b) { return atan2(a ^ b, a * b); } В дальнейшем, свойства произведений помогут нам в определении взаимного расположения точек и, например, прямой, поэтому важно эти свойства понять и крепко запомнить.\n","id":49,"path":"/cs/geometry-basic/products/","title":"Скалярное и векторное произведение"},{"content":"Похожим методом является сортировка выбором (минимума или максимума).\nЧтобы отсортировать массив, $n$ раз выберем минимум среди еще неотсортированных чисел и поставим его на свое место (а именно, на $k$-тую позицию после $k$-той итерации). Чтобы упростить реализацию, на $k$-ой итерации будем искать минимум на отрезке $[k, n - 1]$, меняя его местами с текущим $k$-тым элементом, после чего отрезок $[0, k]$ будет отсортирован.\nvoid selection_sort(int *a, int n) { for (int k = 0; k \u0026lt; n - 1; k++) for (int j = k + 1; j \u0026lt; n; j++) if (a[k] \u0026gt; a[j]) swap(a[j], a[k]); } Доказательства корректности и времени работы аналогичны пузырьковой сортировке: после каждой из $O(n)$ итераций мы за время $O(n)$ получаем отсортированный префикс (первые $k$ элементов), а значит за $O(n^2)$ операций отсортируем весь массив целиком.\n","id":50,"path":"/cs/sorting/selection/","title":"Сортировка выбором"},{"content":"Алгоритм нахождения паросочетания далеко не настолько сложный, насколько сложно сводить задачи к нему.\nНачнём с простых примеров.\n#КубикиДано $n$ кубиков, у каждого из них 6 граней, на каждой гране написана какая-то буква. Дано слово $s$, и требуется каждой букве слова $s$ сопоставить уникальный кубик, так чтобы мы могли повернуть этот кубик и получить нужную нам букву.\nПример — даны три кубика, на гранях которых написаны буквы:\n(a, a, a, a, a, a) (а, а, а, а, а, б) (а, а, а, б, б, с) Если задано слово «acb», тогда ответ ровно один: 132.\nРешение. Сделаем двудольный граф, одна доля которого — номера кубиков, а другая — номер буквы в слове $s$. Проведем ребра из номера кубика в номер буквы только если мы можем взять этот кубик на эту позицию в строке, то есть если у него есть грань с соответствующей буквой. Теперь ответ — максимальное паросочетание в этом графе.\nПо определению паросочетания мы не сопоставим ни один кубик нескольким буквам, но так как наше паросочетание — максимально, то мы покроем максимально возможное количество букв.\n#ДоминошкиЕсть прямоугольное поле $n \\times m$, которое содержит какие-то выколотые клетки. Надо положить на это поле как можно больше костей домино (прямоугольников размера $1 \\times 2$), но с условием, что поверх выколотых полей ничего лежать не должно.\nСоставим граф, в котором вершины будут полями, а рёбрами будут возможности положить доминошку на два соседних свободных поля. Заметим, что такой граф будет двудольным: можно его покрасить как шахматную доску, и тогда черные клетки будут вершинами первой доли, а белые — второй.\nОтвет — максимальное паросочетание в таком графе. Асимптотика с алгоритмом Куна $O(n^2 m^2)$, потому что у нас будет $O(nm)$ вершин и рёбер.\n#Покрытие путями DAGРазберем более сложную задачу, до решения которой самостоятельно додуматься сложно.\nДан ориентированный ациклический граф $G$ (англ. directed acyclic graph, DAG). Требуется покрыть его наименьшим числом путей, то есть найти наименьшее множество простых путей, где каждая вершина принадлежит ровно одному пути.\nПостроим соответствующие изначальному графу $G$ два двудольных графа $H$ и $\\overline{H}$ следующим образом:\nВ каждой доле графа $H$ будет по $n$ вершин. Обозначим их через $a_i$ и $b_i$ соответственно. Для каждого ребра $(i, j)$ исходного графа $G$ проведём соответствующее ребро $(a_i, b_j)$ в графе $H$. Теперь из графа $H$ сделаем граф $\\overline{H}$, добавив обратное ребро $(b_i, a_i)$ для каждого $i$. Если мы рассмотрим любой путь $v_1, v_2, \\ldots, v_k$ в исходном графе $G$, то в графе $\\overline{H}$ ему будет соответствовать путь $a_{v_1}, b_{v_2}, a_{v_2}, b_{v_3}, \\ldots, a_{v_{k-1}}, b_{v_k}$. Обратное тоже верно: любой путь, начинающийся в левой доле $\\overline{H}$ и заканчивающийся в правой будет соответствовать какому-то пути в $G$.\nИтак, есть взаимно однозначное соответствие между путями в $G$ и путями $\\overline{H}$, идущими из левой доли в правую. Заметим, что любой такой путь в $\\overline{H}$ — это паросочетание в $H$ (напомним, это $\\overline{H}$ без обратных рёбер). Получается, любому пути из $G$ можно поставить в соответствие паросочетание в $H$, и наоборот. Более того, непересекающимся путям в $G$ соответствуют непересекающиеся паросочетания в $H$.\nЗаметим, что если есть $p$ непересекающихся путей, покрывающих все $n$ вершин графа, то они вместе содержат $r = n - p$ рёбер. Отсюда получаем, что чтобы минимизировать число путей $p$, мы должны максимизировать число рёбер $r$ в них.\nМы теперь можем свести задачу к нахождению максимального паросочетания в двудольном графе $H$. После нахождения этого паросочетания мы должны преобразовать его в набор путей в $G$. Это делается тривиальным алгоритмом: возьмем $a_1$, посмотрим, с какой $b_k$ она соединена, посмотрим на $a_k$ и так далее. Некоторые вершины могут остаться ненасыщенными — в таком случае в ответ надо добавить пути нулевой длины из каждой из этих вершин.\n#Минимальное вершинное покрытиеЗадача. Назовем вершинным покрытием графа такое множество вершин, что каждое ребро графа инцидентно хотя бы одной вершине из множества. Необходимо найти вершинное покрытие наименьшего размера.\nВ общем случае это NP-полная задача, но для двудольных графов она имеет достаточно простое решение.\nОбозначим за $V_{min}$ наименьшее вершинное покрытие, а за $M$ — максимальное паросочетание в графе. Тогда сразу заметим, что $|V_{min}| \\ge |M|$, потому что $M$ — множество независимых ребер. Теперь приведем алгоритм, который строит вершинное покрытие размера ровно $|M|$. Очевидно, оно будет минимальным.\nАлгоритм. Мысленно ориентируем ребра графа: ребра из $M$ проведем из правой доли в левую, остальные — из левой в правую, после чего запустим обход в глубину из всех вершин левой доли, не включенных в $M$.\nЗаметим, что граф разбился на несколько множеств: $L^+, L^-, R^+, R^-$, где «плюсовые» множества — это множества посещенных в процессе обхода вершин. В графе такого вида не бывает ребер $L^+ \\rightarrow R^-$ и $L^- \\leftarrow R^+$ по очевидным соображениям. Ребер $L^+ \\leftarrow R^-$ не бывает, потому что в противном случае паросочетание $M$ не максимальное — его можно дополнить ребрами такого типа.\n$$ L^- \\cup R^+ = V_{min} $$\nПонятно, что данное множество покроет все ребра. Осталось выяснить, почему $L^- \\cup R^+$. Это верно потому, что $L^- \\cup R^+$ покрывает все ребра $M$ ровно один раз (ведь ребра $L^- \\rightarrow R^+$ не принадлежат $M$), а также потому, что в нем нет вершин, не принадлежащих $M$ (для $L^-$ это справедливо по определению, для $R^+$ можно провести доказательство от противного с использованием чередующихся цепей).\nУпражнение. Подумайте, как это можно применить к решению задачи о нахождении максимального независимого множества.\n#Паросочетание минимального весаПусть у вершин левой доли есть какие-то веса, и нам нужно набрать максимальное паросочетание минимального веса.\nВыясняется, что можно просто отсортировать вершины левой доли по весу и пытаться в таком порядке добавлять их в паросочетание стандартным алгоритмом Куна. Для доказательства этого факта читатель может прочитать про жадный алгоритм Радо-Эдмондса, частным случаем которого является такая модификация алгоритма Куна.\nАналогичную задачу, но когда у ребер есть веса, проще всего решать сведением к нахождению потока минимальной стоимости.\n","id":51,"path":"/cs/matching/matching-problems/","title":"Задачи на паросочетания"},{"content":"Многочленами или полиномами (англ. polynomial) называются конечные суммы вида\n$$ P(x_1, x_2, \\ldots, x_n) = \\sum_{I={i_1,i_2,\\ldots,i_n}} c_I \\cdot x_1^{i_1} \\cdot x_2^{i_2} \\cdot \\ldots \\cdot x_n^{i_n} $$\nВ частности, многочлены от одной переменной это суммы вида\n$$ P(x) = c_0 + c_1 x^1 + \\ldots + c_n x^n $$\nГоворят, что степень многочлена равна $n$, если его наибольший ненулевой коэффициент стоит у $x^n$.\nМногочлены, подобно скалярам, можно складывать, вычитать и умножать, получая другие многочлены. Многочлены также можно делить, но результат не всегда получается многочленом. В этом смысле множество многочленов является кольцом, подобно кольцу остатков по модулю.\n#Представление многочленов и чиселПрограммно представлять многочлен проще всего через vector или обычный статический массив, содержащий его коэффициенты по порядку. Помимо непосредственно многочленов, удобно представлять в таком виде и длинные числа, притворившись, что $x$ равно $10$ или основанию любой другой системы счисления:\n$$ \\begin{aligned} A(x) \u0026amp;= a_0 + a_1\\cdot x + a_2 \\cdot x^2 + \\dots + a_n \\cdot x^n \\ \u0026amp;= a_0 + a_1\\cdot 10 + a_2 \\cdot 10^2 + \\dots + a_n \\cdot 10^n \\end{aligned} $$\nПри операциях с длинными числами (например, при умножении), можно проводить соответствующую операцию с многочленами, а затем производить каррирование результата: проходить от нижних разрядов получившегося многочлена к верхним и «сдвигать» переполнившиеся разряды:\nconst int base = 10; vector\u0026lt;int\u0026gt; normalize(vector\u0026lt;int\u0026gt; a) { int carry = 0; for (int \u0026amp;x : a) { x += carry; carry = x / base; x %= base; } while (carry \u0026gt; 0) { a.push_back(carry % base); carry /= base; } return a; } vector\u0026lt;int\u0026gt; multiply(vector\u0026lt;int\u0026gt; a, vector\u0026lt;int\u0026gt; b) { return normalize(poly_multiply(a, b)); } Из соображений производительности следует выбирать настолько большое основание, насколько вмещается в используемый тип данных (например, $10^9$ или $2^{30}$ для int).\n#Коэффициенты разложенийКоэффициенты при $a^x b^y$, получаемые при возведении бинома $(a+b)$ в $n$-ную степень, называются биномиальными коэффициентами:\n$$ (a + b)^n = \\sum_{k=0}^n C_n^k \\cdot a^k \\cdot b^{n-k} $$\nИх удобно считать по следующей формуле:\n$$ C_n^k = \\binom{n}{k} = \\frac{n!}{(n - k)! k!} $$\nВ более общем случае определяют полиномиальный коэффициент, равный количеству раз, которое элемент $a_1^{x_1} a_2^{x_2} \\ldots a_k^{x_k}$ появится при раскрытии скобки $(a_1+a_2+\\ldots+a_k)^n$:\n$$ P(x_1, x_2, \\ldots, x_k) = \\frac{n!}{\\prod (x_i!)} $$\nБиномиальные коэффициенты применяются в комбинаторике и в отрыве от многочленов. В задачах по программированию их подсчет часто требуется проводить по модулю, для чего нужно уметь искать обратные к факториалам.\n#Умножение многочленовПри умножении двух многочленов степени $n$ и $m$ получается многочлен степени $(n+m)$. Прямая формула для произведения многочленов имеет вид\n$$ \\left(\\sum_{i=0}^n a_i x^i\\right)\\cdot\\left(\\sum_{j=0}^m b_j x^j\\right)=\\sum_{k=0}^{n+m}x^k\\sum_{i+j=k}a_i b_j $$\nЕё наивный подсчёт требует $O(n^2)$ операций, но далее в этой главе мы разберем несколько более эффективных алгоритмов — самый быстрый из которых работает всего за $O(n \\log n)$.\nЭтот факт позволяет сводить много комбинаторных задач к произведению многочленов и использованию уже известных алгоритмов для его подсчета. Разберем несколько примеров таких задач.\n#2-рюкзак Даны два массива $a$ и $b$ размера $n$ и $m$. Требуется найти число различных возможных сумм $(a_i + b_j)$.\n$n, m, a_i, b_i \\le 10^5$.\nРассмотрим многочлены $A(x)$ и $B(x)$, в которых коэффициент при $k$-той степени равен числу равных $k$ элементов в соответствующем массиве.\nРассмотрим произведение $C = A \\cdot B$. В получившемся многочлене коэффициент $c_t$ при $x^t$ будет равен\n$$ c_t \\cdot x^t = \\sum_{p+q=t} a_p \\cdot b_q \\cdot x^{p+q} $$\nчто в свою очередь равно количеству способов набрать сумму ровно $t$.\nЗначит, мы можем перемножить эти два многочлена за $O(n \\log n)$ и просто подсчитать число ненулевых коэффициентов результата.\n#Мульти-рюкзакЗадача «Вор в магазине» является небольшой модификацией предыдущей:\nИмеется $n$ типов предметов различных целых стоимостей $a_i$. Требуется найти количество различный сумм стоимостей наборов из ровно $k$ предметов (возможно, с повторениями).\n$n, k, a_i \\le 1000$\nОпять же, рассмотрим многочлен, в котором коэффициент при $i$-той степени равен единице, если существует предмет со стоимостью $i$, и нулю в противном случае.\nЕсли $k=2$, наша задача свелась к предыдущей: нужно домножить многочлен на самого себя и посмотреть на число ненулевых коэффициентов. В общем же случае нам нужно возвести многочлен в степень $k$ и также посчитать ненулевые коэффициенты результата.\nЕсли возводить многочлен в $k$-ную степень наивно, то асимптотика такого решения будет $O(n k^2 \\log (nk))$: нужно $O(k)$ раз перемножать два многочлена, больший из которых имеет длину $O(nk)$.\nВоспользуемся бинарным возведением в степень: умножение многочленов ведь ассоциативно. В данном случае асимптотика будет не более $O(nk \\log (nk) \\log k)$: нужно $O(\\log k)$ раз умножать два многочлена порядка $O(nk)$. Но на самом деле, так как на каждой итерации размер многочлена будет увеличиваться в два раза, в асимптотике учтется только последнее (самое большое) умножение, и поэтому в действительности время работы составит $O(nk \\log (nk))$.\n#СвёрткиСвёрткой (англ. convolution) называется операция применения некоторой «оконной» функции ко всем отрезкам фиксированной длины исходной функции.\nСвёртка «площадь функции на единичном отрезке» В дискретном случае свертке соответствует сумме вида\n$$ (f * g)(x)= f(1) \\cdot g(x-1) + f(2) \\cdot g(x-2) + \\dots + f(k) \\cdot g(x - k) $$\nВ ещё более узком смысле, свертка это результат перемножения многочленов:\n$$ (A \\cdot B)k = a_0 \\cdot b_k + a_1 \\cdot b{k-1} + \\ldots + a_k \\cdot b_0\n$$\nто есть $k$-тый коэффициент результата равен применению какой-то оконной функции, заданной коэффициентами $B(x)$, к коэффициентам $A(x)$. Значит, подобные функции можно быстро считать через матричное умножение.\nНапример, так можно (неэффективно) искать битовую подстроку $t$ в строке $s$: запишем символы $s$ как коэффициенты многочлена $A(x)$ и символы $t$ в обратном порядке как коэффициенты многочлена $B(x)$ и перемножим. В позициях многочлена-результата, где коэффициенты равны $|t|$, строка $t$ входит в $s$.\nТакже с помощью этого трюка можно решать и другие задачи, например выполнять «fuzzy searching»: коэффициенты, равные $(|t|-d)$, соответствуют вхождениям с ровно $d$ ошибками.\n","id":52,"path":"/cs/algebra/polynomials/","title":"Многочлены"},{"content":"В широком смысле, структура данных — это набор связанных ссылками узлов, в которых хранятся данные.\nОбычный массив тоже попадает под это определение: в нём узел один, в котором хранятся $n$ ячеек.\nРассмотрим немного более узкий класс структур, в котором каждый узел хранит $O(1)$ полей и ссылок. Например, дерево отрезков и декартово дерево, которые мы рассмотрим далее, удовлетворяют этому свойству.\nДля таких структур существует простой и общий способ сделать их полностью персистентными — метод копирования путей.\nРассмотрим в качестве примера сбалансированные деревья. Пусть необходимо сделать какое-то обновление — например, добавить очередной элемент — но при этом нужно не потерять никакую информацию про старое дерево.\nВозьмем вершину, к которой нужно добавить нового ребенка. Вместо того чтобы добавлять нового ребенка напрямую в её, скопируем весь путь от корня до этой вершины вместе со всеми данными и указателями, а затем добавим ребенка к соответствующей вершине в скопированном пути. Все остальные вершины, из которых измененный узел не достижим, мы не трогаем.\nВ результате такой операции из нового корня достижимы все вершины обновленного дерева, а из старого корня и всех ранее существовавших вершин достижимы ровно те же неизмененные вершины, что и раньше — мы ведь ничего не удаляли и не меняли, а всего лишь создали порядка логарифма новых вершин.\nЕсли после каждой операции складывать корни дерева в какой-нибудь отдельный массив, то мы получаем доступ к произвольным предыдущим версиям, от которых можно свободно «форкаться».\n#АсимптотикаВ большинстве операций нам и так нужно делать логарифм работы, так что асимптотика по времени не изменится, хотя расход памяти увеличивается до $O(n \\log n)$.\nНа практике же персистентные структуры могут быть значительно (в 2-5 раз) медленнее, потому что данные перестают переиспользоваться, из-за чего почти исчезает выгода от кэшей процессора.\nТакже в персистентных структурах в худшем случае не работает амортизация: если есть какая-то тяжелая операция (например «сжатие бамбука»), то можно много раз откатываться до её и повторять. Поэтому структуры вроде системы непересекающихся множеств и splay-дерева будут иметь худшую асимптотику.\n#Персистентный стекРеализацию и применение персистентных деревьев рассмотрим в следующих статьях, а пока в учебных целях применим метод к стеку.\nПомимо массива с указателем на последний элементы, стек также тоже можно реализовать на ссылках. В этом случае «корень» стека — это его верхний элемент.\nstruct Node { int val; Node *prev; }; Node *head; void add(int x) { head = new Node(x, head); } int top() { return head.val; } void pop() { Node *old = head; head = head-\u0026gt;prev; // если на память пофиг, можно оставить только эту строчку delete old; } Теперь, чтобы сделать стек персистентным, дополнительные поля нам не нужны, но понадобится добавить глобальный массив версий и дополнительный параметр во все методы:\nvector\u0026lt;Node*\u0026gt; versions; void add(int x, int v); int top(int v); void pop(int v); Чтобы сделать ссылочную структуру персистентной, нужно изменить только те операции, которые как-то меняют её состояние — в нашем случае это add и pop.\nДля add нужно воспользоваться такой же процедурой, но создать новую вершину вместо перезаписи head:\nvoid add(int x, int v) { versions.push_back(new Node(x, versions[v])); } Для pop можно поступить так же, но можно и схитрить, просто добавив в конец списка версий ссылку на уже существующий узел:\nvoid pop(int v) { versions.push_back(versions[v]-\u0026gt;prev); } Обе операции работают за $O(1)$ времени и дополнительной памяти.\nИспользуя персистентный стек, можно легко реализовать персистентную очередь (через два персистентных стека), однако за счёт амортизации она будет работать за $O(n)$ на операцию в худшем случае. Также есть сложные способы реализовать очередь на пяти или шести стеках с чистым $O(1)$ времени на операцию.\n","id":53,"path":"/cs/persistent/path-copying/","title":"Метод копирования пути"},{"content":"Стресс-тестирование — это метод поиска ошибок в решении, заключающийся в генерации случайных тестов и сравнивании результатов работы двух решений:\nПравильное, но медленное. Быстрое, но неправильное. Он особенно полезен на соревнованиях формата IOI — когда есть много времени и/или когда уже написано решение на маленькие подгруппы.\nБолее подробно:\nЕсть решение smart — быстрое, но в котором есть баг, который мы хотим найти. Пишем решение stupid — медленное, но точно корректное. Пишем генератор gen — печатает какой-то корректный тест, сгенерированный случайно. Кормим всё в скрипт checker, который $n$ раз генерирует тест, даёт его на ввод stupid и smart, сравнивает выводы и останавливается, когда они отличаются. В некоторых случаях общая схема может немного отличаться в зависимости от типа задачи — мы поговорим об этом в конце статьи.\n#Конкретный примерЗадача. Есть массив чисел $1 \\le a_1 \u0026hellip; a_n \\le 10^9$. Найдите значение минимального элемента.\nПриведем код решения stupid, который будем использовать в качестве эталонного:\nint a[maxn]; void stupid() { int n; cin \u0026gt;\u0026gt; n; for (int i = 0; i \u0026lt; n; i++) cin \u0026gt;\u0026gt; a[i]; int ans = 1e9; for (int i = 0; i \u0026lt; n; i++) ans = min(ans, a[i]); cout \u0026lt;\u0026lt; ans; } Пусть у нас есть решение smart, которое содержит ошибку в границах цикла:\nint a[maxn]; void smart() { int n; cin \u0026gt;\u0026gt; n; for (int i = 0; i \u0026lt; n; i++) cin \u0026gt;\u0026gt; a[i]; int ans = 1e9; for (int i = 1; i \u0026lt; n; i++) ans = min(ans, a[i]); cout \u0026lt;\u0026lt; ans; } Даже в таком примере можно долго искать ошибку, если подбирать случайные тесты руками и проверять ответ на правильность, поэтому мы хотим найти тест, на котором два решения будут давать разный ответ, чтобы впоследствии найти ошибку в smart.\n#Inline-тестированиеПримечание. Автор не рекомендует так делать, но многим такой подход кажется проще для понимания.\nСамый простой подход заключается в том, чтобы реализовать всю логику стресс-тестирования в одном исходном файле, поместив генератор тестов и оба решения в отдельные функции, которые много раз вызываются в цикле в main.\nГенератор должен куда-то записать один случайный тест. Из самых простых вариантов:\nв свое возвращаемое значение; в переменные, переданные по ссылке; в глобальные переменные. Дальше этот тест по очереди передается функциям-решениям, которые аналогичным образом как-то передают результаты своей работы, которые потом сравниваются, и если ответы не совпали, то мы можем вывести тест и завершиться.\nint a[maxn]; int n; int stupid() { int n; cin \u0026gt;\u0026gt; n; int ans = 1e9; for (int i = 0; i \u0026lt; n; i++) ans = min(ans, a[i]); return ans; } int smart() { int n; cin \u0026gt;\u0026gt; n; int ans = 1e9; for (int i = 1; i \u0026lt; n; i++) ans = min(ans, a[i]); return ans; } void gen() { n = rand() % 10 + 1; for (int i = 0; i \u0026lt; n; i++) a[i] = rand(); } int main() { for (int i = 0; i \u0026lt; 100; i++) { gen(); if (smart() != stupid()) { cout \u0026lt;\u0026lt; \u0026#34;WA\u0026#34; \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; n \u0026lt;\u0026lt; endl; for (int j = 0; j \u0026lt; n; j++) cout \u0026lt;\u0026lt; a[j] \u0026lt;\u0026lt; \u0026#39; \u0026#39;; break; } cout \u0026lt;\u0026lt; \u0026#34;OK\u0026#34; \u0026lt;\u0026lt; endl; } return 0; } Этот подход универсален, но у него есть много недостатков:\nНужно дублировать много кода для тестирования разных задач. Нельзя написать генератор или эталонное решение на другом языке (часто бывает проще и быстрее написать их на каком-нибудь скриптовом языке, вроде Python). Исходный код становится более раздутым, и в нем сложнее ориентироваться. Нужно быть аккуратным в плане использования глобальных переменных. Нужно как-то переключаться между режимами «стресс-тестирование» и обычного «ввод из консоли». Можно вынести всю эту логику в другую программу, а само решение не трогать.\n#Тестирование внешним скриптомСуть в следующем:\nВсе решения и генераторы помещаются в отдельные файлы — которые теперь не обязательно исполняются в одной среде. Передача тестов происходит через перенаправление потоков ввода-вывода. Ввод в программах считывается так, как бы он считывался естественно в тестирующей системе. Запускается внешний скрипт, который $n$ раз запускает генератор, записывает его вывод в файл, а затем кормит этот файл двум решениям и построчно сравнивает выводы. Файлы stupid.cpp, smart.cpp и gen.py содержат уже понятный нам код. Вот примерный код скрипта checker.py:\nimport os, sys _, f1, f2, gen, iters = sys.argv # первый аргумент это название программы, \u0026#34;checker.py\u0026#34;, # поэтому \u0026#34;забудем\u0026#34; его с помощью \u0026#34;_\u0026#34; for i in range(int(iters)): print(\u0026#39;Test\u0026#39;, i + 1) os.system(f\u0026#39;python3 {gen} \u0026gt; test.txt\u0026#39;) v1 = os.popen(f\u0026#39;./{f1} \u0026lt; test.txt\u0026#39;).read() v2 = os.popen(f\u0026#39;./{f2} \u0026lt; test.txt\u0026#39;).read() if v1 != v2: print(\u0026#34;Failed test:\u0026#34;) print(open(\u0026#34;test.txt\u0026#34;).read()) print(f\u0026#39;Output of {f1}:\u0026#39;) print(v1) print(f\u0026#39;Output of {f2}:\u0026#39;) print(v2) break Автор обычно запускает его командой python3 checker.py stupid smart gen.py 100, предварительно скомпилировав stupid и smart в ту же директорию, что и сам checker.py. При желании можно также прописать компиляцию прямо внутри скрипта.\nСкрипт написан под Linux / Mac. Для Windows нужно убрать ./ во внешних командах и вместо python3 писать python.\nНе забывайте, что если хотя бы одна из программ не выводит перевод строки в конце файла, то чекер посчитает, что вывод разный.\n#ВариацииИногда вы даже не можете написать stupid — в задачах на геометрию, например — но вы можете написать несколько разных решений и протестировать их друг против друга, в надежде на то, что множество их багов не сильно пересекается. Если выводы отличаются, то это гарантирует, что хотя бы одно из них неправильное. Также можно в качестве stupid взять чьё-нибудь ещё решение, которое также получает WA, и хотя бы в одном из них найдется баг.\nЕсли задача подразумевает неоднозначный вывод (к примеру, вывести индекс минимума — таких уже может быть несколько), то вместо stupid-решения и v1 != v2 следует использовать сторонний скрипт-чекер, который считывает тест и вывод решения и проверяет его, выводя yes / no.\nИнтерактивные задачи можно тестировать, написав интерактор, вывод из которого перенаправляется в решение и наоборот. Под Linux это делается так:\nmkfifo fifo ./solution \u0026lt; fifo | ./interactor \u0026gt; fifo Однако так нельзя сразу получить протокол взаимодействия — для этого интерактору нужно всю полезную информацию записывать в какой-нибудь отдельный файл.\nЕсть много чего ещё, что может быть полезным:\nполноценная поддержка интерактивных задач, многопоточное тестирование, поддержка лимитов по памяти и времени, автопрогон ручных тестов, детекция изменения исходного кода и автоматическое перетестирование, парсинг тестов из тестирующих систем, цветной diff вывода и прочие кастомные выводы для чекеров. Автор некоторое время назад написал более продвинутую программу для тестирования решений, в которой это всё есть, но забросил её. Если кто-нибудь захочет продолжить её разрабатывать или начать писать свою, дайте знать.\n","id":54,"path":"/cs/programming/stress-test/","title":"Стресс-тестирование"},{"content":"Часто в олимпиадных задачах требуется посчитать какие-то большие комбинаторные величины по простому модулю (чаще всего $10^9 + 7$). Это делают для того, чтобы участникам не приходилось использовать длинную арифметику, и они могли сосредоточиться на самой задаче.\nОбычные арифметические операции по модулю выполняются не сильно сложнее — просто нужно брать модули и заботиться о переполнении. Например:\nc = (a + b) % mod; c = (a - b + mod) % mod; c = a * b % mod; Но вот с делением возникают проблемы — мы не можем просто взять и поделить.\nНапример, $\\frac{8}{2} = 4$, но\n$$ \\frac{8 \\bmod 5}{2 \\bmod 5} = \\frac{3}{2} \\neq 4 $$\nНужно найти некоторый элемент, который будет себя вести как $\\frac{1}{a} = a^{-1}$, и вместо «деления» домножать на него. Такой элемент называется обратным по модулю $m$. Для $a = 0$ обратный по модулю элемент не определён, как и при обычном делении.\n#Через бинарное возведение в степеньМалая теорема Ферма говорит, что для любого простого числа $p$ и любого целого числа $a$,\n$$ a^p \\equiv a \\pmod p $$\nТеперь два раза «поделим» этот известный результат на $a$:\n$$ a^p \\equiv a \\implies a^{p-1} \\equiv 1 \\implies a^{p-2} \\equiv a^{-1} $$\nПолучается, что $a^{p-2}$ ведет себя как $a^{-1}$ относительно умножения по модулю, что нам и нужно.\nПосчитать $a^{p-2}$ можно за $O(\\log p)$ бинарным возведением в степень.\nconst int mod = 1e9 + 7; // бинарное возведение в степень по модулю int binpow(int a, int n) { int res = 1; while (n != 0) { if (n \u0026amp; 1) res = res * a % mod; a = a * a % mod; n \u0026gt;\u0026gt;= 1; } return res; } // находит обратный элемент как a^(p-2) int inv(int x) { return binpow(x, mod - 2); } Этот подход простой и быстрый, однако следует помнить, что он работает только для простых модулей.\nВ случае составных модулей, по теореме Эйлера, число $a$ нужно возводить в степень $(\\phi(m)-1)$, для чего нужно искать факторизацию.\n#Через расширенный алгоритм ЕвклидаРасширенный алгоритм Евклида можно использовать для решения в целых числах уравнений вида\n$$ Ax + By = 1 $$\nПодставим в качестве $A$ и $B$ соответственно $a$ и $m$:\n$$ ax + my = 1 $$\nОдним из решений уравнения и будет $a^{-1}$, потому что если взять уравнение по модулю $m$, то получим\n$$ ax + my = 1 \\iff ax \\equiv 1 \\iff x \\equiv a^{-1} \\pmod m $$\nПреимущества этого метода над возведением в степень:\nЕсли обратное существует, то оно найдется даже если модуль не простой. Алгоритм проще выполнять руками. Алгоритм чуть быстрее, если его соптимизировать Но лично автор почти всегда использует возведение в степень.\n#Упрощенная реализацияСначала приведем реализацию, а потом поймем, почему она работает:\nint inv(int a, int m) { if (a == 1) return 1; return (1 - 1ll * inv(m % a, a) * m) / a + m; } Докажем по индукции, что функция действительно возвращает обратный элемент.\nБазовый случай очевиден: $1 \\cdot 1 \\equiv 1$.\nВо втором случае проверим правильность формулы:\n$(1 - f(m \\bmod a, a) \\cdot m)$ делится на $a$, так как $f(m \\bmod a, a) \\equiv m^{-1} \\pmod a$. $\\frac{f(m \\bmod a, a) \\cdot m}{a}$ делится на $m$, так что итоговое выражение сравнимо с $\\frac{1}{a} = a^{-1}$ по модулю $m$. Почему ответ будет получаться в диапазоне от $0$ до $(m - 1)$, мы оставим читателю в качестве упражнения.\n#Предподсчет обратных элементовЧаще всего нам нужно искать обратный элемент в контексте комбинаторики.\nНапример, особенно часто нужно считать биномиальные коэффициенты, для чего в свою очередь нужно уметь обращать факториалы:\n$$ C_n^k = \\frac{n!}{(n-k)! k!} $$\nПростой способ — это предпосчитать обычные факториалы и каждый раз вызывать inv один или два раза:\nint t[maxn]; // факториалы, можно предподсчитать простым циклом int c(int n, int k) { return t[n] * inv(t[k]) % mod * inv(t[n - k]) % mod; } // или, почти в два раза быстрее: int c(int n, int k) { return t[n] * inv(t[k] * t[n - k] % mod) % mod; } Однако это добавит лишний логарифм в асимптотику в нередком случае, когда какая-то комбинаторная формула лежит внутри горячего цикла. Поэтому имеет смысл предподсчитать и частые обратные элементы.\n#Обратные факториалыЕсли у нас уже написан inv, то нам не жалко потратить лишние $O(\\log m)$ операций, посчитав $(a!)^{-1}$.\nПосле этого обратный к $(a-1)!$ можно посчитать за $O(1)$ по формуле:\n#$$ (a-1)!^{-1}(a!)^{-1} \\cdot a \\equiv \\frac{1}{1 \\cdot 2 \\cdot \\ldots \\cdot (a-1)} \\pmod p $$\nВсе остальные обратные факториалы можно таким же образом итеративно подсчитать из предыдущего.\n// обычные факториалы: int f[maxn]; f[0] = 1; for (int i = 1; i \u0026lt; maxn; i++) f[i] = i * f[i - 1] % mod; // обратные факториалы: int r[maxn]; r[maxn - 1] = inv(f[maxn - 1]) for (int i = maxn - 1; i \u0026gt;= 1; i--) r[i-1] = r[i] * i % mod; Также существует метод нахождения обратных для всех чисел от $1$ до $(p - 1)$, но так как обычно модули большие, он не часто применим.\n#Почему $10^9+7$?Несколько причин:\nЭто выражение довольно легко вбивать (1e9+7). Простое число. Достаточно большое. int не переполняется при сложении. long long не переполняется при умножении. Кстати, $10^9 + 9$ обладает всеми теми же свойствами. Иногда используют и его.\nИногда можно встретить $998244353$. Оно обладает всеми свойствами кроме первого, но зато имеет применение в одном из вариантов быстрого преобразования Фурье. Его иногда добавляют даже в задачи, которые к нему не относятся, чтобы не раскрывать участникам тему.\n","id":55,"path":"/cs/modular/reciprocal/","title":"«Деление» по модулю"},{"content":"Эта статья — одна из серии. Рекомендуется сначала прочитать все предыдущие.\nВозьмём исходную формулу для $f$ и раскроем скобки в cost:\n$$ \\begin{aligned} f[i, j] \u0026amp;= \\min_{k \u0026lt; i} { f[k, j-1] + (x_{i-1}-x_k)^2 } \\ \u0026amp;= \\min_{k \u0026lt; i} { f[k, j-1] + x_{i-1}^2 - 2x_{i-1} x_k + x_k^2 } \\end{aligned} $$\nЗаметим, что $x_{i-1}^2$ не зависит от $k$, значит его можно вынести. Под минимумом тогда останется только\n$$ \\underbrace{(f[k, j-1] + x_k^2)}{a_k} + \\underbrace{(-2 x_k)}{b_k} \\cdot x_{i-1} $$\nВыполнив перегруппировку, получаем, что исходное выражение можно переписать как\n$$ f[i, j] = \\min_k { (a_k, b_k) \\cdot (1, x_{i-1}) } $$\nгде под «$\\cdot$» имеется в виду скалярное произведение.\n#АлгоритмПусть мы хотим найти оптимальное $k$ для $f[i, j]$. Представим все уже посчитанные релевантные динамики с предыдущего слоя как точки $(a_k, b_k)$ на плоскости.\nЧтобы эффективно находить среди них точку с минимальным скалярным произведением, можно поддерживать их нижнюю огибающую — вектор $(1, x_{i-1})$ «смотрит» всегда вверх, поэтому нам интересна только она — и бинпоиском по ней находить оптимальную точку.\nХранить нижнюю огибающую можно просто в стеке. Так как добавляемые точки отсортированы по $x$, её построение будет занимать линейное время, а асимптотика всего алгоритма будет упираться в асимптотику бинарного поиска, то есть будет равна $O(n m \\log n)$\nstruct line { int k, b; line() {} line(int a, int _b) { k = a, b = _b; } int get(int x) { return k * x + b; } }; vector\u0026lt;line\u0026gt; lines; // храним прямые нижней огибающей vector\u0026lt;int\u0026gt; dots; // храним x-координаты точек нижней огибающей // ^ первое правило вещественных чисел // считаем, что в dots лежит округленная вниз x-координата int cross(line a, line b) { // считаем точку пересечения // считаем a.k \u0026gt; b.k int x = (b.b - a.b) / (a.k - b.k); if (b.b \u0026lt; a.b) x--; // боремся с округлением у отрицательных чисел return x; } void add(line cur) { while (lines.size() \u0026amp;\u0026amp; lines.back().get(dots.back()) \u0026gt; cur.get(dots.back())) { lines.pop_back(); dots.pop_back(); } if (lines.empty()) dots.push_back(-inf); else dots.push_back(cross(lines.back(), cur)); lines.push_back(cur); } int get(int x) { int pos = lower_bound(dots.begin(), dots.end(), x) - dots.begin() - 1; return lines[pos].get(x); } В случае нашей конкретной задачи, алгоритм можно и дальше соптимизировать, если вспомнить, что $opt[i, j] \\leq opt[i][j+1]$, то есть что оптимальная точка всегда будет «правее». Это позволяет вместо бинпоиска применить метод двух указателей, и таким образом соптимизировать решение до $O(n m)$.\n","id":56,"path":"/cs/layer-optimizations/convex-hull-trick/","title":"Convex Hull Trick"},{"content":"Алгоритм Грэхэма — это оптимизация алгоритма Джарвиса, основанная на следующем наблюдении: если отсортировать все точки по полярному углу относительно точки $p_0$, то выпуклая оболочка будет какой-то подпоследовательностью такого отсортированного массива точек.\nАлгоритм последовательно строит выпуклые оболочки для каждого префикса этого отсортированного массива. Можно заметить, что при добавлении $i$-й точки в оболочку нужно лишь удалить сколько-то последних добавленных точек, которые не будут входить в новую оболочку, а именно тех, которые «покрываются» новой точкой и своей предыдущей.\nЧтобы проводить это удаление эффективно, мы можем хранить выпуклую оболочку в стеке и в цикле while смотреть на три последние точки и проверять, образуют ли они правый поворот. Если это так, то среднюю следует удалить — мы нашли треугольник $(p_0, p_i, p_{i-2})$, который содержит $p_{i-1}$, а значит её можно удалить.\nКаждая точка будет добавлена один раз удалена не более одного раза, что занимает константное количество операций. Соответственно, время работы будет упираться во время работы сортировки, то есть $O(n \\log n)$.\nvector\u0026lt;r\u0026gt; graham(vector\u0026lt;r\u0026gt; points) { // находим p0, как и раньше r p0 = points[0]; for (r p : points) if (p.x \u0026lt; p0.x || (p.x == p0.x \u0026amp;\u0026amp; p.y \u0026lt; p0.y)) p0 = p; // сортируем точки по полярному углу sort(points.begin(), points.end(), [\u0026amp;](r a, r b){ return (a - p0) ^ (b - p0) \u0026gt; 0; }); vector\u0026lt;r\u0026gt; hull; for (r p : points) { // удаляем последнюю точку со стека пока она образует невыпуклость while (hull.size() \u0026gt;= 2) { r new_vector = p - hull.back(); r last_vector = hull.back() - hull[hull.size() - 2]; // если два последних вектора заворачивают влево, удаляем последнюю точку if (new_vector ^ last_vector \u0026gt; 0) hull.pop_back(); else break; } hull.push_back(p); } return hull; } Интерактивная визуализация, где на заданном наборе точек прогоняются все шаги алгоритма.\n","id":57,"path":"/cs/convex-hulls/graham/","title":"Алгоритм Грэхэма"},{"content":"Алгоритм Дейкстры (англ. Dijkstra\u0026rsquo;s algorithm) находит кратчайшие пути от заданной вершины $s$ до всех остальных в графе без ребер отрицательного веса.\nСуществует два основных варианта алгоритма, время работы которых составляет $O(n^2)$ и $O(m \\log n)$, где $n$ — число вершин, а $m$ — число ребер.\n#Основная идеяЗаведём массив $d$, в котором для каждой вершины $v$ будем хранить текущую длину $d_v$ кратчайшего пути из $s$ в $v$. Изначально $d_s = 0$, а для всех остальных вершин расстояние равно бесконечности (или любому числу, которое заведомо больше максимально возможного расстояния).\nВо время работы алгоритма мы будем постепенно обновлять этот массив, находя более оптимальные пути к вершинам и уменьшая расстояние до них. Когда мы узнаем, что найденный путь до какой-то вершины $v$ оптимальный, мы будем помечать эту вершину, поставив единицу ($a_v=1$) в специальном массиве $a$, изначально заполненном нулями.\nСам алгоритм состоит из $n$ итераций, на каждой из которых выбирается вершина $v$ с наименьшей величиной $d_v$ среди ещё не помеченных:\n$$ v = \\argmin_{u | a_u=0} d_u $$\n(Заметим, что на первой итерации выбрана будет стартовая вершина $s$.)\nВыбранная вершина отмечается в массиве $a$, после чего из из вершины $v$ производятся релаксации: просматриваем все исходящие рёбра $(v,u)$ и для каждой такой вершины $u$ пытаемся улучшить значение $d_u$, выполнив присвоение\n$$ d_u = \\min (d_u, d_v + w) $$\nгде $w$ — длина ребра $(v, u)$.\nНа этом текущая итерация заканчивается, и алгоритм переходит к следующей: снова выбирается вершина с наименьшей величиной $d$, из неё производятся релаксации, и так далее. После $n$ итераций, все вершины графа станут помеченными, и алгоритм завершает свою работу.\n#КорректностьОбозначим за $l_v$ расстояние от вершины $s$ до $v$. Нам нужно показать, что в конце алгоритма $d_v = l_v$ для всех вершин (за исключением недостижимых вершин — для них все расстояния останутся бесконечными).\nДля начала отметим, что для любой вершины $v$ всегда выполняется $d_v \\ge l_v$: алгоритм не может найти путь короче, чем кратчайший из всех существующих (ввиду того, что мы не делали ничего кроме релаксаций).\nДоказательство корректности самого алгоритма основывается на следующем утверждении.\nУтверждение. После того, как какая-либо вершина $v$ становится помеченной, текущее расстояние до неё $d_v$ уже является кратчайшим, и, соответственно, больше меняться не будет.\nДоказательство будем производить по индукции. Для первой итерации его справедливость очевидна — для вершины $s$ имеем $d_s=0$, что и является длиной кратчайшего пути до неё.\nПусть теперь это утверждение выполнено для всех предыдущих итераций — то есть всех уже помеченных вершин. Докажем, что оно не нарушается после выполнения текущей итерации, то есть что для выбранной вершины $v$ длина кратчайшего пути до неё $l_v$ действительно равна $d_v$.\nРассмотрим любой кратчайший путь до вершины $v$. Обозначим первую непомеченную вершину на этом пути за $y$, а предшествующую ей помеченную за $x$ (они будут существовать, потому что вершина $s$ помечена, а вершина $v$ — нет). Обозначим вес ребра $(x, y)$ за $w$.\nТак как $x$ помечена, то, по предположению индукции, $d_x = l_x$. Раз $(x,y)$ находится на кратчайшем пути, то $l_y=l_x+w$, что в точности равно $d_y=d_x+w$: мы в какой-то момент проводили релаксацию из уже помеченный вершины $x$.\nТеперь, может ли быть такое, что $y \\ne v$? Нет, потому что мы на каждой итерации выбираем вершину с наименьшим $d_v$, а любой вершины дальше $y$ на пути расстояние от $s$ будет больше. Соответственно, $v = y$, и $d_v = d_y = l_y = l_v$, что и требовалось доказать.\n#Время работы и реализацияЕдинственное вариативное место в алгоритме, от которого зависит его сложность — как конкретно искать $v$ с минимальным $d_v$.\n#Для плотных графовЕсли $m \\approx n^2$, то на каждой итерации можно просто пройтись по всему массиву и найти $\\argmin d_v$.\nconst int maxn = 1e5, inf = 1e9; vector\u0026lt; pair\u0026lt;int, int\u0026gt; \u0026gt; g[maxn]; int n; vector\u0026lt;int\u0026gt; dijkstra(int s) { vector\u0026lt;int\u0026gt; d(n, inf), a(n, 0); d[s] = 0; for (int i = 0; i \u0026lt; n; i++) { // находим вершину с минимальным d[v] из ещё не помеченных int v = -1; for (int u = 0; u \u0026lt; n; u++) if (!a[u] \u0026amp;\u0026amp; (v == -1 || d[u] \u0026lt; d[v])) v = u; // помечаем её и проводим релаксации вдоль всех исходящих ребер a[v] = true; for (auto [u, w] : g[v]) d[u] = min(d[u], d[v] + w); } return d; } Асимптотика такого алгоритма составит $O(n^2)$: на каждой итерации мы находим аргминимум за $O(n)$ и проводим $O(n)$ релаксаций.\nЗаметим также, что мы можем делать не $n$ итераций а чуть меньше. Во-первых, последнюю итерацию можно никогда не делать (оттуда ничего уже не прорелаксируешь). Во-вторых, можно сразу завершаться, когда мы доходим до недостижимых вершин ($d_v = \\infty$).\n#Для разреженных графовЕсли $m \\approx n$, то минимум можно искать быстрее. Вместо линейного прохода заведем структуру, в которую можно добавлять элементы и искать минимум — например std::set так умеет.\nБудем поддерживать в этой структуре пары $(d_v, v)$, при релаксации удаляя старый $(d_u, u)$ и добавляя новый $(d_v + w, u)$, а при нахождении оптимального $v$ просто беря минимум (первый элемент).\nПоддерживать массив $a$ нам теперь не нужно: сама структура для нахождения минимума будет играть роль множества ещё не рассмотренных вершин.\nvector\u0026lt;int\u0026gt; dijkstra(int s) { vector\u0026lt;int\u0026gt; d(n, inf); d[root] = 0; set\u0026lt; pair\u0026lt;int, int\u0026gt; \u0026gt; q; q.insert({0, s}); while (!q.empty()) { int v = q.begin()-\u0026gt;second; q.erase(q.begin()); for (auto [u, w] : g[v]) { if (d[u] \u0026gt; d[v] + w) { q.erase({d[u], u}); d[u] = d[v] + w; q.insert({d[u], u}); } } } return d; } Для каждого ребра нужно сделать два запроса в двоичное дерево, хранящее $O(n)$ элементов, за $O(\\log n)$ каждый, поэтому асимптотика такого алгоритма составит $O(m \\log n)$. Заметим, что в случае полных графов это будет равно $O(n^2 \\log n)$, так что про предыдущий алгоритм забывать не стоит.\n#С кучейВместо двоичного дерева «правильнее» использовать более специализированную структуру, которая поддерживает именно добавление элементов и нахождение минимума: кучу. Удалять произвольные элементы в ней немного сложнее, поэтому вместо этого будем просто игнорировать все повторные вершины.\nvector\u0026lt;int\u0026gt; dijkstra(int s) { vector\u0026lt;int\u0026gt; d(n, inf); d[root] = 0; // объявим очередь с приоритетами для *минимума* (по умолчанию ищется максимум) using pair\u0026lt;int, int\u0026gt; Pair; priority_queue\u0026lt;Pair, vector\u0026lt;Pair\u0026gt;, greater\u0026lt;Pair\u0026gt;\u0026gt; q; q.push({0, s}); while (!q.empty()) { auto [cur_d, v] = q.top(); q.pop(); if (cur_d \u0026gt; d[v]) continue; for (auto [u, w] : g[v]) { if (d[u] \u0026gt; d[v] + w) { d[u] = d[v] + w; q.push({d[u], u}); } } } } На практике вариант с priority_queue немного быстрее.\nПомимо обычной двоичной кучи, можно использовать и другие. С теоретической точки зрения, особенно интересна Фибоначчиева куча: у неё все почти все операции кроме работают за $O(1)$, но удаление элементов — за $O(\\log n)$. Это позволяет облегчить релаксирование до $O(1)$ за счет увеличения времени извлечения минимума до $O(\\log n)$, что приводит к асимптотике $O(n \\log n + m)$ вместо $O(m \\log n)$.\n#Восстановление путейЧасто нужно знать не только длины кратчайших путей, но и получить сами пути.\nДля этого можно создать массив $p$, в котором в ячейке $p_v$ будет хранится родитель вершины $v$ — вершина, из которой произошла последняя релаксация по ребру $(p_v, v)$.\nОбновлять его можно параллельно с массивом $d$. Например, в последней реализации:\nif (d[u] \u0026gt; d[v] + w) { d[u] = d[v] + w; p[u] = v; // \u0026lt;-- кратчайший путь в u идет через ребро (v, u) q.push({d[u], u}); } Для восстановления пути нужно просто пройтись по предкам вершины $v$:\nvoid print_path(int v) { while (v != s) { cout \u0026lt;\u0026lt; v \u0026lt;\u0026lt; endl; v = p[v]; } cout \u0026lt;\u0026lt; s \u0026lt;\u0026lt; endl; } Обратим внимание, что код распечатает путь в обратном порядке.\n","id":58,"path":"/cs/shortest-paths/dijkstra/","title":"Алгоритм Дейкстры"},{"content":"Другой способ использовать лемму о безопасном ребре — отсортировать все ребра и пытаться добавлять их в изначально пустой остов в порядке возрастания их весов.\nЕсли очередное ребро соединяет какие-то две уже соединенные вершины, то проигнорируем его. Иначе оно является безопасным, так как оно минимальное из соединяющих какие-то две различные компоненты, и его можно добавить.\nЗвучит очень просто: отсортировать все рёбра, пройтись по ним циклом и делать проверку, что вершины в разных компонентах. Но наивная проверка dfs-ом от концов всех ребер будет работать за $O(nm)$. Асимптотику можно улучшить до $O(m \\log m)$ — до стоимости сортировки ребер — если для проверок использовать систему непересекающихся множеств.\nЗа исключением реализации СНМ, код получается очень коротким:\nstruct Edge { int from, to, weight; }; vector\u0026lt;Edge\u0026gt; edges; sort(edges.begin(), edges.end(), [](Edge a, Edge b) { return a.weight \u0026lt; b.weight; }); for (auto [a, b, w] : edges) { // компоненты разные, если лидеры разные if (p(a) != p(b)) { // добавим ребро (a, b) unite(a, b); } } Раз остовные деревья являются частным случаем матроида, то алгоритм Краскала является частным случаем алгоритма Радо-Эдмондса.\n","id":59,"path":"/cs/spanning-trees/kruskal/","title":"Алгоритм Краскала"},{"content":"Пусть есть строка $s$ и мы хотим найти в ней все подпалиндромы.\nМы сразу сталкиваемся с очевидной трудностью: их в строке может быть $O(n^2)$, что можно видеть на примере строки $s = aa \\ldots a$. Поэтому будем использовать следующий формат: для каждой позиции $s_i$ найдём наибольший палиндром, центр которого совпадает с $s_i$ (чётные и нечётные палиндромы будем рассматривать отдельно). Половину его длины, округлённую вниз, будем называть радиусом.\nНаивное решение — перебрать $s_i$, а для него вторым циклом находить наибольшую искомую длину:\nvector\u0026lt;int\u0026gt; pal_array(string s) { int n = s.size(); // окружим строку спецсимволами, чтобы не рассматривать выход за границы s = \u0026#34;#\u0026#34; + s + \u0026#34;$\u0026#34;; // в этом массиве будем хранить расстояние от центра до границы палиндрома vector\u0026lt;int\u0026gt; t(n, 0); for(int i = 1; i \u0026lt;= n; i++) while (s[i - t[i - 1]] == s[i + t[i - 1]]) r[i-1]++; return r; } Тот же пример $s = aa\\dots a$ показывает, что данная реализация работает за $O(n^2)$.\nДля оптимизации применим идею, знакомую из алгоритма z-функции: при инициализации $t_i$ будем пользоваться уже посчитанными $t$. А именно, будем поддерживать $(l, r)$ — интервал, соответствующий самому правому из найденных подпалиндромов. Тогда мы можем сказать, что часть наибольшего палиндрома с центром в $s_i$, которая лежит внутри $s_{l:r}$, имеет радиус хотя бы $\\min(r-i, ; t_{l+r-i})$. Первая величина равна длине, дальше которой произошел бы выход за пределы $s_{l:r}$, а вторая — значению радиуса в позиции, зеркальной относительно центра палиндрома $s_{l:r}$.\nvector\u0026lt;int\u0026gt; manacher_odd(string s) { int n = (int) s.size(); vector\u0026lt;int\u0026gt; d(n, 1); int l = 0, r = 0; for (int i = 1; i \u0026lt; n; i++) { if (i \u0026lt; r) d[i] = min(r - i + 1, d[l + r - i]); while (i - d[i] \u0026gt;= 0 \u0026amp;\u0026amp; i + d[i] \u0026lt; n \u0026amp;\u0026amp; s[i - d[i]] == s[i + d[i]]) d[i]++; if (i + d[i] - 1 \u0026gt; r) l = i - d[i] + 1, r = i + d[i] - 1; } return d; } Так же, как и z-функция, алгоритм работает за линейное время: цикл while запускается только когда $t_i = r - i$ (иначе палиндром уже во что-то упёрся), и каждая его итерация сдвигает увеличивает $r$ на единицу. Так как $r \\leq n$, получаем, что суммарно эти циклы сделают $O(n)$ итераций.\nДля случая чётных палиндромов меняется только индексация:\nvector\u0026lt;int\u0026gt; manacher_even(string s) { int n = (int) s.size(); vector\u0026lt;int\u0026gt; d(n, 0); int l = -1, r = -1; for (int i = 0; i \u0026lt; n - 1; i++) { if (i \u0026lt; r) d[i] = min(r - i, d[l + r - i - 1]); while (i - d[i] \u0026gt;= 0 \u0026amp;\u0026amp; i + d[i] + 1 \u0026lt; n \u0026amp;\u0026amp; s[i - d[i]] == s[i + d[i] + 1]) d[i]++; if (i + d[i] \u0026gt; r) l = i - d[i] + 1, r = i + d[i]; } return d; } Также можно было не писать отдельно две реализации, а воспользоваться следующим трюком — сделать замену:\n$$ S = s_1 s_2 \\dots s_n \\to S^* = s_1 # s_2 # \\dots # s_n $$\nТеперь нечётные палиндромы с центром в $s_i$ соответствуют нечётным палиндромам исходной строки, а нечётные палиндромы с центром в # — чётным.\n","id":60,"path":"/cs/string-searching/manacher/","title":"Алгоритм Манакера"},{"content":"Как уже упоминалось ранее, иногда существенную роль играет то, что алгоритм Джарвиса работает за $O(nh)$, а не $O(n^2)$: когда известно, что оболочка небольшая, он лучше алгоритма Грэхэма за $O(n \\log n)$.\nАлгоритм Чана пытается получить лучшее из двух и объединить алгоритмы Джарвиса и Грэхэма, чтобы получить асимптотику $O(n \\log h)$.\nАлгоритм. Разделим все точки на группы по $m$ точек. В каждой группе построим выпуклую оболочку за $O(m \\log m)$ алгоритмом Грэхэма. Точки никак не упорядочены, и эти оболочки могут пересекаться — это нормально. Суммарно для всех групп понадобится $O(n \\log m)$ операций.\nЗатем, начиная с самой левой нижней точки, мы будем строить общую выпуклую оболочку аналогично алгоритму Джарвиса, но теперь «самую правую точку» можно находить каждый раз не за $O(n)$, а за $O(\\frac{n}{m} \\log m)$, если делать бинарный поиск в каждой из $\\frac{n}{m}$ оболочек.\nЗдесь группы разделены по $x$, но это только для демонстрации Получается, что такое решение будет работать за $O(h \\frac{n}{m} \\log m + n \\log m)$. Если заранее приблизительно знать $h$, то можно положить $m = h$, и тогда асимптотика составит $O(n \\log h)$.\nПонятно, что ни при каких применимых на практике ограничениях, даже если оболочка состоит из трёх точек, этот алгоритм не будет быстрее обычного алгоритма Грэхэма, так что автор не будет приводить его реализацию. Этот способ просто интересен с теоретической точки зрения.\nУпражнение. Придумайте, как при неизвестном $h$ подбирать $m$ так, чтобы асимптотика оставалась $O(n \\log h)$.\n","id":61,"path":"/cs/convex-hulls/chan/","title":"Алгоритм Чана"},{"content":"В этой статье на примере нескольких задач мы рассмотрим важную разновидность бинарного поиска — бинарный поиск по ответу — заключающийся в том, чтобы сформулировать задачу как «найдите максимальное $x$ такое, что такое-то легко вычислимое свойство от $x$ выполняется» и найти этот $x$ бинпоиском.\n#«Коровы в стойла» На прямой расположены $n$ стойл (даны их координаты на прямой), в которые необходимо расставить $k$ коров так, чтобы минимальное расстояние между коровами было как можно больше.\nГарантируется, что $1 \u0026lt; k \u0026lt; n$.\nЕсли решать задачу в лоб, то вообще неясно, что делать. Вместо этого нужно решим более простую задачу: предположим, что мы знаем это расстояние $x$, ближе которого коров ставить нельзя. Тогда сможем ли мы расставить самих коров?\nОтвет — да, причём довольно просто: самую первую ставим в самое левое стойло, потому что это всегда выгодно. Следующие несколько стойл надо оставить пустыми, если они на расстоянии меньше $x$, а в самое левое стойло из оставшихся надо поставить вторую корову, и так далее.\nКак это реализовать: надо идти слева направо по отсортированному массиву стойл, хранить координату последней коровы, и в зависимости от расстояния до предыдущей коровы либо пропускать стойло, либо ставить в него новую корову.\nbool check(int x) { int cows = 1; int last_cow = coords[0]; for (int c : coords) { if (c - last_cow \u0026gt;= x) { cows++; last_cow = c; } } return cows \u0026gt;= k; } Если в конце такого жадного алгоритма коровы у нас кончились раньше, чем безопасные стойла, то ответ точно не меньше $x$, а если у нас не получилось, то ответ точно меньше $x$.\nТеперь мы можем перебрать $x$ и сделать $X = \\frac{\\max x_i - \\min x_i}{k}$ проверок за $O(n)$, но можно и ещё быстрее.\nЗапустим бинпоиск по $x$ — ведь для каких-то маленьких $x$ коров точно можно расставить, а начиная с каких-то больших — уже нельзя, и как раз это границу нас и просят найти в задаче.\nint solve() { sort(coords.begin(), coords.end()); int l = 0; // так как коров меньше, чем стойл, x = 0 нам всегда хватит // по условию есть хотя бы 2 коровы, // которых мы в лучшем случае отправим в противоположные стойла: int r = coords.back() - coords[0] + 1; while (r - l \u0026gt; 1) { int m = (l + r) / 2; if (check(m)) l = m; else r = m; } return l; } Каждая проверка у нас работает за $O(n)$, а внешний бинпоиск — за $O(\\log n)$ проверок, так что асимптотика будет $O(n \\log X)$.\n#«Принтеры» Есть два принтера. Один печатает лист раз в $x$ минут, другой раз в $y$ минут. За сколько минут они напечатают $n$ листов?\n$n \u0026gt; 0$\nЗдесь, в отличие от предыдущей задачи, кажется, существует прямое решение с формулой. Но вместо того, чтобы о нем думать, можно просто свести задачу к обратной. Давайте подумаем, как по числу минут $t$ (ответу) понять, сколько листов напечатается за это время? Очень легко:\n$$ \\left \\lfloor \\frac{t}{x} \\right \\rfloor + \\left \\lfloor \\frac{t}{y} \\right \\rfloor $$\nЯсно, что за $0$ минут $n$ листов распечатать нельзя, а за $x \\cdot n$ минут один только первый принтер успеет напечатать $n$ листов. Поэтому $0$ и $xn$ — это подходящие изначальные границы для бинарного поиска.\n","id":62,"path":"/cs/interactive/answer-search/","title":"Бинарный поиск по ответу"},{"content":"В предыдущей главе мы рассматривали различные способы сделать последовательности упорядоченными; в этой мы обсудим, где это может быть полезным.\n","id":63,"path":"/cs/interactive/","title":"Бинпоиск и интерактивки"},{"content":"Дерево палиндромов (англ. palindromic tree, EERTREE) — структура данных, использующая другой, более мощный формат хранения информации обо всех подпалиндромах, чем размеры $n$ палиндромов. Она была предложена Михаилом Рубинчиком на летних петрозаводских сборах в 2014-м году.\nЛемма. В строке есть не более $n$ различных подпалиндромов.\nДоказательство. Пусть мы дописываем к строке по одному символу и в данный момент, записав $r$ символов, имеем наибольший суффикс-палиндром $s_{l:r}$. Пусть у него, в свою очередь, есть суффикс-палиндром $s_{l\u0026rsquo;:r} = t$. Тогда он также имеет более раннее вхождение в строку как $s_{l:l+r-l\u0026rsquo;} = t$. Таким образом, с каждым новым символом у строки появляется не более одного нового палиндрома, и если таковой есть, то это всегда наибольший суффикс-палиндром.\nЭтот факт позволяет сопоставить всем палиндромам строки сопоставить следующую структуру: возьмём от каждого палиндрома его правую половину (например, $caba$ для $abacaba$ или $ba$ для $abba$; будем рассматривать пока что только чётные палиндромы) и добавим все эти половины в префиксное дерево — получившуюся структуру и будем называть деревом палиндромов.\nНаивный алгоритм построения будет в худшем случае работать за $O(n^2)$, но это можно делать и более эффективно.\n#Построение за линейное времяБудем поддерживать наибольший суффикс-палиндром. Когда мы будем дописывать очередной символ $c$, нужно найти наибольший суффикс этого палиндрома, который может быть дополнен символом $c$ — это и будет новый наидлиннейший суффикс-палиндром.\nДля этого поступим аналогично алгоритму Ахо-Корасик: будем поддерживать для каждого палиндрома суффиксную ссылку $l(v)$, ведущую из $v$ в её наибольший суффикс-палиндром. При добавлении очередного символа, будем подниматься по суффиксным ссылкам, пока не найдём вершину, из которой можно совершить нужный переход.\nЕсли в подходящей вершине этого перехода не существовало, то нужно создать новую вершину, и для неё тоже понадобится своя суффиксная ссылка. Чтобы найти её, будем продолжать подниматься по суффиксным ссылкам предыдущего суффикс-палиндрома, пока не найдём второе такое место, которое мы можем дополнить символом $c$.\nconst int maxn = 1e5, k = 26; int s[maxn], len[maxn], link[maxn], to[maxn][k]; int n, last, sz; void init() { s[n++] = -1; link[0] = 1; len[1] = -1; sz = 2; } int get_link(int v) { while (s[n-len[v]-2] != s[n-1]) v = link[v]; return v; } void add_char(int c) { s[n++] = c; last = get_link(last); if (!to[last][c]) { len[sz] = len[last] + 2; link[sz] = to[get_link(link[last])][c]; to[last][c] = sz++; } last = to[last][c]; } Здесь мы использовали обычный массив для хранения переходов. Как и для любых префиксных деревьев, вместо него можно использовать бинарное дерево поиска, хеш-таблицу, односвязный список и другие структуры, позволяющие обменять время на память, немного изменив асимптотику.\n#АсимптотикаПокажем линейность алгоритма. Рассмотрим длину наибольшего суффикс-палиндрома строки. Каждый новый символ увеличивает её не более, чем на 2. При этом каждый переход по суффиксной ссылке уменьшает её, поэтому нахождение первого суффикс-палиндрома амортизировано работает за линейное время.\nАналогичными рассуждениями о длине второго суффикс-палиндрома (его длина увеличивается тоже не более, чем на 2) получаем, что пересчёт суффиксных ссылок при создании новых вершин тоже суммарно работает за линейное время.\n","id":64,"path":"/cs/string-structures/palindromic-tree/","title":"Дерево палиндромов"},{"content":"Массив — это набор однотипных переменных, доступ к которым осуществляется по индексу.\nДинамический или расширяющийся массив — это массив, который может изменять свой размер в зависимости от количества элементов в нём.\nДинамические массивы обычно используют, когда заранее предсказать размер массива сложно или невозможно. В таком контексте у динамических массивов помимо операций доступа и изменения произвольных элементов есть ещё три:\nДобавить в конец массива элемент $x$. Удалить последний элемент массива. Узнать размер массива. При этом все операции должны выполняться за $O(1)$ — необязательно в худшем случае, но амортизировано.\n#РеализацияДинамические массивы, как и все структуры этого раздела, есть почти во всех языках программирования, однако для полноты понимания очень рекомендуется научиться писать их с нуля, потому что они в дальнейшем будут использоваться для всех остальных структур.\nЕсли обычные массивы — это просто последовательные области в памяти, то динамический массив обычно реализуют как структуру, которая содержит:\nуказатель на массив $t$, размер этого массива, текущее число элементов (меньшее размера массива $t$). При этом внутренний массив $t$ расширяют (деаллоцируют и заново аллоцируют с большим размером), когда он становится полностью заполненным, и требуется добавить ещё один элемент. Также опционально можно сжимать массив, когда доля заполненных элементов станет малой — это позволит вернуть не использующуюся память.\ntemplate \u0026lt;typename T\u0026gt; struct dynamic_array { T *t; int size = 0, capacity; dynamic_array(int capacity) : capacity(capacity) { t = new T[capacity]; } void resize(int new_capacity) { T *new_t = new T[new_capacity]; memcpy(new_t, t, sizeof(T) * size); delete[] t; t = new_t; } T get(int k) { return t[k]; } T set(int k, T x) { t[k] = x; } void add(T x) { if (size == capacity) resize(2 * capacity); t[size++] = x; } void del() { // если хотим сэкономить память: if (4 * size \u0026lt; capacity) resize(capacity / 2); size--; } }; #Время работыВ худшем случае операции добавления и удаления работают за линейное время, потому что нам нужно пересоздавать весь массив размера $O(n)$. Однако амортизировано все операции будут работать за $O(1)$. Применим метод предоплаты чтобы это показать.\n#addПусть единицей стоимости операции является одна монетка. Тогда при каждой операции add, при которой нам не требуется копирование, мы будем платить три монетки: одна из них пойдёт на стоимость самой этой операции, а две будут в резерве — если мы добавили $k$-ый элемент, мы будем класть по одной монетке к элементам с номерами $k$ и $(k−\\frac{n}{2})$.\nК тому моменту, как массив будет заполнен, рядом с каждым элементом будет лежать по одной монетке, которой мы и сможем оплатить его копирование в новый массив. Таким образом, амортизационная стоимость каждой операции add — 3, и среднее время её работы — $O(1)$.\n#delПри каждой обычной операции del будем платить две монетки. Одну из них потратим на непосредственно удаление последнего ($k$-того) элемента, другую положим рядом с элементом, стоящим на позиции $(k \\bmod \\frac{n}{4})$. Тогда даже в худшем случае — если мы только что расширились, а потом удалили $\\frac{n}{4}$ элементов с конца — у каждого элемента из первых $\\frac{n}{4}$ будет по монете, которые мы и потратим на их перемещение.\n#В языках программирования #std::vectorВ С++ динамический массив реализован в структуре vector из стандартной библиотеки.\n// создать пустой вектор vector\u0026lt;int\u0026gt; a; // вставляет x в конец a a.push_back(x); // возвращает размер вектора а a.size(); // сделать размер вектора = x // либо удаляются последние элементы, либо добавляются нули a.resize(x); // сделать размер вектора = x, добавляются y a.resize(x, y); // при инициализации можно изначально задать размер и элементы массива vector\u0026lt;int\u0026gt; a(8); // {0, 0, 0, 0, 0, 0, 0, 0} vector\u0026lt;int\u0026gt; b(5, 42); // {42, 42, 42, 42, 42} vector\u0026lt;int\u0026gt; c = {1, 2, 3} // {1, 2, 3} При попытке записи в массив нового элемента в момент полного заполнения памяти происходит увеличение размера — в 2 раза при компиляции через GCC и в 1.5 при компиляции через MSVC. При удалении элементов уменьшение размера массива не происходит.\nПолучить capacity у vector можно с помощью одноимённой функции:\nvector\u0026lt;int\u0026gt; a; for (int i = 0; i \u0026lt; 10; i++) { a.push_back(i); cout \u0026lt;\u0026lt; \u0026#34;size: \u0026#34; \u0026lt;\u0026lt; a.size() \u0026lt;\u0026lt; \u0026#34;, capacity \u0026#34; \u0026lt;\u0026lt; a.capacity() \u0026lt;\u0026lt; endl; } Будет выведено:\nsize: 1, capacity 1 size: 2, capacity 2 size: 3, capacity 4 size: 4, capacity 4 size: 5, capacity 8 size: 6, capacity 8 size: 7, capacity 8 size: 8, capacity 8 size: 9, capacity 16 size: 10, capacity 16 При инициализации vector по-умолчанию начальный размер (который capacity) равен 0, однако многие использующие его внутри структуры часто резервируют какой-то начальный размер — например, 16 или 32 элементов — чтобы сэкономить время из предположения, что там будет храниться не один элемент.\n#PythonВ Питоне обычные списки выполняют роль расширяемых массивов.\na = [1, 2, 3] a.append(4) ","id":65,"path":"/cs/basic-structures/vector/","title":"Динамический массив"},{"content":"Корневые эвристики — это обобщённое название различных методов и структур данных, опирающихся на тот факт, что если мы разделим какое-то множество из $n$ элементов на блоки по $\\sqrt{n}$ элементов, то самих этих блоков будет не более $\\sqrt{n}$.\nЦентральное равенство этой статьи: $\\sqrt x = \\frac{x}{\\sqrt x}$.\n#Деление на тяжелые и легкие объектыВсем известный алгоритм факторизации за корень опирается на тот факт, что каждому «большому» делителю $d \\geq \\sqrt n$ числа $n$ соответствует какой-то «маленький» делитель $\\frac{n}{d} \\leq n$.\nПодобное полезное свойство (что маленькие объекты маленькие, а больших объектов не много) можно найти и у других объектов.\n#Длинные и короткие строкиЗадача. Требуется в онлайне обрабатывать три типа операций над множеством строк:\nДобавить строку в множество. Удалить строку из множества. Для заданной строки, найти количество её вхождений как подстроку среди всех строк множества. Одно из решений следующее: разделим все строки на короткие ($|s| \u0026lt; \\sqrt l$) и длинные ($|s| \\geq \\sqrt l$), где $l$ означает суммарную длину всех строк. Заметим, что длинных строк немного — не более $\\sqrt l$.\nС запросами будем справляться так:\nЗаведём хеш-таблицу, и когда будем обрабатывать запрос добавления или удаления, будем прибавлять или отнимать соответственно единицу по хешам всех её коротких подстрок. Это можно сделать суммарно за $O(l \\sqrt l)$: для каждой строки нужно перебрать $O(\\sqrt l)$ разных длин и окном пройтись по всей строке. Для запроса третьего типа для короткой строки, просто посчитаем её хеш и посмотрим на значение в хеш-таблице. Для запроса третьего типа для длинной строки, мы можем позволить себе посмотреть на все неудаленные строки, потому что таких случаев будет немного, и если мы можем за линейное время найти все вхождения новой строки, то работать это будет тоже за $O(l \\sqrt l)$. Например, можно посчитать z-функцию для всех строк вида s#t, где $s$ это строка из запроса, а $t$ это строка из множества; здесь, правда, есть нюанс: $s$ может быть большой, а маленьких строк $t$ много — нужно посчитать z-функцию сначала только от $s$, а затем виртуально дописывать к ней каждую $t$ и досчитывать функцию. Иногда отдельный подход к тяжелым и лёгким объектам не требуется, но сама идея помогает увидеть, что некоторые простые решения работают быстрее, чем кажется.\n#Треугольники в графеЗадача. Дан граф из $n$ вершин и $m \\approx n$ рёбер. Требуется найти в нём количество циклов длины три.\nБудем называть вершину тяжелой, если она соединена с более чем $\\sqrt n$ другими вершинами, и лёгкой в противном случае.\nПопытаемся оценить количество соединённых вместе троек вершин, рассмотрев все возможные 4 варианта:\nВ цикле нет тяжелых вершин. Рассмотрим какое-нибудь ребро $(a, b)$ цикла. Третья вершина $c$ должна лежать в объединении списков смежности $g_a$ и $g_b$, а раз обе эти вершины лёгкие, то таких вершин найдётся не более $\\sqrt n$. Значит, всего циклов этого типа может быть не более $O(m \\sqrt n)$. В цикле одна тяжелая вершина. Аналогично — есть одно «лёгкое» ребро, а значит таких циклов тоже $O(m \\sqrt n)$. В цикле две тяжелые вершины — обозначим их как $a$ и $b$, а лёгкую как $c$. Зафиксируем пару $(a, c)$ — способов это сделать $O(m)$, потому что всего столько рёбер. Для этого ребра будет не более $O(\\sqrt n)$ рёбер $(a, b)$, потому что столько всего тяжелых вершин. Получается, что всего таких циклов может быть не более $O(m \\sqrt n)$. Все вершины тяжелые. Аналогично — тип третьей вершины в разборе предыдущего случая нигде не использовался; важно лишь то, что тяжелых вершин $b$ немного. Получается, что циклов длины 3 в графе может быть не так уж и много — не более $O(m \\sqrt n)$.\nСамо решение максимально простое: отсортируем вершины графа по их степени и будем перебирать все пути вида $v \\rightarrow u \\rightarrow w, v \\le u \\le w$ и проверять существование ребра $v \\rightarrow w$.\nvector\u0026lt;int\u0026gt; g[maxn], p(n); // исходный граф и список номеров вершин iota(p.begin(), p.end(), 0); // 0, 1, 2, 3, ... // чтобы не копипастить сравнение: auto cmp = [\u0026amp;](int a, int b) { return g[a].size() \u0026lt; g[b].size() || (g[a].size() == g[b].size() \u0026amp;\u0026amp; a \u0026lt; b); }; // в таком порядке мы будем перебирать вершины sort(p.begin(), p.end(), cmp); // теперь отсортируем списки смежности каждой вершины for (int v = 0; v \u0026lt; n; ++v) sort(g[v].begin(), g[v].end(), cmp); // рядом с каждой вершиной будем хранить количество // ранее просмотренных входящих рёбер (v -\u0026gt; w) vector\u0026lt;int\u0026gt; cnt(n, 0); int ans = 0; for (int v : p) { for (int w : g[v]) cnt[w]++; for (int u : g[v]) { if (cmp(v, u)) break; for (int w : g[u]) { if (cmp(u, w)) break; // если ребро плохое -- не берем треугольник ans += cnt[w]; // если в графе нет петель, то cnt[w] = {0, 1} } } // при переходе к следующему v массив нужно занулить обратно for (int w : g[v]) cnt[w]--; } #Рюкзак за $O(S \\sqrt S)$Если у нас есть $n$ предметов с весами $w_1$, $w_2$, $\\ldots$, $w_n$, такими что $\\sum w_i = S$, то мы можем решить задачу о рюкзаке за время $O(S \\cdot n)$ стандартной динамикой. Чтобы решить задачу быстрее, попытаемся сделать так, чтобы число предметов стало $O(\\sqrt S)$.\nЗаметим, что количество различных весов будет $O(\\sqrt S)$, так как для любых $k$ различных чисел с суммой $S$\n$$ S = w_1 + w_2 + \\ldots + w_n \\geq 1 + 2 + \\ldots + k = \\frac{k \\cdot (k+1)}{2} $$\nОткуда значит, что $k \\leq 2\\sqrt S = O(\\sqrt S)$.\nРассмотрим теперь некоторый вес $x$, который $k$ раз встречается в наборе весов. «Разложим» $k$ по степеням двойки и вместо всех $k$ вхождений этого веса добавим веса $x$, $2 \\cdot x$, $4 \\cdot x$, $\\ldots$, $(k - 1 - 2^t) \\cdot x$, где $t$ это максимальное целое число, для которого выполняется $2^t − 1 \\leq k$. Легко видеть, что все суммы вида $q \\cdot x$ ($q \\leq k$) и только их по-прежнему можно набрать.\nАлгоритм в этом, собственно, и заключается: проведем данную операцию со всеми уникальными значениями весов и после чего запустим стандартное решение. Уже сейчас легко видеть, что новое количество предметов будет $O(\\sqrt S \\log S)$, потому что для каждого веса мы оставили не более $\\log S$ весов, а всего различных весов было $O(\\sqrt S)$.\nУпражнение. Докажите, что предметов на самом деле будет $O(\\sqrt S)$.\nПримечание. Классическое решение рюкзака можно ускорить на несколько порядков, если использовать bitset.\nТакже корневые эвристики можно использовать в контексте структур данных и обработки запросов.\n","id":66,"path":"/cs/decomposition/sqrt-heuristics/","title":"Корневые эвристики"},{"content":"В этой статье мы докажем важную теорему об асимптотике большого класса «разделяек», в которых задача размера $n$ делится на $a$ задач в $b$ раз меньшего размера, с «мерджем» за $\\Theta(n^c)$.\nМастер-теорема. Пусть имеется рекуррента:\n$$ T(n) = \\begin{cases} a T(\\frac{n}{b}) + \\Theta(n^c), \u0026amp; n \u0026gt; n_0 \\ \\Theta(1), \u0026amp; n \\leq n_0 \\end{cases} $$\nТогда:\nA. Если $c \u0026gt; \\log_b a$, то $T(n) = \\Theta(n^c)$. B. Если $c = \\log_b a$, то $T(n) = \\Theta(n^c \\log n)$. C. Если $c \u0026lt; \\log_b a$, то $T(n) = \\Theta(n^{\\log_b a})$. Дерево рекурсии Доказательство. Рассмотрим «дерево рекурсии» этого соотношения. В нём будет $\\log_b n$ уровней. На $k$-том уровне будет $a^k$ вершин, каждая из которых будет стоить $\\left(\\frac{n}{b^k}\\right)^c$ операций. Просуммируем значения во всех вершинах по всем уровням:\n$$ T(n) = \\sum_{k=0}^{\\log_b n} a^k \\left(\\frac{n}{b^k}\\right)^c = n^c \\sum_{k=0}^{\\log_b n} \\left(\\frac{a}{b^c}\\right)^k $$\nA. Если $c \u0026gt; \\log_b a$, то $\\sum (\\frac{a}{b^с})^k$ это сумма убывающей геометрической прогрессии, которая не зависит от $n$ и просто равна какой-то константе. Значит, $T(n) = \\Theta(n^c)$.\nB. Если $c = \\log_b a$, то\n$$ T(n) = n^c \\sum_{k=0}^{\\log_b n} \\left(\\frac{a}{b^c}\\right)^k = n^c \\sum_{k=0}^{\\log_b n} 1^k = \\Theta(n^c \\log_b n) $$\nC. Если $c \u0026lt; \\log_b a$, то так как сумма прогрессии асимптотически эквивалентна своему старшему элементу,\n$$ T(n) = n^c \\sum_{k=0}^{\\log_b n} \\left(\\frac{a}{b^c}\\right)^k = \\Theta\\left(n^c \\left(\\frac{a}{b^c}\\right)^{\\log_b n}\\right) = \\Theta\\left(n^c \\cdot \\frac{a^{\\log_b n}}{n^c}\\right) = \\Theta(a^{\\log_b n}) = \\Theta(n^{\\log_b a}) $$\nПримечание. Для более точных оценок асимптотики «мерджа» теорема ничего не говорит. Например, если мердж занимает $\\Theta(n \\log n)$ и задача разбивается каждый раз на две части, то асимптотика будет равна:\n$$ \\sum_{k=0}^{\\log n} n \\log \\frac{n}{2^k} = \\sum_{k=0}^{\\log n} n (\\log n - k) = n \\sum_{k=0}^{\\log n} k = \\Theta (n \\log^2 n) $$\nВ то же время эта рекуррента под условия теоремы не попадает. Можно лишь получить неточные границы $\\Omega (n \\log n)$ и $O(n^{1+\\varepsilon})$, если подставить $c = 1$ и $c = 1 + \\varepsilon$ соответственно. Заметим, что $n \\log n$ и $n \\log^2 n$ асимптотически меньше $n^{1+\\varepsilon}$, каким бы маленьким $\\varepsilon$ ни был.\n","id":67,"path":"/cs/complexity/master-theorem/","title":"Мастер-теорема"},{"content":"Компонентой связности неориентированного графа называется подмножество вершин, достижимых из какой-то заданной вершины. Как следствие неориентированности, все вершины компоненты связности достижимы друг из друга.\nГраф с двумя компонентами связности Дан неориентированный граф $G$ с $n$ вершинами и $m$ рёбрами. Требуется найти в нём все компоненты связности, то есть разбить вершины графа на несколько групп так, что внутри одной группы можно дойти от одной вершины до любой другой, а между разными группами путей не существует.\nДля решения задачи модифицируем обход в глубину так, чтобы запустившись от вершины какой-то компоненты, от пометил все вершины этой компоненты — то есть все достижимые вершины — заданным номером этой компоненты. Для этого можно массив used заменить массивом номеров компонент для каждой вершины, изначально заполненный нулями:\nconst int maxn = 1e5; int component[maxn]; // тут будут номера компонент void dfs(int v, int num) { component[v] = num; for (int u : g[v]) if (!component[u]) // если номер не присвоен, то мы там ещё не были dfs(u, num); } Теперь проведем серию обходов: сначала запустим обход из первой вершины, и все вершины, которые он при этом обошёл, образуют первую компоненту связности. Затем найдём первую из оставшихся вершин, которые ещё не были посещены, и запустим обход из неё, найдя тем самым вторую компоненту связности. И так далее, пока все вершины не станут помеченными.\nЗаписывается это очень компактно:\nint num = 0; for (int v = 0; v \u0026lt; n; v++) if (!component[v]) dfs(v, ++num); После этого переменная num будет хранить число компонент связности, а массив component — номер компоненты для каждой вершины, который, например, можно использовать, чтобы быстро проверять, существует ли путь между заданной парой вершин.\nИтоговая асимптотика составит $O(n + m)$, потому что такой алгоритм не будет запускаться от одной и той же вершины дважды, и каждое ребро будет просмотрено ровно два раза (с одного конца и с другого).\n","id":68,"path":"/cs/graph-traversals/connectivity/","title":"Поиск компонент связности"},{"content":"Отрезок можно задать двумя точками своих концов. В любом порядке — ведь он, в отличие от вектора, неориентирован.\nstruct segment { r p, q; }; В вычислительной геометрии часто требуется уметь определять взаимное положение геометрических объектов. Например, проверять, пересекаются ли отрезки. В этом нам поможет скалярное и векторное произведение, а также небольшой разбор случаев:\nВ терминах произведений, «концы отрезка лежат по разные стороны относительно другого отрезка», например, записывается как:\n$$ [(M_1 - P_1) \\times (P_2 - P_1)] \\cdot [(M_2 - P_1) \\times (P_2 - P_1)] \u0026lt; 0 $$\nЗаписав ещё одно дополнительное условие, относительно $(M_1, M_2)$ в качестве разделительного отрезка, мы получим легко вычислимый предикат, который верен тогда и только тогда, когда отрезки пересекаются.\n#Лучи и прямыеЛуч — это отрезок, бесконечный в одном из направлений. Его можно хранить как точку и либо явный вектор направления, либо любую другую его точку.\nС прямыми дела обстоят немного сложнее. Здесь есть целых 4 имеющих право на жизнь способа их хранить:\nНормальным уравнением: $A \\cdot x + B \\cdot y + C = 0$. Двумя точками. Точкой и направляющим вектором (любым): $\\vec{r} = \\vec{a}t + \\vec{b}$. Точкой и нормальным вектором (любым). Также их иногда удобно хранить уравнением $y = kx + b$, как в школе, однако для вертикальных прямых придётся делать отдельный костыль.\nЧасто во входе прямые заданы в одном виде, но удобнее работать с другим. Реализуются основные переходы так:\n$2 \\rightarrow 1$: достаточно найти любое решение системы из 2 уравнений и 3 неизвестных. $1 \\rightarrow 3$: если прямая задаётся уравнением, то вектор $\\overline{(-B; A)}$ — направляющий к прямой (важно помнить, что у прямой есть два «направления»). $1 \\rightarrow 4$: если прямая задаётся уравнением, то вектор $\\overline{(A; B)}$ — нормальный к прямой (важно помнить, что упрямой есть 2 «направления нормали»). $4,3 \\rightarrow 1$: нужно решить систему из 1 уравнения и 1 неизвестного, раз уж из предыдущих пунктов мы уже знаем подходящие $A$ и $B$. $1 \\rightarrow 2$: достаточно выбрать любые две точки на прямой — например: // даны A, B, C (A^2 + B^2 != 0) r a, b; if (eq(A, 0)) // если это горизонтальная прямая a = {0, -C/B}, b = {1, -C/B); else a = {-C/A, 0}, b = {-(C+B)/A, 1); #ПроверкиДля способов 2-4 все проверки почти такие же, как для отрезков. На самом деле, в геометрических задачах часто можно ввести «bounding box» — квадрат с очень далекими границами, все точки за пределами которого считаются «бесконечностью». Тогда бесконечные прямые и лучи можно заменить на отрезки, упирающиеся в эти бесконечные границы, и делать все проверки как с обычными отрезками.\nВ случае прямых, заданных формулой, всё совсем по-другому.\n#Расстояние от точки до прямойЗаметим, что если прямая задается уравнением вида $Ax + By + C = 0$, то полуплоскость можно задать таким же неравенством. Это можно сразу использовать для проверки, по какую сторону от прямой находится точка.\nВектор нормали этой прямой будет иметь координаты $(A, B)$. Он перпендикулярен прямой, а в случае с полуплоскостью $Ax + By + C \\geq 0$ будет указывать в сторону самой полуплоскости.\nЧтобы найти расстояние от точки $(x_0, y_0)$ до прямой $Ax + By + C = 0$, можно воспользоваться следующей формулой:\n$$ d = \\frac{|Ax_0+By_0+C|}{\\sqrt{A^2+B^2}} $$\nОб этой формуле можно думать как о скалярном произведении вектора-точки на нормированный ($\\frac{1}{\\sqrt{A^2+B^2}}$) вектор нормали, геометрически равный проекции точки на него.\nЕсли же прямая задана 2 точками, то можно выразить высоту из формулы для площади треугольника:\n$$ A = \\frac{1}{2} bh \\implies h = \\frac{2A}{b} $$\nИ посчитать эту высоту так:\n$$ \\rho(P, L(A, B)) = \\frac{|\\overrightarrow{PA} \\times \\overrightarrow{PB}|}{|\\overrightarrow{AB}|} $$\nОбратите внимание, что в числителе стоит векторное произведение — мы воспользовались тем, что по модулю оно равно удвоенной площади треугольника $\\angle PAB$,\n#Точка пересечения прямыхНайти точку пересечения двух прямых — это то же самое, что и найти точку, которая удовлетворяет обоим условиям их уравнений:\n$$ \\begin{cases} A_1 x + B_1 y + C_1 = 0 \\ A_2 x + B_2 y + C_2 = 0 \\end{cases} $$\nЕсли выразить из обоих уравнений $x$ и приравнять, то получается\n$$ \\begin{cases} -x = \\frac{B_1 y + C_1}{A_1} \\ -x = \\frac{B_2 y + C_2}{A_2} \\end{cases} \\implies \\frac{B_1 y + C_1}{A_1} = \\frac{B_2 y + C_2}{A_2} \\implies y = - \\frac{A_1 C_2 - A_2 C_1}{A_1 B_2 - A_2 B_1} $$\nАналогично, $x = \\frac{B_1 C_2 - B_2 C_1}{A_1 B_2 - A_2 B_1}$ (обратите внимание на знаки).\nЗаметим, что знаменатель может оказаться нулем. Это означает, что векторное произведение векторов нормали нулевое, а значит прямые параллельны — тогда точек пересечения либо нет, либо, в случае совпадающих прямых, бесконечность. Этот случай нужно обрабатывать отдельно.\nПриведем несколько примеров конструктивного подхода.\n#Проекция точки на прямуюЧтобы спроецировать точку $P$ на прямую $L$, достаточно прибавить к ней нормальный вектор прямой, приведённый к длине $\\rho(P, L)$ и направленный от точки к прямой. Поскольку нормальный вектор может быть направлен в двух разных (но противоположных друг другу!) направлениях, то достаточно попробовать оба варианта, и принять тот, в котором расстояние от получившейся точки до прямой будет меньше.\n#Отражение от прямойПусть нам надо отразить точку $(x_0, y_0)$ симметрично относительно заданной прямой $ax+by+c=0$.\nЧисто в педагогических целях, начнём решать эту задачу как математики, чтобы никогда потом так не делать.\n$$ \\Pr_a b = \\frac{a \\cdot b}{|a|} \\frac{a}{|a|} = \\frac{|a| |b| \\cos \\alpha}{|a|} \\frac{a}{|a|} = |b| \\cos \\alpha \\frac{a}{|a|} $$\nГеометрический смысл: длина на единичный вектор направления.\nМы не хотим раскрывать эти формулы покоординатно и предъявлять готовый ответ. Мы знаем, что он получится громоздким. Нам не жалко посчитать всё по частям — здесь нет смысла заниматься оптимизациями. Также мы хотим делать всё по частям, потому что так становится более наглядной логика алгоритма, и, как следствие, его проще дебажить.\n// прямая r = at + b, точка c r pr(r a, r b, r c) { c -= b; // пусть c и a выходят из одной точки return b + (a * b / len(a) / len(a)) * a; } r reflect(r a, r b, r c) { return c + 2*(pr(a, b, c)-c); } ","id":69,"path":"/cs/geometry-basic/segments/","title":"Прямые и отрезки"},{"content":"Разреженная таблица (англ. sparse table) — структура данных, позволяющая отвечать на запросы минимума на отрезке за $O(1)$ с препроцессингом за $O(n \\log n)$ времени и памяти.\nОпределение. Разреженная таблица — это следующий двумерный массив размера $\\log n \\times n$:\n$$ t[k][i] = \\min { a_i, a_{i+1}, \\ldots, a_{i+2^k-1} } $$\nПо-русски: считаем минимумы на каждом отрезке длины $2^k$.\nТакой массив можно посчитать за его размер, итерируясь либо по $i$, либо по $k$:\n$$ t[k][i] = \\min(t[k-1][i], t[k-1][i+2^{k-1}]) $$\nИмея таком массив, мы можем для любого отрезка быстро посчитать минимум на нём. Заметим, что у любого отрезка имеется два отрезка длины степени двойки, которые пересекаются, и, главное, покрывают его и только его целиком. Значит, мы можем просто взять минимум из значений, которые соответствуют этим отрезкам.\nПоследняя деталь: для того, чтобы константа на запрос стала настоящей, нужно научиться считать сам логарифм за константу. Для этого можно воспользоваться доступной в GCC функцией __lg. Она внутри использует инструкцию clz (\u0026ldquo;count leading zeros\u0026rdquo;), которая присутствует в большинстве современных процессоров и возвращает количество нулей до первой единицы в бинарной записи, из чего за несколько процессорных тактов можно получить нужный округленный логарифм.\nint a[maxn], mn[logn][maxn]; int rmq(int l, int r) { // полуинтервал [l; r) int t = __lg(r - l); return min(mn[t][l], mn[t][r - (1 \u0026lt;\u0026lt; t)]); } // Это считается где-то в первых строчках main: memcpy(mn[0], a, sizeof a); for (int l = 0; l \u0026lt; logn - 1; l++) for (int i = 0; i + (2 \u0026lt;\u0026lt; l) \u0026lt;= n; i++) mn[l+1][i] = min(mn[l][i], mn[l][i + (1 \u0026lt;\u0026lt; l)]); Для больших таблиц порядок итерирования и расположение данных в памяти сильно влияет на скорость построения — это связано с работой кэшей. Но в большинстве случаев время построения не критично.\nУпражнение. Подумайте, в чём недостатки других 4 вариантов итерирования и layout-а.\n#ПримененияРазреженная таблица является статической структурой данных, то есть её нельзя дёшево обновлять (но можно достраивать на ходу — см. задачу «Антиматерия» с РОИ-2017).\nРазреженную таблицу часто применяют для решения задачи о наименьшем общем предке, так как её можно свести к RMQ.\n#2d Static RMQЭту структуру тоже можно обобщить на большие размерности. Пусть мы хотим посчитать RMQ на подквадратах. Тогда вместо массива t[k][i] у нас будет массив t[k][i][j], в котором вместо минимума на отрезах будет храниться минимум на квадратах тех же степеней двоек. Получение минимума на произвольном квадрате тогда уже распадется на четыре минимума на квадратах длины $2^k$.\nВ общем же случае от нас просят минимум на прямоугольниках $d$-мерного массива. Тогда делаем предподсчет, аналогичный предыдущему случаю, только теперь тут будет $O(n \\log^d n)$ памяти и времени на предподсчет — нужно хранить минимумы на всех гиперпрямоугольниках со сторонами степени двойки.\n#Ограничения на операциюРазреженную таблицу можно применять не только для минимума или максимума. От операции требуется только ассоциативность ($a ∘ (b ∘ c) = (a ∘ b) ∘ c$), коммутативность ($a ∘ b = b ∘ a$) и идемпотентность ($a ∘ a = a$). Например, её можно применять для нахождения $\\gcd$.\nЕсли операция не идемпотентна, то для нахождения её результата можно действовать так: возьмём самый длинный упирающийся в левую границу запроса отрезок, прибавим его к ответу, сдвинем указатель на его правый конец и будем так продолжать, пока не обработаем весь запрос целиком.\nint sum(int l, int r) { // [l, r) int res = 0; for (int d = logn - 1; d \u0026gt;= 0; d--) { if (l + (1 \u0026lt;\u0026lt; d) \u0026lt; r) { res += t[l][d]; l += (1 \u0026lt;\u0026lt; d); } } return res; } Это работает быстрее, чем, например, дерево отрезков, но тоже асимптотически за $O(\\log n)$, да ещё и с дополнительной памятью. Но есть способ это ускорить.\n#Disjoint Sparse TableМы хотим иметь какую-то структуру, которая может считать функцию $f$ на отрезке, при том что $f$ не удовлетворяет условию идемпотентности. Стандартная разреженная таблица тут не подойдёт — в ней нельзя найти $O(1)$ непересекающихся отрезков.\nСделаем следующее: мысленно построим на массиве дерево отрезков и (уже не мысленно) для каждого его отрезка $[l, r)$ посчитаем $f$ на всех отрезках от его центрального элемента — то есть от элемента с индексом $m = \\lfloor \\frac{l + r}{2} \\rfloor$ — до всех остальных элементов $k \\in [l, r)$. Для каждого элемента массива будет $O(\\log n)$ центральных, а значит суммарно на это потребуются те же $O(n \\log n)$ времени и памяти.\nУтверждение. Любой запрос $[l, r)$ разбивается на $O(1)$ непересекающихся предподсчитаных интервалов.\nДоказательство. Возьмем самый высокий центральный элемент $m$, принадлежащий запросу. Его отрезок полностью покрывает запрос — если бы это было не так, то самым высоким был бы не $m$, а какая-то из его границ . Раз отрезок запроса $[l, r)$ полностью покрыт, и $m$ лежит внутри него, то $[l, r)$ можно разбить на предподсчитаные $[l, m)$ и $[m, r)$.\nРешать задачу мы так и будем: найдём нужный центральный элемент и сделаем два запроса от него.\n#РеализацияСложная часть — найти этот центральный элемент за константное время — станет чуть проще, если мы будем работать только с массивами длины степени двойки и, соответственно, полными деревьями отрезков. Массивы неподходящей длины дополним до ближайшей степени двойки специальным нейтральным элементом, зависящим от самой операции (например, $0$ для сложения или $1$ для умножения).\nБудем хранить всю структуру (предподсчитаные значения на отрезках) в массиве t[logn][maxn], в котором первым параметром будет уровень в дереве отрезков (число $d$ для отрезков размера $2^d$), а вторым — граница соответствующего интервала (число $k$). Этой информации достаточно, чтобы однозначно восстановить отрезок.\nДля ответа на запрос нам достаточно найти только уровень нужного центрального элемента. Чтобы научиться делать это эффективно, нам понадобится немного поразмышлять о природе дерева отрезков.\nЗаметим, что любая вершина $k$-того уровня соответствует какому-то отрезку $[l, l + 2^k)$, причём $l$ делится на $2^k$. Двоичное представление всех индексов на этом отрезке будет иметь какой-то общий префикс, а последние $k$ знаков будут различными.\nНам нужно найти уровень нужного центрального элемента — это то же самое, что и уровень наименьшего общего отрезка для элементов $l$ и $r$. Используя предыдущий факт, получаем, что искомый уровень будет равен позиции самого значимого бита, который отличается у чисел $l$ и $r$. Его можно найти за константное время выражением $h_{[l,r)]} = \\lfloor \\log_2 (l \\oplus r) \\rfloor$, если заранее предпосчитать логарифмы.\nДля примера, построим DST для умножения по составному модулю:\nconst int maxn = (1 \u0026lt;\u0026lt; logn); int a[maxn], lg[maxn], t[logn][maxn]; const int neutral = 1; int f(int a, int b) { return (a * b) % 1000; } void build(int l, int r, int level = logn - 1) { int m = (l + r) / 2; int cur = neutral; for (int i = m + 1; i \u0026lt; r; i++) { cur = f(cur, a[i]); t[level][i] = cur; } cur = neutral; for (int i = m; i \u0026gt;= l; i--) { cur = f(cur, a[i]); t[level][i] = cur; } if (r - l \u0026gt; 1) { build(l, mid, level+1); build(mid, r, level+1); } } int rmq(int l, int r) { // [l, r) int level = lg[l ^ r]; int res = t[level][l]; // и, если правый отрезок не пустой: if (r \u0026amp; ((1 \u0026lt;\u0026lt; lg[l ^ r]) - 1))) res = f(res, t[level][r]); return res; } Примечание: очень вероятно, тут есть баги.\n","id":70,"path":"/cs/range-queries/sparse-table/","title":"Разреженная таблица"},{"content":"Определение. Целое положительное число называется простым, если оно имеет ровно два различных натуральных делителя — единицу и самого себя. Единица простым числом не считается.\nРешето Эратосфена (англ. sieve of Eratosthenes) — алгоритм нахождения всех простых чисел от $1$ до $n$.\nОсновная идея соответствует названию алгоритма: запишем ряд чисел $1, 2,\\ldots, n$, а затем будем вычеркивать\nсначала числа, делящиеся на $2$, кроме самого числа $2$, потом числа, делящиеся на $3$, кроме самого числа $3$, с числами, делящимися на $4$, ничего делать не будем — мы их уже вычёркивали, потом продолжим вычеркивать числа, делящиеся на $5$, кроме самого числа $5$, …и так далее.\nСамая простая реализация может выглядеть так:\nvector\u0026lt;bool\u0026gt; sieve(int n) { vector\u0026lt;bool\u0026gt; is_prime(n + 1, true); for (int i = 2; i \u0026lt;= n; i++) if (is_prime[i]) for (int j = 2 * i; j \u0026lt;= n; j += i) is_prime[j] = false; return is_prime; } Этот код сначала помечает все числа, кроме нуля и единицы, как простые, а затем начинает процесс отсеивания составных чисел. Для этого мы перебираем в цикле все числа от $2$ до $n$, и, если текущее число простое, то помечаем все числа, кратные ему, как составные.\nЕсли память позволяет, то для оптимизации скорости лучше использовать не вектор bool, а вектор char — но он займёт в 8 раз больше места. Компьютер не умеет напрямую оперировать с битами, и поэтому при индексации к vector\u0026lt;bool\u0026gt; он сначала достаёт нужный байт, а затем битовыми операциями получает нужное значение, что занимает приличное количество времени.\n#Время работыДовольно легко показать, что асимптотическое время работы алгоритма хотя бы не хуже, чем $O(n \\log n)$: даже если бы мы входили в цикл вычёркиваний для каждого числа, не проверяя его сначала на простоту, суммарно итераций было бы\n$$ \\sum_k \\frac{n}{k} = \\frac{n}{2} + \\frac{n}{3} + \\frac{n}{4} + \\ldots + \\frac{n}{n} = O(n \\log n) $$\nЗдесь мы воспользовались асимптотикой гармонического ряда.\nУ исходного алгоритма асимптотика должна быть ещё лучше. Чтобы найти её точнее, нам понадобятся два факта про простые числа:\nПростых чисел от $1$ до $n$ примерно $\\frac{n}{\\ln n}$ . Простые числа распределены без больших «разрывов» и «скоплений», то есть $k$-тое простое число примерно равно $k \\ln k$. Мы можем упрощённо считать, что число $k$ является простым с «вероятностью» $\\frac{1}{\\ln n}$. Тогда, время работы алгоритма можно более точнее оценить как\n$$ \\sum_k \\frac{1}{\\ln k} \\frac{n}{k} \\approx n \\int \\frac{1}{k \\ln k} = n \\ln \\ln k \\Big |_2^n = O(n \\log \\log n) $$\nАсимптотику алгоритма можно улучшить и дальше, до $O(n)$.\n#Линейное решетоОсновная проблема решета Эратосфена состоит в том, что некоторые числа мы будем помечать как составные несколько раз — столько, сколько у них различных простых делителей. Чтобы достичь линейного времени работы, нам нужно придумать способ, как рассматривать все составные числа ровно один раз.\nОбозначим за $d(k)$ минимальный простой делитель числа $k$ и заметим следующий факт: у составного числа $k$ есть единственное представление $k = d(k) \\cdot r$, и при этом у числа $r$ нет простых делителей меньше $d(k)$.\nИдея оптимизации состоит в том, чтобы перебирать этот $r$, и для каждого перебирать только нужные множители — а именно, все от $2$ до $d(r)$ включительно.\n#АлгоритмНемного обобщим задачу — теперь мы хотим посчитать для каждого числа $k$ на отрезке $[2, n]$ его минимальный простой делитель $d_k$, а не только определить его простоту.\nИзначально массив $d$ заполним нулями, что означает, что все числа предполагаются простыми. В ходе работы алгоритма этот массив будет постепенно заполняться. Помимо этого, будем поддерживать список $p$ всех найденных на текущий момент простых чисел.\nТеперь будем перебирать число $k$ от $2$ до $n$. Если это число простое, то есть $d_k = 0$, то присвоим $d_k = k$ и добавим $k$ в список $p$.\nДальше, вне зависимости от простоты $k$, начнём процесс расстановки значений в массиве $d$ — переберем найденные простые числа $p_i$, не превосходящие $d_k$, и сделаем присвоение $d_{p_i k} = p_i$.\nconst int n = 1e6; int d[n + 1]; vector\u0026lt;int\u0026gt; p; for (int k = 2; k \u0026lt;= n; k++) { if (p[k] == 0) { d[k] = k; p.push_back(k); } for (int x : p) { if (x \u0026gt; d[k] || x * d[k] \u0026gt; n) break; d[k * x] = x; } } Алгоритм требует как минимум в 32 раза больше памяти, чем обычное решето, потому что требуется хранить делитель (int, 4 байта) вместо одного бита на каждое число. Линейное решето хоть и имеет лучшую асимптотику, но на практике проигрывает также и по скорости оптимизированному варианту решета Эратосфена.\n#ПримененияМассив $d$ позволяет искать факторизацию любого числа $k$ за время порядка размера этой факторизации:\n$$ factor(k) = {d(k)} \\cup factor(k / d(k)) $$\nЗнание факторизации всех чисел — очень полезная информация для некоторых задач. Линейное решето интересно не своим временем работы, а именно этим массивом минимальных простых делителей.\n","id":71,"path":"/cs/factorization/eratosthenes/","title":"Решето Эратосфена"},{"content":"Для большого класса задач требуется решить следующую вспомогательную задачу.\nЗадача. Дано корневое дерево. Требуется отвечать на запросы нахождения наименьшего общего предка вершин $u_i$ и $v_i$, то есть вершины $w$, которая лежит на пути от корня до $u_i$, на пути от корня до $v_i$, и при этом самую глубокую (нижнюю) из всех таких.\nПо-английский эта задача называется least common ancestor — наименьший общий предок.\nВершина $i$ является LCA для вершин $k$ и $n$ Для лучшего понимания — медленно (за линейное время) наименьшего общего предка можно искать так:\nbool ancestor(int u, int v) { return tin[u] \u0026lt;= tin[v] \u0026amp;\u0026amp; tin[v] \u0026lt; tout[u]; } int lca(int u, int v) { while (!ancestor(u, v)) u = p[u]; return u; } Есть много самых разных способов её решать, и далее в этой главе мы рассмотрим основные. Конкретно в этой статье мы сведем её к задаче нахождения минимума на отрезке (и наоборот).\n#Сведение к Static RMQПройдёмся по дереву dfs-ом и выпишем два массива: глубины вершин и номера вершин. Записывать мы их будем как при входе в вершину, так и при выходе.\nПусть теперь поступил запрос: найти LCA вершин $v$ и $u$. Для определенности предположим, что $v$ в обходе встретилась раньше: $tin_v \u0026lt; tin_u$. Посмотрим на часть выписанного пути между моментом, когда мы вышли из $v$ и моментом, когда мы первый раз вошли в $u$. Так как любой простой путь между двумя вершинами в дереве единственный, где-то на этом отрезке мы должны были прийти в наименьший общий предок. При этом мы на этом пути точно не поднимались выше LCA, а значит LCA — это самая высокая вершина на этом пути.\nПолучается, что чтобы найти LCA, можно найти позицию минимума на отрезке $[tout_v, tin_u]$ в массиве глубин (первый выписанный массив) и посмотреть, какой вершине она соответствует в эйлеровом обходе (второй выписанный массив). Таким образом, задачу LCA можно свести к задаче RMQ (нахождению минимума на отрезке), что можно сделать, например, деревом отрезков за $O(\\log n)$ на запрос.\nАсимптотику времени запроса можно улучшить, используя тот факт, что мы на самом деле решаем задачу static RMQ, то есть у нас нет изменений массива. Для этого есть более подходящая структура — разреженная таблица, которая позволяет отвечать на запрос минимума за $O(1)$, но использует $O(n \\log n)$ операций и памяти на препроцессинг с малой константой.\nОказывается, можно свести и обратно.\n#Алгоритм Фарака-Колтона и БендераDisclaimer: алгоритм нахождения LCA, описываемый в этой секции, абсолютно бесполезен на практике, однако очень интересен с теоретической точки зрения.\nОказывается, что и LCA, и static RMQ можно считать за $O(1)$ времени на запрос и $O(n)$ времени на предподсчет.\nНа самом деле, в сведении LCA к RMQ, мы решаем не совсем полноценную задачу RMQ. Мы работаем не со всеми массивами целых чисел от 1 до $n$, а только с некоторыми — с теми, в которых любые два элемента отличаются ровно на единицу, потому что каждый переход это либо спуск, либо подъём в dfs. Это ограничение позволяет находить минимум на подотрезках подобных массивов быстрее.\n#ПредподсчетСделаем следующее: раз каждый элемента либо на единицу больше, либо на единицу меньше предыдущего, то сопоставим исходному массиву глубин булевый массив размера $(n - 1)$: на $i$-той позиции будет стоять единица, если следующее значение больше, и ноль если меньше. Этот массив нужно будет хранить в бинарном виде, чтобы можно было за константу получать булеву маску небольших подотрезков.\nПервая часть предподсчёта. Возьмем константу $k = \\lfloor \\frac{\\log n}{2} \\rfloor$, и разделим исходный массив на блоки по $k$ элементов. На каждом блоке посчитаем минимум, а над этими минимумами построим sparse table.\nВсего блоков таких блоков $O(\\frac{2 n}{\\log n})$, и поэтому построение работает за линейное время:\n$$ O(\\frac{2 n}{\\log n} \\log \\frac{2 n}{\\log n}) = O(\\frac{2 n}{\\log n} (\\log 2n - \\log \\log n)) = O(n) $$\nВторая часть предподсчёта. Посчитаем для каждой возможной маски подъёмов / спусков размера $k$ максимальный спуск на ней — то есть пройдёмся по ней, поддерживая разницу встретившихся нулей и единиц, и запомним минимальное значение этого баланса. Это можно сделать за длину маски, помноженное на их количество:\n$$ O(k \\cdot 2^k) = O(\\frac{\\log n}{2} 2^{\\frac{\\log n}{2}}) = O(\\sqrt n \\log n) $$\nВозможных масок получается немного — ради этого мы и делили логарифм на два при определении $k$\n#Ответ на запросТеперь нам нужно с помощью посчитанных структур найти RMQ на каком-то отрезке $[l, r]$. Он включает в себя какие-то последовательные блоки и сколько-то оставшихся ячеек слева и справа, не вошедших ни в какой целый блок.\nДля блочной части мы можем просто сделать запрос в sparse table — он будет работать за константу.\nДля обеих не-блочных частей посчитаем ещё по кандидату на ответ. Для этого нужно прибавить к граничному элементу предподсчитанное значение минимума на маске оставшихся неблочных элементов — её можно за константу получить битовыми операциями над булевым массивом.\nПросто посчитать все суффиксные и префиксные минимумы для всех блоков, чтобы обрабатывать второй случай, к сожалению, нельзя — есть один частный случай, когда запрос маленький и не накрывает никакой блок целиком. В данном случае нужно просто взять граничный элемент и прибавить к нему минимум от нужной маски из массива подъёмов.\n#Static RMQ → LCAЭтот алгоритм очень важен с теоретической точки зрения, потому что позволяет решать не только LCA, но и в общем случае static RMQ за линейный предподсчет и константу на запрос.\nПостроим декартово дерево, в котором в качестве ключей $x_i$ возьмём индексы элементов, а в качестве приоритетов $y_i$ возьмём сами значения. Декартово дерево могло получиться несбалансированным (так как нет рандомизации приоритетов), но это нам и не нужно. Дальше просто применим описанный выше алгоритм к этому дереву, и теперь для нахождения минимума в исходном массиве можно просто запросить общего предка $l$-той и $r$-той вершины в дереве — его приоритет в декартовом дереве и будет искомым минимумом.\nЧуть более подробно и с реализацией (автор это никогда не кодил и вам не советует) можно почитать у Емакса. Впрочем, на практике этот алгоритм использовать нецелесообразно из-за большой константы: слишком много чего нужно считать, чтобы избавиться от логарифма в асимптотике.\n","id":72,"path":"/cs/trees/lca-rmq/","title":"Связь задачи LCA и static RMQ"},{"content":"Пусть на $k$-ом шаге у нас уже отсортирован префикс длины $k$. Чтобы увеличить этот отсортированный префикс, мы можем взять элемент, следующий после него, и менять его с левым соседом, пока этот элемент не окажется больше своего левого соседа. Когда это произойдет, это будет означать, что он будет больше всех элементов слева и меньше всех элементов префикса справа, и значит мы правильно вставили этот элемент в отсортированную часть массива.\nvoid insertion_sort(int *a, int n) { for (int k = 1; k \u0026lt; n; k++) for (int i = k; i \u0026gt; 0 \u0026amp;\u0026amp; a[i - 1] \u0026lt; a[i]; i--) // мы ещё не дошли до начала массива и предыдущий элемент меньше swap(a[i], a[i - 1]); } Время работы в худшем случае тоже квадратичное — например, когда массив упорядочен по убыванию.\nОднако, в отличие от двух предыдущих квадратичных сортировок, в некоторых хороших случаях время работы может быть меньше, потому что мы добавили условие ранней остановки во внутреннем цикле. Например, алгоритм сделает $O(n)$ операций, если массив изначально отсортирован.\nУпражнение. Покажите, что алгоритм делает $O(n k)$ операций, если массив «почти отсортирован» в том смысле, что каждый элемент находится на расстоянии не более $k$ от его позиции в отсортированном массиве.\n","id":73,"path":"/cs/sorting/insertion/","title":"Сортировка вставками"},{"content":"В этой главе рассматриваются алгоритмы для неотсортированных последовательностей.\n","id":74,"path":"/cs/sequences/","title":"Последовательности"},{"content":"Общий подход к персистентности почти любой ссылочной структуры — это создавать копии всех вершин, в которых хоть что-то меняется, и менять это что-то в копиях, никогда не изменяя уже существующие вершины.\nПерсистентное дерево отрезков здесь не исключение. Просто будем при спуске создавать копию вершины перед тем, как что-либо менять в ней самой или её потомках. Асимптотика операций от этого не изменится, разве что общее потребление памяти увеличится до $O(m \\log n)$.\nУдобнее всего по аналогии с push в отложенных операциях определить функцию copy, копирующую детей текущей вершины, и просто без разбору вызывать её во всех вершинах, в которых что-то может измениться.\nstruct Segtree { int lb, rb; int s = 0; Segtree *l = 0, *r = 0; Segtree(int lb, int rb) : lb(lb), rb(rb) { if (lb != rb) { int t = (lb + rb) / 2; l = new Segtree(lb, t); r = new Segtree(t, rb); } } void copy() { if (l) { l = new Segtree(*l); r = new Segtree(*r); } } void add(int k, int x) { copy(); s += x; if (l) { if (k \u0026lt; l-\u0026gt;rb) l-\u0026gt;add(k, x); else r-\u0026gt;add(k, x); } } int sum(int lq, int rq) { // этот метод ничего не меняет -- он и так хороший if (lq \u0026lt;= lb \u0026amp;\u0026amp; rb \u0026lt;= rq) return s; if (max(lb, lq) \u0026gt;= min(rb, rq)) return 0; return l-\u0026gt;sum(lq, rq) + r-\u0026gt;sum(lq, rq); } }; Теперь осталось только создать список версий, и после каждой операции копировать туда новый корень:\nvector\u0026lt;Segtree*\u0026gt; roots; roots.push_back(new Segtree(0, n)); void add(int k, int x, int v) { Segtree *root = new Segtree(*roots[v]); root-\u0026gt;add(k, x); roots.push_back(root); } В качестве альтернативного подхода можно копировать не детей, а текущую вершину, и возвращать её из рекурсии и переподвешивать за родителя. Это вычислительно эффективнее, но нужно сильно изменить реализацию.\n#ПримененияВ контексте задач на структуры данных персистентное дерево отрезков особенно часто применяют в задачах, где применили бы метод сканирующей прямой, если бы запросы были известны заранее.\n#Сумма на прямоугольникеЗадача. Даны $n$ точек на плоскости. Нужно в онлайн ответить на $q$ запросов числа точек на прямоугольнике.\nЕсли бы можно было отвечать в оффлайн, мы бы разбили запрос на два запроса на префиксах и прошлись бы сканлайном слева направо, добавляя $y$-координаты точек в дерево отрезков и отвечая на запросы суммы на подотрезках — но так делать мы не можем.\nНо точки в оффлайн известны. Добавим их в таком же порядке в персистентное дерево отрезков, а не обычное, и сохраним корни деревьев с разными $x$-координатами в порядке увеличения.\nТеперь мы можем ответить на запрос, так же декомпозировав его на два и перенаправив их в две разные версии дерева отрезков, соответствующие большей и меньшей $x$-координатам. Таким образом, можно отвечать на запросы в онлайн за $O(q \\log n)$ времени и памяти.\n#Порядковые статистикиЗадача. Дан отрезок из $n$ целых чисел и $q$ запросов $k$-той порядковой статистики на подотрезке.\nВо-первых, сожмем все числа, чтобы они были от 1 до $n$.\nЗатем, пройдёмся с персистентным деревом отрезков для суммы по массиву слева направо, и когда будем обрабатывать элемент $a_k$, добавим единицу к $a_k$-ому элементу в дереве отрезков. Заметим, что сумма с $l$ по $r$ в таком дереве будет равна количеству элементов между $l$ и $r$ на текущем префиксе.\nДальше определим разность деревьев как дерево отрезков, которое получилось бы, если бы оно было построено поверх разности соответствующих массивов.\nЧто будет находиться в разности $r$-го и $l$-го дерева ($r \u0026gt; l$)? Там будут находиться количества вхождений чисел на отрезке массива с $l$ по $r$ — какой-то массив целых неотрицательных чисел. Если в таком разностном дереве сделать спуск, который находит последнюю позицию, у которой сумма на префиксе не превышает $k$ — она и будет ответом на запрос.\nЕсли не обязательно строить разностное дерево явно, чтобы делать по нему спуск — можно просто спускаться одновременно в двух деревьях и везде вместо суммы $s$ использовать $(s_r - s_l)$:\nint kth(Segtree *l, Segtree *r, int k) { if (k == 0) return l-\u0026gt;lb; int s = r-\u0026gt;l-\u0026gt;s - l-\u0026gt;l-\u0026gt;s; if (s \u0026lt;= k) return kth(l-\u0026gt;l, r-\u0026gt;l, k); else return kth(l-\u0026gt;r, r-\u0026gt;l, k - s); } Отметим, что эту и предыдущую задачу также можно решить через mergesort tree за $O(n \\log^2 n)$, а также двумерными структурами за такую же асимптотику.\n#Доминирующий элементЗадача. Дан массив из $n$ элементов. Требуется ответить на $m$ запросов, есть ли на отрезке $[l, r)$ доминирующий элемент — тот, который встречается на нём хотя бы $\\frac{r-l}{2}$ раз.\nУ этой задачи есть удивительно простое решение — взять около 100 случайных элементов для каждого проверить, является ли он доминирующим. Проверку можно выполнять за $O(\\log n)$, посчитав для каждого значения отсортированный список позиций, на которых он встречается, и сделав в нём два бинпоиска. Вероятность ошибки в худшем случае равна $\\frac{1}{2^{100}}$, и ей на практике можно пренебречь.\nНо проверять 100 сэмплов — долго. Построим такое же дерево отрезков, как в прошлой задаче, и решим задачу «найти число, большее $\\frac{n}{2}$ в массиве из $n$ неотрицательных целых чисел с суммой не более $n$» относительно разностного дерева. Это тоже будет спуском по нему: каждый раз идём в того сына, где сумма больше (потому что сын с меньшей суммой точно не доминирующий). Если в листе, куда мы пришли, значение больше нужного, возвращаем true, иначе false.\nУпражнение. Придумайте, как модифицировать спуск в задаче где доминирующим считается элемент, встречающийся хотя бы $\\frac{r-l}{k}$ раз для маленького $k$ (от 2 до 5).\n#Как персистентный массивС помощью дерева отрезков обычно и реализуют полностью персистентный массив — в общем случае быстрее $O(\\log n)$ на запрос не получается. Персистентный массив в свою очередь можно использовать, чтобы делать персистентными не-ссылочные структуры — например, систему непересекающихся множеств.\nВ СНМ мы храним всё состояние в массивах, которые можно просто сделать персистентными через дерево отрезков. Асимптотика такой структуры будет $O(n \\log n)$ времени и памяти — причем логарифм здесь и от самого СНМа (нужно пройтись по $O(\\log n)$ предков — эвристику сжатия путей мы использовать не можем), так и от персистентного ДО (обновить значение какого-то $p_v$ и $s_v$ / $h_v$).\n","id":75,"path":"/cs/persistent/persistent-segtree/","title":"Персистентное дерево отрезков"},{"content":"Абстрактно, изоморфизмом называют обратимое отображение (биекцию) между двумя множествами, наделенными какой-то структурой, с сохранением этой структуры.\nНапример, два графа называются изоморфными, если вершинам одного графа можно сопоставить вершины другого графа, так чтобы соединённым вершинам первого графа соответствовали соединённые вершины второго графа и наоборот. Иными словами, два графа изоморфны, если они «одинаковы» с точностью до переименования вершин.\nХороший общий способ выполнять проверки на изоморфизм — посчитать от них какую-то хеш-функцию, значение которой не изменяется от переименования объектов, и сравнить её значение.\n#МножестваПусть мы хотим научиться сравнивать множества чисел или строк на равенство с точностью до перестановки.\nНапример:\n$$ {1, 2, 3} = {2, 1, 3} \\ {\u0026ldquo;lal\u0026rdquo;, \u0026ldquo;abc\u0026rdquo;} = {\u0026ldquo;abc\u0026rdquo;, \u0026ldquo;lal\u0026rdquo;} \\ {4, 5} \\neq {5, 6} \\ {\u0026ldquo;lal\u0026rdquo;, \u0026ldquo;abc\u0026rdquo;} \\neq {\u0026ldquo;abc\u0026rdquo;, \u0026ldquo;all\u0026rdquo;} $$\nЭту задачу можно решить, если отсортировать это множество и посчитать полиномиальный хеш от получившейся последовательности. Результат не будет зависеть от изначальной перестановки элементов, и таким образом мы решим задачу за $O(n \\log n)$. Но задачу можно решить и за линейное время.\nСопоставим всем возможным ключам различные 64-битные случайные числа (будем называть их хешом ключа), а хешом множества тогда будем считать xor-сумму хешей всех входящих в него ключей. Тогда хеши равных с точностью до перестановки множеств будут одинаковые из-за коммутативности xor, а вероятность коллизии будет в точности равна $\\frac{1}{2^{64}}$.\nСопоставить хеши ключам можно либо хеш-функцией, либо, если количество различных значений ключей невелико, предподсчитанной таблицей случайных чисел. В обоих случаях подсчет хеша множества потребует подсчет $O(n)$ хешей ключей.\nЭтот метод также примечателен тем, что пересчет хешей от множеств можно проводить за $O(1)$ при определенных изменениях, таких как добавление элемента в множество, удаление элемента из множества или слияние двух непересекающихся множеств.\n#ИгрыПоследний метод можно обобщить для хеширования позиций в абстрактных играх вроде шахмат или шашек.\nБыстрый способ пересчета хеша позиций критически важен для реализации компьютерных стратегий для этих игр, потому что большинство из них основываются на переборе всех возможных ходов, который будет работать быстрее, если запоминать результаты и не запускать перебор дважды из одной и той же вершины. Для этого нужно завести хеш-таблицу, в которую нужно складывать хеши и посчитанные результаты от посещенных позиций, для чего в свою очередь нужно уметь их быстро пересчитывать.\nДерево дебютов «сицилианской защиты». В некоторые дебюты можно попасть более чем одним способом На примере шахмат, у нас есть $8 \\times 8 = 64$ поля, каждое из которых может быть в одном из $6 \\times 2 + 1 = 13$ возможных состояний (на нем может стоять король, ферзь, ладья, конь, слон или пешка одного из двух цветов или ничего). Сгенерируем $64 \\times 13 = 832$ случайных 64-битных чисел — каждое будет соответствовать факту, что на такой-то позиции стоит такая-то фигура. Теперь пройдемся по всем полям и посчитаем xor-сумму. Получившееся 64-битное число и будет хешом позиции.\nПомимо того, что его легко посчитать (64 раз посмотреть в lookup-таблицу быстрее, чем считать полиномиальные хеши по модулю), его ещё можно очень быстро пересчитывать, если изменение позиции было небольшим. А именно, если сделан один ход, то будут затронуты всего две ячейки, и хеш позиции нужно проксорить с ровно 4 числами (двумя старыми, чтобы их удалить, и двумя новыми, чтобы их добавить).\nС поправкой на возможность рокировки, взятие на проходе, симметрию цветов, невозможность поместить пешки на первый и последний ряд, и несколько других шахматных нюансов, мы описали то, что называется хешированием Зобриста.\n#ПодматрицыЗадача. Дана матрица $A_{i, j}$ размера $n \\times m$, состоящая из чисел. Требуется отвечать на запросы, равны ли подматрицы $A_{x_1:x_2, y_1:y_2}$ и $A_{x_3:x_4, y_3:y_4}$.\nЕсли посмотреть на матрицу как на набор строк, то сравнивать подматрицы это то же самое, что сравнивать набор подстрок на равенство, что мы уже умеем делать хешами. Если предпосчитать массив полиномиальных хешей от каждой строки матрицы и сравнивать хеши за константу, то каждый запрос будет работать за $O(n)$, что не очень быстро, но хотя бы не $O(n^2)$.\n#Двумерный хешДавайте обобщим идею полиномиального хеширования строк на матрицы. Для этого нам понадобится два разных $k$ — «горизонтальный» $k_h$ и «вертикальный» $k_v$. Положим теперь\n#$$ h(A_{x_1:x_2, y_1:y_2})\\sum_{i=x_1}^{x_2} \\sum_{j=y_1}^{y_2} A_{i,j} \\cdot k_h^i \\cdot k_v^j $$\nПредподсчитаем такие хеши для всех угловых подматриц (обобщение понятия префикс строки):\n$$ h(x, y) = h(A_{:x, :y}) $$\nЗаметим, что для всех угловых подматриц, которые содержат только первую строку или первый столбец, определение хеша совпадает с определением полиномиального хеша для строки, а значит такие хеши мы умеем предподсчитать за линию.\nТеперь заметим, что для всех остальных подматриц\n$$ h(x, y) = A_{x, y} k_h^x k_v^y + h(x - 1, y) + h(x, y - 1) - h(x - 1, y - 1) $$\nТаким образом, мы можем посчитать хеш за $O(n^2)$ таким же методом, как для двумерных префиксных сумм.\nТеперь мы можем выполнять проверку на равенство за $O(1)$, считая хеш от подматриц через префиксные суммы. Единственным нюансом остается то, что этот хеш будет «сдвинут» на $k_h^{x_1} \\cdot k_v^{y_1}$ — это можно исправить, либо поделив по модулю на него, либо домножив хеш от подматриц на $k_h^{n-x_1} \\cdot k_v^{m-y_1}$.\nПримечание. Можно определять не два разных $k_h$ и $k_v$, а одно, если мысленно соединить все строки матрицы в одну. Это эквивалентно тому, что сделать $k_h = k_v^m$.\n#Корневые деревьяТеперь мы хотим научиться проверять корневые деревья на изоморфизм, то есть на равенство с точностью до перенумерования вершин, при том, что известно, что корень одного дерева обязательно переходит в корень другого дерева.\n#Хеш от вершиныЗаметим, что поскольку мы не можем апеллировать к номерам вершин, единственная информация, которую мы можем использовать — это структура нашего дерева.\nПоложим тогда хешем вершины без детей какую-нибудь константу (например, 179), а для вершины с детьми положим в качестве хеша некоторую функцию от отсортированного (поскольку мы не знаем истинного порядка, в котором дети должны идти, нужно привести их к одинаковому виду) списка хешей детей.\nХешом корневого дерева будем считать хеш корня. Индукцией построению можно показать, что у изоморфных корневых деревьев хеши совпадают.\nЕдинственный нюанс заключается в том, что стандартный полиномиальный хеш для агрегации хешей от детей не подходит. Например, рассмотрим 2 дерева:\n$$ \\begin{aligned} T_1 \u0026amp;= { (1, 2), (1, 3), (1, 4) } \\ T_2 \u0026amp;= { (1, 2), (1, 3), (3, 4), (3, 5) } \\end{aligned} $$\nЕсли в качестве функции от детей взять полиномиальный хеш, то получим:\n$$ h(T_1) = 179 + 179p + 179p^2 = 179 + p(179 + 179p) = h(T_2) $$\nВ качестве неплохой хеш-функции подойдёт, например\n$$ h(v) = 42 + \\sum_u \\log(h(u)) $$\nДля этой хеш-функции может показаться, что можно не сортировать хеши детей, однако это не так, потому что при вычислении чисел с плавающей точкой у нас возникает погрешность, и чтобы это результат суммирования был одинаковый для изоморфных деревьев, суммировать детей надо тоже в одинаковом порядке.\nПример более сложной хеш-функции:\n$$ h(v) = \\sum_u h(u)^2 + h(u) \\cdot p^i + 42 \\bmod m $$\nВ любом случае, асимптотика будет $O(n \\log n$, так как всё что нам нужно делать в каждой вершине — это сортировка детей по хешу и суммирование. Так как у нас могут быть два изоморфных ребенка с одинаковым хешом, трюки с xor- и другими суммами не работают, и поэтому быстрее нельзя.\n#Некорневые деревьяДля некорневых деревьев можно найти центроид и запуститься от него как от корневого. Если центроидов два, то можно запуститься от двух центроидов, посчитать два хеша, упорядочить (свапнуть если hash_a меньше hash_b) и посчитать мини-хеш от них (hash = hash_a + k * hash_b).\n","id":76,"path":"/cs/hashing/isomorphism/","title":"Проверки на изоморфизм"},{"content":"Переформулируем лемму о безопасном ребре в частном случае:\nЛемма. Для любой вершины минимальное инцидентное ей реборо является безопасным.\nДоказательство. Пусть есть минимальный остов, в котором для какой-то вершины $v$ нет её минимального инцидентного ребра. Тогда, если добавить это ребро, образуется цикл, из которого можно удалить другое ребро, тоже инцидентное $v$, но имеющее не меньший вес.\nАлгоритм Борувки опирается на этот факт и заключается в следующем:\nДля каждой вершины найдем минимальное инцидентное ей ребро. Добавим все такие рёбра в остов (это безопасно — см. лемму) и сожмем получившиеся компоненты, то есть объединим списки смежности вершин, которые эти рёбра соединяют. Повторяем шаги 1-2, пока в графе не останется только одна вершина-компонента. Алгоритм может работать неправильно, если в графе есть ребра, равные по весу. Пример: «треугольник» с одинаковыми весами рёбер. Избежать такую ситуацию можно, введя какой-то дополнительный порядок на рёбрах — например, сравнивая пары из веса и номера ребра.\n#АсимптотикаЗаметим, что на каждой итерации каждая оставшаяся вершина будет задействована в «мердже». Это значит, что количество вершин-компонент уменьшится хотя бы вдвое, а значит всего итераций будет не более $O(\\log n)$.\nНа каждой итерации мы просматриваем почти все рёбра, так что итоговое время работы составит $O(m \\log n)$.\n#Зачем это нужно?Алгоритм неприятно реализовывать. Настолько неприятно, что автор это делать не будет. Однако, алгоритм очень полезен на практике, потому что в «реальных» графах он работает за линейное время.\nУтверждение. В случае планарных графов алгоритм работает за $O(n)$.\nДоказательство. Из формулы Эйлера нам известно, что рёбер в планарном графе $O(n)$. Так как подграф планарного графа тоже всегда планарен, то после каждой итерации размер нашей задачи уменьшается в честные 2 раза — меньше становится не только вершин, но и рёбер тоже. Значит, алгоритм будет работать за $O(n) + O(\\frac{n}{2}) + O(\\frac{n}{4}) + \\ldots = O(n)$.\nТакже, в отличие от алгоритмов Прима и Крускала, его можно легко распараллелить. «Параллельная сложность» у него $O(\\log^2 v)$: нужно каждую итерацию просто искать минимум по оставшимся рёбрам и мерджить списки смежности, что в свою очередь хоть и очень нетривиально, но можно сделать за $O(\\log v)$.\n","id":77,"path":"/cs/spanning-trees/boruvka/","title":"Алгоритм Борувки"},{"content":"Алгоритм Мо (кит. 莫隊算法) вероятно не был открыт, но точно был популяризован китайским спортивным программистом Мо Тао (莫涛) и его сокомандниками в конце нулевых годов.\nОн позволяет в оффлайн отвечать на самые разные запросы на подотрезках, которые часто невозможно решить даже самыми продвинутыми структурами данных.\nДля примера рассмотрим такую задачу: дан массив размера $n$ целых чисел от $1$ до $n$, и требуется отвечать на $q$ заранее известных запросов «количество различных чисел на отрезке $[l_i, r_i]$».\n#АлгоритмЕсли корневая эвристика по запросам группирует запросы по временным признакам, то алгоритм Мо — по пространственным.\nСгруппируем все запросы в блоки размера $c \\approx \\sqrt n$ по их левой границе и внутри каждого блока отсортируем запросы по правой границе:\n// границы и номер запроса, на который нужно ответить struct query { int l, r, idx; }; int a[maxn], ans[maxq]; // исходный массив и массив ответов на запросы vector\u0026lt;query\u0026gt; b[c]; // где-то в main: for (query q : queries) b[q.l / c].push_back(q); for (int i = 0; i \u0026lt; c; i++) { sort(b[i].begin(), b[i].end(), [](query a, query b){ return a.r \u0026lt; b.r; }); } Будем обрабатывать каждый такой блок запросов по отдельности, заведя следующие переменные:\nграницы $[l, r]$ текущего отрезка (изначально пустого: $l = i \\cdot c$ и $r = i \\cdot c - 1$); массив cnt, размера $n$, в котором для каждого значения хранится число элементов на текущем отрезке с данным значением (изначально заполнен нулями); переменная res, равная числу различных элементов на текущем отрезков, то есть числу ненулевых элементов массива cnt (изначально ноль). Массив cnt и переменную res легко пересчитать за $O(1)$ при расширении или сжатии границ на единицу:\nint cnt[maxn]; int res; void add(int k) { if (cnt[a[k]]++ == 0) res++; } void del(int k) { if (--cnt[a[k]] == 0) res--; } Будем медленно двигать границы влево и вправо таким образом, чтобы текущий отрезок в какой-то момент времени был равен каждому отрезку-запросу, и в этот момент времени мы ответим на запрос, записав res в нужную ячейку массива ответов ans.\nА именно, будем двигать правую границу текущего отрезка, пока она не станет равной правой границе очередного запроса (они отсортированы по правой границе), а затем будем двигать левую границу вправо или влево, пока она не совпадет с его левой границей:\nfor (int i = 0; i \u0026lt; c; i++) { // обнуляем переменные int l = i * c, r = i * c - 1; memset(cnt, 0, sizeof cnt); res = 0; for (query q : b[i]) { // пока правая граница не дошла до границы запроса while (r \u0026lt; q.r) add(++r); // дальше делаем так, чтобы левая граница совпала while (l \u0026lt; q.l) del(l++); while (l \u0026gt; q.l) add(--l); ans[q.idx] = res; } } Трюк в том, что правая граница суммарно сдвинется на $O(n)$, потому что отрезки отсортированы, а левая каждый раз сдвинется не более чем на $O(\\sqrt n)$, так как все левые границы сгруппированы по корневым блокам. Изменение левых границ суммарно по всем блокам займёт $O(q \\sqrt n)$ операций, а правых $O(n \\sqrt n)$, так что итоговая асимптотика решения будет $O(q \\sqrt n + n \\sqrt n)$.\n#ВариацииЗадача. Требуется находить наиболее часто встречающийся элемент на отрезке.\nМожно так же поддерживать массив cnt, и ещё помимо него вспомогательную внутреннюю структуру, в которой можно упорядоченно хранить пары (встречаемость, элемент) и доставать максимум. Для этого можно использовать двоичные структуры вроде set, хотя это добавит лишний логарифм в асимптотику.\nЧтобы решить задачу за чистый $O(n \\sqrt n)$, можно в качестве внутренней структуры завести массив двусвязных списков (встречаемость элемента → какие элементы имеют такую встречаемость), а также массив указателей-итераторов — для каждого элемента, где и в каком списке он сейчас находится. Тогда при добавлении / удалении элемента нужно его удалить из его текущего списка и вставить в более верхний / нижний, а при ответе на запрос просто взять какой-нибудь элемент из самого верхнего непустого списка.\nРешение можно упростить, обойдясь без структур вообще: будем поддерживать только массив cnt и сам ответ res (количество самого частого элемента), но при этом последний будем релаксировать только при добавлении элементов, а при удалении ничего делать не будем:\nvoid add(int k) { res = max(res, ++cnt[a[k]]); } void del(int k) { cnt[a[k]]--; // res тут не обновляется } Теперь в основной процедуре при откатывании левой границы будем откатывать res, просто запомнив его значение до обработки запроса (до начала расширения левой границы):\nfor (int i = 0; i \u0026lt; c; i++) { int l = i * c, r = i * c - 1; memset(cnt, 0, sizeof cnt); res = 0; for (query q : b[i]) { while (r \u0026lt; q.r) add(++r); int backup = res; while (l \u0026lt; q.l) del(l++); ans[q.idx] = res; while (l \u0026gt; q.l) add(--l); res = backup; } } Задача. Требуется находить $k$-ую порядковую статистику на отрезке.\nМожно хранить все числа из запроса в декартовом или любом другом дереве — например дереве отрезков для суммы или даже дереве Фенвика — и обновлять его и делать по нему спуск за $O(\\log n)$.\nЧтобы избавиться от логарифма, неожиданно можно использовать корневую декомпозицию как внутреннюю структуру. Нужно сжать все числа в промежуток $1$ до $n$, завести массив «сколько раз встречается число $x$» и поддерживать сумму для каждого его корневого блока. При обновлении нужно сделать $\\pm 1$ ровно в двух ячейках за $O(1)$, а при запросе $k$-той порядковой статистики пройтись сначала по $O(\\sqrt n)$ блокам, а потом по $O(\\sqrt n)$ элементам внутри одного блока.\nТак как запросов обновления во внутренней структуре в $O(n \\sqrt n)$, а запросов чтения всего $O(n)$, то асимптотика $O(\\sqrt n)$ на вторые ни на что не повлияет. Это довольно редкая ситуация, когда корневая оптимизации асимптотически побеждает дерево для ровно той же задачи — внутри другой корневой оптимизации.\nНа деревьях. В некоторых задачах требуется считать результаты операций на путях в дереве. Например: рядом с каждой вершиной есть число, и нужно возвращать количества различных значений на путях.\nИдея примерно такая же, как при сведении LCA к RMQ: если выписать эйлеров обход дерева (каждую вершину дважды), то задача сводится к задаче на массиве, только с одним исключением — мы должны добавлять в структуру только те элементы, которые содержатся на отрезке ровно 1 раз.\nПодробнее можно почитать в статье про запросы на деревьях.\n«3D Мо». Если очень захотеть, этот подход можно применять и в задачах, где надо обрабатывать запросы обновления. Для этого нужно ввести третий параметр get-запроса $t$, который будет равен числу update-запросов до текущего get («время»).\nСнова отсортируем все get-запросы, но на этот раз в порядке\n$$ \\left(\\lfloor \\frac{t}{n^{\\frac 2 3}} \\rfloor, \\lfloor \\frac{l}{n ^{\\frac{2}{3}}} \\rfloor, r\\right) $$\nи обработаем их таким же алгоритмом, только в трёх измерениях.\nЕдинственный нюанс — теперь при переключении между версиями массива надо будет менять ровно один элемент, и иногда менять его в структуре, если он принадлежал текущему отрезку.\nВремя работы такого подхода будет $O(n^{\\frac{5}{3}})$, что доказывается аналогично исходному алгоритму.\n","id":78,"path":"/cs/decomposition/mo/","title":"Алгоритм Мо"},{"content":"Корректной раскраской графа в два цвета называется такая раскраска, что никакое ребро не соединяет две вершины одного цвета. Графы, которые можно так раскрасить, называют двудольными.\nЗаметим, что если такая раскраска существует, и если зафиксировать цвет одной вершины, то все цвета всех достижимых из неё вершин определяются однозначно: пусть цвет этой вершины белый, тогда все её соседи будут иметь черный цвет, все вершины на расстоянии 2 будут иметь снова белый цвет, все вершины на расстоянии 3 снова черный, и так далее.\nПроверять граф на двудольность и выводить раскраску можно обходом в глубину. На этот раз наш dfs будет принимать параметром цвет, в который нужно покрасить вершину, и он будет рекурсивно запускаться от всех соседей, крася их в противоположный цвет. По окончании работы алгоритма мы либо обнаружим, что граф не двудолен (мы когда-то посмотрели на две соседние вершины, которым нужно присвоить один и тот же цвет), либо найдём разбиение вершин графа на две доли.\nconst int maxn = 1e5; int color[maxn]; void dfs(int v, int col) { color[v] = col; for (int u : g[v]) { if (!color[u]) { dfs(u, -col); } else if (color[u] != -col) { cout \u0026lt;\u0026lt; \u0026#34;Graph is not bipartite\u0026#34; \u0026lt;\u0026lt; endl; exit(0); } } } Так как граф может быть несвязным, его возможно придется запускать много раз от разных вершин:\nfor (int v = 0; v \u0026lt; n; v++) if (!color[v]) dfs(v, 1); Так как мы просмотрим каждую вершину и каждое ребро по одному разу, асимптотика будет $O(n + m)$.\n#Полезные утверждения про раскраски графовХроматическим числом графа называется минимальное число цветов, в которые его можно корректно раскрасить. В общем случае задача нахождения хроматического числа не решается за полиномиальное время.\nЛюбую карту можно представить как планарный граф, где страны — вершины, а границы — ребра Однако про раскраски можно доказать некоторые полезные утверждения, например:\nГраф можно раскрасить в два цвета — или, что эквивалентно, граф является двудольным — тогда и только тогда, когда все его простые циклы имеют чётную длину. Если степень любой вершины не превосходит $k$, то граф можно раскрасить в $k$ цветов. Любое планарный граф возможно раскрасить в 4 цвета. Рекомендуем в качестве упражнения доказать первые два утверждения; последнее же пока умеют доказывать только на компьютере.\n","id":79,"path":"/cs/graph-traversals/bipartite/","title":"Двудольные графы и раскраски"},{"content":"Эта статья — одна из серии. Рекомендуется сначала прочитать все предыдущие.\nВ качестве финальной оптимизации мы рассмотрим метод, вдохновленный методом множителей Лагранжа, также в олимпиадной среде называемый «лямбда-оптимизацией».\n#Модификация задачиРассмотрим немного измененную версию исходной задачи. Пусть нам нужно покрыть те же точки, но теперь нас не ограничивают жёстко в количестве отрезков, а просто штрафуют на какую-то константу $\\lambda$ за использование каждого дополнительного отрезка.\nНашу оптимизируемую функцию $g$ тогда можно выразить через $f$ следующим образом:\n$$ g[i] = \\min_{k \u0026lt; i} {f[i, k] + k \\cdot \\lambda } $$\nОднако её можно считать по более оптимальной формуле, не сводя к вычислению $f$:\n$$ g[i] = \\lambda + \\min_{k \u0026lt; i} {g[k] + (x_{i-1} - x_k)^2 } $$\nЭту динамику можно посчитать за $O(n)$: мы ровно это и делали в прошлой статье про Convex Hull Trick.\n#ИдеяНаблюдение 1. Если в оптимальном решении для $g_i$ мы для какого-то $\\lambda$ использовали ровно $k$ отрезков, то это же решение будет оптимальным и для $f[i, k]$.\nНаблюдение 2. Если уменьшать $\\lambda$, то оптимальное количество отрезков для $g_i$ будет увеличиваться.\nОсновная идея оптимизации: сделаем бинпоиск по $\\lambda$, внутри которого будем находить оптимальное решение для $g_i$ с заданным $\\lambda$. Если оптимальное $k^\\star$ получилось больше $k$, то следующая $\\lambda$ должна быть меньше, а в противном случае наоборот. Когда $k^\\star$ совпадёт с $k$, просто выведем чистую стоимость получившегося решения без штрафов.\nТаким образом, задача решается за $O(n \\log n + n \\log m)$, если сортировку точек для CHT делать заранее, а не внутри бинпоиска.\n#Реализацияpair\u0026lt;ll, int\u0026gt; dp[maxn]; // dp[i] - (ответ, число отрезков) void init() { for (int i = 0; i \u0026lt; maxn; i++) { dp[i] = make_pair(inf, 0); } } pair\u0026lt;ll, int\u0026gt; check(ll x) { // это можно соптимизировать init(); dp[0] = make_pair(0ll, 0); // 1-индексация for (int i = 1; i \u0026lt;= n; i++) { for (int j = 0; j \u0026lt; i; j++) { dp[i] = min(dp[i], {dp[j].first + cost[j + 1][i] + x, dp[j].second + 1}); } } return dp[n]; } ll solve() { ll l = -1e14; // границы надо подбирать очень аккуратно! ll r = 1; while (l + 1 \u0026lt; r) { ll mid = (l + r) / 2; pair\u0026lt;ll, int\u0026gt; x = check(mid); if (x.second \u0026gt;= k) l = mid; else r = mid; } pair\u0026lt;ll, int\u0026gt; result = check(l); return result.first - l * return.second; // вычитаем штрафы } Автор извиняется за качество реализации и призывать читателя её переписать и закомитить.\n#Существование решенияМы пропустили один нюанс: почему вообще существует такая $\\lambda$, что оптимальное $k^\\star = k$?\nВ общем случае возможно, что функция $k^\\star(\\lambda)$ через него «перескакивает», и это действительно проблема: одной лишь монотонности не достаточно, чтобы решать подобным образом произвольные задачи с ограничением на число объектов.\nНо в нашей задаче можно заметить следующее: функция $f[i, j]$ нестрого вогнутая (выпуклая вверх) по своему второму аргументу, то есть\n$$ f[i, j] - f[i, j-1] \\leq f[i, j+1] - f[i, j] $$\nИными словами, дополнительная «выгода» от добавления каждого следующего отрезка не увеличивается.\nТеперь, на выражение для модифицированной динамики\n$$ g[i] = \\min_{k \u0026lt; i} {f[i, k] + k \\cdot \\lambda } $$\nможно смотреть как на минимизацию скалярного произведение точек $(f[i, k], k)$ и вектора $(1, \\lambda)$.\nТак как верхняя огибающая точек $(f[i, k], k)$ выпукла, для каждой точки будет существовать $\\lambda$, которая упирается в него, и следовательно $k^\\star = k$ гарантированно найдется.\n","id":80,"path":"/cs/layer-optimizations/lagrange/","title":"Дискретный метод Лагранжа"},{"content":"Экстраполяцией называется построение математических моделей, предсказывающих значения вне некоторого известного промежутка — например, следующее значение по известным предыдущим.\nИнтерполяцией же называется приближенное нахождение промежуточных значений между известными.\nВ более узком смысле, в контексте алгебры, интерполяцией называется нахождение многочлена, проходящего через заданный набор точек $(x_i, y_i)$.\n#Интерполяционный многочлен ЛагранжаТеорема. Пусть есть набор различных точек $x_0, x_1, \\dots, x_{n}$. Многочлен степени $n$ однозначно задаётся своими значениями в этих точках.\nПримечание: коэффициентов у этого многочлена столько же, сколько и точек.\nПримеры интерполирующих многочленов для множеств из 2-5 точек Доказательство будет конструктивным — можно явным образом задать многочлен, который принимает заданные значения $y_0, y_1, \\ldots, y_n$ в этих точках:\n$$ y(x)=\\sum\\limits_{i=0}^{n}y_i\\prod\\limits_{j\\neq i}\\dfrac{x-x_j}{x_i-x_j} $$\nЭтот многочлен называется интерполяционным многочленом Лагранжа.\nКорректность. Проверим, что в точке $x_i$ значение действительно будет равно $y$:\nДля $i$-го слагаемого внутреннее произведение будет равно единице, если вместо $x$ подставить $x_i$: в этом случае просто перемножается $(n-1)$ единица. Эта единица помножится на $y_i$ и войдёт в сумму.\nДля всех остальных слагаемых произведение занулится: один из множителей будет равен $(x_i - x_i)$.\nУникальность. Предположим, есть два подходящих многочлена степени $n$: $A(x)$ и $B(x)$. Рассмотрим их разность. В точках $x_i$ значение получившегося многочлена $A(x) - B(x)$ будет равняться нулю. Если так, то точки $x_i$ должны являться его корнями, и тогда разность можно записать так:\n$$ A(x) - B(x) = \\alpha \\prod_{i=0}^n (x-x_i) $$\nдля какого-то числа $\\alpha$. Тут мы получаем противоречие: если раскрыть это произведение, то получится многочлен степени $n+1$, который нельзя получить разностью двух многочленов степени $n$.\n#Интерполяция на практикеПримечание. На практике интерполяцию решают методом Гаусса: её можно свести к решению линейного уравнения $aX = y$, где $X$ это матрица следующего вида:\n$$ \\begin{pmatrix} 1 \u0026amp; x_0 \u0026amp; x_0^2 \u0026amp; \\ldots \u0026amp; x_0^n \\ 1 \u0026amp; x_1 \u0026amp; x_1^2 \u0026amp; \\ldots \u0026amp; x_1^n \\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ 1 \u0026amp; x_n \u0026amp; x_n^2 \u0026amp; \\ldots \u0026amp; x_n^n \\ \\end{pmatrix} $$\nВажный факт — многочлен степени $n$ можно однозначно задать\nсвоими $(n+1)$ коэффициентами, значениями хотя бы в $(n + 1)$ точке, своими $n$ корнями (возможно, комплексными и дублирующимися) и коэффициентом $\\alpha$: $$ A(x) = \\alpha \\prod (x-x_i)^{k_i} $$\nПоследнее утверждение (что у любого многочлена существует $n$ комплексных корней) называется основной теоремой алгебры, и его доказательство выходит немного за пределы этой статьи.\n","id":81,"path":"/cs/algebra/interpolation/","title":"Интерполяция"},{"content":"Стандартный подход в динамическом программировании — создать массив для ответов на подзадачи и пройтись по нему циклом, пересчитывая неизвестные значения из известных. Однако, иногда сложно или вообще невозможно придумать такой порядок обхода, что все необходимые значения уже посчитаны.\nВ подобных случаях вместо циклов можно использовать подход, называемый мемоизацией: будем считать динамику рекурсивной функцией, в которой запоминается посчитанный результат и в следующий раз сразу возвращается, когда функция вызывается от тех же входных аргументов.\n#Простой примерРешим в таком стиле задачу о нахождении $n$-ого числа Фибоначчи.\nИзначально положим все $f[i] = -1$: это будет обозначать, что значение для соответствующего состояния еще не посчитано. Далее, положим $f[0] = 0$ и $f[1] = 1$ как базовые значения.\nТеперь напишем функцию-переход, в которая просто в самом начале проверяет, было ли уже посчитано искомое значение — и если нет, то рекурсивно его считает.\nint f[n] = {-1}; f[0] = 0; f[1] = 1; int g(int k) { if (f[k] == -1) f[k] = g(k - 2) + g(k - 1); return f[k]; } Время работы так же составит $O(n)$, так как каждое значение мы считаем только один раз, но реальное время работы будет в несколько раз больше, потому что константа на вызовы функции значительно выше, чем на простой цикл.\nТакже можно заметить, что в такой динамике мы гарантированно посещаем только действительно нужные состояния, что в некоторых задачах приводит к более оптимальной асимптотике.\n#КэшированиеВ более общем смысле, то, что мы делаем, называется кэшированием — запоминанием и переиспользованием промежуточных результатов. Это очень распространенная концепция в программировании, и современные языки программирования — особенно поддерживают — обычно поддерживают мемоизацию как встроенную оптимизацию.\nНапример, в Python есть декоратор cache, который делает ровно это:\nfrom functools import cache @cache def f(n): if n == 0: return 0 if n == 1: return 1 return f(n - 2) + f(n - 1) При этом этому декоратору не нужно знать о каких-либо ограничениях на $n$: он запоминает результаты для любых входных и выходных данных, которые можно положить как кортежи в хеш-таблицу.\nПопробуем подобное реализовать на C++.\n#Сложный примерРассмотрим задачу «Игра финансистов».\nИмеется массив $a_i$ из $n \\le 4000$ чисел, и два игрока по очереди берут из него числа, пытаясь максимизировать разницу свой суммы и суммы оппонента. На каждом ходу игрок может взять сколько-то крайних чисел с одной стороны — первый игрок слева, второй справа. При этом, если игрок на предыдущем ходе взял $k$ чисел, то его оппонент на следующем ходе может взять либо $k$, либо $(k + 1)$ чисел (на первом ходе можно взять 1 или 2).\nИгра завершается, когда игрок не может сделать ход.\nВведем следующую динамику: $f[l, r, k, p] = $ максимальная достижимая разность сумм игрока $p$ и его оппонента, если остались только элементы с $l$ по $r$， и на предыдущем ходу было взято $k$ чисел. Так как у нас есть всего два выбора — брать $k$ или $(k+1)$ чисел — то переход это просто максимум из этих двух возможностей:\n$$ f[l, r, k, 0] = \\max \\begin{cases} \\sum_{i=l}^{k} a_i + f[l + k, r, k, 1] \\ \\sum_{i=l}^{k + 1} a_i + f[l + k + 1, r, k + 1, 1] \\end{cases} $$\n(И аналогично для другого игрока $p=1$.)\nПопытаемся оценить, за сколько такое работает. Пересчет можно делать за $O(1)$ с помощью единожды посчитанных префиксных сумм исходного массива, значит асимптотика равна числу состояний. Но тут вроде как возникает проблема:\n$$ l \\in [1, n] \\ r \\in [1, n] \\ k \\in [1, n] \\ p \\in { 0, 1 } $$\nЕсли наивно хранить все состояния в четырехмерном массиве, то для него потребуется $O(n^3)$ ячеек, что нас не устраивает, так как $n=4000$. Оказывается, достижимых состояний сильно меньше.\nУтверждение 1. $k = O(\\sqrt n)$.\nЧтобы получить данное $k$, нужно, чтобы на каких-то предыдущих шагах были взяты $k, (k-1), \\ldots, 2, 1$ чисел, для чего размер массива $n$ должен быть не менее\n$$ n \\ge \\sum_{i=1}^x i \\approx \\frac{k^2}{2} $$\nИз чего следует, что $k = O(\\sqrt n)$.\nУтверждение 2. Для заданной левой границы $l$ существует не более $O(\\sqrt n)$ возможных правых границ $r$.\nРассмотрим разность $d$ количеств чисел, взятых первым и вторым игроком:\n$$ d = l - (n - r) $$\nТак как один игрок берет сколько-то элементов, а другой после этого либо берет столько же, либо на один больше, то за любые два последовательных хода $d$ изменится не более, чем на единицу. Более того, чтобы $d$ изменилось на единицу, необходимо, чтобы $k$ тоже увеличилось. Как мы уже установили, $k = O(\\sqrt n)$, а значит и $d = O(\\sqrt n)$. Так как по паре $(l, d)$ из определения выше восстанавливается $r$, получаем, что для каждой $l$ различных $r$ тоже будет $O(\\sqrt n)$.\nПолучается, что на самом деле у нас не $O(n^3)$, а $O(n \\cdot \\sqrt n \\cdot \\sqrt n \\cdot 2) = O(n^2)$ достижимых состояний. Значит, можно либо придумать какой-нибудь более умный формат хранения динамики и более умный обход, либо применить мемоизацию.\nТак как просто создать четырехмерный массив не влезает в память (даже если он на $O(n^{2.5})$ элементов), воспользуемся вместо этого хеш-таблицей. Проще всего определить её как unordered_map из int в int и найти какую-нибудь нумерацию кортежей $(l, r, k, p)$ в какой-нибудь помещающийся в int промежуток, и вместо тюпла из 4 чисел использовать этот номер.\nconst int N = 4000, K = 80; int n; int s[N + 1]; unordered_map\u0026lt;int, int\u0026gt; dp; long f(int l, int r, int k, bool p) { if ((r - l + 1) \u0026lt; k) return 0; int key = l * N * K * 2 + r * K * 2 + k * 2 + p; if (dp.count(key)) return dp[key]; return dp[key] = (p ? max( s[l + k] - s[l] - f(l + k, r, k, 0), s[l + k + 1] - s[l] - f(l + k + 1, r, k + 1, 0) ) : max( s[r + 1] - s[r - k + 1] - f(l, r - k, k, 1), s[r + 1] - s[r - k] - f(l, r - k - 1, k + 1, 1) ) ); } int main(){ dp.rehash(6.2e7); cin \u0026gt;\u0026gt; n; for (int i = 0; i \u0026lt; n; i++) { cin \u0026gt;\u0026gt; s[i + 1]; s[i + 1] += s[i]; } cout \u0026lt;\u0026lt; f(0, n - 1, 1, 1); return 0; } Если можно заранее оценить число элементов в хеш-таблице, то выгодно в самом начале зарезервировать место под неё через dp.rehash: это обычно ускоряет решение в 2-3 раза. Если написать свою хеш-таблицу, будет ещё в 3-4 раза быстрее.\n","id":82,"path":"/cs/general-dynamic/memoization/","title":"Ленивая динамика"},{"content":"Многоугольник определяется как часть плоскости, ограниченная какой-то замкнутой ломаной.\nРазные типы многоугольников Многоугольник называется простым, если граничная ломаная не имеет точек самопересечения. На картинке все многоугольники кроме последнего являются простыми.\nМногоугольник называется выпуклым, если для любых его двух точек, все точки на отрезке между ними тоже принадлежат многоугольнику. Первые два многоугольника являются выпуклыми.\nВыпуклый многоугольник называется правильным, если у него равны все стороны и все углы. Все правильные многоугольники также можно вписать в окружность, но обратное неверно.\nВ трёхмерном пространстве обобщением многоугольника является многогранник.\n#ПлощадиВ самом простом случае — для треугольника — площадь можно посчитать по готовым формулам через основание и высоту, а можно воспользоваться свойством векторного произведения:\n$$ V = \\frac{1}{2} (B-A) \\times (C-A) $$\nВ случае произвольного многоугольника, заданного последовательностью вершин в порядке обхода, можно поступить так: пройдемся по всем ребрам и для каждого добавим в сумму его ориентированную площадь треугольника, заданного этим ребром и началом координат.\nНа примере из картинки, площадь будет равна такой сумме векторных произведений:\n$$ S = - \\frac{1}{2} \\cdot (\\sum_{i=1}^{5} A_i \\times A_{i+1} + A_6 \\times A_1) $$\nВсе пары вершин, идущие по часовой стрелке, учтутся с отрицательным знаком, а против часовой — с положительным, и поэтому отменят лишнюю площадь.\nФормула верна в случае любых многоугольников без самопересечений, даже невыпуклых. Чтобы убедиться в этом, попробуйте зафиксировать какую-нибудь точку на плоскости и проверить, сколько раз и с какими знаками она учтется в сумме.\nОтметим, что знак итогового результата зависит от того, в каком порядке обходить многоугольник (чтобы про это не думать, можно взять его по модулю).\ndouble area(vector\u0026lt;r\u0026gt; p) { int n = (int) p.size(); double s = 0; for (int i = 0; i \u0026lt; n; i++) s += p[i] ^ p[(i + 1) % n]; return abs(s) / 2; } Также существует возможно более интуитивный, но и более громоздкий метод трапеций, где мы так же проходимся по всем ребрам и складываем ориентированные площади, но не треугольников относительно начала координат, а трапеций с основанием на оси $x$, в котором «нижние» трапеции аналогично отменяют лишнюю площадь «верхних».\nВне зависимости от метода подсчета площадей, заметим, что из формулы для площади треугольника следует, что если все координаты целочисленные, то площадь любой фигуры будет либо целым числом, либо рациональным с двойкой в знаменателе. Часто в задачах все входные данные целочисленные, и чтобы оставаться в целых числах, иногда имеет смысл умножить все входные числа на $2$ — тогда все площади будут целыми (и более того, чётными).\n#Принадлежность точкиОпять же, начнём с треугольников. Требуется определить, находится ли точка точка $P$ в треугольнике $ABC$, заданном против часовой стрелки. Тогда она должна лежать слева от всех трёх отрезков $AB$, $BC$ и $CA$. Это условие задаст пересечение трёх полуплоскостей, которое и будет нужным треугольником.\n$$ \\text{P лежит внутри ABC} \\iff \\begin{cases} (B-A) \\times (P-A) \\geq 0 \\ (C-B) \\times (P-B) \\geq 0 \\ (A-C) \\times (P-C) \\geq 0 \\ \\end{cases} $$\nПодобную проверку можно обобщить на случай выпуклых многоугольников. Для этого нужно пройтись по вершинам против часовой стрелки и проверить, что точка $P$ лежит «слева» от всех ребер.\nЧтобы проверить, что многоугольник выпуклый, можно пройтись по ребрам многоугольника и проверять векторным произведением, что мы «поворачиваем» всегда в одну сторону, то есть для всех последовательных точек $a$, $b$, $c$ проверить, что $(b-a)\\times(c-a) \u0026gt; 0$.\nВ постановке задачи, когда у нас многоугольник фиксирован, и поступают запросы проверки вхождения, задачу можно решить быстрее чем за линейное время.\n#Произвольный многоугольникВ более общем случае есть два популярных подхода, оба за $O(n)$.\nПервый заключается в подсчете углов. Пройдемся по всем вершинам в порядке обхода и будем последовательно рассматривать углы с вершиной в точке $P$ и лучами, проходящими через соседние вершины многоугольника. Если просуммировать эти ориентированные углы, то получится какая-то величина $\\theta$. Если точка $P$ лежит внутри многоугольника, то $\\theta = \\pm 2 \\pi$, иначе $\\theta = 0$.\nВторой заключается в подсчете, сколько раз луч, выпущенный из $P$, пересекает ребра многоугольника.\nЕсли из произвольной точки пустить любой луч, и если этот луч пересечет многоугольник, то он рано или поздно выйдет из этого многоугольника — потому что луч бесконечный, а многоугольник нет.\nЕсли посчитать число пересечений с многоугольником, то для точки, находящейся внутри, это число будет нечетным («наружу-внутрь-наружу», 3 пересечения), в противном случае — четным.\nЧтобы не обрабатывать отдельно случаи, когда луч пересекает вершину (сразу 2 ребра), его можно делать случайным — вероятность, что действительнозначное направление совпадет с конечным числом точек пренебрежимо мала.\n","id":83,"path":"/cs/geometry-basic/polygons/","title":"Многоугольники"},{"content":"Напомним, что циклом в графе $G$ называется ненулевой путь, ведущий из вершины $v$ в саму себя. Граф называют ацикличным, если в нем нет циклов.\nДля нахождения цикла, рассмотрим такой альтернативные способ делать обход в глубину:\nvoid dfs(int v, int p = -1) { for (int u : g[v]) if (u != p) dfs(u, v); } Здесь мы вместо массива used передаем в рекурсию параметр $p$, равный номеру вершины, откуда мы пришли, или $-1$, если мы начали обход в этой вершине.\nЭтот способ корректен только для деревьев — проверка u != p гарантирует, что мы не пойдем обратно по ребру, однако если в графе есть цикл, то мы в какой то момент вызовем dfs второй раз с одними и теми же параметрами и попадем в бесконечный цикл.\nЕсли мы можем определять, попали ли мы в бесконечный цикл, то это ровно то, что нам нужно. Модифицируем dfs так, чтобы мы могли определять момент, когда мы входим в цикл. Для этого просто вернем массив used обратно, но будем использовать его для проверки, были ли мы когда-то в вершине, которую мы собираемся посетить — это будет означать, что цикл существует.\nconst int maxn = 1e5; bool used[maxn]; void dfs(int v, int p = -1) { if (used[v]) { cout \u0026lt;\u0026lt; \u0026#34;Graph has a cycle\u0026#34; \u0026lt;\u0026lt; endl; exit(0); } used[v] = true; for (int u : g[v]) if (u != p) dfs(u, v); } Если нужно восстанавливать сам цикл, то можно вместо завершения программы возвращаться из рекурсии несколько раз и выписывать вершины, пока не дойдем до той, в которой нашелся цикл.\n// возвращает -1, если цикл не нашелся, и вершину начала цикла в противном случае int dfs(int v, int p = -1) { if (used[v]) { cout \u0026lt;\u0026lt; \u0026#34;Graph has a cycle, namely:\u0026#34; \u0026lt;\u0026lt; endl; return v; } used[v] = true; for (int u : g[v]) { if (u != p) { int k = dfs(u, v); if (k != -1) { cout \u0026lt;\u0026lt; v \u0026lt;\u0026lt; endl; if (k == v) exit(0); return k; } } } return -1; } Как и со всеми обходами, если в графе больше одной компоненты связности, или если граф ориентированный, то dfs нужно запускать несколько раз от вершин разных компонент.\nfor (int v = 0; v \u0026lt; n; v++) if (!used[v]) dfs(v); ","id":84,"path":"/cs/graph-traversals/cycle/","title":"Нахождение цикла"},{"content":"Рассмотрим такую модификацию задачи. Дан массив размера $n$, и требуется отвечать на запросы двух типов:\nЗаменить все значения на отрезке $[l, r)$ на $x$ (выполнить a[k] = x для всех $k$ от $l$ до $r$). Вывести сумму элементов $a_i$ на отрезке с $l$ по $r$. То есть теперь наш запрос обновления — это присвоение значения $x$ всем элементам некоторого отрезка $[l, r)$, а не только одному, и при этом мы всё ещё хотим обрабатывать оба запроса за $O(\\log n)$.\n#Основная идеяМы не хотим спускаться до каждого элемента, где меняется сумма — их может быть очень много.\nВместо этого мы схитрим, и при запросе присваивания будем по возможности «помечать» некоторые вершины, что они и все их дети должны быть равны какому-то числу $d_v = x$, но спускаться дальше и непосредственно выполнять эти присвоения мы не будем.\nНапример, если пришел запрос «присвой число $x$ на всем массиве», то мы вообще фактических присвоений нигде делать не будем — только оставим пометку в корне дерева, что все элементы в нем равны $x$.\nКогда нам позже понадобятся правильные значения таких вершин и их детей, то мы по пути от корня будем делать «проталкивание» информации из текущей вершины в её детей: если метка стоит, пересчитаем сумму текущего отрезка и передадим эту метку детям (не запускаясь рекурсивно от детей). Когда нам потом понадобятся поля детей, мы сделаем то же самое — пересчитаем в них сумму и передадим метку их детям, и так далее. Подобная операция будет гарантировать корректность данных в вершине ровно к тому моменту, когда они нам понадобятся.\nЭкономия работы получается за счет того, что когда мы передаем метку в ребенка, у которого уже есть какая-то метка, то она просто перезаписывается, и нам уже не нужно выполнять старые присвоения. Выигрыш откладывания работы в том, что какие-то из этих отложенных операций в будущем делать будет не нужно.\nОт использования таких «запаздывающих» обновлений асимптотика никак не ухудшается — мы просто делаем на $O(1)$ больше операций, выполняя одну отложенную операцию в каждой вершине на пути — так что мы всё так же отвечаем на все запросы за $O(\\log n)$.\n#РеализацияДобавим поле-метку assign и создадим вспомогательную функцию push, которая будет производить проталкивание информации из этой вершины в обоих её сыновей.\nstruct Segtree { int lb, rb; int sum = 0, assign = -1; // \u0026#34;-1\u0026#34; означает, что присвоения делать не нужно Segtree *l = 0, *r = 0; Segtree(int lb, int rb) : lb(lb), rb(rb) { if (lb + 1 \u0026lt; rb) { int t = (lb + rb) / 2; l = new segtree(lb, t); r = new segtree(t, rb); } } void push() { if (assign != -1) { sum = (rb - lb) * assign; if (l) { // если дети есть l-\u0026gt;assign = assign; r-\u0026gt;assign = assign; } } assign = -1; } }; Вызывать метод push стоит в самом начале обработки любого запроса — тогда он гарантирует, что в текущей вершине все значения корректны, а в сыновьях достаточно информации, чтобы при рекурсивном спуске и вызове push от них значения тоже стали корректными.\nvoid add(int lq, int rq, int x) { push(); if (lq \u0026lt;= lb \u0026amp;\u0026amp; rb \u0026lt;= rq) assign = x; else if (l \u0026amp;\u0026amp; max(lb, lq) \u0026lt; min(rb, rq)) { // если есть дети и отрезок запроса хоть как-то пересекается с нашим l-\u0026gt;add(lq, rq, x); r-\u0026gt;add(lq, rq, x); // ...дальше они сами разберутся } } int sum(int lq, int rq) { push(); if (lb \u0026gt;= lq \u0026amp;\u0026amp; rb \u0026lt;= rq) return s; if (max(lb, lq) \u0026gt;= min(rb, rq)) return 0; return l-\u0026gt;sum(lq, rq) + r-\u0026gt;sum(lq, rq); } По-английски эта техника называется lazy propagation. Общая идея «давайте будем всё делать в последний момент» применима не только в дереве отрезков, но и в других структурах и в реальной жизни.\n","id":85,"path":"/cs/segment-tree/lazy-propagation/","title":"Отложенные операции"},{"content":"Лемма Холла, также известная как теорема о свадьбах — очень удобный критерий в задачах, где нужно проверить, что паросочетание существует, но при этом не требуется строить его явно.\nЛемма Холла. Полное паросочетание существует тогда и только тогда, когда любая группа вершин левой доли соединена с не меньшим количеством вершин правой доли.\nДоказательство. В одну сторону понятно — если совершенное паросочетание есть, то для любого подмножества вершин левой доли можно взять вершины правой, соединенные с ним паросочетанием.\nВ другую сложнее — нужно воспользоваться индукцией. Будем доказывать, что если паросочетание не полное, то можно в таком графе найти увеличивающую цепь, и с её помощью увеличить паросочетание на единицу.\nБаза индукции: одна вершина из $L$, которая по условию соединена с хотя бы одной вершиной из $R$.\nИндукционный переход: пусть после $k \u0026lt; n$ шагов построено паросочетание $M$. Докажем, что в $M$ можно добавить вершину $v$ из $L$, не насыщенную паросочетанием.\nРассмотрим множество вершин $H$ — все вершины, достижимые из $x$, если можно ходить из правой доли в левую только по рёбрам паросочетания, а из левой в правую — по любым (мы такой граф по сути строим, когда ищем увеличивающую цепь в алгоритме Куна)\nТогда в $H$ найдется вершина $y$ из $R$, не насыщенная паросочетанием. Иначе, если такой вершины нет, то получается, что если рассмотреть вершины $H_L$ (вершины левой доли, насыщенные паросочетанием), то для них не будет выполнено условие, что $|H_L| \\leq |N(H_L)|$ (здесь $N(X)$ — множество вершин, соединенным паросочетанием с $X$).\nТогда должен существовать путь из $x$ в $y$, и он будет увеличивающим для паросочетания $M$, потому что из $R$ в $L$ мы всегда шли только по ребрам паросочетания. Проведя чередование вдоль этого пути, получим большее паросочетание, следовательно предположение индукции верно.\n","id":86,"path":"/cs/matching/hall/","title":"Теорема о свадьбах"},{"content":"Так же, как и с деревом отрезков, персистентной версией декартова дерева можно решать интересные задачи — особенно часто связанные со строками. Разберем сначала несколько примеров, а потом перейдем к реализации.\n#Примеры задачЗадача. Дана строка. Требуется выполнять в ней копирования, удаления и вставки в произвольные позиции.\nПостроим персистентное ДД поверх символов. Тогда просто вызвав два split-а, мы можем получить копию любой подстроки (указатель вершину), которую потом можно вставлять куда угодно, при этом оригинальную подстроку мы не изменим.\nЗадача. Дана строка. Требуется выполнять в ней копирования, удаления, вставки в произвольные позиции и сравнение произвольных подстрок.\nМожно в вершинах хранить полиномиальный хеш соответствующей подстроки. Тогда мы можем проверять равенство подстрок сравниванием хешей вершин, полученных теми же двумя сплитами.\nЧтобы сравнивать стоки на больше-меньше, а не только на равенство, можно стандартный трюк с бинарным поиском: перебрать длину совпадающего суффикса, и, когда она найдется, посмотреть на следующий символ.\nЗадача. Дана — не поверите — строка. Требуется выполнять в ней такие же операции изменения и после каждого запроса находить минимальный период строки, то есть такое минимальное число $k$, что из префикса длины $k$ повторением $\\frac{n}{k}$ раз получается вся строка.\nНаучимся сначала с помощью построенного дерева проверять, что $k$ является периодом (каким-нибудь, не обязательно минимальным). Выделим первые $k$ символов и посчитаем от них полиномиальный хеш $h_k$, а также хеш $h$ от всей строки. Если вся строка — это просто $\\frac{n}{k}$ записанных вместе копий префикса $k$, то хеш от неё должен быть равен:\n$$ h = h_k + h_k \\cdot p^k + h_k \\cdot p^{2k} + \\ldots + h_k \\cdot p^{n-k} $$\nСоответственно, мы можем выполнять проверку за $O(\\log n)$, посчитав выделением или спуском хеш от префикса длины $k$.\nТеперь мы можем перебрать разные кандидаты $k$ среди делителей $n$ и каждый за логарифм проверить. Можно и ещё быстрее, если заметить, что если $p$ является делителем какого-то периода, то $\\frac{n}{p}$ тоже будет периодом. Можно посмотреть на разложение $n$ и такой проверкой убирать по одному простому множителю за раз, пока не останется только факторизация минимального периода.\nТак как у числа $n$ не более $O(\\log n)$ множителей в факторизации, то решение будет работать за $O(\\log^2 n)$ на запрос (не считая факторизации — но если есть разумное ограничение на длину строки, то её можно предпосчитать).\n#РеализацияРеализация почти такая же, как и для всех персистентных структур на ссылках — перед тем, как идти в какую-то вершину, нужно создать её копию и идти в неё. Создадим для этого вспомогательную функцию copy:\nNode* copy(Node *v) { return new Node(*v); } Во всех методах мы будем начинать с копирования всех упоминаемых в ней вершин. Например, персистентный split начнётся так:\nPair split(Node *p, int x) { p = copy(p); // ... } В дереве отрезков просто создавать копии вершин было достаточно — этого обычно достаточно для всех детерминированных структур данных, но с декартовым деревом всё сложнее.\nСуществует тест, который «валит» приоритеты: можно раскопировать много версий одной вершины, а все остальные — удалить. Тогда у всех вершин будет один и тот же приоритет, и дерево превратится в «бамбук», в котором все операции будут работать за $O(n)$.\nУ этой проблемы есть очень элегантное решение — избавиться от приоритетов и делать следующее переподвешивание: если размер левого дерева равен $L$, а размер правого $R$, то будем подвешивать за левое с вероятностью $\\frac{L}{L+R}$, иначе за правое.\nТеорема. Такое переподвешивание эквивалентно приоритетам.\nДоказательство. Покажем, что все вершины всё так же имеют равную вероятность быть корнем. Докажем по индукции:\nЛист имеет вероятность 1 быть корнем себя (база индукции) Переход индукции — операция merge. Любая вершина левого дерева была корнем с вероятностью $\\frac{1}{L}$ (по предположению индукции), а после слияния она будет корнем всего дерева с вероятностью $\\frac{1}{L} \\cdot \\frac{L}{L+R} = \\frac{1}{L+R}$. С вершинами правого дерева аналогично. Получается, что при таком переподвешивании всё так же каждая вершина любого поддерева равновероятно могла быть его корнем, а на этом основывалось наше доказательство асимптотики декартова дерева.\nNode* merge(Node *l, Node *r) { if (!l) return r; if (!r) return l; l = copy(l), r = copy(r); if (rand() % (size(l) + size(r)) \u0026lt; size(l)) { // ... } else { // ... } } Философский вопрос: можно ли декартово дерево называть декартовым, если из него удалить и $x$, и $y$?\n","id":87,"path":"/cs/persistent/persistent-treap/","title":"Персистентное декартово дерево"},{"content":"Рассмотрим какую-нибудь задачу на перебор подмножеств, которую мы умеем решать за $O(2^n \\cdot poly(n))$, где $poly(n)$ — какой-то полином от размера задачи $n$. Метод meet-in-the-middle (дословно, «встреча в середине») позволяет соптимизировать перебор до $O(2^{n/2} \\cdot poly(n))$ в большом классе таких задач.\nВ качестве конкретного примера, рассмотрим задачу о рюкзаке — нужно выбрать подмножество $a_i$ с суммарным весом $w$:\nbool find_subset(int *a, int n, int w) for (int mask = 0; mask \u0026lt; (1 \u0026lt;\u0026lt; n); mask++) { int s = 0; for (int i = 0; i \u0026lt; n; i++) if (mask \u0026gt;\u0026gt; i \u0026amp; 1) s += a[i]; if (s == w) return true; } return false; } Здесь мы перебираем все подмножества и каждое проверяем за $O(n)$, что дает асимптотику $O(2^n \\cdot n)$.\nВ теории можно избавиться от проверки за $O(n)$, если перебирать маску рекурсивно и поддерживать текущую сумму на префиксе, возможно добавляя во время спуска только один элемент. Однако мы погонимся за более мощной оптимизацией.\n#РешениеРазделим массив на две части. Заметим, что искомое подмножество имеет какое-то количество элементов из левой половины и какое-то количество элементов из правой (возможно, нулевое). Попытаемся посчитать все суммы слева и справа по отдельности и найти пару, дающую нужную общую сумму.\nСначала посчитаем суммы для всех подмножеств среди первых $l = \\lfloor \\frac{n}{2} \\rfloor$ элементов и положим в хеш-таблицу:\nunordered_set\u0026lt;int\u0026gt; t; int l = n / 2; for (int mask = 0; mask \u0026lt; (1 \u0026lt;\u0026lt; l); mask++) { int s = 0; for (int i = 0; i \u0026lt; n; i++) if (mask \u0026gt;\u0026gt; i \u0026amp; 1) s += a[i]; t.insert(s); } Затем переберем все суммы среди оставшихся $r = n - l = \\lceil \\frac{n}{2} \\rceil$ элементов и для каждой попытаемся найти подходящую половину (с суммой $s_l = w - s_r$) через предподсчитанную хеш-таблицу:\nint r = n - l; for (int mask = 0; mask \u0026lt; (1 \u0026lt;\u0026lt; r); mask++) { int s = 0; for (int i = 0; i \u0026lt; r; i++) if (mask \u0026gt;\u0026gt; i \u0026amp; 1) s += a[l + i]; if (t.count(w - s)) return true; } Обе фазы (а значит и сам алгоритм) работают за $O(2^{n/2} \\cdot n)$: мы перебираем $2^{n/2}$ подмножеств и для каждого за $O(n)$ считаем сумму и делаем запрос добавления / проверки наличия в хеш-таблицу за $O(1)$.\nЗаметим, что оба перебора всё ещё можно так же соптимизировать в $O(n)$ раз через трюк с рекурсией.\n#Другие примерыЗадача. Найти количество чисел до $10^{14}$ с суммой цифр $s$.\nРазделим число на левую часть (старшие 7 разрядов) и правую (младшие 7 разрядов). Для всех чисел левой части предподсчитаем, сколько из них имеют сумму $s$ и запишем это в массив предподсчёта (его размер будет $7 \\times 9 + 1 = 64$). Затем переберем правую часть, и тогда для «левой» суммы $s$ нам нужно найти количество правых частей с суммой $(w - s)$, для чего мы просто за $O(1)$ обращаемся к предподсчитанному массиву.\nНа больших ограничениях эта задача решается через динамику по цифрам.\nЗадача. Дан граф из $n$ вершин. Нужно найти количество клик — подграфов, в котором все вершины связаны со всеми.\nСначала научимся решать задачу полным перебором. Пусть у нас есть матрица смежности графа. Как быстро проверить, что подмножество вершин $m$ является кликой?\nЗа $O(n^2)$ можно пройтись по всем парам включенных вершин и для каждой проверить, есть ли единичка в матрице смежности. Проверку можно соптимизировать до $O(n)$, посчитав маску $m\u0026rsquo;$, равную побитовому «И» строчек матрицы смежности, соответствующих вершинам $m$. Теперь, если $m$ является подмножеством $m\u0026rsquo;$, то есть\n$$ m ; \u0026amp; ; m\u0026rsquo; = m $$\nто подграф $m$ является кликой: для всех его вершин есть ребро из всех других.\nВоспользуемся этим трюком для слияния ответов в meet-in-the-middle. Разделим граф на две части, найдем для левой все клики и пометим их маски единицами в специальном массиве is_clique[mask] размера $2^{n/2}$.\nТеперь будем перебирать подграфы второй половины, и для каждой клики нам нужно найти количество клик левой половины, являющихся подграфами пересечения списков смежности для правой половины ($m\u0026rsquo;$ из проверки выше).\nЧтобы сделать это быстро, предподсчитаем поверх массива is_clique динамику «как много подмасок данной маски являются кликами». Эту динамику можно посчитать за $O(2^{n/2})$, если для каждой маски $m$ рассмотреть два варианта — когда первая вершина $v$ включена в клику и когда не включена:\n$$ f[m] = f[m ; \u0026amp; ; g_v \\oplus 2^v] + f[m \\oplus 2^v] + is_clique[m] $$\nИтоговая асимптотика алгоритма будет $O(2^{n/2} \\cdot n)$.\nЗадача. Неявно задан очень большой невзвешенный граф, в котором у каждой вершины не более $k$ переходов. Требуется найти расстояние от $s$ до $t$, если известно, что оно не превышает $n$.\nЗапустим обход в ширину одновременно из обеих вершин, записывая рядом с вершинами расстояния от $s$ или $t$ найденные соответствующим обходом. Когда обход с одной стороны зайдет в вершину, посещенную другим обходом, то восстанавливаем путь размера $(d_s + d_t)$ и завершаемся.\nТак как расстояние не превосходит $n$, то алгоритм гарантированно завершится за $O(\\frac{n}{2})$ «волн». Так как каждая вершина связана с не более $k$ другими, и в «родителя» алгоритм не заходит, на следующей волне будет не более чем в $(k-1)$ раз больше вершин, чем на предыдущей.\nЗначит суммарно будет посещено $O((k-1)^{n/2})$ вершин, чему и будет равна асимптотика алгоритма.\n","id":88,"path":"/cs/decomposition/mitm/","title":"Meet-in-the-middle"},{"content":"В этой главе представлены общие способы решения задач путем разбиения на более простые.\n","id":89,"path":"/cs/decomposition/","title":"Декомпозиция задач"},{"content":"Рассмотрим следующую игру. Даны $n$ кучек, в каждой из них какое-то количество камней. За один ход игрок может выбрать кучку и выбросить оттуда любое ненулевое число камней. Выигрывает тот игрок, который забрал последний камень.\nПри оптимальной игре в ниме на кучках размера 3, 4 и 5 выигрывает первый игрок Немного переформулируем условие. Состояние игры однозначно описывается неупорядоченным набором неотрицательных чисел — пронумеруем их и обозначим количество камней в $i$-й кучке как $a_i$. Теперь, за один ход разрешается строго уменьшить любое из чисел. Терминальное состояние — когда все числа стали нулями.\nТеорема. Состояние игры выигрышное тогда и только тогда, когда xor-сумма размеров кучек отлична от нуля:\n$$ S = a_1 \\oplus a_2 \\oplus \\ldots \\oplus a_n \\ne 0 $$\nДоказательство проведём по индукции.\nДля терминального состояния кучек уже нет, и xor-сумма равна нулю, и оно действительно проигрышное. База доказана — теперь докажем переходы.\nИз состояния с нулевой xor-суммой все переходы ведут в выигрышные состояния, то есть в состояния с ненулевой суммой. В самом деле, достаточно убрать сколько угодно спичек из любой кучки — xor-сумма изменится с нуля на $a_i \\oplus b_i $, где $b_i \u0026lt; a_i$ равно числу камней в $i$-й кучке после нашего действия.\nПротивоположный случай сложнее. Нужно показать, что если xor-сумма ненулевая, то всегда существует такой $b_i \u0026lt; a_i$, что xor-сумма станет нулевой, то есть\n$$ S \\oplus a_i \\oplus b_i = 0 $$\nДля этого посмотрим на старший единичный бит $S$ и возьмём любой $a_i$, у которого этот бит тоже единичный. Такой $a_i$ найдётся хотя бы один — по свойствам xor, их должно быть нечетное число. Из условия выше следует, что искомый $b_i$ должен быть равен $S \\oplus a_i$.\nВыясняется, что это корректный новый размер кучки, то есть $b_i \u0026lt; a_i$. Почему так? Потому что все старшие биты в выражении остались нетронутыми, $k$-й бит изменился на единицу, а что происходило с дальнейшими битами нам не важно, потому что эти изменения точно не больше, чем $2^k$.\nПолучается, что оптимальная стратегия такая: посчитать xor-сумму всех $a_i$, найти такой $a_i$, у которого старший бит взведен, и заменить его на $S \\oplus a_i$. (А если xor-сумма $S$ оказалась нулевая, то сдаться.)\n#Модификации #Ним в поддавки (misère nim)В противоположность обычному ниму, существует также «ним в поддавки»: когда игрок, совершивший последний ход, не выигрывает, а проигрывает.\nРешение такого нима удивительно просто. Выигрышность позиции определяется так же, как и в обычном ниме — xor-суммой размеров кучек — но с одним исключением: если размеры всех кучек равны единице, то выигрышность и проигрышность меняются местами.\nС учетом этого исключения, будем делать ходы как в обычном ниме, переходя в позицию с нулевой xor-суммой, но если такой ход ведёт в позицию, в которой размеры всех кучек равны единице, то этот ход надо изменить так, чтобы количество остающихся непустых кучек изменило свою чётность.\nДоказательство. Рассмотрим некоторое течение игры: выберем произвольную стартовую позицию и выпишем ходы игроков вплоть до завершения игры. В любой игре двух оптимальных игроков рано или поздно наступает момент, когда размеры всех непустых кучек равны единице. Обозначим через $k$ число непустых кучек в этот момент — тогда для текущего игрока эта позиция выигрышна тогда и только тогда, когда $k$ чётно.\nОткатимся теперь на один ход назад. Мы оказались в позиции, где ровно одна кучка имеет размер $a_i \u0026gt; 1$, а все остальные кучки (возможно, их было ноль) имеют размер $1$. Эта позиция выигрышна, так как мы всегда можем сделать такой ход, после которого останется нечетное число кучек размера $1$:\nЕсли всего непустых кучек нечетное количество, то заменяем $a_i$ на $1$. В противном случае, заменяем $a_i$ на $0$. Далее, если продолжим откатываться по игре назад, то для всех состояний выигрышность также будет совпадать с «нормальным» нимом — просто потому, что когда у нас есть более одной кучки размера $\u0026gt;1$, то все переходы ведут в состояния с одной и более кучкой размера $\u0026gt;1$, а для всех них, как мы уже показали, ничего по сравнению с «нормальным» нимом не изменилось.\nТаким образом, изменения в ниме «в поддавки» затрагивают только состояния, когда все кучки имеют размер, равный единице — что и требовалось доказать.\n#Ним Мура (k-ним)Условие. Есть $n$ кучек камней размера $a_i$. Также задано натуральное число $k$. За один ход игрок может уменьшить размеры от одной до $k$ кучек (то есть теперь разрешаются одновременные ходы в нескольких кучках сразу). Проигрывает тот, кто не может сделать хода.\nОчевидно, при $k=1$ ним Мура превращается в обычный ним.\nРешение. Запишем размер каждой кучки в двоичной системе счисления. Затем просуммируем эти нули и единицы вдоль каждого разряда и возьмём эту сумму по модулю $(k+1)$. Если во всех разрядах получился ноль, то текущая позиция проигрышная, иначе — выигрышная.\nДоказательство, как и для обычного нима, заключается в описании стратегии игроков — нужно показать, что\nиз игры с нулевым значением мы можем перейти только в игры с ненулевыми значениями, и из любой игры с ненулевым значением найдется ход в игру с нулевым значением. Первый пункт доказывается следующим рассуждением. Если для всех разрядов сумма бит по модулю $(k+1)$ была равна нулю, то после изменения от одного до $k$ элементов снова получить нулевую сумму можно только «уравновешиванием» всех изменений: для всех элементов, где определенный бит удаляется, должен быть другой элемент, где этот бит добавляется. В частности, это должно выполняться и для самого старшего разряда, для которого хотя бы один бит у любого числа меняется. Но тогда это будет означать, что существует какой-то элемент, для которого добавилась единица в этом разряде, а все более старших разрядах ничего не менялось — значит, этот элемент будет больше исходного, что нарушает правила.\nОсталось научиться переходить из состояний с ненулевыми суммами в нулевые. Назовём проблемными те позиции, для которых сумма битов получилась ненулевой. Переберем все биты от старшего к младшему и будем по очереди делать каждую проблемную сумму нулевой, уменьшая какое-то количество элементов.\nОбозначим за $u$ количество кучек, которые мы уже начали изменять; изначально, $u = 0$. Обратим внимание, что так в этих $u$ кучках мы уменьшали какой-то из предыдущих, более старших, битов, то все более младшие биты мы можем ставить как угодно.\nПусть мы рассматриваем текущий бит, в котором сумма по модулю $(k+1)$ получилась ненулевой. В идеале, мы хотим сделать её нулевой, изменяя в этом разряде только те $u$ элементов, которые мы и так уже собрались изменять. Мысленно поставим во всех из этих $u$ элементов единицы в соответствующем разряде и пересчитаем сумму, обозначив её за $s$. Теперь рассмотрим два случая:\nЕсли $s \\le u$, то мы можем обойтись уже выбранными элементами, убрав в $s$ из них единичный бит. В противном случае, если $s \u0026gt; u$ найдем $(s - u)$ дополнительных кучек, у которых рассматриваемый бит единичный, и будем уменьшать их вместе с $u$ уже имеющимися. После каждой итерации число $u$ изменяемых кучек станет равным $u\u0026rsquo; = \\max(s, u) \\le k$.\nТаким образом, мы показали способ выбирать множество изменяемых кучек и какие биты следует в них изменять, чтобы общее их количество $u$ никогда не превысило $k$. Следовательно, мы доказали, что искомый переход из состояния с ненулевой суммой в состояние с нулевой суммой всегда существует, что и требовалось доказать.\n#Зачем это вообще надо?Цугцвангом (от немецкого zug — ход, и zwang — принуждение) в шахматах и многих других стратегических играх называют ситуацию, когда у игрока закончились хорошие ходы, и если бы правила позволяли, он бы просто стоял на месте и передал ход оппоненту.\nХод белых. Мат в 2 хода. Выясняется, что игр, основанных на идее приведения противника к цугцвангу, очень много, и они все эквивалентны ниму — в том смысле, что их графы с точки зрения выигрышности суммы отдельных игр работают точно так же, как графы нима.\nВ дальнейших статьях мы разберемся, как сводить подобные игры к ниму.\n","id":90,"path":"/cs/games/nim/","title":"Игра «Ним»"},{"content":"В 1960-м году Андрей Колмогоров вместе с другими пионерами советской информатики собрались на научном семинаре и выдвинули «гипотезу $n^2$»: невозможно перемножить два $n$-значных числа, быстрее, чем за $O(n^2)$. Это подразумевает, что умножение «в столбик», придуманное шумерами как минимум четыре тысячи лет назад и никем на тот момент не побитое, является асимптотически оптимальным алгоритмом умножения двух чисел.\nЧерез неделю 23-летний аспирант Анатолий Карацуба предложил метод умножения с оценкой времени работы $O(n^{\\log_2 3})$ и тем самым опроверг гипотезу. В этой статье мы и рассмотрим этот метод.\n#АлгоритмОсновная идея алгоритма очень простая: в нём умножение двух чисел длины $n$ небольшим алгебраическим трюком сводится к трём умножениям чисел длины $\\frac{n}{2}$. Число элементов на каждом уровне рекурсии будет расти, но на самом нижнем будет суммарно всего $O(n^{\\log_2 3})$ элементов, чем и объясняется такая странная асимптотика.\nВ нашей версии алгоритма мы будем перемножать не числа, а многочлены. Это эквивалентная задача — если вместо $x$ подставить основание системы счисления, то в качестве коэффициентов можно взять последовательность цифр числа:\n$$ \\begin{aligned} A(x) \u0026amp;= a_0 + a_1\\cdot x + a_2 \\cdot x^2 + \\dots + a_n \\cdot x^n \\ \u0026amp;= a_0 + a_1\\cdot 10 + a_2 \\cdot 10^2 + \\dots + a_n \\cdot 10^n \\end{aligned} $$\nИтак, пусть у нас есть два многочлена $a(x)$ и $b(x)$ равной длины $n = 2k$ и мы хотим их перемножить. Разделим их коэффициенты на две равные части и представим как\n$$ a(x) = a_1 (x) + x^k a_2(x) \\b(x) = b_1 (x) + x^k b_2(x) $$\nТеперь рекурсивно вычислим многочлены-произведения $p_1$ и $p_2$:\n$$ p_1(x) = a_1(x) \\cdot b_1(x) \\ p_2(x) = a_2(x) \\cdot b_2(x) $$\nА также многочлен $t$:\n$$ t(x) = ( a_1(x) + a_2(x) ) \\cdot (b_1(x) + b_2(x)) $$\nРезультат умножения исходных многочленов (многочлен размера $2n$) теперь можно посчитать по следующей формуле — внимание, алгебра:\n$$ c(x) = a(x) \\cdot b(x) = p_1(x) + x^k \\cdot (t(x) - p_1(x) - p_2(x)) + x^{2k} \\cdot p_2(x) $$\nКорректность формулы можно проверить, просто выполнив нужные подстановки.\n#АнализЕсли посчитать необходимые операции, то выясняется, что для перемножения двух многочленов размера $n$ нам нужно посчитать три произведения — $p_1$, $p_2$ и $t$ — размера $\\frac{n}{2}$ и выполнить константное количество сложений, вычитаний и сдвигов (домножений на $x^k$), которые суммарно можно выполнить за $O(n)$.\nМастер-теорема утверждает, что в данном случае асимптотика всего алгоритма будет $\\Theta (n^{\\log_2 3}) \\approx \\Theta (n^{1.58})$: наша задача разбивается на $a = 3$ части в $b = 2$ раз меньшего размера, а объединение происходит за $O(n)$.\n#РеализацияДля простоты будем предполагать, что $n$ это степень двойки. Если это не так, то в зависимости от обстоятельств это можно исправить одним из двух костылей:\nМожно дополнить коэффициенты многочлена нулями до ближайшей степени двойки — в худшем случае это будет работать в $2^{1.58} \\approx 3$ раза дольше. Можно «отщепить» последний коэффициент от многочленов и свести задачу размера $(2k + 1)$ к задаче размера $2k$ и константному количество сложений. Мы будем использовать первый метод, так как он проще в реализации.\nОсновные соображения по поводу эффективной реализации:\nНужно выделять как можно меньше лишней памяти, для чего нужно переиспользовать имеющиеся массивы. Все арифметические операции нужно реализовать как простые линейные проходы по массивам, чтобы компилятор смог их векторизовать. Вместо использования базы вида if (n == 1) c[0] = a[0] * b[0], имеет смысл, начиная с какого-то размера задачи, использовать более эффективное наивное умножение за квадрат. Сначала приведем код основной рекурсивной процедуры, а потом объясним, почему он работает:\nvoid karatsuba(int *a, int *b, int *c, int n) { if (n \u0026lt;= 64) { for (int i = 0; i \u0026lt; n; i++) for (int j = 0; j \u0026lt; n; j++) c[i + j] += a[i] * b[j]; } else { int k = n / 2; int l[k], r[k], t[n] = {0}; for (int i = 0; i \u0026lt; k; i++) { l[i] = a[i] + a[k + i]; r[i] = b[i] + b[k + i]; } karatsuba(l, r, t, k); // считает t karatsuba(a, b, c, k); // считает p1 karatsuba(a + k, b + k, c + n, k); // считает p2 int *t1 = t, *t2 = t + k; int *s1 = c, *s2 = c + k, *s3 = c + 2 * k, *s4 = c + 3 * k; for (int i = 0; i \u0026lt; k; i++) { int c1 = s2[i] + t1[i] - s1[i] - s3[i]; int c2 = s3[i] + t2[i] - s2[i] - s4[i]; c[k + i] = c1; c[n + i] = c2; } } } После трёх рекурсивных вызовов массив $c$ — это конкатенация $p_1$ и $p_2$.\nПосле этого, для подсчета самого многочлена $c$ проще всего мысленно разделить его на четыре равные части, а многочлен $t$ — на две половины $t_1$ и $t_2$, а затем посмотреть на формулу и подумать, как изменится каждая часть:\n$s_1$: не меняется — это первая половина $p_1$. $s_2$: выражается как $s_2 + t_1 - s_1 - s_3$, то есть изменяется на «первую» половину $t - p_1 - p_2$. $s_3$: выражается как $s_3 + t_2 - s_2 - s_4$, то есть изменяется на «вторую» половину $t - p_1 - p_2$. $s_4$: не меняется — это вторая половина $p_2$. Из-за векторизации важно использовать максимально «лёгкий» тип данных и при возможности компилировать с AVX:\n#pragma GCC optimize(\u0026#34;O3\u0026#34;) #pragma GCC target(\u0026#34;avx2\u0026#34;) Реализация достаточно эффективна: она может перемножить два многочлена размера $4 \\cdot 10^5$ за секунду.\n#Обобщение идеиПохожий метод можно применить к матричному умножению — это называется алгоритмом Штрассена. В нём две матрицы разбиваются на $8 = 4 + 4$ частей, перемножаются блочно, и сложной алгеброй от одного из 8 умножений получается избавиться, что даёт асимптотику $O(n^{\\log_2 7}) \\approx O(n^{2.81})$.\nИ в алгоритме Штрассена, и в алгоритме Карацубы можно достичь и лучшей асимптотики, если разбивать объекты на большее число частей. Однако, в реальности это не применяется, потому что в асимптотиках подобных алгоритмов скрыта непрактично большая константа.\n","id":91,"path":"/cs/algebra/karatsuba/","title":"Алгоритм Карацубы"},{"content":"Для большого количества применений нам на самом деле нужны не выпуклые оболочки, а только их половины (правые и левые или верхние и нижние), которые называют огибающими (англ. envelope).\nОгибающие строить немного проще: можно отсортировать точки по $x$ и пройтись по ним в таком порядке, поддерживая на стеке верхнюю огибающую для текущего префикса. При добавлении очередной точки нам нужно аналогичным образом проверить и удалить сколько-то верхних точек на стеке:\nvector\u0026lt;r\u0026gt; upper_envelope(vector\u0026lt;r\u0026gt; points) { sort(points.begin(), points.end(), [](r a, r b){ return a.x \u0026lt; b.x; }); vector\u0026lt;r\u0026gt; s = {p0}; for (r p : points) { while (...) { s[s.size()-2] = s[s.size()-1]; s.pop_back(); } s.push_back(p); } return s; } С огибающими работать легче, чем с целыми оболочками: их легко пересекать, объединять и делать бинпоиск (например, чтобы находить касательные). Объединение, кстати, можно производить за линейное время, пройдясь двумя указателями по обеим оболочкам одновременно, что позволяет альтернативно строить огибающую методом «разделяй и властвуй», тоже за $O(n \\log n)$.\nИногда имеет смысл разбить оболочку на две огибающие и работать с ними по отдельности. Основной минус такого подхода в том нужно либо писать в два раза больше кода, либо писать его так, чтобы все внутренние процедуры не отличали «выше» от «ниже» — тогда точки нижней огибающей можно передавать в те же рутины, что и для верхней, заменив $y_i$ на $-y_i$.\nТакже для построения выпуклой оболочки можно поступить следующим образом: построить верхнюю огибающую, построить нижнюю огибающую, а затем одну из них перевернуть и добавить в конец другой. Если точки уже отсортированы по $x$, $y$ или вдоль любой другой прямой, алгоритм будет работать за $O(n)$. Такой подход называется алгоритмом Эндрю.\n#Динамические выпуклые оболочкиОперируя с оболочкой как с двумя огибающими, можно относительно просто обрабатывать запросы добавления точек, обернув огибающие в std::set или любое другое дерево так, что они оказываются отсортированными по $x$. При добавлении точки нужно найти её позицию в дереве, проверить, что она находится выше огибающей, и, возможно, удалить сколько-то её левых и правых соседей.\nОбрабатывать удаление точки из множества сложнее. Если запросы известны заранее, то проще воспользоваться идеями корневой эвристики или dynamic connectivity problem и поддерживать только те точки, которые существуют на всем текущем блоке, и сводить удаление к добавлению.\nЕсли же запросы удаления требуется обрабатывать онлайн (fully dynamic convex hull), то для этого есть очень неприятный алгоритм на 250-300 строк кода, заключающийся в поддержании огибающих для всех поддеревьев в этом дереве поиска и быстром объединении огибающих при перестроении дерева. Алгоритм работает за $O(\\log^2 n)$ на запрос: при слиянии огибающих при объединении вершин используется бинпоиск с разбором 7 случаев.\n","id":92,"path":"/cs/convex-hulls/envelope/","title":"Верхние и нижние огибающие"},{"content":"В контексте решения задачи LCA и не только популярен следующий метод.\n#ПредподсчетПредподсчитаем для каждой вершины её 1-го предка, 2-го предка, 4-го и так далее. Сохраним их в двумерном массиве up размера $n \\times \\lceil \\log n \\rceil$: в up[v][d] будет храниться предок вершины $v$ на расстоянии $2^d$, а если такой вершины не существует, то корень.\nТакой препроцессинг можно выполнить за $O(n \\log n)$, используя тот факт, что предок на расстоянии $2^{d+1}$ — это предок на расстоянии $2^d$ предка на расстоянии $2^d$:\nint up[maxn][logn]; void dfs(int v) { for (int l = 1; l \u0026lt; logn; l++) up[v][l] = up[up[v][l-1]][l-1]; tin[v] = t++; for (int u : g[v]) { up[u][0] = v; dfs(u); } tout[v] = t; } Препроцессинг суммарно занимает $O(n \\log n)$ времени и памяти, потому что таков размер массива up, и каждый его элемент вычисляется за константу.\n#ЗапросПусть теперь поступил запрос нахождения LCA — пара вершин $(u, v)$:\nПроверим, не является ли одна вершина предком другой — в таком случае она и является результатом. Иначе, пользуясь массивом up, будем подниматься по предкам одной из них, пока не найдём самую высокую вершину, которая ещё не является предком другой. Следующая за ней будет искомым LCA. Подробнее про второй пункт. Присвоим $i = \\lceil \\log n \\rceil$ и будем уменьшать эту переменную на единицу, пока up[v][i] не перестанет быть предком $u$. Когда это произойдёт, подвинем указатель на $2^i$-го предка $v$ и продолжим дальше:\nint lca(int v, int u) { if (a(v, u)) return v; if (a(u, v)) return u; for (int l = logn-1; l \u0026gt;= 0; l--) if (!ancestor(up[v][l], u)) v = up[v][l]; return up[v][0]; } Указатель up[v][i] изначально является корнем дерева, а затем будет каждую итерацию спускаться на $2^i$. Когда он станет потомком lca, нам достаточно подняться всего лишь один раз, потому что два раза прыгнуть на расстояние $2^i$ это то же самое, что один раз прыгнуть на $2^{i+1}$, а мы могли это сделать на предыдущем шаге.\nОтвет на произвольный запрос будет работать за $O(\\log n)$, потому что мы фактически выполняем бинпоиск.\n#Запросы на путяхПусть нас вместо LCA спрашивают, например, о минимуме на произвольном пути (на всех рёбрах записаны какие-то числа).\nМы можем сделать такой же предподсчет, как в методе двоичных подъемов, но хранить вместе с номером $2^d$-ого предка минимум на соответствующем пути.\nЗаметим, что минимум на пути от $u$ до $v$ — это минимум от минимума на пути от $u$ до $lca(u, v)$ и от минимума на пути от $v$ до $lca(u, v)$. В свою очередь, оба этих минимума — это минимум на всех двоичных подъемах до LCA.\nint get_min(int v, int u) { int res = inf; for (int l = logn-1; l \u0026gt;= 0; l--) if (!ancestor(up[v][l], u)) v = up[v][l], res = min(res, mn[v][l]); for (int l = logn-1; l \u0026gt;= 0; l--) if (!ancestor(up[u][l], v)) u = up[u][l], res = min(res, mn[u][l]); return min({res, mn[v][0], mn[u][0]}) } Аналогичным образом можно считать сумму, gcd, полиномиальный хеш и много других странных функций на пути, но только в статическом случае — когда у нас нет обновлений. Для динамического случая существует гораздо более сложный метод, называемый heavy-light декомпозицией.\n","id":93,"path":"/cs/trees/binary-lifting/","title":"Метод двоичных подъемов"},{"content":"Рассмотрим нашу любимую задачу суммы на подотрезках, но теперь все индексы лежат не в пределах $10^5$ или $10^6$, а до $10^9$ или даже $10^{18}$.\nВсе асимптотики нас по прежнему более-менее устраивают:\n$$ \\log_2 10^6 \\approx 20 \\ \\log_2 10^9 \\approx 30 \\ \\log_2 10^{18} \\approx 60 $$\nЕдинственная проблема — это этап построения, работающий за линейное от $n$ время и память.\nРешить её можно отказавшись от явного создания всех вершин дерева в самом начале. Изначально создадим только лишь корень, а остальные вершины будем создавать на ходу, когда в них потребуется записать что-то не дефолтное — как в lazy propagation:\nstruct Segtree { int lb, rb; int s = 0; Segtree *l = 0, *r = 0; Segtree(int lb, int rb) : lb(lb), rb(rb) { // а тут ничего нет } // создает детей, если нужно void extend() { if (!l \u0026amp;\u0026amp; lb + 1 \u0026lt; rb) { int t = (lb + rb) / 2; l = new segtree(lb, t); r = new segtree(t, rb); } } // ... }; Теперь по аналогии с push будем в начале всех методов будем проверять, что дети-вершины созданы, и создавать их, если это не так.\nvoid add(int k, int x) { extend(); s += x; if (l) { if (k \u0026lt; l-\u0026gt;rb) l-\u0026gt;add(k, x); else r-\u0026gt;add(k, x); } } int sum(int lq, int rq) { if (lb \u0026gt;= lq \u0026amp;\u0026amp; rb \u0026lt;= rq) return s; if (max(lb, lq) \u0026gt;= min(rb, rq)) return 0; extend(); return l-\u0026gt;sum(lq, rq) + r-\u0026gt;sum(lq, rq); } Такой метод в основном приеним только к реализации на указателях, однако при использовании индексаций можно хранить вершины не в массиве, а в хеш-таблице: запросы станут работать дольше, но не асимптотически. Отметим также, что в любом случае на каждый запрос потребуется $O(\\log n)$ дополнительной памяти для создания новых вершин, что обычно приводит к асимптотике $O(q \\log n)$ по памяти.\nТакже, если все запросы известны заранее, то их координаты можно просто сжать перед обработкой запросов. Автор обычно делает это так:\nvector\u0026lt;int\u0026gt; compress(vector\u0026lt;int\u0026gt; a) { vector\u0026lt;int\u0026gt; b = a; sort(b.begin(), b.end()); b.erase(unique(b.begin(), b.end()), b.end()); for (int \u0026amp;x : a) x = int(lower_bound(b.begin(), b.end(), x) - b.begin()); return a; } В большинстве случаев, использовать динамическое построение — это как стрелять из пушки по воробьям. Попробуйте сначала более простые методы: возможно, всё просто в set влезает.\n","id":94,"path":"/cs/segment-tree/lazy-initialization/","title":"Отложенное построение"},{"content":"Задача топологической сортировки графа звучит так: дан ориентированный граф, и требуется найти такой порядок вершин, в котором все рёбра графа вели из более ранней вершины в более позднюю.\nЭто может быть полезно, например, при планировании выполнения связанных задач: вам нужно одеться, в правильном порядке надев шорты (1), штаны (2), ботинки (3), подвернуть штаны (4) — как хипстеры — и завязать шнурки (5).\nВо-первых, сразу заметим, что граф с циклом топологически отсортировать не получится — как ни располагай цикл в массиве, все время идти вправо по ребрам цикла не получится.\nВо-вторых, верно обратное. Если цикла нет, то его обязательно можно топологически отсортировать — сейчас покажем, как.\nЗаметим, что вершину, из которой не ведет ни одно ребро, можно всегда поставить последней, а такая вершина в ациклическом графе всегда есть (иначе можно было бы идти по обратным рёбрам бесконечно). Из этого сразу следует конструктивное доказательство: будем итеративно класть в массив вершину, из которой ничего не ведет, и убирать ее из графа. После этого процесса массив надо будет развернуть.\nЭтот алгоритм проще реализовать, обратив внимание на времена выхода вершин в dfs. Вершина, из которой мы выйдем первой — та, у которой нет новых исходящих ребер. Дальше мы будем выходить только из тех вершин, которые если и имеют исходящие ребра, то только в те вершины, из которых мы уже вышли.\nСледовательно, достаточно просто выписать вершины в порядке выхода из dfs, а затем полученный список развернуть, и мы получим какую-то из корректных топологических сортировок.\nconst int maxn = 1e5; bool used[maxn]; vector\u0026lt;int\u0026gt; t; void dfs(int v) { used[v] = true; for (int u : g[v]) if (!used[u]) dfs(u); t.push_back(v); } void topological_sort() { for (int v = 0; v \u0026lt; n; v++) if (!used[v]) dfs(v); reverse(t.begin(), t.end()); } Топологическую сортировку можно использовать для проверки достижимости, сравнивая номера вершин в получившемся массиве. Факт того, что вершина $a$ идёт позже вершины $b$, говорит о том, что из $a$ недостижима $b$ — однако $a$ может быть как достижима, так и недостижима из $b$.\n","id":95,"path":"/cs/graph-traversals/topological-sorting/","title":"Топологическая сортировка"},{"content":"Определение. Эйлеров путь — это путь в графе, проходящий через все его рёбра.\nОпределение. Эйлеров цикл — это эйлеров путь, являющийся циклом.\nДля простоты в обоих случаях будем считать, что граф неориентированный.\nГраф на пяти вершинах и один из его эйлеровых циклов: CDCBBADEBC Также существует понятие гамильтонова пути и цикла — они посещают все вершины по разу, а не рёбра. Нахождение гамильтонова цикла (задача коммивояжера, англ. travelling salesman problem) — одна из самых известных NP-полных задач, в то время как нахождение эйлерова цика решается за линейное время, и мы сейчас покажем, как.\n#Нахождение эйлерова циклаТеорема. Эйлеров цикл существует тогда и только тогда, когда граф связный и степени всех вершин чётны.\nДоказательство. Необходимость показывается так: можно просто взять эйлеров цикл и ориентировать все его ребра в порядке обхода. Тогда из каждой вершины будет выходить столько же рёбер, сколько входить, а значит степень у всех вершин исходного неориентированного графа была четной.\nДостаточность докажем конструктивно — предъявим алгоритм нахождения цикла:\nvoid euler(int v) { while (!g[v].empty()) { u = *g[v].begin(); // берем любое ребро remove_edge(v, u); // удаляем его euler(u); // запускаемся от противоположного конца } cout \u0026lt;\u0026lt; v \u0026lt;\u0026lt; \u0026#34; \u0026#34;; // выписываем текущую вершину } Заметим, что:\nвыведется последовательность из ровно $(m + 1)$ вершин, между каждой соседней парой выписанных вершин есть ребро, каждое ребро будет выписано ровно один раз. Значит, если условия на связность и четность степеней выполняются, то выведенная последовательность вершин действительно будет эйлеровым циклом, причём и в случае ориентированного графа тоже.\n#Как удалять ребраПроще всего хранить все списки смежности в set-подобной структуре и удалять их напрямую там:\nconst int maxn = 1e5; set\u0026lt;int\u0026gt; g[maxn]; void euler(int v) { while (!g[v].empty()) { u = *g[v].begin(); g[v].erase(u); g[u].erase(v); // если граф ориентированный, обратное ребро удалять не надо euler(u); } cout \u0026lt;\u0026lt; v \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } Это будет работать за $O(m \\log n)$, однако просто заменив дерево на хеш-таблицу (unordered_set) можно уменьшить время до линейного.\nТакже можно использовать более общий подход, который часто применяется в задачах, где ребра как-то изменяются. Создадим отдельный массив с мета-информацией о ребрах и будем хранить в списках смежности не номера вершин, а номера рёбер:\nstruct edge { int to; bool deleted; }; vector\u0026lt;edge\u0026gt; edges; stack\u0026lt;int\u0026gt; g[maxn]; void add_edge(int u, int v) { g[u].push(edges.size()); edges.add({v, false}); g[u].push(edges.size()); edges.add({u, false}); } Если добавлять каждое ребро неориентированного графа через add_edge, то у получившейся нумерации ребер будет интересное свойство: чтобы получить обратное к ребру e, нужно вычислить e^1.\nТогда во время обхода можно поддерживать эту информацию вместо какой-то сложной модификации структур:\nvoid euler(int v) { while (!g[v].empty()) { e = g[v].top(); g[v].pop(); if (!edges[e].deleted) { edges[e].deleted = edges[e^1].deleted = true; euler(e.to); } } cout \u0026lt;\u0026lt; v \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } Во всех вариантах реализации нужно быть чуть аккуратнее в случае, когда в графе есть петли и кратные ребра.\n#Эйлеров путьПоговорим теперь про эйлеровы пути. Может, всё-таки можно что-нибудь сделать, даже если степени не всех вершин чётные?\nЗаметим, что, так как каждое ребро меняет четность степеней ровно двух вершин, и в пустом графе все степени изначально нулевые, то число вершин с нечетными степенями будет четным.\nЕсли нечетных вершин ровно две, то можно сделать следующее: соединить их ребром, построить эйлеров цикл (ведь теперь степени всех вершин четные), а затем удалить это ребро из цикла. Если правильно сдвинуть этот цикл, мы получили эйлеров путь.\nАльтернативно, можно просто запустить обход из одной из нечетных вершин и получить такой же результат.\nЕсли нечетных вершин больше двух, то мы уже построить эйлеров путь не сможем, потому что любой эйлеров путь входит или покидает каждую вершину четное число раз, кроме, возможно двух своих концов.\nСледствие. Эйлеров путь существует тогда и только тогда, когда граф связен и количество вершин с нечётными степенями не превосходит 2.\nУпражнение. Дан связный неориентированный граф. Требуется покрыть его ребра наименьшим количеством непересекающихся путей. Асимптотика $O(n + m)$.\nУпражнение. Сформулируйте и докажите аналогичные утверждения для случая ориентированного графа.\n","id":96,"path":"/cs/graph-traversals/euler-cycle/","title":"Эйлеров цикл"},{"content":"Корневые оптимизации можно использовать много для чего, в частности в контексте структур данных.\n#Корневая декомпозиция на массивахСделаем вид, что про дерево отрезков мы не знаем, и рассмотрим следующую задачу.\nЗадача. Дан массив $a$ длины $n$. Требуется отвечать на $q$ запросов одного из двух типов:\nНайти сумму на отрезке $[l, r]$. Увеличить все элементы на отрезке [l, r] на $x$. Разделим весь массив на блоки по $c \\approx \\sqrt{n}$ элементов и посчитаем сумму на каждом блоке. Так как блоки не пересекаются, суммарно это будет работать за $O(n)$.\n// c это и количество блоков, и также их размер; оно должно быть чуть больше корня const int maxn = 1e5, c = 330; int a[maxn], b[c], add[c]; for (int i = 0; i \u0026lt; n; i++) b[i / c] += a[i]; Заведем также массив add размера $\\sqrt n$, который будем использовать для отложенной операции прибавления на блоке: будем считать, что реальное значение $i$-го элемента равно a[i] + add[i / c].\nТеперь мы можем отвечать на запросы первого типа за $O(\\sqrt n)$ операций на запрос:\nДля всех блоков, лежащих целиком внутри запроса, просто возьмём уже посчитанные суммы и сложим. Для блоков, пересекающихся с запросом только частично (их максимум два — правый и левый), проитерируемся по нужным элементам и поштучно прибавим к ответу. Есть много разных стилей, как это реализовать, но автор предпочитает такой:\nint sum(int l, int r) { int res = 0; while (l \u0026lt;= r) { // если мы находимся в начале блока и он целиком в запросе if (l % c == 0 \u0026amp;\u0026amp; l + c - 1 \u0026lt;= r) { res += b[l / c]; l += c; // мы можем прыгнуть сразу на блок } else { res += a[l] + add[l / c]; l += 1; } } return res; } Обновление пишется примерно так же — для целиком лежащих кусков обновляем add и сумму, для остальных отдельные элементы:\nvoid upd(int l, int r, int x) { while (l \u0026lt;= r) { if (l % c == 0 \u0026amp;\u0026amp; l + c - 1 \u0026lt;= r) { b[l / c] += c * x; add[l / c] += x; l += c; } else { b[l / c] += x; a[l] += x; l++; } } } Обе операции будут работать за $O(\\sqrt n)$, потому что нужных «центральных» блоков всегда не более $\\sqrt n$, а в граничных блоках суммарно не более $2 \\sqrt n$ элементов.\n#Внутренние структурыВ блоках корневой декомпозиции можно хранить не только значения каких-то функций для подотрезка, а ещё и самые разные структуры.\nНапример, хеш-таблица внутри каждого блока позволяет отвечать на запросы вида «число элементов равных $x$ на отрезке», а отсортированным массивом можно решать задачи вида «количество чисел меньших $x$ на отрезке» или «сумма на прямоугольнике».\nНа скорость работы в этих случаях очень сильно влияет размер блока. Ранее мы для простоты использовали одну и ту же константу и для количества блоков, и для их размера, но на практике их часто нужно подбирать.\nДля этого нужно смотреть даже не на асимптотику «блочной» и «поштучной» частей, а скорее на относительное реальное время их работы — учитывая, что походы в какое-нибудь декартово дерево совсем не в логарифм раз медленнее линейного, хорошо векторизуемого прохода по массиву. По этой же причине очень часто в корневых эвристиках корень «меньше» логарифма.\n#Подвижные элементыЗадача. Дан массив $a$ длины $n$. Требуется отвечать на $q$ запросов одного из трёх видов:\nВставить элемент $x$ на позицию $k$ (слева от него окажется ровно $k$ элементов). Удалить элемент с позиции $k$. Найти минимум на интервале $[l, r]$. Здесь предыдущий подход напрямую нельзя применить по той же причине, почему нельзя применить дерево отрезков — вставка и удаление элементов меняют индексы всех правых соседей. Здесь нужно хранить элементы в какой-то более гибкой структуре, не привязанной к статичным индексам.\nРазделим все элементы на корневые блоки, и в каждом блоке будем хранить список (vector или любой другой подобный контейнер) всех его элементов, а также минимум на этом блоке.\nТеперь, когда приходит запрос вставки элемента, мы проходимся по всем блокам, находим нужный блок (такой, что до него идет меньше $k$ элементов, а вместе с ним уже больше), и вставляем в него этот элемент, просто перестраивая весь блок с нуля за его размер.\nДля удаления делаем почти то же самое — находим нужный блок, находим нужный элемент и удаляем. Для минимума — берем минимум из всех полностью покрытых блоков, и в двух граничных линейным проходом берем минимумы на префиксе или суффиксе.\nОтметим, что чтобы находить граничные блоки, мы теперь не можем просто поделить границы на какую-то константу — у блоков динамические размеры, и чтобы найти индекс начала какого-то блока, нам нужно просуммировать размеры всех предыдущих блоков.\nvector\u0026lt; vector\u0026lt;int\u0026gt; \u0026gt; blocks; // возвращает индекс блока и индекс элемента внутри блока pair\u0026lt;int, int\u0026gt; find_block(int pos) { int idx = 0; while (blocks[idx].size() \u0026lt;= pos) pos -= blocks[idx++].size(); return {idx, pos}; } Решение в таком виде будет хорошо работать только первое время, когда каждая операция будет просматривать не более $O(\\sqrt n)$ блоков и не более $O(\\sqrt n)$ элементов по отдельности, но с течением времени может получиться, что либо блоков слишком много, либо есть слишком большие блоки. Есть два способа решать эту проблему:\nПосле каждой операции добавления или удаления смотреть на затронутый блок, и если его размер больше $2 \\cdot \\sqrt n$, то разделять его на два, а также если он и его сосед в сумме дают меньше $n$, то смерджить их в один. Завести глобальный счетчик операций и просто перестраивать целую структуру каждые $\\sqrt q$ запросов (выписать обратно в массив и заново разделить на равные блоки). В первом случае мы потенциально тратим $O(\\sqrt n)$ операций на сплиты и мерджи, но гарантируем инварианты, а во втором получаем ровно $\\sqrt n$ помножить на стоимость построения.\nВторой вариант проще кодить (ведь структуру в любом случае нужно изначально строить). Примечательно, что бывают ситуации, когда перестраивать структуру не так выгодно, как поддерживать в сбалансированном состоянии — когда мердж дешевле перестроения с нуля.\nБолее реальный пример задачи: IOI 2011 «Dancing Elephants».\n","id":97,"path":"/cs/range-queries/sqrt-structures/","title":"Корневые структуры"},{"content":"Причина, по которой сортировка выбором работает за квадратичное время — это линейный поиск минимума на каждом шаге.\nЧтобы соптимизировать время работы алгоритма, мы можем завести специальную структуру данных, которая поддерживает быстрое извлечение минимума из набора неупорядоченных элементов, добавить весь исходных массив в неё, а затем по одному извлекать минимум в новый отсортированных массив.\nЭто ровно то, что делает двоичная куча — за $O(\\log n)$ на операцию. Воспользовавшись ей, получаем алгоритм, работающий за $O(n \\log n)$:\nvoid heapsort(int* a, int n) { // копируем массив в кучу t_n = n; copy(a, a + n, t + 1); build_heap(); for (int i = 1; i \u0026lt;= n; i++) { // удаляем минимум, он останется в ячейке t[n + 1 - i] swap(t[1], t[n + 1 - i]); t_n--; sift_down(1); } // получили массив t[1..n], в котором все элементы упорядочены по убыванию reverse(t, t + n + 1); copy(t, t + n, a); } Примечательно, что в случае «почти отсортированного» массива алгоритм можно немного ускорить. Если гарантируется, что каждый элемент находится на расстоянии не более $k$ от своей позиции в отсортированном массиве, то нам достаточно поддерживать кучу размера $O(k)$, считать массив «окном» из $k$ элементов, и после добавления очередного элемента выписывать минимум. Такой алгоритм будет работать за $O(n \\log k)$.\n","id":98,"path":"/cs/sorting/heapsort/","title":"Сортировка кучей"},{"content":"Быстрое преобразование Фурье — один из самых важных алгоритмов XX века, если не самый важный.\nОно применяется, как можно догадаться, для вычисления преобразований Фурье, которые в свою очередь используются для обработки звука, электромагнитных волн, задач оптики, сжатия данных, физического моделирования и прочих сложных математических и физических задач.\nВ этой статье же мы подойдем немного с другой стороны и рассмотрим алгоритм быстрого преобразования Фурье в контексте задачи умножения чисел и многочленов, часто встречающейся в олимпиадах.\n#Умножение через интерполяциюМногочлен степени $(n - 1)$ можно однозначно задать не только своими коэффициентами, но и значениями в $n$ различных точках.\nПри прямом перемножении многочленов, заданных своими коэффициентами, нужно потратить $O(n^2)$ операций. Но если многочлены заданы своими значениями в $2n$ точках, то их можно перемножить за $O(n)$: значение многочлена-произведения $A(x) \\cdot B(x)$ в точке $x_i$ просто становится равным $A(x_i) \\cdot B(x_i)$.\nОсновная идея алгоритма заключается в том, что если мы посчитаем значения в каких-то различных $(n + m)$ точках для обоих многочленов $A$ и $B$, то, попарно перемножив их, мы за $O(n + m)$ операций можем получить значения в тех же точках для многочлена $A(x) \\cdot B(x)$, и с их помощью интерполяцией получить коэффициенты многочлена-произведения и решить задачу.\nvector\u0026lt;int\u0026gt; poly_multiply(vector\u0026lt;int\u0026gt; a, vector\u0026lt;int\u0026gt; b) { vector\u0026lt;int\u0026gt; A = evaluate(a); vector\u0026lt;int\u0026gt; B = evaluate(b); for (int i = 0; i \u0026lt; A.size(); i++) A[i] *= B[i]; return interpolate(A); } Если притвориться, что evaluate и interpolate работают за линейное время, то такое умножение тоже будет работать за линейное время. Но, к сожалению, непосредственное вычисление значений требует $O(n^2)$ операций, а интерполяция — как методом Гаусса, так и через символьное вычисление многочлена Лагранжа — и того больше, $O(n^3)$.\nНо что, если бы мы могли вычислять значения в точках и делать интерполяцию быстрее? Выясняется, что это можно сделать, если рассматривать не произвольные точки, а только специальные — а именно, комплексные корни из единицы.\n#Корни из единицыФакт. Для любого натурального $n$ есть ровно $n$ «корней из единицы», то есть чисел $w_k$, для которых\n$$ w_k^n = 1 $$\nА именно, это будут числа вида\n$$ w_k = e^{i \\tau \\frac{k}{n}} $$\nгде $\\tau$ обозначает $2 \\pi$, «целый круг». Это довольно новая нотация.\nНа комплексной плоскости эти числа располагаются на единичном круге на равном расстоянии друг от друга:\nВсе 9 комплексных корней степени 9 из единицы Первый корень $w_1$ (точнее второй — единицу считаем нулевым корнем) называют образующим корнем степени $n$ из единицы. Возведение его в нулевую, первую, вторую и так далее степени порождает последовательность нужных корней единицы, при этом на $n$-ном элементе последовательность зацикливается:\n$$ w_n = e^{i \\tau \\frac{n}{n}} = e^{i \\tau} = e^{i \\cdot 0} = w_0 = 1 $$\nБудем обозначать $w_1$ как просто $w$.\n#Дискретное преобразование ФурьеДискретным преобразованием Фурье собственно и называется вычисление значений многочлена в комплексных корнях из единицы:\n$$ y_j = \\sum_{k=0}^{n-1} x_k e^{i\\tau \\frac{kj}{n}} = \\sum_{k=0}^{n-1} x_k w_1^{kj} $$\nОбратным дискретным преобразованием Фурье называется, как можно догадаться, обратная операция — интерполяция коэффициентов $x_i$ по значениям $y_i$.\nУтверждение. Обратное ДПФ можно вычислить по формуле\n$$ x_j = \\frac{1}{n} \\sum_{k=0}^{n-1} y_k e^{-i\\tau \\frac{kj}{n}} = \\frac{1}{n} \\sum_{k=0}^{n-1} y_k w_{n-1}^{kj} $$\nДоказательство. При вычислении ПФ мы фактически применяем матрицу к вектору:\n$$ \\begin{pmatrix} w^0 \u0026amp; w^0 \u0026amp; w^0 \u0026amp; w^0 \u0026amp; \\dots \u0026amp; w^0 \\ w^0 \u0026amp; w^1 \u0026amp; w^2 \u0026amp; w^3 \u0026amp; \\dots \u0026amp; w^{-1} \\ w^0 \u0026amp; w^2 \u0026amp; w^4 \u0026amp; w^6 \u0026amp; \\dots \u0026amp; w^{-2} \\ w^0 \u0026amp; w^3 \u0026amp; w^6 \u0026amp; w^9 \u0026amp; \\dots \u0026amp; w^{-3} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ w^0 \u0026amp; w^{-1} \u0026amp; w^{-2} \u0026amp; w^{-3} \u0026amp; \\dots \u0026amp; w^1 \\end{pmatrix} \\begin{pmatrix} a_0 \\ a_1 \\ a_2 \\ a_3 \\ \\vdots \\ a_{n-1} \\end{pmatrix} = \\begin{pmatrix} y_0 \\ y_1 \\ y_2 \\ y_3 \\ \\vdots \\ y_{n-1} \\end{pmatrix} $$\nТо есть преобразование Фурье — это просто линейная операция над вектором: $W a = y$. Значит, обратное преобразование можно записать так: $a = W^{-1}y$.\nКак будет выглядеть эта $W^{-1}$? Автор не будет пытаться изображать логичный способ рассуждений о её получении и сразу её приведёт:\n$$ W^{-1} = \\dfrac 1 n \\begin{pmatrix} w^0 \u0026amp; w^0 \u0026amp; w^0 \u0026amp; w^0 \u0026amp; \\dots \u0026amp; w^0 \\ w^0 \u0026amp; w^{-1} \u0026amp; w^{-2} \u0026amp; w^{-3} \u0026amp; \\dots \u0026amp; w^{1} \\ w^0 \u0026amp; w^{-2} \u0026amp; w^{-4} \u0026amp; w^{-6} \u0026amp; \\dots \u0026amp; w^{2} \\ w^0 \u0026amp; w^{-3} \u0026amp; w^{-6} \u0026amp; w^{-9} \u0026amp; \\dots \u0026amp; w^{3} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ w^0 \u0026amp; w^{1} \u0026amp; w^{2} \u0026amp; w^{3} \u0026amp; \\dots \u0026amp; w^{-1} \\end{pmatrix} $$\nПроверим, что при перемножении $W$ и $W^{-1}$ действительно получается единичная матрица:\nЗначение $i$-того диагонального элемента будет равно $\\frac{1}{n} \\sum_k w^{ki} w^{-ki} = \\frac{1}{n} n = 1$. Значение любого недиагонального ($i \\neq j$) элемента $(i, j)$ будет равно $$ \\frac{1}{n} \\sum_k w^{ik} w^{-jk} = \\frac{1}{n} \\sum_k w^k w^{i-j} = \\frac{w^{i-j}}{n} \\sum_k w^k = 0 $$\nПоследний переход верен, потому что все комплексные корни суммируются в ноль, то есть $\\sum w^k = 0$.\nВнимательный читатель заметит симметричность форм $W$ и $W^{-1}$, а также формул для прямого и обратного преобразования. Эта симметрия нам сильно упростит жизнь: для обратного преобразования Фурье можно использовать тот же алгоритм, только вместо $w^k$ использовать $w^{-k}$, а в конце результат поделить на $n$.\n#АлгоритмНапомним, что мы изначально хотели перемножать многочлены следующим алгоритмом:\nПосчитаем значения в $(n+m)$ каких-нибудь точках обоих многочленов. Перемножим эти значения попарно за $O(n + m)$. Интерполяцией получим многочлен-произведение. В общем случае быстро посчитать интерполяцию и даже просто посчитать значения в точках нельзя, но для корней единицы — можно. Если научиться быстро считать значения в корнях и интерполировать (прямое и обратное преобразование Фурье), но мы сможем решить исходную задачу.\nСоответствующий алгоритм и называется быстрым преобразованием Фурье (англ. fast Fourier transform). Он использует парадигму «разделяй-и-властвуй» и работает за $O(n \\log n)$.\n#Схема Кули-ТьюкиОбычно, алгоритмы «разделяй-и-властвуй» делят задачу на две половины: на первые $\\frac{n}{2}$ элементов и вторые $\\frac{n}{2}$ элементов. Здесь же мы поступим по-другому: поделим все элементы на чётные и нечётные.\nПредставим многочлен в виде $P(x)=A(x^2)+xB(x^2)$, где $A(x)$ состоит из коэффициентов при чётных степенях $x$, а $B(x)$ — из коэффициентов при нечётных.\nПусть $n = 2k$. Тогда заметим, что для любого целого числа $t$\n$$ w^{2t} = w^{2t \\bmod n} = w^{2t \\bmod 2k} = w^{2(t \\bmod k)} $$\nЗная это, исходную формулу для значения многочлена в точке $w^t$ можно записать так:\n$$ P(w^t) = A(w^{2t}) + w^t B(w^{2t}) = A\\left(w^{2(t\\bmod k)}\\right)+w^tB\\left(w^{2(t\\bmod k)}\\right) $$\nКлючевое замечание: различных корней вида $w^{2t}$, значения в которых нам потребуются для пересчета, будет в два раза меньше, а также в обоих многочленах будет в два раза меньших коэффициентов — значит, мы только что успешно разбили нашу задачу на две, каждая из которых в два раза меньше.\nСам алгоритм заключается в следующем: рекурсивно посчитаем БПФ для многочленов $A$ и $B$ и объединим ответы с помощью формулы выше. При этом в рекурсии нам нужно считать значения на корнях степени не $n$, а $k = \\frac{n}{2}$, то есть на всех «чётных» корнях степени $n$ (вида $w^{2t}$). Заметим, что если $w$ это образующий корень степени $n = 2k$ из единицы, то $w^2$ будет образующим корнем степени $k$, то есть в рекурсию мы можем просто передать другое значение образующего корня.\nТаким образом, мы свели преобразование размера $n$ к двум преобразованиям размера $\\dfrac n 2$, и, следовательно, общее время вычисления БПФ составит\n$$ T(n)=2T\\left(\\dfrac n 2\\right)+O(n)=O(n\\log n) $$\nОтметим также, что предположение о делимости $n$ на $2$ имело существенную роль. Значит, $n$ должно быть чётным на каждом уровне, кроме последнего, из чего следует, что $n$ должно быть степенью двойки.\n#РеализацияПриведём код, вычисляющий БПФ по схеме Кули-Тьюки:\ntypedef complex\u0026lt;double\u0026gt; ftype; const double pi = acos(-1); // принимает массив и n-ный корень из единицы, и заменяет его на значения в корнях void fft(vector\u0026lt;ftype\u0026gt; \u0026amp;p, ftype wn) { int n = (int) p.size(); if (n == 1) return; // разделяем массив на четный и нечетный vector\u0026lt;ftype\u0026gt; a(n / 2), b(n / 2); for (int i = 0; i \u0026lt; n / 2; i++) { a[i] = p[2 * i]; b[i] = p[2 * i + 1]; } // рекурсивно считаем БПФ fft(a, wn * wn); fft(b, wn * wn); // объединяем результат по формуле ftype w = 1; for (int i = 0; i \u0026lt; n / 2; i++) { // можно не использовать модуль, а сразу раскрыть его для двух половин p[i] = a[i] + w * b[i]; p[i + n / 2] = a[i] - w * b[i]; // w^(i+n/2) = -w^i w *= wn; } } При изначальном запуске следует дополнить массив до степени двойки:\nvector\u0026lt;ftype\u0026gt; evaluate(vector\u0026lt;int\u0026gt; p) { while (__builtin_popcount(p.size()) != 1) p.push_back(0); return fft(p, polar(1., 2 * pi / p.size())); } Как обсуждалось ранее, обратное преобразование Фурье удобно выразить через прямое:\nvector\u0026lt;int\u0026gt; interpolate(vector\u0026lt;ftype\u0026gt; p) { int n = p.size(); auto inv = fft(p, polar(1., -2 * pi / n)); vector\u0026lt;int\u0026gt; res(n); for(int i = 0; i \u0026lt; n; i++) // мы хотим получать целые числа, для этого результаты нужно округлить res[i] = round(real(inv[i]) / n); return res; } Теперь мы умеем перемножать два многочлена за $O(n \\log n)$:\nvector\u0026lt;int\u0026gt; poly_multiply(vector\u0026lt;int\u0026gt; a, vector\u0026lt;int\u0026gt; b) { vector\u0026lt;ftype\u0026gt; A = evaluate(a); vector\u0026lt;ftype\u0026gt; B = evaluate(b); for (int i = 0; i \u0026lt; A.size(); i++) A[i] *= B[i]; return interpolate(A); } Приведённый выше код, являясь корректным и имея асимптотику $O(n \\log n)$, имеет весьма большую константу — в основном из-за рекурсии и дополнительных аллокаций.\n#Оптимизированная версияПопробуем избавиться от аллокаций вообще. Сейчас они происходят, потому что мы каждый раз разбиваем массив на два. Что произойдет, если вместо того, чтобы создавать новые массивы, просто сдвинуть все четные элементы в левую половину, а нечетные — в правую, и запуститься рекурсивно от половин?\nНаблюдение. Элемент с индексом k на последнем уровне рекурсии будет записан в ячейку revbits(k), где функция revbits(x) «разворачивает» биты числа x.\nДействительно, первой итерации все четные элементы (с нижним битом, равным нулю) будут записаны в первую половину (в позицию с верхним битом, равным нулю), а для нечетных наоборот. Дальше, все элементы со вторым самым младшим битом равным нулю будут внутри своих половин записаны в меньшую половину (со вторым самым старшим битом, равным нулю), и так далее.\nЕсли мы знаем, где окажется каждый элемент, то давайте тогда даже не будем делать какие-либо перестановки внутри рекурсии, а просто выполним их один раз в самом начале, а в рекурсивной функции будем просто запускаться от половин.\nvoid solve(ftype *a, int n, ftype wn) { if (n \u0026gt; 1) { int k = (n \u0026gt;\u0026gt; 1); solve(a, k, wn * wn); solve(a + k, k, wn * wn); ftype w = 1; for (int i = 0; i \u0026lt; k; i++) { // тут нужно быть чуть аккуратней с перезаписыванием, // потому что мы читаем и пишем из одного и того же массива ftype t = w * a[i + k]; a[i + k] = a[i] - t; a[i] = a[i] + t; w *= wn; } } } void fft(ftype *a, int n, int inverse) { const int logn = __lg(n); for (int i = 0; i \u0026lt; n; i++) { // переворачиваем биты числа i int k = 0; for (int l = 0; l \u0026lt; logn; l++) k |= ((i \u0026gt;\u0026gt; l \u0026amp; 1) \u0026lt;\u0026lt; (logn - l - 1)); // делаем только один swap -- из того элемента, который идет раньше if (i \u0026lt; k) swap(a[i], a[k]); } ftype wn = polar(1., inverse * 2 * pi / n); // inverse = {-1, +1} solve(a, n, wn); } У алгоритма довольно неплохая численная стабильность, однако в олимпиадных задачах часто требуется считать какой-нибудь большой целочисленный ответ, либо посчитать его по модулю. Однако от этой проблемы можно избавиться.\n#Number-theoretic transformНам от комплексных чисел на самом деле нужно было только одно свойство: что у единицы есть $n$ «корней». На самом деле, помимо комплексных чисел, есть и другие алгебраические объекты, обладающие таким свойством — например, элементы кольца вычетов по модулю.\nНайдем пару $m$ и $g$ (играющее роль $w_n^1$), такую что $g$ является образующим элементом, то есть $g^n \\equiv 1 \\pmod m$ и для всех остальных $k \u0026lt; n$ все степени $g^k$ различны по модулю $m$. В качестве $m$ на практике часто специально берут «удобные» модули, например\n$$ m = 998244353 = 7 \\cdot 17 \\cdot 2^{23} + 1 $$\nЭто число простое, и при этом является ровно на единицу больше числа, делящегося на большую степень двойки. При $n=2^{23}$ подходящим $g$ является число $31$. Заметим, что, как и для комплексных чисел, если для некоторого $n=2^k$ первообразный корень $g$, то для $n=2^{k-1}$ первообразным корнем будет $(g^2 \\bmod m)$. Таким образом, для $m=998244353$ и $n=2^k$ первообразный корень будет равен $g=31^{2^{23-k}} \\bmod m$.\nРеализация при этом практически не отличается: нужно просто использовать модулярную арифметику во всех операциях и страшные предподсчитанные константы для $w$ и $w^{-1}$.\nТакже с недавнего времени некоторые проблемсеттеры начали использовать именно этот модуль вместо стандартного $10^9+7$, чтобы намекнуть (или сбить с толку), что задача на FFT.\n","id":99,"path":"/cs/algebra/fft/","title":"Быстрое преобразование Фурье"},{"content":"Ро-алгоритм Полларда — рандомизированный алгоритм факторизации целых чисел, работающий за время $O(n^\\frac{1}{4})$ и основывающийся на следствии из парадокса дней рождений:\nВ мультимножество нужно добавить $O(\\sqrt{n})$ случайных чисел от 1 до $n$, чтобы какие-то два совпали.\n#$\\rho$-алгоритм ПоллардаИтак, мы хотим факторизовать число $n$. Предположим, что $n = p q$ и $p \\approx q$. Понятно, что труднее случая, наверное, нет. Алгоритм итеративно ищет наименьший делитель и таким образом сводит задачу к как минимум в два раза меньшей.\nВозьмём следующую функцию:\n$$ f(x) = (x+1)^2 \\bmod n $$\nОдно из свойств, которое нам понадобится в алгоритме, заключается в том, что эта функция «достаточно случайная» с точки зрения теории чисел. Мы к этой функции ещё вернемся.\nОпределение. Граф, в котором из каждой вершины есть единственное ребро $x \\to f(x)$, называется функциональным.\nЕсли в функциональном графе нарисовать «траекторию» произвольного элемента, то получится какой-то путь, превращающийся в цикл.\nТраектория каждого элемента получается похожей на букву $\\rho$ (ро). Алгоритм из-за этого так и назван. Рассмотрим траекторию какого-нибудь элемента $x_0$: {$x_0$, $f(x_0)$, $f(f(x_0))$, $\\ldots$}. Сделаем из неё новую последовательность, мысленно взяв каждый элемент по модулю $p$ — меньшего из простых делителей $n$.\nУтверждение. Ожидаемая длина цикла в этой последовательности $O(\\sqrt[4]{n})$.\nДоказательство: так как $p$ — меньший делитель, то $p \\leq \\sqrt n$. Теперь просто подставим в следствие из парадокса дней рождений: в множество нужно добавить $O(\\sqrt{p}) = O(\\sqrt[4]{n})$ элементов, чтобы какие-то два совпали, а значит последовательность зациклилась.\nЕсли мы найдём цикл в такой последовательности, то есть такие $i$ и $j$, что\n$$ f^i(x_0) \\equiv f^j(x_0) \\pmod p $$\nто мы сможем найти и какой-то делитель $n$, а именно\n$$ d = \\gcd(|f^i(x_0) - f^j(x_0)|, n) $$\nЭто число меньше $n$ и делится на $p$, а значит должно быть равно ему.\nАлгоритм по сути находит цикл в этой последовательности, используя для этого стандартный алгоритм «черепаха и заяц»: будем поддерживать два удаляющихся друг от друга указателя $j$ и $i = 2j$ и проверять, что $f^i(x_0) \\equiv f^j(x_0)$, что эквивалентно проверке\n$$ \\gcd(|f^i(x_0) - f^j(x_0)|, n) \\not \\in { 1, n } $$\nПростая, но не самая надежная реализация:\ntypedef long long ll; inline ll f(ll x, ll n) { return (__int128_t) (x + 1) * (x + 1) % n; } ll find_divisor(ll n, ll seed = 1) { ll x = seed, y = seed; ll d = 1; while (d == 1 || d == n) { // двигаем первый указатель на шаг y = f(y); // а второй -- на два x = f(f(x)); // пытаемся найти общий делитель d = gcd(abs(x - y), n); } return d; } Так как алгоритм рандомизированный, при полной реализации нужно учитывать разные детали. Например, что иногда делитель не находится (нужно запускать несколько раз), или что при попытке факторизовать простое число он будет работать за $O(\\sqrt n)$ (нужно добавить тест на простоту или отсечение по времени).\nАсимптотика этой реализации будет $O(\\sqrt[4]{n} \\log n)$, потому что мы на каждой итерации ищем $\\gcd$ за $O(\\log n)$. От логарифма можно избавиться следующим трюком: объявим константу $M = \\Theta(\\log n)$ и будем поддерживать произведение значений $P = \\prod |f^i(x_0) - f^j(x_0)|$ по модулю $n$ для $M$ последовательных итераций, и, вместо проверок через подсчет $\\gcd$ на каждом шаге, будем каждые $M$ итераций считать $\\gcd(P, n)$.\nКогда мы дойдем до делителя, он будет учтен в произведении, и мы не более чем через $M = \\Theta(\\log n)$ шагов найдем его в $\\gcd$ произведения. Так как мы теперь лишь каждые $\\Theta(\\log n)$ считаем $\\gcd$ за $O(\\log n)$, то логарифм из асимптотики исчезнет.\n#ПримечанияФакторизация больших чисел интересна в контексте криптографии — на предположение невозможности факторизации за линейное время опирается, например, алгоритм RSA.\nСуществуют также субэкспоненциальные, но не полиномиальные алгоритмы факторизации. Человечество умеет факторизовывать числа порядка $10^{250}$.\nАлгоритм Шора позволяет факторизовывать числа за полиномиальное время на квантовом компьютере. Но на 2019 год все квантовые вычисления проще симулировать на обычном компьютере. Самое большое число, факторизованное на реальном квантовом компьютере — 4088459.\n","id":100,"path":"/cs/factorization/pollard/","title":"Ро-алгоритм Полларда"},{"content":"В этой главе мы обсудим представление чисел в памяти и операции с ними.\n","id":101,"path":"/cs/arithmetic/","title":"Арифметика"},{"content":"Определение. Мостом называется ребро, при удалении которого связный неориентированный граф становится несвязным.\nОпределение. Точкой сочленения называется вершина, при удалении которой связный неориентированный граф становится несвязным.\nПример задачи, где их интересно искать: дана топология сети (компьютеры и физические соединения между ними) и требуется установить все единые точки отказа — узлы и связи, без которых будут существовать два узла, между которыми не будет пути.\nНаивный алгоритм поочередного удаления каждого ребра $(u, v)$ и проверки наличия пути $u \\leadsto v$ потребует $O(m^2)$ операций. Чтобы научиться находить мосты быстрее, сначала сформулируем несколько утверждений, связанных с обходом в глубину.\nЗапустим DFS из произвольной вершины. Введем новые виды рёбер:\nПрямые рёбра — те, по которым были переходы в dfs.\nОбратные рёбра — то, по которым не было переходов в dfs.\nЗаметим, что никакое обратное ребро $(u, v)$ не может являться мостом: если его удалить, то всё равно будет существовать какой-то путь от $u$ до $v$, потому что подграф из прямых рёбер является связным деревом.\nЗначит, остается только проверить все прямые рёбра. Это уже немного лучше — такой алгоритм будет работать за $O(n m)$.\nСоптимизировать его до линейного времени (до одного прохода dfs) поможет замечание о том, что обратные рёбра могут вести только «вверх» — к какому-то предку в дереве обхода графа, но не в другие «ветки» — иначе бы dfs увидел это ребро раньше, и оно было бы прямым, а не обратным.\nТогда, чтобы определить, является ли прямое ребро $v \\to u$ мостом, мы можем воспользоваться следующим критерием: глубина $h_v$ вершины $v$ меньше, чем минимальная глубина всех вершин, соединенных обратным ребром с какой-либо вершиной из поддерева $u$.\nДля ясности, обозначим эту величину как $d_u$, которую можно считать во время обхода по следующей формуле:\n$$ d_v = \\min \\begin{cases} h_v, \u0026amp;\\ d_u, \u0026amp;\\text{ребро } (v \\to u) \\text{ прямое} \\ h_u, \u0026amp;\\text{ребро } (v \\to u) \\text{ обратное} \\end{cases} $$\nЕсли это условие ($h_v \u0026lt; d_u$) не выполняется, то существует какой-то путь из $u$ в какого-то предка $v$ или саму $v$, не использующий ребро $(v, u)$, а в противном случае — наоборот.\nconst int maxn = 1e5; bool used[maxn]; int h[maxn], d[maxn]; void dfs(int v, int p = -1) { used[v] = true; d[v] = h[v] = (p == -1 ? 0 : h[p] + 1); for (int u : g[v]) { if (u != p) { if (used[u]) // если ребро обратное d[v] = min(d[v], h[u]); else { // если ребро прямое dfs(u, v); d[v] = min(d[v], d[u]); if (h[v] \u0026lt; d[u]) { // если нельзя другим путем добраться в v или выше, // то ребро (v, u) -- мост } } } } } Примечание. Более известен алгоритм, вместо глубин вершин использующий их $tin$, но автор считает его чуть более сложным для понимания.\n#Точки сочлененияЗадача поиска точек сочленения не сильно отличается от задачи поиска мостов.\nВершина $v$ является точкой сочленения, когда из какого-то её ребёнка $u$ нельзя дойти до её предка, не используя ребро $(v, u)$. Для конкретного прямого ребра $v \\to u$ этот факт можно проверить так: $h_v \\leq d_u$ (теперь неравенство нестрогое, так как если из вершины можно добраться только до нее самой, то она все равно будет точкой сочленения).\nИспользуя этот факт, можно оставить алгоритм практически прежним — нужно проверить этот критерий для всех прямых рёбер $v \\to u$:\nvoid dfs(int v, int p = -1) { used[v] = 1; d[v] = h[v] = (p == -1 ? 0 : h[p] + 1); int children = 0; for (int u : g[v]) { if (u != p) { if (used[u]) d[v] = min(d[v], h[u]); else { dfs(u, v); d[v] = min(d[v], d[u]); if (h[v] \u0026lt;= d[u] \u0026amp;\u0026amp; p != -1) { // корень (p == -1) обрабатываем отдельно // v -- точка сочленения // (это условие может выполниться много раз для разных детей) } children++; } } } if (p == -1 \u0026amp;\u0026amp; children \u0026gt; 1) { // v -- корень и точка сочленения } } Единственный крайний случай — это корень, так как в него мы по определению войдём раньше других вершин. Но фикс здесь очень простой — достаточно посмотреть, было ли у него более одной ветви в обходе (если корень удалить, то его поддеревья станут несвязными между собой).\n","id":102,"path":"/cs/graph-traversals/bridges/","title":"Мосты и точки сочленения"},{"content":"Центроидная декомпозиция — обобщение метода «разделяй-и-властвуй» на деревья. Обычно её используют для решения двух типов задач: «сколько есть путей с такими-то свойствами» и «есть ли такое-то свойство у этого пути».\nИногда вместо неё можно написать heavy-light декомпозицию, что чуть сложнее, или метод переливаний, что чуть проще.\nОпределение. Центром или центроидом (англ. centroid) дерева будем называть вершину, при удалении которой размеры оставшихся компонент будут не более $\\frac{n}{2}$.\nЦентроид всегда существует — это следует из алгоритма его поиска:\nint s[maxn]; int sizes (int v) { s[v] = 1; for (int u : g[v]) // для простоты считаем, что дерево корневое s[v] += sizes(u); return s[v]; } // второй параметр -- размер дерева int centroid (int v, int n) { for (int u : g[v]) if (s[u] \u0026gt; n/2) return centroid(u, n); return v; } Утверждение. centroid действительно находит цетроид.\nДоказательство:\ncentroid вернет вершину, у которой размеры всех детей не больше $\\frac{n}{2}$ (это явно проверяется в if-е). Мы пришли в эту вершину, когда её размер был больше $\\frac{n}{2}$, а это значит, что в «обратном» направлении есть не более $n - (\\frac{n}{2}+1) = \\frac{n}{2}-1$ вершин. Значит, размеры всех соседей не больше половины $n$, и алгоритм корректен. Иногда центроидов два (пример: 1-2-3-4), и тогда алгоритм вернёт «нижний» центроид.\nОпределение. Центроидной декомпозицией будем называть рекурсивный процесс «выделить центроид, удалить, запуститься от компонент».\nОпределение. Компонентой центроида будем называть множество вершин, достижимых из центроида непосредственно перед его удалением.\nЗаметим, что в конце всё дерево будет удалено, а значит каждая вершина ровно один раз побывала центроидом своей компоненты.\nТеперь поймём, зачем мы всё это делали.\nУтверждение. Каждая вершина входит в $O(\\log n)$ компонент.\nДоказательство. Центроид разбивает дерево на компоненты в хотя бы два раза меньшего размера. Значит, никакая вершина не может «прожить» более $\\lceil \\log_2 n \\rceil$ разделений.\nСледствие. Центроидная декомпозиция (см. определение выше) работает за $O(n \\log n)$.\nУтверждение. Для любого пути $a \\leadsto b$ есть единственный центроид $c$, в чьей компоненте были и $a$, и $b$.\nДоказательство. Каждая вершина когда-то была центроидом. Какая-то из вершин на пути была центроидом первой, и она навсегда разъединила $a$ и $b$.\nЭто очень важные выводы. Получается, что в процессе центроидной декомпозиции и для каждого простого пути будет ровно одна ситуация, когда этот путь содержится в какой-то компоненте и проходит через центроид. Ровно в этот момент мы можем обработать какой-то запрос, пользуясь техникой, похожей на метод переливаний.\n#Подсчет путей с заданным свойствомРассмотрим конкретный пример: подсчёт путей заданной длины.\nВмешаемся в процесс центроидной декомпозиции: для каждого центроида перед его удалением будем прибавлять к ответу число интересующих нас путей, которые проходят через этот центроид.\nКоличество таких путей можно посчитать за размер текущей компоненты: заведём массив d, в котором будем хранить количество вершин на каждом расстоянии от 0 до размера компоненты. Подвесим наше дерево-компоненту за центроид и будем запускать от его непосредственных детей dfs, который будет возвращать временный массив t — список глубин вершин в этом поддереве. Мы можем пройтись по каждому значению x в нём и добавить к ответу d[l-x], а затем добавить все значения из t в d. Можно убедиться, что таким образом каждый интересующий нас путь будет учтён ровно один раз.\nint l = 179; // нужная нам длина int ans = 0; // нам лень явно удалять вершины: заведем массив used -- была ли вершина удалена bool used[maxn]; int s[maxn]; // размеры поддеревьев void sizes(int v, int p) { s[v] = 1; for (int u : g[v]) if (u != p \u0026amp;\u0026amp; !used[u]) sizes(u, v), s[v] += s[u]; } int centroid(int v, int p, int n) { for (int u : g[v]) if (u != p \u0026amp;\u0026amp; !used[u] \u0026amp;\u0026amp; s[u] \u0026gt; n/2) return centroid(u, v, n); return v; } // записывает в t[] глубины вершин void dfs(int v, int p, int d, vector\u0026lt;int\u0026gt; \u0026amp;t) { t.push_back(d); for (int u : g[v]) if (u != p \u0026amp;\u0026amp; !used[u]) dfs(u, v, d + 1, t); } void solve(int v) { /* \u0026lt;единственный зависящий от конкретной задачи код\u0026gt; */ sizes(v); vector\u0026lt;int\u0026gt; d(s[v], 0); d[0] = 1; for (int u : g[v]) { if (!used[u]) { vector\u0026lt;int\u0026gt; t; dfs(u, v, 1, t); for (int x : t) if (x \u0026lt;= l) ans += d[l-x]; for (int x : t) d[x]++; } } /* \u0026lt;/единственный зависящий от конкретной задачи код\u0026gt; */ used[v] = 1; for (int u : g[v]) if (!used[u]) solve(centroid(u, v, s[u])); } Асимптотика $O(n \\log n)$, потому что на каждую из $O(n)$ вершин мы потратим $O(1)$ операций на каждом из $O(\\log n)$ «уровней» центроидной декомпозиции.\n#Запросы на путях — offlineМы находим всё, что нам надо, сразу после того, как получили новую компоненту. Если у нас есть список запросов «посчитать что-то на пути», то мы часто можем обработать их в offline.\nА именно, мы можем хранить вместе с вершиной список относящихся к ней ещё не отвеченных запросов. Этот список мы будем просматривать во время обработки очередной компоненты, и если центроид лежит на этом запросе, то мы ответим на этот запрос и удалим его из списка, а в обратном случае просто проигнорируем.\nНапример, при запросах суммы на пути, мы можем насчитать во внутреннем dfs для каждой вершины сумму на пути от её текущего центроида, и если при обработке текущей компоненты встретим вторую вершину запроса, в какой-то другой ветке, то просто возьмем её сумму и уже посчитанной суммой на пути до первой вершины.\nТаким образом, каждый запрос будет просмотрен $O(\\log n)$ раз, пока не будет удален, и асимптотика составит $O(q \\log n + n \\log n)$.\n#Запросы на путях — onlineАльтернативно, можно сначала «построить» центроидную декомпозицию, а потом отвечать на запросы.\nПусть мы хотим посчитать отвечать на запросы минимума на путях и не знаем, как решать LCA двоичными подъемами. Создадим двумерный массив centroid[][] размера $n \\times \\log n$, в котором для каждой вершины будем хранить $O(\\log n)$ центроидов, в чьи компоненты она когда-то входила. Помимо этого, в таком же массиве будем хранить информацию, которая нам будем нужна для ответа на запросы — в данном случае для каждой пары вершина-центроид будем хранить минимум на соответствующем пути.\nТогда, при ответе на запрос, мы за $O(\\log n)$ или даже $O(\\log \\log n)$ операций находим центроид на нужном нам пути (первые сколько-то значений centroid[v] и centroid[u] будут совпадать — нас интересует последнее) и берём в качестве ответа минимум от минимумов на двух путях до центроида.\n#Асимптотика при более долгих пересчётахПредставим, что мы написали «мердж» (ту часть, которая отвечает за ответ на запросы или подсчёт путей, идущих через центроид) не за линейное время, а за $O(n \\log n)$, например, где-то использовав set. Сильно ли это хуже по времени?\nВ худшем случае (в бамбуке) каждый раз компонента будет разбиваться на две равные части. Просуммируем общее количество операций в дереве рекурсии:\n$$ \\sum_{k=0}^{\\log n} n \\log \\frac{n}{2^k} = \\sum_{k=0}^{\\log n} n (\\log n - k) = n \\sum_{k=0}^{\\log n} k = \\Theta (n \\log^2 n) $$\nМораль: используйте хеш-таблицы.\n","id":103,"path":"/cs/trees/centroid/","title":"Центроидная декомпозиция"},{"content":"Структуры данных — это форматы эффективного хранения и обработки логически связанных данных.\nРеализации структур данных состоят из конкретного способа хранения данных в памяти и логики, реализующей интерфейс. Когда детали реализации не важны, говорят об абстрактных структурах данных, состоящих только из интерфейса.\nСтруктуры данных повсеместно используются в алгоритмах для решения вспомогательных задач, и в этой главе мы рассмотрим несколько базовых абстрактных структур данных, которые несложно реализовать самостоятельно.\n","id":104,"path":"/cs/basic-structures/","title":"Базовые структуры данных"},{"content":"Heavy-light декомпозиция — это мощный метод решения задач на запросы на путях, когда также есть запросы обновлений. Если запросов обновления нет, то лучше написать что-нибудь попроще.\nHeavy-light декомпозицией корневого дерева называется результат следующего процесса: каждой вершины $v$ посмотрим на всех её непосредственных детей $u$, выберем среди них ребёнка $u_{max}$ с самым большим размером поддерева — если таких больше одного, то любого из них — и назовём ребро $(v, u_{max})$ тяжелым (англ. heavy), а все остальные рёбра — лёгкими (англ. light).\nТаким образом, дерево распалось на тяжелые и лёгкие рёбра. Это разбиение и называют heavy-light декомпозицией.\nДокажем несколько полезных свойств такого разбиения.\nУтверждение. Дерево разбивается на непересекающиеся пути из тяжелых рёбер.\nДоказательство. В каждую вершину входит не более одного тяжелого ребра, и из каждой вершины исходит не более одного тяжелого ребра.\nНазовём блоком либо лёгкое ребро, либо вертикальный путь из тяжелых рёбер.\nУтверждение. На любом вертикальном пути будет не более $O(\\log n)$ блоков.\nДоказательство разбивается на две части:\nЛёгких ребер на вертикальном пути будет не более $O(\\log n)$: рассмотрим самую нижнюю вершину и будем идти по вертикальному пути снизу вверх. Каждый раз, когда мы переходим по лёгкому ребру, размер поддерева текущей вершины увеличивается в два раза, потому что если вершина связана с родителем лёгким ребром, то у него есть какой-то другой ребёнок, который не легче текущего. Непрерывных путей из тяжелых рёбер будет не более $O(\\log n$: если это не конец или начало пути, то каждый такой путь окружают два лёгких ребра, а их всего $O(\\log n)$. Следствие. На любом пути будет не более $O(\\log n)$ блоков.\nРади этого мы всё и делали: теперь построим какую-нибудь структуру на каждом тяжелого пути (например, дерево отрезков), а при ответе на запрос (скажем, суммы на пути) разобьем его на $O(\\log n)$ запросов либо к подотрезкам тяжелых путей, либо к лёгким рёбрам.\n#РеализацияБольшинство публичных реализаций HLD — это 120-150 строк кода. Мы же воспользуемся следующим трюком, который сильно упростит нам жизнь: перенумеруем вершины дерева так, что для каждого тяжелого пути все его вершины будут иметь последовательные номера.\nА именно, на этапе подсчёта размера поддеревьев, изменим список смежности каждой вершины так, чтобы в самом начале шел её «тяжелый» ребёнок. Тогда, если запустить обычный эйлеров обход графа, то массив tin будет нужной нумерацией, потому что в каждой вершине мы шли в своего тяжелого ребёнка в первую очередь.\nvector\u0026lt;int\u0026gt; g[maxn]; int s[maxn], p[maxn], tin[maxn], tout[maxn]; int head[maxn]; // «голова» тяжелого пути, которому принадлежит v int t = 0; void sizes(int v = 0) { s[v] = 1; for (int \u0026amp;u : g[v]) { sizes(u); s[v] += s[u]; if (s[u] \u0026gt; s[g[v][0]]) // \u0026amp;u -- это ссылка, так что её легально использовать при swap-е swap(u, g[v][0]); } } void hld(int v = 0) { tin[v] = t++; for (int u : g[v]) { // если это тяжелый ребенок -- его next нужно передать // в противном случае он сам является головой нового пути head[u] = (u == g[v][0] ? head[v] : u); hld(u); } tout[v] = t; } Теперь мы можем построить дерево отрезков или какую-нибудь другую структуру поверх массива размера $n$ и при запросе к какому-нибудь тяжелому пути делать запрос к отрезку в этой структуре.\n#Как им решать задачиПростейший пример задачи на HLD: дано дерево, в каждой вершине которого записано какое-то число, и поступают запросы двух типов:\nУзнать минимальное число на пути между $v_i$ и $u_i$. Заменить число $v_i$-той вершины на $x_i$. Подвесим дерево за произвольную вершину и построим на нём HL-декомпозицию с деревом отрезков в качестве внутренней структуры. Его код мы приводить не будем и посчитаем, что оно реализовано примерно так же, как в соответствующей статье и имеет методы upd(k, x) и get_min(l, r).\nint val[maxn]; segtree st(0, n); При операции обновления нам нужно просто обновить нужную ячейку в дереве отрезков:\nvoid upd(int v, int x) { st.upd(tin[v], x); } Запрос минимума сложнее — нам нужно разбить исходный запрос на запросы к вертикальным путям:\nint ancestor(int a, int b) { return tin[a] \u0026lt;= tin[b] \u0026amp;\u0026amp; tin[b] \u0026lt; tout[a]; } void up(int \u0026amp;a, int \u0026amp;b, int \u0026amp;ans) { while (!ancestor(head[a], b)) { ans = min(ans, st.get_min(tin[head[a]], tin[a])); a = p[head[a]]; } } int get_min(int a, int b) { int ans = inf; up(a, b, ans); up(b, a, ans); if (!ancestor(a, b)) swap(a, b); ans = min(ans, st.get_min(tin[a], tin[b])); return ans; } Заметьте, что чтобы разбить путь на два вертикальных, нам даже не нужно отдельно решать задачу LCA: поднимаясь по тяжелым путям, мы за $O(\\log n)$ подъемов приходим к наибольшему общему предку.\n#Замечания Если в задаче нет запросов обновления, то можно «кэшировать» запросы. Заметим, что большинство запросов к структуре, которые надо сделать, находятся на префиксах тяжелых путей, и на них можно отвечать за $O(1)$ предподсчетом, и префиксных подзапросов будет $O(\\log n)$ на запрос. Также придется сделать $O(1)$ запросов к структуре, которые будут работать за $O(\\log n)$. Получаем $O(\\log n)$ на запрос. Так как наша реализация HLD строит структуру данных на эйлеровом обходе дерева, мы можем добавить запросы к поддеревьям, ведь поддеревья — это подотрезки эйлерова обхода. ","id":105,"path":"/cs/trees/heavy-light/","title":"Heavy-light декомпозиция"},{"content":"Куча (англ. heap) — абстрактная структура данных, поддерживающая следующие операции:\nНахождение минимума. Удаление минимума. Добавление нового элемента в кучу. Другое название, лучше отражающее функциональность — очередь с приоритетами (англ. priority queue).\nКучи используются во многих алгоритмах. Например, кучи используются в алгоритмах поиска кратчайшего пути, а также с помощью кучи можно проводить сортировку (путём превращения массива в кучу, а кучу в отсортированный массив).\n#Устройство двоичной кучиДвоичная куча (пирамида, сортирующее дерево, англ. binary heap) — реализация очереди с приоритетами, использующая корневое дерево, для которого выполнены три условия:\nЗначение в любой вершине не больше, чем значения её потомков. У любой вершины не более двух сыновей. Слои заполняются последовательно сверху вниз и слева направо, без «дырок». Заметим, что двоичная куча строится неоднозначно: например, значения сыновей, которые являются листами, всегда можно менять местами. Фиксирована только сама структура и предикат «родитель не больше детей».\nДвоичная куча для максимума Обозначим высоту дерева как $h$. Так как куча всегда состоит из нескольких слоев заполненных полностью и одного заполненного частично, и каждый следующий слой содержит в два раза больше вершин, чем предыдущий, то высота дерева будет $\\Theta(\\log n)$.\nКак и любая очередь с приоритетами, двоичная куча должна уметь выполнять операции:\nНахождение минимума за $O(1)$. Удаление минимума за $O(h)$. Добавление нового элемента в кучу за $O(h)$. Помимо основных трёх, можно определить и другие, например «изменить значение ключа», но пока мы остановимся на этих трёх.\n#РеализацияХранить кучу будем в виде массива $t$, где у корня индекс равен $1$, а у вершины $k$ индексы ее детей равны $2k$ и $(2k + 1)$. Нулевая ячейка массива при этом остается пустой.\nПредставление кучи в памяти Дальше определим две вспомогательные функции, которые восстанавливают инварианты кучи при изменении значения одного элемента.\nЕсли значение элемента уменьшается, то чтобы исполнялось первое условие кучи, его, возможно, нужно переместить выше. Это можно сделать, несколько раз итеративно меняя элемент с его непосредственным родителем, если его значение больше. Так как каждый раз мы поднимаемся по дереву, то работать такая функция будет за его высоту:\nvoid sift_up(int v) { // если элемент больше своего отца, то всё корректно и больше ничего делать не нужно while (v \u0026gt; 1 \u0026amp;\u0026amp; t[v] \u0026lt; t[v / 2]) { // иначе меняем его с отцом и запускаемся от отца swap(t[v], t[v / 2]); v /= 2; } } Если значение измененного элемента увеличивается, то нам нужно наоборот переместить его ниже. Это можно сделать похожим способом, итеративно меняя его с меньшим из сыновей и продолжая уже от него. Аналогично, каждый раз мы спускаемся глубже, поэтому эта процедура будет работать тоже за высоту дерева:\nvoid sift_down(int v) { // пока не пришли в лист while (2 * v \u0026lt;= n) { // n -- количество элементов в куче int l = 2 * v; // левый сын int r = 2 * v + 1; // правый сын // если правый сын существует и меньше, выбираем его int u = (r \u0026lt;= n \u0026amp;\u0026amp; t[r] \u0026lt; t[l] ? r : l); if (t[v] \u0026lt;= t[u]) break; // инвариант и так выполняется, завершаемся swap(t[v], t[u]); v = u; } } Для понимания рекомендуем посмотреть визуализацию.\nВернемся теперь к «полезным» операциям:\nМинимум мы можем выполнить за константу, просто вернув корень, то есть первый элемент массива. Для удаления минимума мы можем заменить значение корня на значение последнего элемента массива, уменьшить размер кучи, а затем запустить sift_down от корня. Для добавления элемента добавим его в конец массива, а затем вызовем sift_up от него. Примечание. Если рассматривать не двоичную кучу, а так называемую $d$-кучу, в которой у каждой вершины может быть не более $d$ сыновей, то время работы останется таким же для любой константы $d$.\n#Построение за линейное времяКучу для множества из $n$ элементов можно построить, просто добавляя их по одному — такой алгоритм построения работает за $O(n \\log n)$, что в большинстве случаев достаточно быстро. Но это можно делать и чуть быстрее — за линейное время.\nМы на самом деле хотим просто переупорядочить элементы некоторого массива $t$ так, чтобы для любого индекса $1 \\le i \\le n$ выполнялись неравенства $t_i \\le t_{2 i}$ и $t_i \\le t_{2 i + 1}$ (если, конечно, соответствующие элементы существуют).\nДавайте тогда это свойство кучи восстанавливать с конца: будем постепенно увеличивать суффикс, на котором выполняется свойство, с помощью просеивания вниз (sift_down). Получаем следующий алгоритм:\nvoid build_heap() { for (int i = n; i \u0026gt; 0; --i) { // для суффикса t[i + 1 .. n] свойство кучи выполняется sift_down(i); // теперь выполняется для суффикса t[i .. n] } } Почему это работает за $O(n)$? Давайте честно оценим сверху суммарное количество просеиваний вниз. У нас для каждой высоты $i$ будет не больше $2^{h - i}$ вершин на ней, а значит\n$$ \\sum_{i = 1}^h 2^{h - i} \\cdot i = 2^{h + 1} - h - 2 \\le 2 n - h - 2 \\le 2 n = O(n) $$\nПервое равенство несложно доказывается индукцией по $h$.\n#В стандартной библиотекеВ STL доступны несколько примитивов для работы с бинарными кучами поверх массивов:\nint ints[] = {10,20,30,5,15}; vector\u0026lt;int\u0026gt; v(ints, ints + 5); make_heap(v.begin(), v.end()); v.front(); // получить максимальный элемент (30) pop_heap(v.begin(), v.end()); // удалить максимум из кучи v.pop_back(); // уменьшить размер массива на единицу v.push_back(99); // добавить элемент в конец массива push_heap(v.begin(), v.end()); // вызовет sift_up от последнего элемента sort_heap(v.begin(), v.end()); // сортировка кучей: в v будет записан отсортированный массив Однако напрямую они используются довольно редко. Чаще используется интерфейс поверх вектора и этих операций, доступный как priority_queue:\npriority_queue\u0026lt;int\u0026gt; q; for (int x : {1,5,3,4,2,0}) q.push(x); q.top(); // вернуть максимальный элемент (5) q.pop(); // удалить максимальный элемент В priority_queue нахождение максимального (по умолчанию) элемента работает за $O(1)$, а добавление и удаление за $O(\\log n)$. Помимо этого у него есть методы .size(), .empty() и все остальные, ожидаемые от контейнеров STL.\nТакже часто в качестве очередей с приоритетом используют любые деревья поиска, например std::set.\n","id":106,"path":"/cs/basic-structures/heap/","title":"Двоичная куча"},{"content":"Ранее мы научились топологически сортировать ациклические графы. Но в циклических графах тоже иногда требуется найти какую-то структуру, для чего нам нужно ввести понятие сильной связности.\nОпределение. Две вершины ориентированного графа связаны сильно (англ. strongly connected), если существует путь из одной в другую и наоборот. Иными словами, они обе лежат в каком-то цикле.\nПонятно, что такое отношение транзитивно: если $a$ и $b$ сильно связны, и $b$ и $c$ сильно связны, то $a$ и $c$ тоже сильно связны. Поэтому все вершины распадаются на компоненты сильной связности — такое разбиение вершин, что внутри одной компоненты все вершины сильно связаны, а между вершинами разных компонент сильной связности нет.\nГраф с тремя компонентами сильной связности Самый простой пример сильно-связной компоненты — это цикл. Но это может быть и полный граф, или сложное пересечение нескольких циклов.\nЧасто рассматривают граф, составленный из самих компонент сильной связности, а не индивидуальных вершин. Очевидно, такой граф уже будет ациклическим, и с ним проще работать. Задачу о сжатии каждой компоненты сильной связности в одну вершину называют конденсацией графа, и её решение мы сейчас опишем.\n#Конденсация графаЕсли мы уже знаем, какие вершины лежат в каждой компоненте сильной связности, то построить граф конденсации несложно: нужно провести некоторые манипуляции со списками смежности, заменив для всех ребер номера вершин номерами их компонент, а затем объединив списки смежности для всех вершин каждой компоненты. Поэтому сразу сведем исходную задачу к нахождению самих компонент.\nЛемма. Запустим dfs. Пусть $A$ и $B$ — две различные компоненты сильной связности, и пусть в графе конденсации между ними есть ребро $A \\to B$. Тогда:\n$$ \\max\\limits_{a \\in A}(tout_a) \u0026gt; \\max\\limits_{b\\in B}(tout_b) $$\nДоказательство. Рассмотрим два случая, в зависимости от того, в какую из компонент dfs зайдёт первым.\nПусть первой была достигнута компонента $A$, то есть в какой-то момент времени dfs заходит в некоторую вершину $v$ компоненты $A$, и при этом все остальные вершины компонент $A$ и $B$ ещё не посещены. Но так как по условию в графе конденсаций есть ребро $A \\to B$, то из вершины $v$ будет достижима не только вся компонента $A$, но и вся компонента $B$. Это означает, что при запуске из вершины $v$ обход в глубину пройдёт по всем вершинам компонент $A$ и $B$, а, значит, они станут потомками по отношению к $v$ в дереве обхода, и для любой вершины $u \\in A \\cup B, u \\ne v$ будет выполнено $tout_v] \u0026gt; tout_u$, что и утверждалось.\nВторой случай проще: из $B$ по условию нельзя дойти до $A$, а значит, если первой была достигнута $B$, то dfs выйдет из всех её вершин ещё до того, как войти в $A$.\nИз этого факта следует первая часть решения. Отсортируем вершины по убыванию времени выхода (как бы сделаем топологическую сортировку, но на циклическом графе). Рассмотрим компоненту сильной связности первой вершины в сортировке. В эту компоненту точно не входят никакие рёбра из других компонент — иначе нарушилось бы условие леммы, ведь у первой вершины $tout$ максимальный. Поэтому, если развернуть все рёбра в графе, то из этой вершины будет достижима своя компонента сильной связности $C^\\prime$, и больше ничего — если в исходном графе не было рёбер из других компонент, то в транспонированном не будет ребер в другие компоненты.\nПосле того, как мы сделали это с первой вершиной, мы можем пойти по топологически отсортированному списку дальше и делать то же самое с вершинами, для которых компоненту связности мы ещё не отметили.\nvector\u0026lt;int\u0026gt; g[maxn], t[maxn]; vector\u0026lt;int\u0026gt; order; bool used[maxn]; int component[maxn]; int cnt_components = 0; // топологическая сортировка void dfs1(int v) { used[v] = true; for (int u : g[v]) { if (!used[u]) dfs1(u); order.push_back(v); } // маркировка компонент сильной связности void dfs2(int v) { component[v] = cnt_components; for (int u : t[v]) if (component[u] == 0) dfs2(u); } // в содержательной части main: // транспонируем граф for (int v = 0; v \u0026lt; n; v++) for (int u : g[v]) t[u].push_back(v); // запускаем топологическую сортировку for (int i = 0; i \u0026lt; n; i++) if (!used[i]) dfs1(i); // выделяем компоненты reverse(order.begin(), order.end()); for (int v : order) if (component[v] == 0) dfs2(v), cnt_components++; TL;DR:\nСортируем вершины в порядке убывания времени выхода. Проходимся по массиву вершин в этом порядке, и для ещё не помеченных вершин запускаем dfs на транспонированном графе, помечающий все достижимые вершины номером новой компоненты связности. После этого номера компонент связности будут топологически отсортированы.\n","id":107,"path":"/cs/graph-traversals/scc/","title":"Компоненты сильной связности"},{"content":"Все предыдущие алгоритмы работали с массивами, в которых лежат могут лежать абсолютно любые объекты, которые можно сравнивать: числа, строки, пары, другие массивы — почти все что угодно.\nВ особом случае, когда элементы могут принадлежать только какому-то небольшому множеству, можно использовать другой алгоритм — сортировку подсчетом (англ. counting sort).\nПусть, например, нам гарантируется, что все числа натуральные и лежат в промежутке от $1$ до $100$. Тогда есть такой простой алгоритм:\nСоздадим массив размера $100$, в котором будем хранить на $k$-ом месте, сколько раз число $k$ встретилось в этом массиве. Пройдемся по всем числам исходного массива и увеличим соответствующее значение массива на $1$. После того, как мы посчитали, сколько раз каждое число встретилось, можно просто пройтись по этому массиву и вывести $1$ столько раз, сколько встретилась $1$, вывести $2$ столько раз, сколько встретилась $2$, и так далее. Время работы такого алгоритма составляет $O(m + n)$, где $m$ — число возможных значений, $n$ — число элементов в массиве. Если количество возможных различных элементов в множестве относительно невелико, то сортировка подсчетом является одним из самых оптимальных решений.\n#Вариант реализацииСоздадим массив $c$ размера $100$ и заполним его, один раз пройдясь по исходному массиву.\nЗатем заведем указатель $k$ на позицию в исходном массиве, куда нужно записывать очередное число, и будем увеличивать его каждый раз, когда мы уменьшаем очередную ячейку в $c$.\nvoid count_sort(int *a, int n) { int c[100] = {0}; for (int i = 0; i \u0026lt; n; i++) c[a[i]]++; int k = 0; for (int i = 0; i \u0026lt; 100; i++) while (c[i]--) a[k++] = i; } ","id":108,"path":"/cs/sorting/counting/","title":"Сортировка подсчетом"},{"content":"Дерево — одна из наиболее распространенных структур данных в программировании.\nДеревья состоят из набора вершин (узлов, нод) и ориентированных рёбер (ссылок) между ними. Вершины связаны таким образом, что от какой-то одной вершины, называемой корневой (вершина 8 на рисунке), можно дойти до всех остальных единственным способом.\nДеревья принято рисовать корнем вверх Связанные определения:\nРодитель вершины $v$ — вершина, из которой есть прямая ссылка в $v$. Дети (дочерние элементы, сын, дочь) вершины $v$ — вершины, в которые из $v$ есть прямая ссылка. Предки — родители родителей, их родители, и так далее. Потомки — дети детей, их дети, и так далее. Лист (терминальная вершина) — вершина, не имеющая детей. Поддерево — вершина дерева вместе со всеми её потомками. Степень вершины — количество её детей. Глубина вершины — расстояние от корня до неё. Высота дерева — максимальная из глубин всех вершин. Деревья чаще всего представляются в памяти как динамически создаваемые структуры с явными указателями на своих детей, либо как элементы массива связанные отношениями, неявно определёнными их позициями в массиве.\nДеревья также используются в контексте графов.\nБинарные деревья поискаБинарное дерево поиска (англ. binary search tree, BST) — дерево, для которого выполняются следующие свойства:\nУ каждой вершины не более двух детей. Все вершины обладают ключами, на которых определена операция сравнения (например, целые числа или строки). У всех вершин левого поддерева вершины $v$ ключи не больше, чем ключ $v$. У всех вершин правого поддерева вершины $v$ ключи больше, чем ключ $v$. Оба поддерева — левое и правое — являются двоичными деревьями поиска. Более общим понятием являются обычные (не бинарные) деревья поиска — в них количество детей может быть больше двух, и при этом в «более левых» поддеревьях ключи должны быть меньше, чем «более правых». Пока что мы сконцентрируемся только на двоичных, потому что они проще.\nЧаще всего бинарные деревья поиска хранят в виде структур — по одной на каждую вершину — в которых записаны ссылки (возможно, пустые) на правого и левого сына, ключ и, возможно, какие-то дополнительные данные.\nstruct Node { int x; Node *l, *r; }; Как можно понять по названию, основное преимущество бинарных деревьев поиска в том, что в них можно легко производить поиск элементов:\nbool find(Node *v, int x) { if (!v) return false; if (v-\u0026gt;x == x) return true; return (v-\u0026gt;x \u0026lt; x) ? find(v-\u0026gt;l, x) : find(v-\u0026gt;r, x); } Эта функция — как и многие другие основные, например, вставка или удаление элементов — работает в худшем случае за высоту дерева. Высота бинарного дерева в худшем случае может быть $O(n)$ («бамбук»), поэтому в эффективных реализациях поддерживаются некоторые инварианты, гарантирующую среднюю глубину вершины $O(\\log n)$ и соответствующую стоимость основных операций. Такие деревья называются сбалансированными.\n","id":109,"path":"/cs/tree-structures/","title":"Деревья поиска"},{"content":"В этой главе рассматриваются структуры для неупорядоченных множеств.\n","id":110,"path":"/cs/set-structures/","title":"Структуры для множеств"},{"content":"Ликбез. Конъюнкция — это «правильный» термин для логического «И» (обозначается $\\wedge$ или \u0026amp;). Конъюнкция возвращает true тогда и только тогда, когда обе переменные true.\nЛикбез. Дизъюнкция — это «правильный» термин для логического «ИЛИ» (обозначается $\\vee$ или |). Дизъюнкция возвращает false тогда и только тогда, когда обе переменные false.\nРассмотрим конъюнкцию дизъюнктов, то есть «И» от «ИЛИ» от каких-то переменных или их отрицаний. Например, такое выражение:\n(a | b) \u0026amp; (!c | d) \u0026amp; (!a | !b) Если буквами: (А ИЛИ B) И (НЕ C ИЛИ D) И (НЕ A ИЛИ НЕ B).\nМожно показать, что любую логическую формулу можно представить в таком виде.\nЗадача satisfiability (SAT) заключается в том, чтобы найти такие значения переменных, при которых выражение становится истинным, или сказать, что такого набора значений нет. Для примера выше такими значениями являются a=1, b=0, c=0, d=1 (убедитесь, что каждая скобка стала true).\nВ случае произвольных формул эта задача быстро не решается. Мы же хотим решить её частный случай — когда у нас в каждой скобке ровно две переменные (2-SAT).\n#Графы импликацийКазалось бы — причем тут графы? Заметим, что выражение $a | b$ эквивалентно $(!a \\rightarrow b) | (!b \\rightarrow a)$. Здесь «$\\rightarrow$» означает импликацию («если $a$ верно, то $b$ тоже верно»). С помощью этой подстановки приведем выражение к другому виду — импликативному.\nЗатем построим граф импликаций: для каждой переменной в графе будет по две вершины, (обозначим их через $x$ и $!x$), а рёбра в этом графе будут соответствовать импликациям.\nГрафы импликаций полезны тем, что если положить какое-либо выражение как верное, то нужно также положить верными и все выражения, которые следуют из него, то есть достижимые в графе импликаций.\nЗаметим, что если для какой-то переменной $x$ выполняется, что из $x$ достижимо $!x$, а из $!x$ достижимо $x$, то задача решения не имеет. Действительно: какое бы значение для переменной $x$ мы бы ни выбрали, мы всегда придём к противоречию — что должно быть выбрано и обратное ему значение.\nПереформулируем данный критерий в терминах теории графов. Если из одной вершины достижима вторая и наоборот, то эти две вершины находятся в одной компоненте сильной связности. Тогда условие существования решения звучит так: для того, чтобы задача 2-SAT имела решение, необходимо, чтобы для любой переменной $x$ вершины $x$ и $!x$ находились в разных компонентах сильной связности графа импликаций.\nОказывается, что это условие является не только необходимым, но и достаточным. Доказательством этого факта служит описанный ниже алгоритм.\n#Алгоритм решения 2-SATПусть решение существует, и нам надо его найти. Заметим, что для некоторых переменных может выполняться, что из $x$ достижимо $!x$ или из $!x$ достижимо $x$ (но не одновременно). В таком случае выбор одного из значений переменной $x$ будет приводить к противоречию, в то время как выбор другого — не будет. Чтобы выбирать из двух значений то, которое не приводит к возникновению противоречий, можно воспользоваться следующим простым правилом.\nПравило. Пусть $с[x]$ обозначает номер компоненты сильной связности, которой принадлежит вершина $x$, причём номера упорядочены в порядке топологической сортировки компонент сильной связности: если есть путь из $x$ в $y$, то $c[x] \\leq c[y]$. Тогда, если $c[x] \u0026gt; c[!x]$, то выбираем значение $x$, иначе выбираем $!x$.\nДокажем, что при таком выборе значений мы не придём к противоречию. Пусть, для определённости, $c[x] \u0026gt; c[!x]$, и поэтому была выбрана вершина $x$ (для случая $!x$ доказательство аналогичное).\nПокажем, что из $x$ не достижимо $!x$. Так как номер компоненты сильной связности $c[ x ]$ больше номера компоненты $c[ !x ]$, то компонента связности $x$ расположена «левее» компоненты связности $!x$, и из первой никак не может быть достижима последняя. Покажем, что из любой вершины $y$, достижимой из $x$, недостижимо $!y$. Докажем от противного: пусть из $x$ достижимо $y$, а из $y$ достижимо $!y$. Так как мы получали граф импликаций преобразованием $(a | b) \\leftrightarrow ((!a \\rightarrow b) | (!b \\rightarrow a))$, и из $x$ достижимо $y$, то из $!y$ будет достижимо $!x$. Но, по предположению, из $y$ достижимо $!y$. Тогда мы получаем, что из $x$ достижимо $!x$, что противоречит условию. Получается, для любого отмеченного верным выражения не достижимо его отрицание, а также для любых следующих из него выражений не достижимы их отрицания, а значит противоречий при таком правиле не возникнет.\nИтак, мы построили алгоритм, который находит искомые значения переменных в предположении, что для любой переменной $x$ вершины $x$ и $!x$ находятся в разных компонентах сильной связности:\nПостроим граф импликаций, заменив все выражения вида $a | b$ двумя ребрами $!a \\rightarrow b$ и $!b \\rightarrow a$. Найдем все компоненты сильной связности в графе импликаций. Проверим, что для любого значения $x$ его отрицание лежит в другой компоненте сильной связности: $c[ x ] \\neq c[ !x ]$. Если это не так, то решения не существует. Если требуется выводить ответ, то положим условие $x$ верным, если $c[ x ] \u0026lt; c[ !x ]$, и неверным в противном случае. ","id":111,"path":"/cs/graph-traversals/2-sat/","title":"2-SAT"},{"content":"Цифровая сортировка — это способ применить идею сортировки подсчетом на большие ключи.\nПусть нам нужно отсортировать большой массив int-ов — скажем, $10^5$ элементов. Мы можем сначала отсортировать его по первым двум байтам (то есть используя $\\lfloor x / 2^{16} \\rfloor$ как ключ) стабильной сортировкой подсчетом, а затем получившуюся последовательность отсортировать по вторым двум байтам (используя $x \\bmod 2^{16}$ как ключ).\nРеализация, использующая подсчет через массив векторов:\nconst int c = (1\u0026lt;\u0026lt;16); void radix_sort(vector\u0026lt;int\u0026gt; \u0026amp;a) { int n = (int) a.size(); vector\u0026lt;int\u0026gt; b[c]; for (int i = 0; i \u0026lt; n; i++) b[a[i] % c].push_back(a[i]); int k = 0; for (int i = 0; i \u0026lt; c; i++) { for (size_t j = 0; j \u0026lt; b[i].size(); j++) a[k++] = b[i][j]; b[i].clear(); } for (int i = 0; i \u0026lt; n; i++) b[a[i]/c].push_back(a[i]); k = 0; for (int i = 0; i \u0026lt; c; i++) for (size_t j = 0; j \u0026lt; b[i].size(); j++) a[k++] = b[i][j]; } Этот подход можно обобщить на любой тип и размер ключа.\n","id":112,"path":"/cs/sorting/radix/","title":"Цифровая сортировка"},{"content":"Бинарные деревья поиска можно использовать почти для любых запросов про точечные множества: добавление, удаление, сумма, минимум, прибавление на отрезке, $k$-тое меньшее\u0026hellip;\nОднако у общих бинарных деревьев есть и несколько недостатков: их сложно реализовывать, они довольно медленно работают (за счет прохождения по ссылкам), и на самом деле для некоторых запросов они не оптимальны.\nЕсли рассматриваемые множества фиксированного размера (например, массив размера $10^6$), то можно построить поверх них более эффективные статические структуры, которые не меняют свое устройство с течением времени, а только изменяют свои значения. О таких структурах мы и поговорим в этой главе.\n","id":113,"path":"/cs/range-queries/","title":"Запросы на отрезках"},{"content":"Персистентные структуры данных (англ. persistent data structures) — это структуры данных, которые при внесении в них изменений сохраняют доступ ко всем своим предыдущим состояниям.\nЕсть несколько «уровней» персистентности:\nЧастичная — к каждой версии можно делать запросы, но изменять можно только последнюю. Полная — можно делать запросы к любой версии и менять любую версию. Конфлюэнтная — помимо этого можно объединять две структуры данных в одну (например, сливать вместе кучи или деревья поиска). Функциональная — структуру можно реализовать на чистом функциональном языке: для любой переменной значение может быть присвоено только один раз и изменять значения переменных нельзя. Персистентные структуры используются в текстовых редакторах, системах контроля версий, базах данных, в некоторых параллельных алгоритмах, а также в функциональном программировании.\nПомимо этого, персистентные структуры нередко используются и для решения «обычных» алгоритмических задач, что и будет основным фокусом этой главы.\n","id":114,"path":"/cs/persistent/","title":"Персистентность"},{"content":"Дерево отрезков — очень мощная и гибкая структура данных, позволяющая быстро отвечать на самые разные запросы на отрезках.\nРассмотрим конкретную задачу: дан массив $a$ из $n$ целых чисел, и требуется отвечать на запросы двух типов:\nИзменить значение в ячейке (т. е. реагировать на присвоение a[k] = x). Вывести сумму элементов $a_i$ на отрезке с $l$ по $r$. Оба запроса нужно обрабатывать за время $O(\\log n)$.\nСтруктура дерева отрезковЧтобы решить задачу, сделаем с исходным массивом следующие манипуляции.\nПосчитаем сумму всего массива и где-нибудь запишем. Потом разделим его пополам, посчитаем сумму на половинах и тоже где-нибудь запишем. Каждую половину потом разделим пополам ещё раз, и так далее, пока не придём к отрезкам длины 1.\nЭту последовательность разбиений можно представить в виде дерева.\nДерево отрезков для $n=8$ является полным бинарным деревом Корень этого дерева соответствует отрезку $[0, n)$, а каждая вершина (не считая листьев) имеет ровно двух сыновей, которые тоже соответствуют каким-то отрезкам. Отсюда и название — «дерево отрезков».\nРазные полезные свойстваВысота дерева отрезков равна $\\Theta(\\log n)$: на каждом новом уровне длина отрезка уменьшается вдвое. Этот факт будет ключевым для оценки асимптотики операций.\nЛюбой полуинтервал разбивается на $O(\\log n)$ неперекрывающихся полуинтервалов, соответствующих в вершинам дерева: с каждого уровня нам достаточно не более двух отрезков.\nДерево содержит менее $2n$ вершин: первый уровень дерева отрезков содержит одну вершину (корень), второй уровень — в худшем случае две вершины, на третьем уровне в худшем случае будет четыре вершины, и так далее, пока число вершин не достигнет $n$. Таким образом, число вершин в худшем случае оценивается суммой $n + \\frac{n}{2} + \\frac{n}{4} + \\frac{n}{8} + \\ldots + 1 \u0026lt; 2n$. Значит, оно линейное по памяти.\nПри $n$, отличных от степеней двойки, не все уровни дерева отрезков будут полностью заполнены. Например, при $n=3$ левый сын корня есть отрезок $[0, 2)$, имеющий двух потомков, в то время как правый сын корня — отрезок $[2, 3)$, являющийся листом.\nОк, как это нам поможет?Опишем, как с помощью такой структуры решить исходную задачу.\nЗапрос обновления. Нам нужно обновить значения в вершинах таким образом, чтобы они соответствовали новому значению $a[k] = x$.\nИзменим все вершины, в суммах которых участвует $k$-тый элемент. Их будет $\\Theta(\\log n)$ — по одной с каждого уровня.\nЭто можно реализовать как рекурсивную функцию: ей передаётся текущая вершина дерева отрезков, и функция выполняет рекурсивный вызов от одного из двух своих сыновей (от того, который содержит $k$-ый элемент в своём отрезке), а после этого — пересчитывает значение суммы в текущей вершине точно таким же образом, как мы это делали при построении дерева отрезков.\nЗапрос суммы. Мы знаем, что во всех вершинах лежат корректные значения, и нам с помощью них посчитать сумму на отрезке.\nСделаем тоже рекурсивную функцию, рассмотрев три случая:\nЕсли отрезок вершины лежит целиком в отрезке запроса, то вернуть записанную в ней сумму. Если отрезки вершины и запроса не пересекаются, то вернуть 0. Иначе разделиться рекурсивно на 2 половины и вернуть сумму этой функции от обоих детей. Чтобы разобраться, почему это работает за $O(\\log n)$, нужно оценить количество «интересных» отрезков — тех, которые порождают новые вызовы рекурсии. Это будут только те, которые содержат границу запросов — остальные сразу завершатся. Обе границы отрезка содержатся в $O(\\log n)$ отрезках, а значит и итоговая асимптотика будет такая же.\nДерево отрезков можно использовать для гораздо большего, чем только для суммы.\nДалее этой главе мы рассмотрим разные реализации и варианты этой структуры и их применения.\n","id":115,"path":"/cs/segment-tree/","title":"Дерево отрезков"},{"content":" Динамическое программирование — это когда у нас есть задача, которую непонятно как решать, и мы разбиваем ее на меньшие задачи, которые тоже непонятно как решать.\nРассмотрим такую задачу: требуется найти $n$-ое число Фибоначчи $f_n$, определяемое как\n$$ \\begin{aligned} f_0 \u0026amp;= 0 \\ f_1 \u0026amp;= 1 \\ f_n \u0026amp;= f_{n-2} + f_{n-1} \\end{aligned} $$\nЗаписав определение выше как функцию, задачу можно решить рекурсивно:\nint f(int n) { if (n == 0) return 0; if (n == 1) return 1; return f(n - 2) + f(n - 1); } Однако для $n$, превосходящих ~40, такое решение будет работать очень долго. Попытаемся грубо оценить время работы $T(n)$.\nЧтобы посчитать $f_n$, нам нужно рекурсивно посчитать $f_{n-1}$ и $f_{n-2}$:\n$$T(n) = T(n-2) + T(n-1) + \\Theta(1)$$\nДля подсчета $f_{n-1}$ нужно не меньше действий, чем для $f_{n-2}$. Объединяя с предыдущим, это значит, что $f_n$ нужно хотя бы в два раза больше действий, чем для $f_{n-2}$:\n$$ T(n) \\ge 2 \\cdot T(n-2) $$\nА значит, время работы растет экспоненциально:\n$$ T(n) \\ge \\Omega(2^{n/2}) $$\nУпражнение. Какое точное суммарное количество рекурсивных вызовов будет при подсчете $f_n$?\nЗапоминание результатовОсновная идея — сводить задачу к меньшим, и в какой-то момент к базовым — совершенно правильная.\nОднако нужно следить за тем, чтобы число этих задач не росло экспоненциально. Для этого проще всего не совершать никаких лишних действий: если мы один раз посчитали $f_n$, то давайте запомним, чему оно равно, и в следующий раз, когда оно нам понадобится, будем использовать его сразу.\nУдобнее всего завести массив для всех нужных значений $f_n$ и, вместо рекурсивной функции, инициализировать его базовые значения, а затем пройтись в цикле по всем остальным $f_i$ и посчитать их, используя уже известные после предыдущих итераций $f_{i-2}$ и $f_{i-1}$:\nconst int n = 100; int f[n]; f[0] = 0; f[1] = 1; for (int i = 2; i \u0026lt; n; i++) f[i] = f[i - 2] + f[i - 1]; (Для больших $n$ в последней строчке будет происходить переполнение, поэтому часто в подобных задачах все значения рассматривают по модулю.)\nДинамическим программированием (англ. dynamic programming, ДП, динамика) называется как раз приведенный выше подход, обычно заключающийся в следующих общих шагах:\nСвести задачу для $n$ к задаче для чисел, меньших, чем $n$ — то есть найти какую-то рекурсивную формулу. Создать массив (или какую-нибудь другую структуру данных) для хранения ответов на подзадачи. Заполнить начало массива вручную (базовые случае, для которых формула не работает). Обойти массив от известных значений в неизвестные и заполнить ответы по формуле. Вывести ответ — обычно просто записанный в последней ячейке массиве. Чтобы решить задачу динамическим программированием, вы должны ответить на 5 вопросов:\nЧто лежит в массиве? (чаще всего самый важный вопрос) Как инициализировать начало массива? Как обходить массив? (чаще всего слева направо, но не всегда) Какой формулой считать элементы массива? Где в массиве лежит ответ? Этим приемом можно решать большое количество важных задач, и в этом разделе мы в этом убедимся.\n","id":116,"path":"/cs/general-dynamic/","title":"Общие приёмы динамики"},{"content":"Комбинаторная оптимизация заключается в поиске некотрого оптимального объекта в конечном множестве объектов.\nЗа долгое время любую задачу комбинаторной оптимизации можно решить полным перебором, но для большого класса задач есть либо гораздо более быстрые точные решения, либо весьма хорошие аппроксимации. О таких задачах мы и поговорим в этой главе.\n","id":117,"path":"/cs/combinatorial-optimization/","title":"Комбинаторная оптимизация"},{"content":"Теория игр занимается изучением оптимальных стратегий в играх: произвольных процессах, в которых участвуют две и более сторон, ведущие борьбу за реализацию своих интересов.\nПод играми люди обычно подразумевают разные «несерьёзные» активности, в которых люди участвуют для обучения, социализации или утоления психологических потребностей. Например:\n«Крестики-нолики» Шахматы Покер «Колонизаторы» «Мафия» StarCraft II Однако в более общем смысле почти все взаимодействия в реальном мире являются математическими играми:\nТорговля Аукционы Поиск работы Дейтинг Преступность Конкуренция в бизнесе Войны Политика Любые переговоры Чтобы достаточно точно описать всё это разнообразие игр, нужна очень богатая теория, во многом граничащая с экономикой и информатикой.\nВ этом разделе мы сконцентрируемся на отдельном классе комбинаторных игр, которые можно решать с помощью динамического программирования.\n","id":118,"path":"/cs/games/","title":"Теория игр"},{"content":"Задача. Даны $n$ точек на прямой, отсортированные по своей координате $x_i$. Нужно найти $m$ отрезков, покрывающих все точки, минимизировав при этом сумму квадратов их длин.\nБазовое решение — определить состояние динамики $f[i, j]$ как минимальную стоимость покрытия $i$ первых точек используя не более $j$ отрезков. Пересчитывать её можно перебором всех возможных последних отрезков:\n$$ f[i, j] = \\min_{k \u0026lt; i} {f[k, j-1] + (x_{i-1}-x_k)^2 } $$\nИтоговым ответом будет $f[n, m]$, а суммарно такая динамика будет работать за $O(n^2 m)$.\n// x[] — отсортированный массив координат точек, индексация с нуля // квадрат длины отрезка с i-той до j-той точки int cost(int i, int j) { return (x[j] - x[i]) * (x[j] - x[i]); } for (int i = 0; i \u0026lt;= m; i++) f[0][i] = 0; // если нам не нужно ничего покрывать, то всё и так хорошо // все остальные f предполагаем равными бесконечности for (int i = 1; i \u0026lt;= n; i++) for (int j = 1; j \u0026lt;= m; j++) for (int k = 0; k \u0026lt; i; k++) f[i][j] = min(f[i][j], f[k][j - 1] + cost(k, i - 1)); (Заметим, что циклы по i и j можно поменять местами.)\nВ этой главе мы рассмотрим 4 связанных между собой способа её соптимизировать, помимо непосредственно этой задачи обобщающихся и на многие другие динамики.\n","id":119,"path":"/cs/layer-optimizations/","title":"Пересчет динамики по слоям"},{"content":"В этой главе мы рассмотрим основные алгоритмы вычислительной алгебры.\nРекомендуется математическая подготовка на уровне младших курсов технических вузов.\n","id":120,"path":"/cs/algebra/","title":"Алгебра"},{"content":"Компьютеры обычно хранят время в секундах, прошедших с 1 января 1970 года — начала «эпохи Unix».\nЛюди также хранят время относительно какого-то момента в прошлом — обычно имеющего какую-то политическую, культурную или религиозную значимость — в зависимости от календаря. На момент написания этой статьи, прошло примерно 63882260594 секунд с 0 года н. э.\nНо в отличие от компьютеров, мы в повседневной жизни не пользуемся всей этой информацией. В зависимости от задачи, нам достаточно знания о том, что сейчас 14:00 и пора на обед, или что сегодня четверг и в Subway скидка на итальянский BMT. Вместо целого таймстемпа мы используем его остаток, содержащий лишь ту информацию, которая нам нужна.\nСамая удобная вещь про остатки в том, что они цикличны: после 12 на часах идёт 1, и поэтому число всегда остается небольшим. Гораздо проще работать с одно- или двузначными числами вместо 11-значных.\nЗадача. Пусть сегодня четверг. Какой день недели будет через год?\nЕсли мы пронумеруем все дни недели от 0 с 6 начиная с понедельника, то четверг получит номер 3. Нам нужно добавить к этому числу 365 и затем взять остаток от деления на 7. Так как 365 % 7 удобно равен 1, то мы получаем, что через год будет пятница (если это не високосный год, в случае чего будет суббота).\nМодулярная арифметика занимается изучением подобных интересных свойств остатков.\n","id":121,"path":"/cs/modular/","title":"Модулярная арифметика"},{"content":"Одним из важнейших достижений античной греческой математики является доказательство утверждения, которое мы сейчас называем основной теоремой арифметики:\nВсякое число, большее 1, может быть разложено в произведение простых чисел, и это разложение единственно с точностью до порядка множителей.\n— Евклид, IV век до н. э.\nОднако древние греки по какой-то причине не разработали эффективные методы вычисления этого разложения на миллиметровых кристаллах кремния — чем мы в свою очередь и займёмся в этой главе.\n","id":122,"path":"/cs/factorization/","title":"Факторизация и простые числа"},{"content":"Определение. Графом называется пара множеств $G = (V, E)$, где множество $V$ называется множеством вершин графа, а множество $E$, состоящее из пар вершин $(u, v)$, называется множеством ребер графа.\nЕсли существует ребро $(u, v)$, то говорят что оно соединяет вершины $u$ и $v$. Две вершины, соединенные ребром, называют смежными. Для вершины $v$ любое ребро $(v, u)$ называется инцидентным ей. Число ребер, инцидентных вершине $v$, называют степенью вершины $v$.\nЕсли какие-то две вершины соединены более, чем одним ребром, то говорят, что граф содержит кратное ребро. Если ребро соединяет вершину саму с собой, то такое ребро называют петлей. Если граф не содержит петель и кратных ребер, то он называется простым.\nРазные простые неориентированные графы Граф называют ориентированным, если для каждого ребра задано направление, и неориентированным в противном случае. Для ребра $(u, v)$ обратным ребром называется ребро $(v, u)$. Неориентированные графы часто рассматривают как ориентированные, добавив вместо каждого ребра два для каждого направления.\nПутем называется последовательность вершин, соединенных между собой ребрами. Если в пути не повторяются вершины, он называется простым. Из вершины $v$ достижима вершина $u$, если существует путь, начинающийся в $v$ и заканчивающийся в $u$. Если от каждой вершины достижима любая другая, граф называется связным.\nЦиклом называется путь, начинающийся и заканчивающийся в одной и той же вершине. Если помимо первой и последней вершины все остальные вершины на этой пути уникальны, то такой цикл называется простым. Граф называют ацикличным, если в нем нет простых циклов.\nАцикличный неориентированный граф называется лесом. Связный лес называется деревом. У деревьев есть другие эквивалентные определения:\nГраф является деревом, если в нем $n$ вершин и $(n-1)$ ребер и нет циклов. Граф является деревом, если из любой вершины можно дойти в любую другую единственным образом. Если дерево называется корневым, если у него есть ровно одна специальная вершина, называемая корнем. Часто подразумевается, что ребра корневого дерева ориентированы так, что от корня можно дойти от всех остальных вершин.\nГраф называется планарным, если его можно разложить на плоскости так, чтобы его ребра не пересекались в точках, отличных от вершин. Граф называется двудольным, если его вершины которого можно разделить на два множества таких, что ребра соединяют только вершины из разных множеств.\nАнгло-русский графовый словарь граф — graph вершина — vertex вершины — vertices ребро — edge смежный — adjacent степень — degree петля — self-loop ориентированный граф — directed graph неориентированный граф — undirected graph цикл — cycle ацикличный граф — acyclic graph дерево — tree планарный граф — planar graph двудольный граф — bipartite graph В теории графов есть много других важных понятий, до которых мы дойдем позже, когда они понадобятся.\n","id":123,"path":"/cs/graph-traversals/","title":"Обходы графов"},{"content":"Определение. Кратчайшим путем между вершинами $a$ и $b$ в неориентированном графе называется путь между ними, содержащий наименьшее количество ребер.\nВ зависимости от контекста, под длиной пути может иметься в виду как число ребер ($k$), число промежуточных вершин ($k-1$), так и суммарное число вершин ($k+1$), включая начало и конец.\nКратчайший путь между парой вершин не всегда уникален.\nОт A до F три перехода Определение. Взвешенным графом называется граф, в котором каждому ребру сопоставляется число, называемое весом, длиной или стоимостью.\nВо взвешенных графах длиной пути называется суммарная длина всех его ребер.\nНесмотря на то, что в выделенном пути больше ребер, его суммарная стоимость меньше Если не сказано обратного, длина ребра может быть любым числом — в том числе отрицательным. В графах с отрицательными ребрами может возникнуть контринтуитивная ситуация, когда есть цикл отрицательного веса, по которому можно ходить бесконечно и таким образом получить путь произвольного малого веса между двумя вершинами, в промежутке проходя много раз по этому циклу. Поэтому обычно рассматривают графы с неотрицательными длинами ребер.\nЧасто требуется искать кратчайшие пути в графах. Эта задача имеет несколько тесно связанных вариаций:\nНахождение кратчайшего пути между заданной парой вершин $a$ и $b$. Нахождение кратчайших путей между заданной вершиной $a$ и всеми остальными вершинами в графе. Нахождение кратчайших путей между каждой парой вершин в графе. В этой главе мы рассмотрим несколько классических алгоритмов для решения этой и других смежных задач.\n","id":124,"path":"/cs/shortest-paths/","title":"Кратчайшие пути"},{"content":"Рассмотрим следующую задачу:\nАвиакомпания содержит $m$ рейсов между $n$ городами, $i$-ый из них обходится в $w_i$ рублей, причём из любого города можно добраться до любого другого. В стране наступил кризис, и нужно отказаться от как можно большего числа из них таким образом, что содержание оставшихся рейсов будет наиболее дешевым.\nИными словами, нужно найти дерево минимального веса, которое является подграфом данного неориентированного графа. Почему дерево? Потому что в противном случае там был бы цикл, из которого можно удалить какое-то ребро и получить более оптимальный ответ. А если это больше, чем одно дерево, то какие-то две вершины остаются несвязны.\nТакие деревья называют остовами (каркас, скелет; ударение на первый слог, но так мало кто произносит; англ. minimum spanning tree — дословно, минимальное покрывающее дерево).\nВ этой статье мы рассмотрим алгоритмы нахождения остовных деревьев и классические задачи на них.\n","id":125,"path":"/cs/spanning-trees/","title":"Связность и остовные деревья"},{"content":"Задача. Пусть есть $n$ мальчиков и $m$ девочек. Про каждого мальчика и про каждую девочку известно, с кем они не против танцевать. Нужно составить как можно больше пар, в которых партнёры хотят танцевать друг с другом.\nФормализуем эту задачу, представив мальчиков и девочек как вершины в двудольном графе, рёбрами которого будет отношение «могут танцевать вместе». Будем в дальнейшем обозначать левую долю графа как $L$, а правую долю как $R$.\nПаросочетанием $M$ называется набор попарно несмежных рёбер графа (иными словами, любой вершине графа должно быть инцидентно не более одного ребра из $M$).\nВсе вершины, у которых есть смежное ребро из паросочетания (т.е. которые имеют степень ровно один в подграфе, образованном $M$), назовём насыщенными этим паросочетанием.\nМощностью паросочетания назовём количество рёбер в нём. Наибольшим (максимальным) паросочетанием назовём паросочетание, мощность которого максимальна среди всех возможных паросочетаний в данном графе, а совершенным — где все вершины левой доли им насыщенны.\nПаросочетания можно искать не только в двудольных графах, однако общий алгоритм неприятно кодить, и он работает за $O(n^3)$, так что в этой главе мы сфокусируемся только на двудольных графах.\n","id":126,"path":"/cs/matching/","title":"Паросочетания"},{"content":"Дерево называется корневым, если оно ориентировано, и из какой-то вершины (называемой корнем) можно попасть во все остальные.\nПримеры корневых деревьев:\nнаследование классов в языках программирования (если множественное наследование запрещено), дерево факторизации числа на простые (в общем случае не уникальное), иерархия в какой-нибудь организации, дерево парсинга математичеких выражений. Задачи на корневые деревья весьма бесполезны в реальной жизни, но зато очень интересны с алгоритмической точки зрения, и поэтому часто встречаются на олимпиадах по программированию.\n","id":127,"path":"/cs/trees/","title":"Корневые деревья"},{"content":"Задача. Есть дом, вода из которого стекает в реку по водопроводу. Водопровод — это набор однонаправленных труб, концы которых соединены с домом, рекой или одним из $n$ соединительных узлов. Также для каждой трубы известна ее прочность — сколько воды она может выдержать. Необходимо найти максимальный объем воды, который может вытекать из дома в единицу времени.\nПоставим задачу более формально. Есть граф в котором выделены две вершины, соответствующие дому $s$ и реке $t$, а остальные вершины соответствуют узлам. Ребра этого графа соответствуют трубам водопровода, и у каждого ребра есть пропускная способность — максимальное количество воды, которое может по нему течь. Обозначим за $c(v, u)$ пропускную способность ребра из $v$ в $u$, а $f(v, u)$ — количество воды, текущее по нему.\nЗаметим некоторые свойства воды, текущей в водопроводе. Например, для любой вершины, которая соответствует узлу, объем втекающей в неё воды равен объему вытекающей из неё воды:\n$$ \\sum_u f(u, v) = \\sum_u f(v, u) $$\nЭто значит, что вода не может появится из ниоткуда и исчезнуть в никуда.\nТакже удобно считать, что у любого ребра есть ровно одно обратное, и $f(v, u) = -f(u, v)$, то есть если по ребру из $v$ в $u$ перетекает $x$ потока, то по ребру из $u$ в $v$ перетекает $-x$ потока.\nТеперь замечание о втекающем и вытекающем объеме звучит так, что для любой вершины $v$, кроме истока $s$ и стока $t$:\n$$ \\sum_u f(v, u) = 0 $$\nНаша задача тогда — это найти такую функцию $f$, максимизирующую $\\sum f(s, u) = \\sum f(u, t)$, удовлетворяющую предыдущим равенствам, и при этом не превышающая ни одну пропускную способность:\n$$ f(v, u) \\leq c(v, u) $$\nЭта сложная задача имеет множество решений, которые мы разберем в этой главе.\n","id":128,"path":"/cs/flows/","title":"Потоки"},{"content":"В математике, геометрические задачи в основном доказательные: обычно требуется показать, что угол $A$ равен углу $B$, или что такой-то отрезок во столько-то раз больше другого. В школе её используют для знакомства с основными принципами логики и отучивания от слова «очевидно».\nВ программировании (как и в реальной жизни) задачи совершенно другого плана: геометрия больше количественная, чем качественная, и применяется для оптимизации чего-то, для подсчета точных величин, или для выполнения зависящих от конкретных чисел проверок.\nДля решения подобных задач есть два подхода: алгебраический и конструктивный. Когда математик говорит «пересечем две прямые», он представляет громоздкое уравнение, с которым он потом будет работать. Программист же хочет абстрагироваться и написать функцию intersect(a, b), в корректности которой он точно уверен, которую он будет вызывать по необходимости, не беспокоясь об алгебраических выражениях. Это позволяет разбить задачу на много маленьких кусочков, которые можно решать по отдельности, а не возиться с формулами.\nВ этой главе мы рассмотрим как раз такой конструктивный подход к вычислительной геометрии и научимся манипулировать различными геометрическими объектами с помощью ООП в C++.\n","id":129,"path":"/cs/geometry-basic/","title":"Геометрические примитивы"},{"content":"Выпуклое множество — такое множество точек, что, для любых двух точек множества, все точки на отрезке между ними тоже принадлежат этому множеству.\nВыпуклая оболочка множества точек — такое выпуклое множество точек, что все точки фигуры также лежат в нем.\nМинимальная выпуклая оболочка множества точек — это минимальная по площади выпуклая оболочка.\nДля экономии времени дальше минимальные выпуклые оболочки мы будем называть просто выпуклыми оболочками.\nДля практических целей выпуклые оболочки полезны тем, что они компактно хранят всю необходимую информацию о множестве точек, что позволяет быстро отвечать на разнообразные запросы на этом множестве.\nВыпуклые оболочки можно рассматривать в любом пространстве, но в этой статье мы ограничимся двумерным и научимся их эффективно и строить по какому-то множеству из $n$ точек на плоскости и применять для ответов разнообразные на запросы об этом множестве.\n","id":130,"path":"/cs/convex-hulls/","title":"Выпуклые оболочки"},{"content":"Рассмотрим задачу, которая возникает каждый раз, когда вы делаете ctrl+f:\nЕсть большой текст $t$. Нужно найти все вхождения строки $s$ в него.\nНаивное решение со сравнением всех подстрок $t$ длины $|s|$ со строкой $s$ работает за $O(|t| \\cdot |s|)$. Если текст большой, то длинные слова в нем искать становится очень долго.\nОднако существует множество способов решить эту задачу за $O(|s| + |t|)$, два самых распространённых и простых из них: через префикс-функцию и через z-функцию (примечание: не «зи», а «зет»).\n","id":131,"path":"/cs/string-searching/","title":"Поиск подстроки"},{"content":"Хеш — это какая-то функция, сопоставляющая объектам какого-то множества числовые значения из ограниченного промежутка.\n«Хорошая» хеш-функция:\nБыстро считается — за линейное от размера объекта время; Имеет не очень большие значения — влезающие в 64 бита; «Детерминировано-случайная» — если хеш может принимать $n$ различных значений, то вероятность того, что хеши от двух случайных объектов совпадут, равна примерно $\\frac{1}{n}$. Обычно хеш-функция не является взаимно однозначной: одному хешу может соответствовать много объектов. Такие функции называют сюръективными.\nДля некоторых задач удобнее работать с хешами, чем с самими объектами. Пусть даны $n$ строк длины $m$, и нас просят $q$ раз проверять произвольные две на равенство. Вместо наивной проверки за $O(q \\cdot n \\cdot m)$, мы можем посчитать хеши всех строк, сохранить, и во время ответа на запрос сравнивать два числа, а не две строки.\nПрименения в реальной жизни Чек-суммы. Простой и быстрый способ проверить целостность большого передаваемого файла — посчитать хеш-функцию на стороне отправителя и на стороне получателя и сравнить. Хеш-таблица. Класс unordered_set из STL можно реализовать так: заведём $n$ изначально пустых односвязных списков. Возьмем какую-нибудь хеш-функцию $f$ с областью значений $[0, n)$. При обработке .insert(x) мы будем добавлять элемент $x$ в $f(x)$-тый список. При ответе на .find(x) мы будем проверять, лежит ли $x$-тый элемент в $f(x)$-том списке. Благодаря «равномерности» хеш-функции, после $k$ добавлений ожидаемое количество сравнений будет равно $\\frac{k}{n}$ = $O(1)$ при правильном выборе $n$. Мемоизация. В динамическом программировании нам иногда надо работать с состояниями, которые непонятно как кодировать, чтобы «разгладить» в массив. Пример: шахматные позиции. В таком случае нужно писать динамику рекурсивно и хранить подсчитанные значения в хеш-таблице, а для идентификации состояния использовать его хеш. Проверка на изоморфизм. Если нам нужно проверить, что какие-нибудь сложные структуры (например, деревья) совпадают, то мы можем придумать для них хеш-функцию и сравнивать их хеши аналогично примеру со строками. Криптография. Правильнее и безопаснее хранить хеши паролей в базе данных вместо самих паролей — хеш-функцию нельзя однозначно восстановить. Поиск в многомерных пространствах. Детерминированный поиск ближайшей точки среди $m$ точек в $n$-мерном пространстве быстро не решается. Однако можно придумать хеш-функцию, присваивающую лежащим рядом элементам одинаковые хеши, и делать поиск только среди элементов с тем же хешом, что у запроса. Хешируемые объекты могут быть самыми разными: строки, изображения, графы, шахматные позиции, просто битовые файлы.\nВ этом разделе мы в основном сфокусируемся на строках.\n","id":132,"path":"/cs/hashing/","title":"Хеширование"},{"content":"Автоматы — абстрактные математические объекты, как-то реагирующие на некоторые события, или символы.\nАвтомат определяется начальным состоянием, множеством возможных состояний, множеством символов, а также функцией, которая по паре из текущего состояния и символа определяет, в какое новое состояние должен переходить автомат.\nАвтоматы могут описывать много различных процессов и вычислений. Например, компьютер сам по себе является частным случаем реализации автомата: состоянием является содержимое оперативной памяти и регистров, событиями — взаимодействие с пользователем, правилами перехода — архитектура.\nДругим примером являются клеточные автоматы, возможно знакомые читателю по игре «Жизнь» Джона Конвея. В ней состояние и правила переходов автомата определяется состоянием каждой индивидуальной клетки, так, что правила применяются сразу ко всей решетке.\n«Ружьё Госпера» в игре «Жизнь» В этом разделе мы рассмотрим конкретные применения автоматов в контексте строковых задач.\n","id":133,"path":"/cs/string-structures/","title":"Строковые структуры"},{"content":"Во многих задачах ответ требуется найти не абсолютно точно, а с каким-то допустимым уровнем ошибки. Часто это мотивируется тем, что в данных из реального мира и так содержатся ошибки моделирования и измерения, на несколько порядков превосходящие ошибки численных алгоритмов.\nВ этой главе мы рассмотрим несколько подобных общих методов нахождения приближенных решений.\n","id":134,"path":"/cs/numerical/","title":"Численные методы"},{"content":"Везде, где не указано — время работы $O(n)$, а если есть конкретные числа, то TL 1 секунда.\nЗадачи идут в порядке вспоминания/придумывания, то есть в весьма рандомном.\n#ПопугаиВам нужно передать 64 байт некоторой информации. Для этого у вас есть 320 специально обученных попугаев, каждый из которых может запомнить число от 0 до 255. Все попугаи рано или поздно долетают до точки назначения и сообщают свой байт, но, возможно, не в том порядке, в котором были выпущены. Попугаи внешне неразличимы.\n#Минимум и максимумДаны 68 камней разного веса. Требуется за 100 операций взвешивания определить самый лёгкий и самый тяжелый.\n#ПерестановкаЕсть перестановка из 64 элементов. Вы можете спрашивать, какие элементы находятся на заданном множестве позиций (вам сообщается список элементов в произвольном порядке). Восстановите перестановку за 6 запросов.\n#Выпуклая оболочкаТребуется отвечать на два типа запросов:\nДобавить точку в выпуклую оболочку. Проверить, лежит ли точка внутри выпуклой оболочки. Обе операции онлайн за $O(\\log n)$.\n#Геометрическая прогрессияНайдите способ посчитать $\\frac{1-a^n}{1-a}$ по произвольному модулю за $O(\\log n)$.\n#ПокемоныВ турнире участвуют 1024 видов покемонов. Вы, будучи экспертом по покемонам, знаете, кто кого может победить. Иначе говоря, вам полностью известен граф-турнир из 1024 вершин и $1023 \\times 1022 : 2$ рёбер, в котором каждые две вершины соединены ровно одним ориентированным ребром, определяющим победителя в возможном поединке. Обратите внимание, что из $a \\to b$ и $b \\to c$ не следует, что $a \\to c$.\nУ вас есть 10 покеболов. Составьте команду из 10 покемонов такую, что для каждого из 1024 типов покемонов в вашей команде найдется покемон, побеждающий его. Считайте, что в битве одинаковых покемонов побеждает ваш.\n#СортировкаМожно ли отсортировать\n5 камней за 8 взвешиваний? 5 камней за 7 взвешиваний? 20 камней за 60 взвешиваний? #Точки в кругеДаны $n$ точек, равномерно распределенных в единичном круге с центром в начале координат. Предложите алгоритм, который за $O(n)$ в среднем сортирует их по удаленности от начала координат.\n#Бинарный поискНа стороне интерактора был сгенерирован массив из $n$ случайных чисел, равномерно распределенных на некотором промежутке, а затем отсортирован. За один запрос можно по индексу в массиве узнать число, которое там лежит. Требуется за $O(\\log \\log n)$ операций в среднем определять, есть ли в данном массиве число $x$.\n#Случайный бинарный поискДан такой бинарный поиск, в котором мы вместо элемента из середины берем случайный:\nint lower_bound(int x) { int l = 0, r = n - 1; while (l \u0026lt; r) { int m = l + rand() % (r - l); if (t[m] \u0026gt;= x) r = m; else l = m + 1; } return l; } Все ключи и запросы случайные. При $n \\to \\infty$, во сколько в среднем раз больше итераций рандомизированный бинарный поиск будет делать по сравнению с обычным?\n#Замкнутые ломаныеДаны две замкнутые несамопересекающиеся ломаные. Определите, можно ли перевести их друг в друга с помощью параллельного переноса, поворотов и гомотетии?\n#Неубывающий массивДан массив из $n$ целых чисел. Требуется за $2n$ операций «прибавить к одному элементу любой другой» сделать его неубывающим.\n#Чётный циклДан неориентированный граф. Определите, есть ли в нём простой цикл чётной длины.\n#$k$-ая порядковая статистикаДан массив из $n$ целых чисел. Найдите его $k$-й наименьший элемент за $O(n)$.\n#Доминирующий элементДан массив из $n$ элементов. Требуется ответить на $m$ запросов, есть ли на отрезке $[l, r]$ доминирующий элемент — тот, который встречается на нём хотя бы $\\frac{r-l}{2}$ раз. Время работы $O((n+m) \\log n)$.\n#Разрушение дереваДано корневое дерево. Каждую итерацию выбирается вершина (равновероятно из всех оставшихся), и удаляется всё поддерево, соответствующее этой вершине. Найти, сколько ходов в среднем будет продолжаться этот процесс, то есть матожидание номера итерации, на которой будет удалён корень дерева.\n#$k$-ый элемент на отрезкеДан массив из $n$ целых чисел. Требуется ответить на $m$ запросов $k$-ой порядковой статистики на произвольном отрезке. Время работы $O((n+m) \\log n)$.\n#Различные числа на отрезкеДан массив из $n$ целых чисел. Требуется ответить на $m$ запросов количества различных элементов на произвольном отрезке. Время работы $O(m\\sqrt{n})$.\n#ФизкультураЗачёт по физкультуре в одном институте ставится по количеству посещений, поэтому важно знать, сколько учебных дней осталось до конца семестра (особенно когда напропускал пары). Семестр длится $m$ дней. Деканат последовательно издает $n$ приказов двух типов:\nОбъявить все дни с $l$ по $r$ выходными (физру закрывать нельзя) Объявить все дни с $l$ по $r$ учебными (физру закрывать можно) При этом приказ может частично отменить действие предыдущих приказов.\nПосле каждого приказа нужно посчитать суммарное число учебных дней в семестре. Асимптотика $O(n \\log n)$.\n#Нулевая суммаДано мультимножество из $n$ целых чисел. Найдите любое его непустое подмножество, сумма чисел которого делится на $n$.\n#Мета-задачаВ задаче дана произвольная строка, по которой известным только авторам способом генерируется ответ yes/no. В задаче 100 тестов. У вас есть 20 попыток. В качестве фидбэка вам доступны вердикты на каждом тесте. Вердикта всего два: OK (ответ совпал) и WA. Попытки поделить на ноль, выделить терабайт памяти и подобное тоже считаются как WA. «Решите» задачу.\n#Мета-задача 2Условие как в «Мета-задаче», но сообщается только число пройденных тестов.\n100 тестов, 70 попыток.\n#Мета-задача 3Условие как в «Мета-задаче», но сообщается только номер первого не пройденного теста.\n10 тестов, 100 попыток.\n#НиточкаВ плоскую доску вбили $n$ гвоздей радиуса $r$, причём так, что соответствующие точки на плоскости образуют вершины выпуклого многоугольника. На эти гвозди натянули ниточку, причём ниточка «огибает» по кругу гвозди. Найдите длину ниточки, то есть периметр этого многоугольника с учётом закругления.\n#ПельмениКомпания друзей захотела сделать заготовки для пельменей из огромного прямоугольного куска теста. Для этого в ход пошли всевозможные предметы округлой формы: стаканы, кружки, кастрюли… В итоге тесто было разделено $n$ возможно пересекающимися окружностями произвольных радиусов и центров. Нам интересно посчитать, сколько получилось заготовок, то есть на сколько кусков распалось тесто. Асимптотика $O(n^2 \\log n)$.\n#От нуля до единицыДан следующий код:\nx = 0 while x \u0026lt; 1: x += random() Требуется посчитать матожидание x.\n(random в питоне возвращает случайное действительное число от 0 до 1.)\n#ПлощадьДан единичный квадрат и 100 кругов. Нужно посчитать площадь квадрата, которую покрывают эти круги, с ошибкой менее 1%.\n#ОкружностиИмеется окружность радиуса $R$, назовём её внешней. Внутри неё лежит окружность радиуса $r \u0026lt; R$ и соприкасается с ней. Дальше строятся бесконечное число окружностей по следующему правилу: $k$-я окружность должна\nсоприкасаться с внешней, соприкасаться с предыдущей (($k-1$)-ой), иметь при этом максимальный радиус. Найдите (выведите формулу за $O(1)$) радиус $k$-й такой окружности.\n#БлефКатя и Серёжа играют в игру. У Кати есть $n$ карт, у Серёжи — $m$. Одна дополнительная карта лежит на столе рубашкой вверх. Назовём её особой. Цель игроков — её отгадать. Все $n+m+1$ карт различны, игроки изначально знают только свои карты. Игроки ходят по очереди, начинает Катя. Игрок в свой ход может:\nПопытаться угадать особую карту. Если получилось угадать, то игрок победил, иначе проиграл. В любом случае, игра на этом заканчивается. Назвать любую карту из колоды. Если у соперника есть такая карта — он обязан показать ее и вывести из игры. Если же у него нет такой карты — он сообщает об этом. С какой вероятностью выиграет Катя при оптимальной игре обоих игроков? Асимптотика $O(nm)$.\n#ДостижимостьДан ориентированный граф без кратных рёбер. Для всех пар вершин $u$ и $v$ определите, можно ли дойти из $u$ в $v$. Вершин меньше 2000.\n#НумизматЕсть $n$ жадных коллекционеров монет, которые согласны меняться, только если взамен монеты, которых у них более одной, они получают монету, которых у них нет вовсе. Всего есть $k$ типов монет. Известно количество монет каждого типа у каждого коллекционера и у коллекционера Серёжи. Серёжа хочет максимизировать количество различных монет у него путем обмена с жадными коллекционерами. Считайте, что жадные коллекционеры не меняются между собой.\nПридумайте любой полиномиальный алгоритм.\n#ПринцессаВ некотором королевстве жила принцесса. Она была настольно прекрасна, что каждый юноша в королевстве хотел жениться на ней. Принцесса была привередлива, поэтому твердо решила выйти замуж только за самого красивого юношу в государстве.\nОна составила список из $n$ самых красивых юношей и вызывает их каждый день в случайном порядке. После того, как к ней приходит очередной юноша, она принимает решение: либо выйти за него замуж, либо нет. Если она выходит за него замуж, то она все равно просматривает всех остальных, чтобы убедиться, что он действительно самый красивый. Если это так, то они живут долго и счастливо. Если нет (если она вышла замуж не за самого красивого, либо не вышла замуж вообще), то принцесса покончит жизнь самоубийством.\nУ принцессы очень хорошая память, поэтому она может сравнивать красоту очередного юноши со всеми предыдущими, а также у неё тонкий вкус, поэтому никакие двое юношей не являются для неё одинаково красивыми.\nПомогите принцессе разработать стратегию, которая максимизирует вероятность того, что она выйдет замуж за наиболее красивого юношу.\nАсимптотика $O(n^2)$.\n#СпиральОпределим спираль $(2n+1) \\times (2n+1)$ как матрицу следующего вида:\n$$ \\begin{matrix} 21 \u0026amp; 22 \u0026amp; 23 \u0026amp; 24 \u0026amp; 25 \\ 20 \u0026amp; 7 \u0026amp; 8 \u0026amp; 9 \u0026amp; 10 \\ 19 \u0026amp; 6 \u0026amp; 1 \u0026amp; 2 \u0026amp; 11 \\ 18 \u0026amp; 5 \u0026amp; 4 \u0026amp; 3 \u0026amp; 12 \\ 17 \u0026amp; 16 \u0026amp; 15 \u0026amp; 14 \u0026amp; 13 \\ \\end{matrix} $$\nВаша задача — рассчитать ответы на $q$ запросов суммы чисел в произвольной прямоугольной области (по модулю $10^9+7$).\n$q \\leq 100$, $n \\leq 10^9$.\n#Польский лабиринтГруппа из $n$ туристов гуляет в бесконечном лабиринте. Лабиринт имеет форму треугольника Серпинского: клетка $(x, y)$ свободна, только если x \u0026amp; y == 0.\nТуристы устали блуждать по лабиринту и хотят встретиться в какой-нибудь свободной клетке, сумма расстояний от всех туристов до которой наименьшая. Туристы за один ход могут передвигаться вверх, вниз, влево и вправо по свободным клеткам.\n$n \\leq 10^5$, изначальные координаты туристов до $10^9$.\n#Нимные подмножестваЕсть множество $A$, состоящее из $n$ чисел от 0 до $2^{32}-1$. Требуется выбрать его подмножество $B \\subseteq A$ максимальной суммы такое, что нельзя выбрать его подмножество $C \\subseteq B$ такое, что ним на кучках соответствующего размера — проигрышный. Асимптотика $O(n \\log n)$.\n#Баланс степенейДан неориентированный граф. Требуется ориентировать каждое ребро так, чтобы максимизировать число вершин, у которых степень исхода равна степени захода.\n#Два путиДан ориентированный граф. Найдите два непересекающихся по рёбрам пути из $s$ в $t$.\n#ПьяницаПьяница стоит на числовой прямой в точке 0. Каждую секунду он делает единичный шаг вправо (в сторону увеличения координат) с вероятностью $p$ и влево с вероятностью $1-p$. С какой вероятностью он когда-либо окажется в точке с отрицательной координатой?\n#Ксоровый рюкзакДан массив из $10^5$ целых чисел от $0$ до $(2^{30}-1)$. Найти количество различных подпоследовательностей этого массива, xor-сумма которых равна заданному числу $x$.\n#Иван СусанинПольская армия хочет добраться из поселения $s$ в поселение $t$. Ей руководят два гетмана — Камиль и Матеуш.\nКамиль руководит армией днём и водит армию по дорогам. Матеуш руководит армией ночью и совершает маневры по секретным тропам. Каждый гетман перед маршем спрашивает дорогу у Ивана Сусанина. Иван хочет задержать наступление польских войск.\nКарта дорог известна Камилю. Аналогично, карта секретных троп известна Матеушу. Поэтому Ивану не удастся их так просто обмануть — он должен каждый раз выбрать переход так, что минимальное расстояние между поселением $t$ и войском по соответствующей карте строго уменьшилось.\nВы знаете карту дорог и троп вместе с их длинами. Помогите Ивану как можно дольше (желательно, бесконечно) вести армию из $s$ в $t$.\n#ВареньеВ ряд стоят $n$ пустых банок из-под варенья. Вместительность $i$-й банки равна $v_i$ грамм.\nКарлсон наполняет эти банки вареньем в $m$ этапов. На каждом этапе он выбирает числа $l$, $r$, $x$ и $y$, а затем пролетает над банками с $l$ по $r$, выполняя следующие операции: в банку номер $l$ он добавляет $x$ грамм варенья, в банку номер $(l + 1)$ — $(x + y)$ грамм варенья, в банку номер $(l + 2)$ — $(x + 2y)$, и так далее до $r$-той банки, в которую он положит $x + y(r - l)$ грамм варенья.\nМалышу хочется определить для каждой банки наименьший номер операции, после которой она станет полной.\n$n, m \\leq 10^5$\n#ЛабиринтСерёжа потерялся в лабиринте $n \\times m$. Каждая клетка либо свободна, либо стена. Все крайние клетки — стены. Вам известно, где находится выход из лабиринта, но не известно, где находится Серёжа. Составьте для него последовательность направлений (вверх, вниз, влево, вправо) такую, после которой он окажется в клетке с выходом, вне зависимости от его стартовой позиции. Если вы прикажете ему идти в стену, то просто ничего не произойдёт.\nПридумайте любой полиномиальный алгоритм.\n#ОбезьянаДана строка из $10^5$ символов латинского алфавита. Обезьяна нажимает случайные клавиши на клавиатуре (одну из 26 букв), пока не наберёт её целиком, то есть пока исходная подстрока не станет подстрокой набранной строки. Какое ожидание числа нажатых клавиш перед тем, как это произойдёт?\n#Ожидание минимумаДаны $n$ случайных величин, равномерно распределенных на отрезках $[l_i, r_i]$ — у каждой величины свой отрезок. Найдите математическое ожидание минимума этих случайных величин.\nПридумайте любой точный полиномиальный алгоритм.\n#Шумный ксорЗагадано некое число $x$. Вы можете делать запросы следующего типа: назвать число $y$ и получить в ответ число единичных битов в ксор-сумме $x$, $y$ и $m$, где $m$ это случайно сгенерированная маска, в которой каждый бит имеет вероятность $p = \\frac15$ быть единичным, то есть каждый бит $x \\oplus y$ заменяется на противоположный с вероятностью $y$, и вам возвращается количество единичных битов. Для ясности:\nx = # ... def mask(p=0.2): r = 0 for i in range(32): if random.random() \u0026lt; p: r += 2**i return r def query(y): return bin(x ^ y ^ mask()).count(\u0026#39;1\u0026#39;) Ваша задача — отгадать число, используя не более 10000 попыток.\n#КоммивояжерДаны $3 \\cdot 10^5$ точек на плоскости. Выберите среди них любое подмножество из 500 точек и решите для него задачу коммивояжера: найдите минимальный по длине цикл, проходящий через все эти точки.\n#АнаграммыНайдите в строке $s$ первую подстроку, являющуюся анаграммой (пререстановкой символов) строки $t$ за $O(n)$.\n#Функциональный графДан ориентированный граф из $n \u0026lt; 10^5$ вершин, в котором из каждой вершины ведет ровно одно ребро. Требуется ответить на $q \u0026lt; 10^5$ запросов «в какую вершину мы попадем, если начнем в вершине $v_i$ и сделаем $k_i \u0026lt; 10^{18}$ переходов» за время $O(q + n)$.\n#Асинхронная шляпаСерёжа и его $(n - 1)$ друзей решили поиграть в «шляпу», в которой один игрок должен за ограниченное время объяснить как можно больше слов, чтобы его партнер их отгадал.\nКаждый игрок должен пообщаться с любым другим по разу; обычно игра проводится так:\n1-й игрок объясняет в течение минуты слова 2-му, 2-й игрок объясняет слова 3-му, \u0026hellip;, $n$-й игрок объясняет слова 1-му, 1-й игрок объясняет слова 3-му, 2-й игрок объясняет слова 4-му… …и так далее, пока $(n-1)$-й игрок не закончит объяснять слова $(n-2)$-ому.\nЕсли друзей собралось много, то игра может занять приличное время. Серёжу интересует, какое минимальное время она может длиться, если разрешить парам участников общаться между собой одновременно и в любом порядке.\nДля данного $n \\le 500$, найдите минимальное количество времени $k$ и соответствующее ему расписание.\n#Random coffeeВ компании, в которой вы работаете, устроено неизвестное число людей — от одного до бесконечности с равной вероятностью. Для борьбы с одиночеством, каждый сотрудник участвует в «random coffee»: каждую неделю вы встречаетесь со случайным человеком из компании, чтобы попить кофе и обсудить что угодно.\nВы участвовали в random coffee $n$ раз и пообщались с $k$ разными людьми (с некоторыми — более одного раза). Какое наиболее вероятное число человек работает в компании?\n#МафияВ «мафию» играют 13 человек, из которых 10 мирных и 3 мафии. Все роли розданы с помощью стандартной колоды игральных карт: заранее выбрали и перемешали 10 красных и 3 чёрные карты, кто вытянул черную — мафия. Все карты различны и известны всем. Игра начинается с дневного голосования.\nКак мирным гарантированно победить?\n","id":135,"path":"/cs/programming/bayans/","title":"Просто интересные задачи"},{"content":"В этом разделе собрано всё, что относится к программированию в общем, а не только к алгоритмам.\n","id":136,"path":"/cs/programming/","title":"Технологии программирования"},{"content":"Суффиксный массив, автомат и дерево обобщённо называют суффиксными структурами данных. Они применяются в множестве различных задач, встречающихся как на олимпиадах, так и на практике.\nСуффиксные структуры часто (но не всегда) взаимозаменяемые, и более того, конвертируются друг в друга за линейное время. Суффиксный массив — самый простой из них, поэтому мы с него и начнём.\n#МотивацияСуффиксным массивом (англ. suffix array, суфмасс) строки $s$ называется перестановка индексов начал её суффиксов, которая задаёт порядок их лексикографической сортировки. Иными словами, чтобы его построить, нужно выполнить сортировку всех суффиксов заданной строки.\nСортировка всех суффиксов строки «mississippi$» Где это может быть полезно. Пусть вы хотите основать ещё один поисковик, и чтобы получить финансирование, вам нужно сделать хоть что-то минимально работающее — хотя бы просто научиться искать по ключевому слову документы, включающие его, а также позиции их вхождения (в 90-е это был бы уже довольно сильный MVP). Простыми алгоритмами — полиномиальными хешами, z- и префикс-функцией и даже Ахо-Корасиком — это сделать быстро нельзя, потому что на каждый раз нужно проходиться по всем данным, а суффиксными структурами — можно.\nВ случае с суффиксным массивом можно сделать следующее: сконкатенировать все строки-документы с каким-нибудь внеалфавитным разделителем ($), построить по ним суффиксный массив, а дальше для каждого запроса искать бинарным поиском первый суффикс в суффиксном массиве, который меньше искомого слова, а также последний, который меньше. Все суффиксы между этими двумя будут включать искомую строку как префикс.\nНапример, строка iss в суффиксном массиве из примера найти будет зажата между суффиксами issippi$ и ississippi$, которые имеют индексы $2$ и $5$, а значит ровно на этих позициях и будет входить в исходный текст.\nРаботать такой алгоритм будет за $O(|t| \\log |s|)$, и позже это можно будет оптимизировать до $O(|t| + \\log |s|)$, что является одним из самых оптимальных алгоритмов поиска.\nТеперь научимся его строить.\n#Построение суффиксного массиваДля сортировки суффиксов мы могли бы просто взять перестановку от $0$ до $n$, написать компаратор, который сравнивает суффиксы с соответствующими индексами, и скормить это в std::sort, что будет работать за $O(n^2 \\log n)$, потому что внутреннее сравнение работает за $O(n)$. Однако, если сравнивать суффиксы хешами, то уже можно получить алгоритм за $O(n \\log^2 n)$. Но это не самый быстрый и удобный алгоритм.\n#Построение за $O(n \\log n)$Для удобства допишем в конец строки какой-нибудь символ, который лексикографически меньше любого другого. Для стандартной ASCII обычно выбирают либо «$», либо «#», либо просто нулевой символ, если это C-string (в языке Си все строки и так заканчиваются нулевым символом).\nМы будем выполнять сортировку не суффиксов, а циклических сдвигов. Строки мы, соответственно, тоже будем рассматривать циклические — это удобно тем, что у них всех будет одна и та же длина. Легко убедиться, что сортировка таких циклических сдвигов эквивалентна сортировке суффиксов — можно просто убрать всё, что идёт после доллара.\nНаш алгоритм будет состоять из $\\lceil \\log n \\rceil$ этапов. На $k$-том этапе будем сортировать все циклические подстроки длины $2^k$. Так на последнем этапе мы отсортируем строки длины $\\geq n$ (это легально — строки ведь циклические), и мы получим нужный суффиксный массив.\nЗаметим, что в отличие сортировки суффиксов, сортировка подстрок не всегда однозначна — две подстроки могут быть одинаковыми. Поэтому на каждой фазе алгоритма помимо перестановки $p$ индексов циклических подстрок будем поддерживать для каждой циклической подстроки номер её класса эквивалентности $c_i$, которому эта подстрока принадлежит. Номера классов эквивалентности будем давать с сохранением порядка: лексикографически меньшим подстрокам соответствуют меньшие $c_i$. После последнего этапа массив классов эквивалентности должен задавать перестановку, обратную суффиксному массиву.\nПример: $s = aaba$. Этапов будет 3: для подстрок длины 1, 2 и 4.\n$$ p_0 = (0, 1, 3, 2) ;;; c_0 = (0, 0, 1, 0) \\ p_1 = (0, 3, 1, 2) ;;; c_1 = (0, 1, 2, 0) \\ p_2 = (3, 0, 1, 2) ;;; c_2 = (1, 2, 3, 0) $$\nКак настоящие программисты, мы нумеруем этапы с нуля, поэтому на нулевом этапе мы отсортируем строки длины $2^0 = 1$, то есть просто символы. Если алфавит небольшой, то это легко сделать сортировкой подсчётом за $O(n)$.\nСледующие этапы нужно проводить, используя информацию с предыдущих. Как в исходном решении, можем снова создать перестановку индексов и применить к ней std::sort со своим компаратором.\nДля быстрого сравнения двух подстрок размера $2^{k-1}$ мы можем использовать классы эквивалентности $c_i$, сопоставив каждой строке длины $2^k$ биграмму — строку из двух «символов». А именно, строка $s[i \\ldots i+2^k-1]$ с точки зрения сортировки будет эквивалентна паре $(c_i, c_{i+2^{k-1}})$. Соответственно, можно написать компаратор, который смотрит только на эти пары, и работает за $O(1)$ вместо линейного прохода по строкам. Однако, это всё ещё будет работать за $O(n \\log^2 n)$, потому что каждый этап работает за $O(n \\log n)$.\nПримечание. Зачастую этот способ построения работает быстро, поскольку имеет небольшую константу.\nЧтобы соптимизировать каждый этап до линейного, нужно избавиться от логарифма в сортировке. Сортировка слиянием оптимальна только тогда, когда мы ничего не знаем про сортируемые значения — но в нашем случае это не так.\nВоспользуемся идеей цифровой сортировки — сначала отсортируем биграммы по второму символу, а потом по первому. С предыдущего этапа мы уже знаем порядок вторых элементов (это ровно массив $p$). Теперь, чтобы упорядочить их по первому, нам надо просто пройтись по суффиксам в этом порядке, от начала каждого элемента отнять $2^{k-1}$, чтобы получить индекс начала, и провести цифровую сортировку. Таким образом, можно проводить каждый этап за $O(n)$, а весь алгоритм за $O(n \\log n)$.\n// строка -- это последовательность чисел от 1 до размера алфавита vector\u0026lt;int\u0026gt; suffix_array(vector\u0026lt;int\u0026gt; \u0026amp;s) { s.push_back(0); // добавляем нулевой символ в конец строки int n = (int) s.size(), cnt = 0, // вспомогательная переменная: счётчик для сортировки cls = 0; // количество классов эквивалентности vector\u0026lt;int\u0026gt; c(n), p(n); map\u0026lt;int, vector\u0026lt;int\u0026gt;\u0026gt; t; for (int i = 0; i \u0026lt; n; i++) t[s[i]].push_back(i); // «нулевой» этап for (auto \u0026amp;x : t) { for (int u : x.second) c[u] = cls, p[cnt++] = u; cls++; } // пока все суффиксы не стали уникальными for (int l = 1; cls \u0026lt; n; l++) { vector\u0026lt; vector\u0026lt;int\u0026gt; \u0026gt; a(cls); // массив для сортировки подсчётом vector\u0026lt;int\u0026gt; _c(n); // новые классы эквивалентности int d = (1 \u0026lt;\u0026lt; l) / 2; int _cls = cnt = 0; // новое количество классов for (int i = 0; i \u0026lt; n; i++) { int k = (p[i] - d + n) % n; a[c[k]].push_back(k); } for (int i = 0; i \u0026lt; cls; i++) { for (size_t j = 0; j \u0026lt; a[i].size(); j++) { // если суффикс начинает новый класс эквивалентности if (j == 0 || c[(a[i][j] + d) % n] != c[(a[i][j - 1] + d) % n]) _cls++; _c[a[i][j]] = _cls - 1; p[cnt++] = a[i][j]; } } c = _c; cls = _cls; } return vector\u0026lt;int\u0026gt;(p.begin() + 1, p.end()); } Автор извиняется за качество кода и призывает читателей его переписать и законтрибьютить.\n#Наибольшие общие префиксыДля многих применений часто бывает полезным возможность находить длину наибольшего общего префикса (англ. largest common prefix) для различных суффиксов строки.\nНапример, чтобы просто сравнить две подстроки, мы можем найти наибольший префикс соответствующих суффиксов и посмотреть на следующий символ — так же, как это обычно делается хешами.\nУтверждение. Пусть мы знаем lcp для всех суффиксов, которые являются соседями в суффиксном массиве — это будет какой-то массив длины $(n - 1)$. Тогда общий префикс для $i$-того и $j$-того суффикса будет равен минимуму между $c_i$-тым и $c_j$-тым элементом в массиве lcp (напомним, что $c_i$ — класс эквивалентности — это номер $i$-того суффикса в суффиксном массиве).\nДля доказательства можно представить массив lcp в виде гистограммы. Минимум на отрезке между $c_i$ и $c_j$ будет соответствовать суффиксу, который имеет самый короткий общий префикс с ними, а значит все символы до этого префикса и только они и будут наибольшим общим префиксом.\nМассив LCP для строки «BANANA$» Тогда есть мотивация посчитать массив lcp$ в котором окажутся наибольшие общие префиксы соседних суффиксов, а после как-нибудь считать минимумы на отрезках в этом массиве (например, с помощью разреженной таблицы).\nОсталось придумать способ быстро посчитать массив lcp. Можно воспользоваться идеей из построения суффиксного массива за $O(n \\log^2 n)$: с помощью хешей и бинпоиска находить lcp для каждой пары соседей. Такой метод работает за $O(n \\log n)$, но является не самым удобным и популярным.\n#Алгоритм Касаи, Аримуры, Арикавы, Ли, ПаркаАлгоритм в реальности называется как угодно, но не исходным способом (алгоритм Касаи, алгоритм пяти корейцев, и т. д.). Используется для подсчета $lcp$ за линейное время. Автору алгоритм кажется чем-то похожим на z-функцию по своей идее.\nУтверждение. Пусть мы уже построили суфмасс и посчитали $lcp[i]$. Тогда:\n$$ lcp[c[p[i] + 1]] \\ge lcp[i] - 1 $$\nПо-русски: $lcp$ от следующего суффикса в исходной строке не более чем на единицу меньше.\nДоказательство. Если для $p[i]$-го суффикса была общая часть длины $lcp[i]$ с $p[i + 1]$-ым суффиксом, то у $(p[i] + 1)$-го суффикса гарантированно будет общая часть длины $(lcp[i] - 1)$ с $(p[i + 1] + 1)$-ым суффиксом.\nЗаметим, что этот суффикс уже не обязательно будет соседом $(p[i] + 1)$-го в суффиксном массиве, но из свойства $lcp$ двух произвольных суффиксов получим, что $lcp[c[p[i] + 1]] \\ge lcp[i] - 1$. Здесь возникает идея просто считать точное значение $lcp$ наивным образом — увеличиваем, пока увеличивается — но брать его стартовое значение, опираясь на это неравенство. В таком случае, чтобы не делать лишние сравнения, будем считать также считать массив $lcp$ в порядке от длинных суффиксов к коротким.\nКод алгоритма:\nvector\u0026lt;int\u0026gt; calc_lcp(vector\u0026lt;int\u0026gt; \u0026amp;val, vector\u0026lt;int\u0026gt; \u0026amp;c, vector\u0026lt;int\u0026gt; \u0026amp;p) { int n = val.size(); int l = 0; // lcp текущего элемента vector\u0026lt;int\u0026gt; lcp(n); for (int i = 0; i \u0026lt; n; i++) { if (c[i] == n - 1) continue; int nxt = p[c[i] + 1]; while (max(i, nxt) + l \u0026lt; n \u0026amp;\u0026amp; val[i + l] == val[nxt + l]) l++; lcp[c[i]] = l; l = max(0, l - 1); } return lcp; } Чтобы оценить сложность алгоритма, посчитаем, сколько раз мы сделаем наивное прибавление. Каждый $lcp[i] \\le n$, а также мы сделаем не более $n$ вычитаний единицы при переходах. Тогда суммарное число прибавлений — $O(n)$. Следовательно, и сам алгоритм будет работать за $O(n)$.\n","id":137,"path":"/cs/string-structures/suffix-array/","title":"Суффиксный массив"},{"content":"Рассмотрим следующую программу, в которой считается сумма одномерного целочисленного массива:\n#pragma GCC optimize(\u0026#34;O3\u0026#34;) // ^ включает самый \u0026#34;агрессивный\u0026#34; уровень оптимизации // то же самое, что добавить флаг \u0026#34;-O3\u0026#34; при компиляции из консоли #include \u0026lt;iostream\u0026gt; using namespace std; const int n = 1e5; int a[n], s = 0; int main() { for (int t = 0; t \u0026lt; 100000; t++) for (int i = 0; i \u0026lt; n; i++) s += a[i]; return 0; } Если скомпилировать этот код под GCC без всяких дополнительных настроек и запустить, он отработает за 2.43 секунды.\nДобавим теперь следующую магическую директиву в самое начало программы:\n#pragma GCC target(\u0026#34;avx2\u0026#34;) // ...остальное в точности как было Скомпилировавшись и запустившись при тех же условиях, программа завершается уже за 1.24 секунды. Это почти в два раза быстрее, при том, что сам код и уровень оптимизации мы не меняли.\nВсё дело в том, что в современных процессорах есть специальные «векторные» инструкции, которые могут применять какую-то одну операцию сразу к блоку из скольки-то последовательных элементов, а не только к одному скаляру за раз. Такая модель называется SIMD-параллелизмом (англ. single instruction, multiple data).\nНа разных микроархитектурах поддержка SIMD-инструкций разная. Помимо самого набора инструкций, различаются ещё и размеры векторных регистров — сейчас это может быть 128, 256 или 512 бит.\nНа архитектуре x86 (которая используется на подавляющем большинстве десктопных и серверных CPU) все SIMD-расширения в основном сохраняют обратную совместимость: более новое подразумевает все более старые. По умолчанию в компиляторах C++ предполагается только то, что таргет (компьютер, на котором должна исполняться программа) поддерживает набор инструкций «SSE2» — что должно быть верно для почти всех CPU, появившихся в этом веке.\nНа большинстве же современных CPU есть поддержка AVX2, ключевое отличие которого заключается в том, что векторные регистры, с которыми он работает, увеличились в два раза — с 128 до 256 бит — и соответственно за одну операцию можно сложить уже не 4, а целых 8 int-ов. Соответственно, когда мы сообщаем оптимизирующему компилятору дополнительную информацию о микроархитектуре (через #pragma GCC target(\u0026quot;avx2\u0026quot;) или флаги вроде -mavx2 или -march=native), он получает доступ к более широким регистрам и ускоряет нашу программу в ожидаемые два раза.\nОднако векторизация имеет гораздо больше нюансов, чем просто добавление прагм — для более основательного подхода автор рекомендует соответствующую главу «Algorithms for Modern Hardware».\n","id":138,"path":"/cs/arithmetic/simd/","title":"Векторизация"},{"content":"Понятие матроида нужно, чтобы придумывать и доказывать некоторые жадные алгоритмы в задачах, где нужно набрать какое-то множество объектов максимального веса. Например, минимальный остов, максимальное вершинно-взвешенное паросочетание или выполнение заказов с ограничениями по времени.\nЕсли вы не специалист по абстрактной алгебре, рекомендуется подсматривать примеры матроидов в конце статьи.\n#ОпределениеМатроидом называется пара $(X, I)$, где $X$ — множество элементов, называемое носителем матроида, а $I$ — некоторое множество подмножеств $X$, называемое семейством независимых множеств. В матроиде должны выполняться следующие свойства:\nПустое множество является независимым: $\\varnothing \\in I$\nЛюбое подмножество независимого множества тоже независимо:\n$$ A \\subset B, B \\in I \\implies A \\in I $$\nЕсли в независимом множестве $A$ меньше элементов, чем в независимом множестве $B$, то будет существовать элемент из $B$, дополняющий $A$ до независимого множества размера $|A|+1$:\n$$ A, B \\in I, |A| \u0026lt; |B| \\implies \\exists x \\in B \\setminus A: A \\cup {x} \\in I $$\nМатроид называется взвешенным, если на нем есть аддитивная весовая функция: $w(A) = \\sum w(a_i)$.\n#Алгоритм Радо-ЭдмондсаПусть нам нужно найти независимое множество, которое будет включать как можно больше элементов и при этом иметь как можно меньший вес. Утверждается, что можно просто отсортировать элементы по возрастанию весов и пытаться в таком порядке добавлять их в ответ:\nX.sort() s = [] for x in X: if good(s + [x]): s += [x] Здесь под good имеется в виду $s \\cup x \\in I$.\nКорректность этого алгоритма для произвольного матроида следует из следующего утверждения:\nТеорема Радо-Эдмондса. Пусть $A \\in I$ — множество минимального веса среди всех независимых подмножеств $X$ мощности $k$. Возьмем $x: A \\cup x \\in I,;x \\notin A,;w(x)$ — минимальна. Тогда $A \\cup x$ — множество минимального веса среди независимых подмножеств $X$ мощности $k + 1$.\nДоказательство:\nРассмотрим $B$ — множество минимального веса среди независимых подмножеств $X$ мощности $k + 1$.\nИз свойств матроида: $\\exists y \\in B \\setminus A : A \\cup y \\in I$.\nТогда верны два неравенства:\n$$ \\begin{cases} w(A \\cup y) = w(A) + w(y) \\geq w(B) \\implies w(A) \\geq w(B) - w(y) \\ w(B \\setminus y) = w(B) - w(y) \\geq w(A) \\implies w(A) \\leq w(B) - w(y) \\end{cases} $$\nВеличина $w(A)$ с двух сторон ограничивает величину $w(B) - w(y)$. Значит, они равны. Следовательно, $w(A \\cup y) = w(A) + w(y) = w(B)$.\nПолучаем, что если объединить множество $A$ с $x$ — минимальным из таких, что $A \\cup x \\in I$, — то получим множество минимального веса среди независимых подмножеств $X$ мощности $k + 1$.\nИными словами, если у нас есть оптимальное $k$-элементарное независимое множество, то мы можем индуктивно построить оптимальное $(k+1)$-элементарное множество, добавив к нему минимальный элемент, оставляющий построенное множество независимым.\n#ПримерыДля доказательства жадников нужно просто (а иногда не просто) показать, что рассматриваемое множество является матроидом. Первые два критерия доказывать тривиально, третий сложнее.\n#Минимальный остовРассмотрим неориентированный граф $G = (V, E)$. Пусть $I$ — множество лесов графа (ациклических подмножеств $E$). Тогда $M = (E, I)$ является матроидом:\nГраф без ребер является лесом. Если удалить из леса ребра, он останется лесом. Пусть есть два леса $|A| \\leq |B|$. В $A$ будет $|V| - |A|$ компонент связности, в $B$ будет $|V|-|B|$ компонент связности. Так как в $B$ компонент связности меньше, то будет существовать какое-то ребро $x$, связывающее две компоненты связности из $A$. Его и возьмем: $A \\cup {x}$ тоже будет лесом, так как $x$ только соединило две разные компоненты связности. Применив к этому матроиду теорему Радо-Эдмондса, мы получаем обоснование алгоритма Крускала для нахождения минимального остова.\n#РасписанияПусть у нас есть $n$ заданий, на выполнение каждого требуется $1$ час. Награда за выполнение $i$-го задания не позже $d_i$-того часа равна $w_i$. В один час разрешено сделать только одно задание. Цель — максимизировать сумму наград.\nНазовём правильными те наборы заданий, для которых существует порядок, при котором можно их всех успеть сделать до их дедлайнов. Проверять фиксированный набор можно так: отсортировать по дедлайнам ($d_i$) и проверить, что $d_i \\geq i$ для всех $i$.\nТогда $M =$ (множество всех заданий, множество правильных наборов заданий) является матроидом:\nПустой набор заданий всегда можно сделать. Если у нас стало меньше заданий, то их сделать мы тоже успеем. Пусть есть два правильных набора $|A| \\leq |B|$. Тогда в $B$ будет существовать задание $x$ с дедлайном позже $|A|$. Все задания $A$ можно сделать не позже $|A|$-го часа, а в $(|A|+1)$-й час будем делать $x$. Значит, $A \\cup x$ — тоже правильный набор. Значит, можно отсортировать задания по убыванию их стоимости и пытаться в таком порядке добавлять в ответ, проверяя правильность какой-нибудь структурой данных.\n#ПаросочетанияРассмотрим двудольный граф $G = (L, R, E)$. Пусть $I$ — множество наборов вершин левой доли, которых можно покрыть каким-нибудь паросочетанием. Тогда $M = (L, I)$ является матроидом:\nЛюбое паросочетание покрывает пустое множество вершин. Исходное паросочетание покрывает также и любое подмножество исходных вершин. Пусть есть два множества вершин $|A| \\leq |B|$. Раскрасим ребра из паросочетания, соответствующего $A$ в красный цвет, $B$ — в синий, а ребра из обоих паросочетаний — в пурпурный. Рассмотрим граф из красных и синих ребер. Любая компонента связности в нём представляет собой либо путь, либо цикл, состоящий из чередующихся красных и синих ребер. В любом цикле будет равное число красных и синих ребер, а так как всего синих ребер больше, то должен существовать путь, начинающийся и оканчивающийся синим ребром. Поменяем в этом пути красный и синий цвета и сделаем пурпурные ребра обратно красными. Теперь в графе из красных ребер на одно ребро больше, а значит к множеству $A$ добавилась какая-то вершина из левой доли, принадлежавшая ранее $B$. Пусть у вершин левой доли есть веса, и нам нужно найти максимальное паросочетание минимального веса. Согласно теореме, мы можем отсортировать вершины левой доли по их весам, а затем пытаться добавлять их в паросочетание. Сделать это можно стандартным алгоритмом Куна.\n#Линейно независимые вектораМножество $n$-мерных векторов называется линейно независимым, если никакой его вектор нельзя получить линейной комбинацией (взвешенной суммой) других.\nРассмотрим какое-то множество векторов $V$. Пусть $I$ — множество всех его линейно независимых подмножеств. Тогда $M = (V, I)$ является матроидом:\nВ пустом множестве нельзя получить какой-либо вектор. Подмножество линейно независимого множество тоже линейно независимо: способов набрать вектора стало ещё меньше. Пусть есть два линейно независимых множества $A$ и $B$, и $B$ больше. Пусть в нём нельзя выбрать вектор, совместимый с $A$. Тогда получается, что все вектора $B$ можно выразить из $A$. Значит, размерность $B$ уж точно не больше — противоречие. Пример задачи: набрать базис максимального веса, если каждый вектор сколько-то стоит.\nПримечание: $Z_2$ — это тоже пространство, и там тоже можно ввести линейную независимость. Это значит, что то же самое распространяется на задачи в духе «набрать максимальное множество чисел, из которых ксором нельзя получить ноль».\n","id":139,"path":"/cs/combinatorial-optimization/matroid/","title":"Матроиды"},{"content":"Алгоритм имитации отжига (англ. simulated annealing) — эвристический алгоритм глобальной оптимизации, особенно эффективный при решении дискретных и комбинаторных задач.\nАлгоритм вдохновлён процессом отжига в металлургии — техники, заключающейся в нагревании и постепенном охлаждении металла для увеличения его кристаллизованности и уменьшения дефектов. Симулирование отжига в переборных задачах может быть использовано для приближённого нахождения глобального минимума функций с большим количеством свободных переменных.\nМаксимизация функции одной переменной методом отжига Алгоритм вероятностный и не даёт почти никаких гарантий сходимости, однако хорошо работает на практике при решении NP-полных задач. Иногда на контестах им удаётся сдать сложные комбинаторные задачи, у которых есть нормальное решение: например, Ильдар Гайнуллин как-то раз в 8-м классе сдал отжигом div2E на динамику по подмножествам.\n#Общее описание алгоритмаДля конкретного примера будем рассматривать задачу коммивояжёра:\nЕсть $n$ городов, соединённых между собой дорогами. Необходимо проложить между ними кратчайший замкнутый маршрут, проходящий через каждый город только один раз.\nПусть имеется некоторая функция $f(x)$ от состояния $x$, которую мы хотим минимизировать. В данном случае $x$ это перестановка вершин (городов) в том порядке, в котором мы будем их посещать, а $f(x)$ это длина соответствующего пути.\nВозьмём в качестве базового решения какое-то состояние $x_0$ и будем пытаться его улучшать. В нашем примере мы можем взять случайную перестановку как изначальный маршрут.\nВведём температуру $t$ — какое-то действительное число, изначально равное единице, которое будет изменяться в течение оптимизации и влиять на вероятность перейти в соседнее состояние.\nПока не придём к оптимальному решению или пока не закончится время, будем повторять следующие шаги:\nУменьшим температуру $t_{k} = T(t_{k-1})$ по какой-то формуле $T$. Выберем случайного соседа $x$: какое-то состояние $y$, которое может быть получено из $x$ каким-то небольшим изменением. С вероятностью $p(f(x), f(y), t_k)$ сделаем присвоение $x \\leftarrow y$, иначе оставим $x$ как есть. В каждом шаге есть много свободы при реализации. Основные эвристические соображения следующие:\nВ начале оптимизации наше решение и так плохое, и мы можем позволить себе высокую температуру и риск перейти в состояние хуже. В конце наоборот — наше решение почти оптимальное, и мы не хотим терять прогресс. Температура должна быть высокой в начале и медленно уменьшаться к концу.\nАлгоритм будет работать лучше, если функция $f(x)$ «гладкая» относительно этого изменения, то есть изменяется не сильно.\nВероятность должна быть меньше, если новое состояние хуже, чем старое. Также вероятность должна быть больше при высокой температуре.\nНапример, в коммивояжере можно действовать так:\n$t_k = \\gamma \\cdot t_{k-1}$, где $\\gamma$ это какое-то число, близкое к единице (например, $0.99$). Оно должно зависеть от планируемого количества итераций: оптимизация при низкой температуре почти ничего не будет менять.\nВ случае с перестановками этим минимальным изменением может быть, например, своп двух случайных элементов.\nЕсли $y$ не хуже, то есть $f(y) \\leq f(x)$, то переходим в него в любом случае. Иначе делаем переход в $y$, с вероятностью $p = e^\\frac{f(x)-f(y)}{t_k}$ — это экспонента отрицательного числа, и она даст вероятность в промежутке $(0, 1)$.\nНо важно осознавать, что в выборе конкретных эвристик не существует «золотого правила»: все компоненты алгоритма сильно зависят друг от друга и от задачи.\n#РеализацияНа практике применим алгоритм к другой задаче:\nДана шахматная доска $n \\times n$ и $n$ ферзей. Нужно расставить их так, чтобы они не били друг друга.\nБудем кодировать состояние так же перестановкой чисел от $0$ до $(n-1)$: ферзь номер $i$ будет стоять на пересечении $i$-той строки и $p_i$-того столбца.\nТакое представление кодирует не все возможные расстановки, но это даже хорошо: точно не учтутся те расстановки, где ферзи бьют друг друга по вертикали или горизонтали.\nВыберем функцию $f(p)$, равную числу успешно расставленных ферзей.\nПримерно эквивалентный код на C++:\nconst int n = 100; // размер доски const int k = 1000; // количество итераций алгоритма int f(vector\u0026lt;int\u0026gt; \u0026amp;p) { int s = 0; for (int i = 0; i \u0026lt; n; i++) { int d = 1; for (int j = 0; j \u0026lt; i; j++) if (abs(i - j) == abs(p[i] - p[j])) d = 0; s += d; } return s; } // генерирует действительное число от 0 до 1 double rnd() { return double(rand()) / RAND_MAX; } int main() { // генерируем начальную перестановку vector\u0026lt;int\u0026gt; v(n); iota(v.begin(), v.end(), 0); shuffle(v.begin(), v.end()); int ans = f(v); // текущий лучший ответ double t = 1; for (int i = 0; i \u0026lt; k \u0026amp;\u0026amp; ans \u0026lt; n; i++) { t *= 0.99; vector\u0026lt;int\u0026gt; u = v; swap(u[rand() % n], u[rand() % n]); int val = f(u); if (val \u0026gt; ans || rnd() \u0026lt; exp((val - ans) / t)) { v = u; ans = val; } } for (int x : v) cout \u0026lt;\u0026lt; x + 1 \u0026lt;\u0026lt; \u0026#34; \u0026#34;; return 0; } Здесь подсчёт количества свободных диагоналей работает за $O(n^2)$. Его можно соптимизировать до $O(n)$ и делать в $O(n)$ итераций больше: можно создать булев массив, в котором для каждой диагонали хранить, была ли она занята. С этой оптимизацией уже должна быть сдаваема эта задача на информатиксе.\nУпражнение. Соптимизируйте пересчёт до $O(1)$.\nПримечание. На самом деле, задача о ферзях решается конструктивно.\n","id":140,"path":"/cs/combinatorial-optimization/annealing/","title":"Метод отжига"},{"content":"На этом сайте находятся материалы различных CS-курсов, проводящихся в Tinkoff Generation.\nПроект открытый, живёт на гитхабе. Помощь в подготовке статей, исправление ошибок и любая другая обратная связь очень приветствуется. Разрабатывает и поддерживает Сергей Слотин.\n","id":141,"path":"/","title":"Алгоритмика"},{"content":"В контексте графов, система непересекающихся множеств напрямую решает следующую задачу:\nЗадача. Дан изначально пустой граф, и требуется обработать $n$ запросов добавления ребра (+) и проверки связности двух вершин (?).\nЕсли немного подумать, можно решить и обратную ей:\nЗадача. Дан граф, и нужно обрабатывать $n$ заранее известных запросов удаления ребра (-) и проверки связности двух вершин (?).\nЗдесь ключевое условие — что все запросы известны заранее. Это позволяет заменить все - на + и пройтись по всем запросам в обратном порядке. Если в конце граф становится пустым, то это будет эквивалентно предыдущей задаче, а если же граф удаляется не полностью, то все неудаленные ребра нужно просто добавить в СНМ в самом начале.\nЕсли же есть одновременно и добавления, и удаления, то задача сильно усложняется. Если на запросы нужно отвечать в режиме онлайн, то для этого существует весьма сложная структура, называемая Lunk-Cut Tree, которую мы в этой статье разбирать не будем. Но если запросы известны заранее, можно применить уже известные методы декомпозиции запросов и откатывания структур.\n#Dynamic Connectivity ProblemЗадача. Дан изначально пустой граф, и нужно отвечать на $n$ заранее известных запросов добавления ребра (+), удаления ребра (-) и проверки связности двух вершин (?).\nПопытаемся решить задачу корневой декомпозицией запросов. Разделим запросы на корневые блоки, и для каждого блока построим СНМ только для тех ребер, которые существуют на всем блоке.\nДля ответа на каждый запрос добавим все недостающие ребра на текущем блоке в СНМ, сделаем непосредственно запрос к нему, а затем — важно — откатим все изменения, которые мы делали на текущем блоке, чтобы получить чистый СНМ, который можно таким же образом использовать для других запросов текущего блока.\nКак откатывать СНМ? Можно воспользоваться либо трюком с занулением (только здесь «нулём» будет состояние СНМ для начала блока), либо поддерживать список изменений и проходиться по нему в обратном порядке.\nАсимптотика операций СНМ, правда, немного поменяется. Амортизация через сжатие путей в худшем случае выгоды не даст — можно много раз заставлять структуру делать сжатие и затем откатываться на состояние до него. Поэтому остается только весовая или ранговая эвристика, и асимптотика с ней будет $O(\\log n)$.\nТакое решение будет работать за $O(n \\sqrt n \\log n)$, однако можно быстрее.\n#Divide-and-conquer по запросамДавайте вместо корневой эвристики заведем рекурсивную функцию solve(l, r), которая будет отвечать на все запросы с $l$ по $r$, имея СНМ, соответствующий всем ребрам, которые существуют на всем этом промежутке.\nЭта функция будет действовать следующим образом:\nЕсли в промежутке всего один запрос, то найдем ответ на него через СНМ и выйдем. В противном случае: Разделим промежуток времени пополам: t = (l + r / 2). Рекурсивно разрешим левую половину: solve(l, t). Добавим в СНМ те ребра, которые существуют на всей правой половине запросов. Рекурсивно запустимся от правой половины: solve(t, r). Откатим СНМ до изначального состояния. Так как мы всегда поддерживаем инвариант «когда мы запускаемся и выходим из рекурсии, СНМ всегда чистый для этого промежутка», алгоритм действительно ответит на все запросы и будет работать за $O(n \\log^2 n)$.\nЗаметим, что мы нигде не использовали ничего конкретно про связность — можно отвечать на любые запросы, поддерживаемые СНМ, например о размерах компонент или числе ребер. Также существуют модификации для других задач, например для нахождения мостов или компонент двусвязности — подробнее можно почитать в дипломной работе Сергея Копелиовича.\n","id":142,"path":"/cs/spanning-trees/dcp/","title":"Динамическая связность"},{"content":"Нумерованные множества можно представлять в виде булевых массивов: если элемент $x$ присутствует в множестве, то $x$-тый элемент массива будет равен единице, и нулю в противном случае.\nРазные теоретико-множественные операции часто можно свести к поэлементным операциям булевыми массивами. Например:\nОбъединение множеств — побитовое ИЛИ. Пересечение множеств — побитовое И. Симметрическая разность — побитовый XOR (исключающее ИЛИ). Процессор устроен так, что работает не с отдельными битами, а сразу с блоками по, например, 32 или 64 бита — эта величина называется машинным словом — и поэтому операции, затрагивающие лишь один бит, на самом деле «стоят» столько же, сколько и операции над всеми битами int или long.\nЗдесь появляется следующая идея оптимизации: сгруппировать элементы булева массива в блоки размера 64 и каждый такой блок представить 64-битным двоичным числом. Тогда можно применять соответствующие побитовые операцию сразу к 64 элементам и тратить на это один процессорный такт вместо 64-х.\n#std::bitsetЭто всё несложно кодить и вручную, но в STL всё уже сделано до нас. bitset — структура, ведущая себя как большое двоичное число со всеми стандартными битовыми операциями:\nconst int lim = 1000; bitset\u0026lt;lim\u0026gt; b; // создать битсет размера lim (должно быть константой) b.set(); // заполнить единицами b.reset(); // заполнить нулями b.flip(); // заменить единички на нули и наоборот b.count(); // посчитать число единичек cout \u0026lt;\u0026lt; b; // вывести бинарную строку cout \u0026lt;\u0026lt; b[42]; // вывести 43-ий (нумерация с нуля, как у массивов) бит \u0026#34;справа\u0026#34; Также для битсетов работает вся битовая арифметика: \u0026amp;, |, ^, ~, \u0026lt;\u0026lt;, \u0026gt;\u0026gt; и их варианты с [operator]=.\nПримечание. Часто «асимптотику» использующих битовое сжатие алгоритмов пишут как $O(f / 64)$. Автору не нравится такая нотация, потому что численную константу формально можно сократить, и, вообще, на разных архитектурах и у разных реализаций будут разные константы. Вместо этого будем везде писать $O(f / w)$, где $w$ означает размер машинного слова.\n#Задача о рюкзаке Даны $n$ предметов с положительными целыми весами $a_i$ и рюкзак размера $m$. Требуется выбрать подмножество предметов с максимальной суммой, не превышающий размер рюкзака.\nОбычное решение за $O(n \\cdot m)$:\nbool dp[m] = {}; // так можно его заполнить нулями dp[0] = 1; for (int i = 0; i \u0026lt; n; i++) for (int x = m - a[i]; x \u0026gt;= 0; x--) dp[x + a[i]] |= dp[x]; …битовым сжатием разгоняется до $O(n \\cdot m / w)$ так:\nbitset\u0026lt;m\u0026gt; b; b[0] = 1; for (int i = 0; i \u0026lt; n; i++) b |= b \u0026lt;\u0026lt; a[i]; #Цикл длины 3 Нужно узнать, есть ли цикл длины 3 в ориентированном графе из $n$ вершин, заданном своей матрицей смежности.\nОбернем матрицу смежности в массив битсетов, и тогда задача решается за $O(n^3 / w)$:\nbitset\u0026lt;maxn\u0026gt; g[maxn]; // матрица смежности for (int a = 0; a \u0026lt; n; a++) { for (int b = 0; b \u0026lt; n; b++) { if (g[a][b] \u0026amp;\u0026amp; (~g[a] \u0026amp; g[b]).any()) { // цикл найден } } } Бенчмарк: на серверах CodeForces этот код при $n = 5000$ работает за 7 секунд.\n#Перемножение матрицВ задачах на подсчет числа путей часто используется факт, что матрица смежности графа, возведенная в степень $n$, имеет комбинаторный смысл: на пересечении $a$-ой строки и $b$-того столбца матрицы $G^n$ будет записано количество способов дойти из вершины $a$ в вершину $b$, используя ровно $n$ переходов.\nВ некоторых задачах нам не нужно знать именно число способов дойти из $a$ в $b$ — нам достаточно знания, можно ли вообще там оказаться за $n$ ходов. Тогда вместо числового умножения нам хватит битового умножения, то есть \u0026amp;:\ntypedef bitset\u0026lt;maxn\u0026gt; t; typedef array\u0026lt;t, maxn\u0026gt; matrix; matrix matmul(matrix a, matrix b) { matrix c; for (int i = 0; i \u0026lt; n; i++) for (int j = 0; j \u0026lt; n; j++) if (a[i][j]) c[i] |= b[j]; return c; } #Метод ГауссаИногда встречаются задачи, требующие решения системы линейных уравнений. Большую часть из них на самом деле можно решить над полем $\\mathbb{Z}_2$ — «по модулю 2».\nЕсть $n$ переключателей лампочек. Каждый активированный переключатель меняет состояние (включает или выключает) какого-то подмножества из $n$ лампочек. Известно текущее состояние всех лампочек, требуется восстановить по нему состояние переключателей.\nНас по сути просят решить следующую систему:\n$$ \\begin{cases} a_{11} x_1 + a_{12} x_2 + \\ldots + a_{1n} x_n \\equiv b_1 \\pmod 2 \\\\ a_{21} x_1 + a_{22} x_2 + \\ldots + a_{2n} x_n \\equiv b_2 \\pmod 2 \\\\ \\ldots \\\\ a_{n1} x_1 + a_{n2} x_2 + \\ldots + a_{nn} x_n \\equiv b_n \\pmod 2 \\end{cases} $$\nЗдесь $x$ — состояния переключателей, $b$ — состояния лампочек, а бит $a_{ij}$ отражает, влияет ли переключатель $i$ на лампочку $j$.\nВ таком случае можно значительно ускорить и упростить обычный метод Гаусса:\nt gauss(matrix a) { for (int i = 0; i \u0026lt; n; i++) { int nonzero = i; for (int j = i+1; j \u0026lt; n; j++) if (a[j][i]) nonzero = j; swap(a[nonzero], a[i]); for (int j = 0; j \u0026lt; n; j++) if (j != i \u0026amp;\u0026amp; a[j][i]) a[j] ^= a[i]; } t x; for (int i = 0; i \u0026lt; n; i++) x[i] = a[i][n] ^ a[i][i]; return x; } Код находит вектор $x$ из уравнения $Ax = b$ при условии, что решение существует и единственно. Для простоты кода предполагается, что вектор $b$ приписан справа к матрице $A$.\n#Детали реализацииНа самом деле, на высоких уровнях оптимизации (g++ -O3 -march=native ...) компилятор в конечном итоге будет использовать группы не по 64, а по 256 бит, используя SIMD-инструкции. Однако, для реализации .count() самое быстрое, что можно придумать — вызывать инструкцию popcnt по очереди от каждого 64-битного числа (в GCC она доступна как встроенный интринзик __builtin_popcount), поэтому этот метод работает в несколько раз медленнее побитовых операций.\nОперации с битсетами довольно просто реализовывать и с нуля: нужен только цикл с достаточно простым телом, и компилятор сделает всё сам, никакой тёмной магии при этом не требуется.\n","id":143,"path":"/cs/set-structures/bitset/","title":"Битсет и битовое сжатие"},{"content":"Неполные гайдлайны, которые постепенно будут пополняться.\nЕсли есть какие-либо вопросы, даже глупые, напишите мне.\n#Как начать #Если у меня маленькая правкаНа любой странице сайта можно нажать кнопку с карандашом сверху справа. Откроется интерфейс prose.io, в котором нужно залогиниться через GitHub, после чего можно редактировать markdown-исходник страницы. При первом сохранении автоматически создастся ветка и pull request от вашего имени, и при дальнейших он будет обновляться. Когда закончили, оставьте как есть — кто-нибудь придет и апрувнет.\nПолного preview там нет — осторожнее с правкой сложных формул, если не уверены в корректности.\nИногда редактор немного меняет формат блока мета-информации в начале статьи (о нём ниже). Это нормально — только проверьте, что published / draft / date стоят такие, какие нужно.\n#Если у меня большая правкаДля чего-либо серьёзного рекомендуется счекаутить репозиторий и поднять сайт локально. Это можно сделать так (предполагается, что вы знакомы с работой в терминале):\nПоставить Hugo: скорее всего одно из sudo apt-get install hugo, sudo pacman -Syu hugo, brew install hugo или choco install hugo -confirm в зависимости от системы. Форкнуть репозиторий и сделать git clone https://github.com/$USERNAME/algorithmica.git. Выполнить hugo serve в корне репозитория и зайти на localhost:1313 для английской или localhost:1314 для русской версии (порты могут быть другие). Найти или создать нужную статью, внести правки (в любом текстовом редакторе; они автоматически отрендерятся в браузере), запушить изменения и создать pull request в master. Если хотите отредактировать какую-то статью, помеченную как draft (неопубликованную), нужно добавить флаг -D. Рекомендуется перед созданием PR пройтись спеллчекером (например, hunspell-ом).\nЕсли кому-то нужна помощь с любым из этих пунктов, опять же, пишите.\n#Если хочу написать новую статьюУсловно готовыми считаются статьи, в которых:\nалгоритм описан и доказан, есть реализация, в реализации нет багов, есть хотя бы один пример задачи. Для минимизации бесполезной работы рекомендуется сначала спросить (в telegram или в issue на гитхабе), есть ли где-то уже статья на эту тему, начал ли её уже кто-нибудь писать, нужна ли она вообще, и в какой раздел её тогда следует положить.\nМногие статьи помечены как draft — это означает, что статья запланирована, но ещё только готовится. Если в них не указан автор, или указан, но последнее изменение было очень давно, то смело берите.\n#Технические возможности #MarkdownГайд по синтаксису.\nПомимо основного синтаксиса, поддерживаются ещё таблицы, блоки кода, strikethrough, latex-формулы (через один или два $) и tikz-диаграммы (через две @).\n#Front matterВ начале каждой статьи есть отделяемый через --- блок метаинформации, среди которой из важной есть:\ntitle: название статьи authors: список авторов (поддерживает markdown: можете вставлять там ссылки, например) editors: такой же список редакторов date: время последнего изменения (записывать в формате 2021-08-19) created: когда статья была изначально опубликована (в любом формате) prerequisites: список ссылок (лучше относительных) на статьи-пререквизиты; лучше не указывать слишком много и избегать циклов (это не техническое ограничение, а здравый смысл) Пример заполненной метаинформации.\n#Правила русского языкаРевьюер всё равно поправит, но, пожалуйста, имейте в виду:\nКавычки: « и ». Дефисы, минусы и тире: -, $a-b$ (через latex) и —. Предложения с формулами должны читаться как нормальные предложения (если идёт display-style формула, параграф перед ней скорее всего не заканчивается знаком препинания, а параграф после начинается со строчной, если он не является отдельным предложением). Списки с перечислениями тоже должны читаться либо как отдельные предложения, либо как часть одного предложения. Хеш, стек, дек — все через «е». Удобно научиться набирать кавычки и прочие особые символы без копирования. В Linux это делается через Compose Key.\nТакже мы игнорируем правило про тире вместо дефиса в названиях алгоритмов с более чем одним автором («Ахо-Корасик» вместо «Ахо — Корасик») и не очень строго относимся к правилам пунктуации.\n#Частые ошибки и конвенцииНебольшие технические тонкости и гайдлайны:\nИспользуйте только заголовки второго (##) и третьего (###) уровней. Если это очень маленькая секция, имеет смысл её выделить просто параграфом, начинающимся с bold-словом. Используйте либо относительные ссылки, либо абсолютные от корня сайта: /cs/tree-structures/treap/. Картинки вставляются через ![описание](../img/picture.png) и заливаются в локальную директорию img. Старайтесь по возможности использовать .svg вместо .png и .png вместо .jpg. Не используйте html, если это не какой-то исключительный случай. Помечайте блоки кода тегом языка, на котором они были написаны (cpp / python), иначе подсветка синтаксиса работать не будет. Комментарии можно и нужно использовать. Комментарии в духе общего \u0026ldquo;todo\u0026rdquo; можно указать в front matter через #, а контекстные комментарии или секции-черновики можно сделать через html-комментарии: \u0026lt;!-- ... --\u0026gt;. #КодстайлВезде используются пробелы, если это не Makefile или Go.\nPython. В полном соответствии с PEP.\nC++:\nЛучше использовать конкретные типы вместо метапрограммирования. Абсолютно точно никаких #define forn. Если в статье была однобуквенная переменная, её можно и нужно использовать в реализации. В противном случае у неё должно быть самоочевидное название. Решение конкретной задачи должно быть функцией или методом. Предподсчет можно указывать либо отдельной функцией, либо как в main без самого main. Ввод-вывод никому не интересен. Оставляйте комментарии на русском языке, но переменные и функции называйте на английском. Кодстайл проще описать примерами:\ntemplate \u0026lt;typename T\u0026gt; void f(T a, T *b, T \u0026amp;c) { // ... } // однострочные форы предпочтительнее for (/* ... */) // ... for (/* ... */) for (/* ... */) for (/* ... */) // однострочные ифы тоже предпочтительнее if (cond) // ... // но не так: if (cond) // ... // при этом однострочные функции ок int sum(Node *v) { return v ? v-\u0026gt;sum : 0; } if (cond) { // ... } else if { // ... } else { // ... } // иногда: if (cond) // ... else { // ... } // бесконечный цикл делается так while (true) { // ... } sort(a.begin(), a.end(), [](int x, int y){ return x \u0026lt; y; }); int a[5] = {1, 2, 3, 4, 5}; int a[n] = {0}; // заполнить нулями Если не указано обратного, во всех реализациях неявно предполагается, что компилятор GCC, стандарт не старше C++17, и перед блоком кода идут импорты:\n#include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; Если ваш привычный кодстайл отличается, рекомендуется воспользоваться clang-format или другими форматерами.\n","id":144,"path":"/contributing/","title":"Как добавлять и редактировать статьи"},{"content":"","id":145,"path":"/categories/","title":"Categories"},{"content":"","id":146,"path":"/tags/","title":"Tags"},{"content":"Рассмотрим ориентированный граф $G = (V, E)$ с истоком $s$ и стоком $t$, в котором у каждого ребра $(u, v)$ задана целая стоимость $w_{uv}$ и целая положительная пропускная способность $c_{uv}$. Требуется найти максимальный поток, стоимость которого минимальна:\n$$ \\sum_{(u, v) \\in E} f_{uv} \\to \\max $$ $$ \\sum_{(u, v) \\in E} f_{uv} w_{uv} \\to \\min $$\nЗаметим, что рёбра отрицательной стоимости по условию возможны. Мы дополнительно предполагаем, что циклов отрицательного веса нет.\nМодифицируем сеть, добавив стандартным образом обратные рёбра, позволяющие «отменять» операции: для каждого ребра $(u, v)$ добавим $(v, u)$, для которого $c_{vu} = 0$ и $w_{vu} = -w_{uv}$. Напомним, что остаточной сетью называется граф из рёбер, остаточная пропускная способность которых ненулевая ($c_{uv}-f_{uv} \u0026gt; 0$).\n#Критерий оптимальностиУтверждение. Если в остаточной сети нет циклов отрицательного веса, то поток оптимален (и наоборот).\nДоказательство:\n$\\rightarrow$ Рассмотрим произвольный неоптимальный поток $f$ и оптимальный поток $f^\\ast$. Рассмотрим разность $f^\\ast-f$. Она является циркуляцией, а любая циркуляция может быть разложена на сумму простых циклов. Хотя бы один из этих циклов будет иметь отрицательную стоимость, так как стоимость $f^\\ast$ меньше стоимости $f$, что противоречит предположению.\n$\\leftarrow$ Пусть цикл существует, тогда мы можем пропустить поток по этому циклу и получить поток меньшей стоимости.\n#Отмена цикловЭтот критерий сразу даёт нам относительно простой алгоритм: найдем какой-нибудь максимальный поток и будем «отменять» циклы отрицательного веса в остаточной цепи, пока такие циклы существуют. Искать цикл нам придётся не более $mUC$ раз где $U$ — величина потока, $C$ — максимальная пропускная способность ребра. Этой величиной ограничен модуль минимальной стоимости ответа, а каждый отмененный цикл уменьшает ответ хотя бы на единицу.\nЕсли искать цикл алгоритмом Форда-Беллмана, то асимптотика алгоритма составит $O(m^2nUC)$ (предполагая, что какой-нибудь максимальный поток мы уже нашли).\n#Дополняющие путиВспомним общий алгоритм поиска максимального потока, основанный на теореме Форда-Фалкерсона: найти какой-нибудь дополняющий путь, пропустить по нему поток и модифицировать сеть, снова найти дополняющий путь и так далее, пока путь из истока в сток существует. Что будет, если мы каждый раз будем искать не произвольный путь, а путь минимальной стоимости? Утверждается, что такой алгоритм найдет максимальный поток минимальной стоимости.\nУтверждение. Алгоритм не создает в остаточной сети циклов отрицательного веса.\nИзначально в остаточной сети нет циклов отрицательного веса. Мы нашли минимальный путь из $s$ в $t$ и модифицировали сеть, возможно добавив какие-то обратные рёбра. Могли ли из-за этих рёбер появиться циклы отрицательного веса? Пусть какое-нибудь обратное ребро $(v, u)$ находится в цикле отрицательного веса. Тогда есть путь Из $u$ в $v$ стоимости меньше, чем $w_{uv}$. Но такое не могло произойти: если бы такой путь существовал, то на этапе поиска дополняющего пути мы выбрали бы его вместо ребра $(u, v)$.\nДля поиска дополняющего пути можно использовать алгоритм Форда-Беллмана. Асимптотика в данном случае составит $O(nmU)$ — искать каждый дополняющий путь мы будем не более $U$ раз.\nПочему мы не использовали алгоритм Дейкстры? Проблема в рёбрах отрицательного веса. Даже если в исходном графе их нет, они могут в процессе алгоритма появиться как обратные. Покажем, как изменить веса рёбер так, чтобы они стали неотрицательными, но информация о кратчайших путях не утратилась: это можно сделать, если дать каждой вершине так называемый «потенциал», который будет учитываться при пересчете стоимостей ребер.\n#Потенциалы ДжонсонаПотенциалом вершины $v$ будем называть расстояние $d_v$ от вершины $s$. Рассмотрим граф из всех достижимых вершин и тех же рёбер, только с изменёнными весами:\n$$ w_{uv}\u0026rsquo; = w_{uv} + d_u - d_v $$\nУтверждение 1. Веса всех рёбер графа неотрицательные.\nДоказательство. Пусть вес какого-то ребра $(u, v)$ отрицателен, то есть $w_{uv}\u0026rsquo; = w_{uv} + d_u - d_v \u0026lt; 0$. Тогда $d_u + w_{uv} \u0026lt; d_v$, и нарушилось неравенство треугольника: почему мы тогда не использовали ребро $(u, v)$, когда искали кратчайший путь до $v$?\nАналогично можно показать, что рёбра на кратчайших путях из $s$ имеют нулевую стоимость. Заметим, что стоимость обратных рёбер на кратчайших путях тоже будет нулевой: $$ w_{vu}\u0026rsquo; = w_{vu} + d_v - d_u = -w_{uv} - d_u + d_v = -(w_{uv}) = 0 $$\nУтверждение 2. Кратчайшие пути между любыми вершинами остались кратчайшими.\nДоказательство. Распишем новую стоимость пути из $a$ в $z$.\n$$ \\begin{aligned} w_{ab}\u0026rsquo; + \\ldots + w_{yz}' \u0026amp;= (w_{ab} + \\ldots + w_{yz}) + (d_a + \\ldots + d_y) - (d_b + \\ldots + d_z) \\\u0026amp;= (w_{ab} + \\ldots + w_{yz}) + d_a - d_z \\end{aligned} $$\nПолучаем, что стоимость всех путей из $a$ в $z$ лишь изменилась на константу.\nБолее того, если мы добавим или удалим некоторые рёбра из графа, потенциалы тоже никак не повлияют на кратчайшие пути.\nЗаметьте, что в доказательстве мы не использовали то, что $d_v$ — кратчайшие расстояния. Это вообще могут быть произвольные числа.\nУтверждение 3. Когда мы проталкиваем поток вдоль кратчайшего пути, удаляя ребра и возможно добавляя обратные, веса в изменённом графе тоже остались корректными (все рёбра неотрицательного веса и все кратчайшие пути остались кратчайшими).\nДоказательство. Все добавленные обратные рёбра на кратчайшем пути будут иметь нулевую стоимость (утверждение 1), а добавления или удаления рёбер на кратчайшие пути не повлияли (утверждение 2).\n#Итоговый алгоритм Модифицируем сеть, добавив обратные рёбра. Если в исходном графе есть рёбра отрицательного веса (но нет циклов отрицательного веса), то посчитать изначальные потенциалы (расстояния) алгоритмом Форда-Беллмана. Иначе достаточно положить потенциалы изначально равными нулю. Пока максимальный поток не найден: Посчитать алгоритмом Дейкстры кратчайшие расстояния от $s$, используя для веса формулу с потенциалами, записать их в $d$. Протолкнуть максимально возможный поток вдоль кратчайшего пути $s \\leadsto t$ и обновить остаточную сеть. #РеализацияНиже приведено решение задачи о назначениях (паросочетание минимального веса). Для нахождения дополняющего пути используется алгоритм Дейкстры для плотных графов (без очереди с приоритетами — каждую итерацию ищется минимум за $O(n)$).\ncost, cap — параметры сети pot — потенциалы par — предок вершины в алгоритме Дейкстры (нужен для проталкивания потока) d — временный массив для алгоритма Дейкстры, куда будут записаны новые расстояния const int maxn = 305, inf = 1e9; int n; int cost[maxn][maxn], cap[maxn][maxn]; int d[maxn], pot[maxn], par[maxn]; bool dijkstra(int s, int t) { used[maxn] = {0}; fill(d, d+n, inf); d[s] = 0; while (1) { int v = -1; for (int u = 0; u \u0026lt; n; u++) if (!used[u] \u0026amp;\u0026amp; (v == -1 \u0026amp;\u0026amp; d[u] \u0026lt; d[v])) v = u; if (v == -1 || d[v] == inf) break; used[v] = 1; for (int u = 0; u \u0026lt; n; u++) { int w = cost[v][u] + pot[v] - pot[u]; if (cap[v][u] \u0026amp;\u0026amp; d[u] \u0026gt; d[v] + w) { d[u] = d[v] + w; par[u] = v; } } } return d[t] \u0026lt; inf; } int mincost_maxflow(int s, int t) { int ans = 0; while (dijkstra(s, t)) { memcpy(pot, d, sizeof(d)); int delta = inf; for (int v = t; v != s; v = par[v]) delta = min(delta, cap[par[v]][v]); for (int v = t; v != s; v = par[v]) { cap[par[v]][v] -= delta; cap[v][par[v]] += delta; ans += cost[par[v]][v]*delta; } } return ans; } #АсимптотикаВ общем случае, алгоритм работает за $O(U m \\log n)$ или $O(U n^2)$ в случае плотных графов.\nВ наиболее популярных задачах рёбра обычно с единичной пропускной способностью, и $U \\leq n$ или $U \\leq m$. Например, в задаче о назначениях $U = n$, и алгоритм работает за $O(n^3)$, что совпадает с асимптотикой венгерского алгоритма.\n","id":147,"path":"/cs/flows/mincost-maxflow/","title":"Поток минимальной стоимости"},{"content":"Система непересекающихся множеств (англ. disjoint set union) — структура данных, позволяющая объединять непересекающиеся множества и отвечать на разные запросы про них, например «находятся ли элементы $a$ и $b$ в одном множестве» и «чему равен размер данного множества».\nБолее формально, изначально имеется $n$ элементов, каждый из которых находится в отдельном (своём собственном) множестве. Структура поддерживает две базовые операции:\nОбъединить два каких-либо множества. Запросить, в каком множестве сейчас находится указанный элемент. Обе операции выполняются в среднем почти за $O(1)$ (но не совсем).\nСНМ часто используется в графовых алгоритмах для хранения информации о связности компонент — например, в алгоритме Краскала.\n#Устройство структурыМножества элементов мы будем хранить в виде деревьев: одно дерево соответствует одному множеству. Корень дерева — это представитель (лидер) множества. Для корней деревьев будем считать, что их предки — они сами.\nЗаведём массив p, в котором для каждого элемента мы храним номер его предка в дереве:\nint p[maxn]; for (int i = 0; i \u0026lt; n; i++) p[i] = i; Для запроса «в каком множестве элемент $v$» нужно подняться по ссылкам до корня:\nint leader(int v) { if (p[v] == v) return v; else return leader(p[v]); } Для объединения двух множеств нужно подвесить корень одного за корень другого:\nvoid unite(int a, int b) { a = leader(a), b = leader(b); p[a] = b; } К несчастью, в худшем случае такая реализация работает за $O(n)$ — можно построить «бамбук», подвешивая его $n$ раз за новую вершину. Сейчас мы это исправим.\n#ОптимизацииСначала привидем идеи оптимизации, а потом проанализируем, насколько хорошо они работают.\nЭвристика сжатия пути. Оптимизируем работу функции leader. Давайте перед тем, как вернуть ответ, запишем его в p от текущей вершины, то есть переподвесим его за самую высокую.\nint leader(int v) { return (p[v] == v) ? v : p[v] = leader(p[v]); } Следующие две эвристики похожи по смыслу и стараются оптимизировать высоту дерева, выбирая оптимальный корень для переподвешивания.\nРанговая эвристика. Будем хранить для каждой вершины её ранг — высоту её поддерева. При объединении деревьев будем делать корнем нового дерева ту вершину, у которой ранг больше, и пересчитывать ранги (ранг у лидера должен увеличиться на единицу, если он совпадал с рангом другой вершины). Эта эвристика оптимизирует высоту дерева напрямую.\nvoid unite(int a, int b) { a = leader(a), b = leader(b); if (h[a] \u0026gt; h[b]) swap(a, b); h[b] = max(h[b], h[a] + 1); p[a] = b; } Весовая эвристика. Будем вместо ранга хранить размеры поддеревьев для каждой вершины, а при объединении — подвешивать за более «тяжелую».\nvoid unite(int a, int b) { a = leader(a), b = leader(b); if (s[a] \u0026gt; s[b]) swap(a, b); s[b] += s[a]; p[a] = b; } Эвристики выбора вершины-лидера взаимоисключающие, но их можно использовать вместе со сжатием путей.\n#РеализацияФинальная реализация, использующая весовую эвристику и эвристику сжатия путей:\nint p[maxn], s[maxn]; int leader(int v) { return (p[v] == v) ? v : p[v] = leader(p[v]); } void unite(int a, int b) { a = leader(a), b = leader(b); if (s[a] \u0026gt; s[b]) swap(a, b); s[b] += s[a]; p[a] = b; } void init(n) { for (int i = 0; i \u0026lt; n; i++) p[i] = i, s[i] = 1; } Автор предпочитает именно весовую эвристику, потому что часто в задачах размеры компонент требуются сами по себе.\n#АсимптотикаЭвристика сжатия путей улучшает асимптотику до $O(\\log n)$ в среднем. Здесь используется именно амортизированная оценка — понятно, что в худшем случае нужно будет сжимать весь бамбук за $O(n)$.\nСжатие пути после запроса $p(7)$ Индукцией несложно показать, что весовая и ранговая эвристики ограничивают высоту дерева до $O(\\log n)$, а соответственно и асимптотику нахождения корня тоже.\nПри использовании эвристики сжатия плюс весовой или ранговой асимптотика будет $O(a(n))$, где $a(n)$ — обратная функция Аккермана (очень медленно растущая функция, для всех адекватных чисел не превосходящая 4).\nТратить время на изучения доказательства или даже чтения статьи на Википедии про функцию Аккермана автор не рекомендует.\n","id":148,"path":"/cs/set-structures/dsu/","title":"Система непересекающихся множеств"}]