<!doctype html><html lang=en-us><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-WBN59M8Y5S"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-WBN59M8Y5S")</script><script type=text/javascript>(function(e,t,n,s,o,i,a){e[o]=e[o]||function(){(e[o].a=e[o].a||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)})(window,document,"script","https://mc.yandex.ru/metrika/tag.js","ym"),ym(53961409,"init",{clickmap:!0,trackLinks:!0,accurateTrackBounce:!0,webvisor:!0})</script><noscript><div><img src=https://mc.yandex.ru/watch/53961409 style=position:absolute;left:-9999px alt></div></noscript><meta charset=utf-8><link rel=stylesheet href=/style.min.a3a4a7a8e8602aaa85b7cb3d655edde028ac80d73f2a97389e2cbcf995dd672d.css integrity="sha256-o6SnqOhgKqqFt8s9ZV7d4CisgNc/Kpc4niy8+ZXdZy0="><link rel=stylesheet href=/syntax.css id=syntax-theme><link rel=stylesheet type=text/css href=https://tikzjax.com/v1/fonts.css><script src=https://tikzjax.com/v1/tikzjax.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.9/lunr.min.js></script><script src=/scripts/lunr.stemmer.support.min.js></script><script src=/scripts/lunr.ru.min.js></script><script src=/scripts/lunr.multi.min.js></script><link rel=stylesheet id=theme><script>function toggleSidebar(){console.log("Toggling sidebar visibility");var e=document.getElementById("sidebar"),t=document.getElementById("wrapper");(e.classList.contains("sidebar-toggled")||window.getComputedStyle(e).display=="block")&&(e.classList.toggle("sidebar-hidden"),t.classList.toggle("sidebar-hidden")),e.classList.add("sidebar-toggled"),t.classList.add("sidebar-toggled")}function switchTheme(e){console.log("Changing theme:",e),document.getElementById("theme").href=e=="dark"?"/dark.min.b3ae1169831434b11b48de5b3e3210547eea6b7884c295ab0030cb973ea0dc49.css":"",document.getElementById("syntax-theme").href=e=="dark"?"/syntax-dark.css":"/syntax.css",localStorage.setItem("theme",e)}async function toggleSearch(){console.log("Toggling search");var e=document.getElementById("search");if(window.getComputedStyle(e).display=="none"?(e.style.display="block",window.scrollTo({top:0}),document.getElementById("search-bar").focus()):e.style.display="none",!index){console.log("Fetching index");const e=await fetch("/searchindex.json"),t=await e.json();index=lunr(function(){this.use(lunr.multiLanguage("en","ru")),this.field("title",{boost:5}),this.field("content",{boost:1}),t.forEach(function(e){this.add(e),articles.push(e)},this)}),console.log("Ready to search")}}var articles=[],index=void 0;function search(){var n,e=document.getElementById("search-bar").value,s=document.getElementById("search-results"),o=document.getElementById("search-count");if(e==""){s.innerHTML="",o.innerHTML="";return}n=index.search(e),o.innerHTML="Found <b>"+n.length+"</b> pages";let t="";for(const a in n){const i=articles[n[a].ref];t+='<li><a href="'+i.path+'">'+i.title+"</a> <p>";const s=i.content,o=80;if(s.includes(e)){const n=s.indexOf(e);n>o&&(t+="…"),t+=s.substring(n-o,n)+"<b>"+e+"</b>"+s.substring(n+e.length,n+e.length+o)}else t+=s.substring(0,o*2);t+="…</p></li>"}s.innerHTML=t}localStorage.getItem("theme")=="dark"&&switchTheme("dark"),window.addEventListener("load",function(){var e=document.getElementById("active-element");e&&e.scrollIntoView({block:"center"})}),window.addEventListener("scroll",function(){var e=document.getElementById("menu");window.scrollY<120?e.classList.remove("scrolled"):e.classList.add("scrolled")}),window.addEventListener("keydown",function(e){if(e.altKey)return;if(document.activeElement.tagName=="INPUT")return;e.key=="ArrowLeft"?document.getElementById("prev-article").click():e.key=="ArrowRight"&&document.getElementById("next-article").click()})</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})'></script><title>Auto-Vectorization and SPMD - Algorithmica</title></head><body><nav id=sidebar><div class=title><a href=/>Algorithmica</a>
<span class=slash>/</span>
<a href=/hpc/ class=divisionAbbr>HPC</a></div><ul><li class=part>Performance Engineering</li><li><a href=/hpc/complexity/>Complexity Models</a></li><ol><li><a href=/hpc/complexity/hardware/>Modern Hardware</a></li><li><a href=/hpc/complexity/languages/>Programming Languages</a></li></ol><li><a href=/hpc/architecture/>Computer Architecture</a></li><ol><li><a href=/hpc/architecture/isa/>Instruction Set Architectures</a></li><li><a href=/hpc/architecture/assembly/>Assembly Language</a></li><li><a href=/hpc/architecture/loops/>Loops and Conditionals</a></li><li><a href=/hpc/architecture/functions/>Functions and Recursion</a></li><li><a href=/hpc/architecture/indirect/>Indirect Branching</a></li><li><a href=/hpc/architecture/layout/>Machine Code Layout</a></li></ol><li><a href=/hpc/pipelining/>Instruction-Level Parallelism</a></li><ol><li><a href=/hpc/pipelining/hazards/>Pipeline Hazards</a></li><li><a href=/hpc/pipelining/branching/>The Cost of Branching</a></li><li><a href=/hpc/pipelining/branchless/>Branchless Programming</a></li><li><a href=/hpc/pipelining/tables/>Instruction Tables</a></li><li><a href=/hpc/pipelining/throughput/>Throughput Computing</a></li></ol><li><a href=/hpc/compilation/>Compilation</a></li><ol><li><a href=/hpc/compilation/stages/>Stages of Compilation</a></li><li><a href=/hpc/compilation/flags/>Flags and Targets</a></li><li><a href=/hpc/compilation/situational/>Situational Optimizations</a></li><li><a href=/hpc/compilation/contracts/>Contract Programming</a></li><li><a href=/hpc/compilation/precalc/>Precomputation</a></li></ol><li><a href=/hpc/profiling/>Profiling</a></li><ol><li><a href=/hpc/profiling/instrumentation/>Instrumentation</a></li><li><a href=/hpc/profiling/events/>Statistical Profiling</a></li><li><a href=/hpc/profiling/simulation/>Program Simulation</a></li><li><a href=/hpc/profiling/mca/>Machine Code Analyzers</a></li><li><a href=/hpc/profiling/benchmarking/>Benchmarking</a></li><li><a href=/hpc/profiling/noise/>Getting Accurate Results</a></li></ol><li><a href=/hpc/arithmetic/>Arithmetic</a></li><ol><li><a href=/hpc/arithmetic/float/>Floating-Point Numbers</a></li><li><a href=/hpc/arithmetic/ieee-754/>IEEE 754 Floats</a></li><li><a href=/hpc/arithmetic/errors/>Rounding Errors</a></li><li><a href=/hpc/arithmetic/newton/>Newton's Method</a></li><li><a href=/hpc/arithmetic/rsqrt/>Fast Inverse Square Root</a></li><li><a href=/hpc/arithmetic/integer/>Integer Numbers</a></li><li><a href=/hpc/arithmetic/division/>Integer Division</a></li></ol><li><a href=/hpc/number-theory/>Number Theory</a></li><ol><li><a href=/hpc/number-theory/modular/>Modular Arithmetic</a></li><li><a href=/hpc/number-theory/exponentiation/>Binary Exponentiation</a></li><li><a href=/hpc/number-theory/euclid-extended/>Extended Euclidean Algorithm</a></li><li><a href=/hpc/number-theory/montgomery/>Montgomery Multiplication</a></li></ol><li><a href=/hpc/external-memory/>External Memory</a></li><ol><li><a href=/hpc/external-memory/hierarchy/>Memory Hierarchy</a></li><li><a href=/hpc/external-memory/virtual/>Virtual Memory</a></li><li><a href=/hpc/external-memory/model/>External Memory Model</a></li><li><a href=/hpc/external-memory/sorting/>External Sorting</a></li><li><a href=/hpc/external-memory/list-ranking/>List Ranking</a></li><li><a href=/hpc/external-memory/policies/>Eviction Policies</a></li><li><a href=/hpc/external-memory/oblivious/>Cache-Oblivious Algorithms</a></li><li><a href=/hpc/external-memory/locality/>Spatial and Temporal Locality</a></li></ol><li><a href=/hpc/cpu-cache/>RAM & CPU Caches</a></li><ol><li><a href=/hpc/cpu-cache/bandwidth/>Memory Bandwidth</a></li><li><a href=/hpc/cpu-cache/latency/>Memory Latency</a></li><li><a href=/hpc/cpu-cache/cache-lines/>Cache Lines</a></li><li><a href=/hpc/cpu-cache/sharing/>Memory Sharing</a></li><li><a href=/hpc/cpu-cache/mlp/>Memory-Level Parallelism</a></li><li><a href=/hpc/cpu-cache/prefetching/>Prefetching</a></li><li><a href=/hpc/cpu-cache/alignment/>Alignment and Packing</a></li><li><a href=/hpc/cpu-cache/pointers/>Pointer Alternatives</a></li><li><a href=/hpc/cpu-cache/associativity/>Cache Associativity</a></li><li><a href=/hpc/cpu-cache/paging/>Memory Paging</a></li><li><a href=/hpc/cpu-cache/aos-soa/>AoS and SoA</a></li></ol><li><a href=/hpc/simd/>SIMD Parallelism</a></li><ol><li><a href=/hpc/simd/intrinsics/>Intrinsics and Vector Types</a></li><li><a href=/hpc/simd/moving/>Moving Data</a></li><li><a href=/hpc/simd/reduction/>Reductions</a></li><li><a href=/hpc/simd/masking/>Masking and Blending</a></li><li><a href=/hpc/simd/shuffling/>In-Register Shuffles</a></li><li><a href=/hpc/simd/auto-vectorization/ id=active-element>Auto-Vectorization and SPMD</a></li></ol><li><a href=/hpc/algorithms/>Algorithms Case Studies</a></li><ol><li><a href=/hpc/algorithms/gcd/>Binary GCD</a></li><li><a href=/hpc/algorithms/factorization/>Integer Factorization</a></li><li><a href=/hpc/algorithms/argmin/>Argmin with SIMD</a></li><li><a href=/hpc/algorithms/prefix/>Prefix Sum with SIMD</a></li><li><a href=/hpc/algorithms/matmul/>Matrix Multiplication</a></li></ol><li><a href=/hpc/data-structures/>Data Structures Case Studies</a></li><ol><li><a href=/hpc/data-structures/binary-search/>Binary Search</a></li><li><a href=/hpc/data-structures/s-tree/>Static B-Trees</a></li><li><a href=/hpc/data-structures/b-tree/>Search Trees</a></li><li><a href=/hpc/data-structures/segment-trees/>Segment Trees</a></li></ol></ul></nav><div id=wrapper><menu id=menu><div class=left><a><img src=/icons/bars-solid.svg onclick=toggleSidebar() title='open table of contents'>
</a><a><img src=/icons/adjust-solid.svg style=position:relative;top:-1px onclick='switchTheme(localStorage.getItem("theme")=="dark"?"light":"dark")' title='dark theme'>
</a><a><img src=/icons/search-solid.svg onclick=toggleSearch() title=search></a></div><div class=title>Auto-Vectorization and SPMD</div><div class=right><a onclick=window.print()><img src=/icons/print-solid.svg title=print>
</a><a href=https://prose.io/#algorithmica-org/algorithmica/edit/master//hpc%2fsimd%2fauto-vectorization.md><img src=/icons/edit-solid.svg title=edit style=width:18px;position:relative;right:-2px;top:-1px>
</a><a href=https://github.com/algorithmica-org/algorithmica/blob/master//hpc/simd/auto-vectorization.md class=github-main><img src=/icons/github-brands.svg title='view on github'></a></div></menu><main><div id=search><input id=search-bar type=search placeholder='Search this book…' oninput=search()><div id=search-count></div><div id=search-results></div></div><header><h1>Auto-Vectorization and SPMD</h1><div class=info></div></header><article><p>SIMD parallelism is most often used for <em>embarrassingly parallel</em> computations: the kinds where all you do is apply some elementwise function to all elements of an array and write it back somewhere else. In this setting, you don&rsquo;t even need to know how SIMD works: the compiler is perfectly capable of optimizing such loops by itself — you just need to be aware that such optimization exists and that it usually yields a 5-10x speedup.</p><p>Doing nothing and relying on auto-vectorization is actually the most popular way of using SIMD. In fact, in many cases, it even advised to stick with the plain scalar code for its simplicity and maintainability.</p><p>But often even the loops that seem straightforward to vectorize are not optimized because of some technical nuances. <a href=/hpc/compilation/contracts>As in many other cases</a>, the compiler may need some additional input from the programmer as he may know a bit more about the problem than what can be inferred from static analysis.</p><span class=anchor id=potential-problems></span><h3><a class=anchor-link href=https://en.algorithmica.org/hpc/simd/auto-vectorization/#potential-problems>#</a>Potential Problems</h3><p>Consider the &ldquo;a + b&rdquo; example we <a href=../intrinsics/#simd-intrinsics>started with</a>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=kt>void</span> <span class=nf>sum</span><span class=p>(</span><span class=kt>int</span> <span class=o>*</span><span class=n>a</span><span class=p>,</span> <span class=kt>int</span> <span class=o>*</span><span class=n>b</span><span class=p>,</span> <span class=kt>int</span> <span class=o>*</span><span class=n>c</span><span class=p>,</span> <span class=kt>int</span> <span class=n>n</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>c</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>a</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+</span> <span class=n>b</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Let&rsquo;s step into a compiler&rsquo;s shoes and think about what can go wrong when this loop is vectorized.</p><p><strong>Array size.</strong> If the array size is unknown beforehand, it may be that it is too small for vectorization to be beneficial in the first place. Even if it is sufficiently large, we need to insert an additional check for the remainder of the loop to process it scalar, which would cost us a branch.</p><p>To eliminate these runtime checks, use array sizes that are compile-time constants, and preferably pad arrays to the nearest multiple of the SIMD block size.</p><p><strong>Memory aliasing.</strong> Even when array size issues are out of the question, vectorizing this loop is not always technically correct. For example, the arrays <code>a</code> and <code>c</code> can intersect in a way that their beginnings differ by a single position — because who knows, maybe the programmer wanted to calculate the Fibonacci sequence through a convolution this way. In this case, the data in the SIMD blocks will intersect and the observed behavior will differ from the one in the scalar case.</p><p>When the compiler can&rsquo;t prove that the function may be used for intersecting arrays, it has to generate two implementation variants — a vectorized and a &ldquo;safe&rdquo; one — and insert runtime checks to choose between the two. To avoid them, we can tell the compiler that we are that no memory is aliased by adding the <code>__restrict__</code> keyword:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=kt>void</span> <span class=nf>add</span><span class=p>(</span><span class=kt>int</span> <span class=o>*</span> <span class=n>__restrict__</span> <span class=n>a</span><span class=p>,</span> <span class=k>const</span> <span class=kt>int</span> <span class=o>*</span> <span class=n>__restrict__</span> <span class=n>b</span><span class=p>,</span> <span class=kt>int</span> <span class=n>n</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>a</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+=</span> <span class=n>b</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>The other way, specific to SIMD, is the &ldquo;ignore vector dependencies&rdquo; pragma. It is a general way to inform the compiler that there are no dependencies between the loop iterations:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=cp>#pragma GCC ivdep
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1>// ...
</span></span></span></code></pre></div><p><strong>Alignment.</strong> The compiler also doesn&rsquo;t know anything about the alignment of these arrays and has to either process some elements at the beginning of these arrays before starting the vectorized section or potentially lose some performance by using <a href=../moving>unaligned memory accesses</a>.</p><p>To help the compiler eliminate this corner case, we can use the <code>alignas</code> specifier on static arrays and the <code>std::assume_aligned</code> function to mark pointers aligned.</p><p><strong>Checking if vectorization happened.</strong> In either case, it is useful to check if the compiler vectorized the loop the way you intended. You can either <a href=/hpc/compilation/stages>compiling it to assembly</a> and look for blocks for instructions that start with a &ldquo;v&rdquo; or add the <code>-fopt-info-vec-optimized</code> compiler flag so that the compiler indicates where auto-vectorization is happening and what SIMD width is being used. If you swap <code>optimized</code> for <code>missed</code> or <code>all</code>, you may also get some reasoning behind why it is not happening in other places.</p><p>There are <a href=https://software.intel.com/sites/default/files/m/4/8/8/2/a/31848-CompilerAutovectorizationGuide.pdf>many other ways</a> of telling the compiler exactly what we mean, but in especially complex cases — e.g., when there are a lot of branches or function calls inside the loop — it is easier to go one level of abstraction down and vectorize manually.</p><span class=anchor id=spmd></span><h3><a class=anchor-link href=https://en.algorithmica.org/hpc/simd/auto-vectorization/#spmd>#</a>SPMD</h3><p>There is a neat compromise between auto-vectorization and the manual use of SIMD intrinsics: &ldquo;single program, multiple data&rdquo; (SPMD). This is a model of computation in which the programmer writes what appears to be a regular serial program, but that is actually executed in parallel on the hardware.</p><p>The programming experience is largely the same, and there is still the fundamental limitation in that the computation must be data-parallel, but SPMD ensures that the vectorization will happen regardless of the compiler and the target CPU architecture. It also allows for the computation to be automatically parallelized across multiple cores and, in some cases, even offloaded to other types of parallel hardware.</p><p>There is support for SPMD is some modern languages (<a href=https://docs.julialang.org/en/v1/base/base/#Base.SimdLoop.@simd>Julia</a>), multiprocessing APIs (<a href=https://www.openmp.org/spec-html/5.0/openmpsu42.html>OpenMP</a>), and specialized compilers (Intel <a href=https://ispc.github.io/>ISPC</a>), but it has seen the most success in the context of GPU programming where both problems and hardware are massively parallel.</p><p>We will cover this model of computation in much more depth in Part 2</p></article><div class=nextprev><div class=left><a href=https://en.algorithmica.org/hpc/simd/shuffling/ id=prev-article>← In-Register Shuffles</a></div><div class=right><a href=https://en.algorithmica.org/hpc/algorithms/ id=next-article>../Algorithms Case Studies →</a></div></div></main><footer>Copyright 2021–2022 Sergey Slotin<br></footer></div></body></html>